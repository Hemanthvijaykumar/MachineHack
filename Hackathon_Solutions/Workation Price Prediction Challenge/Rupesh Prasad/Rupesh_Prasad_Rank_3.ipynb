{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.9","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":28,"outputs":[{"name":"stdout","text":"/kaggle/input/workation-price-prediction-challengemachinehack/sample submission.csv\n/kaggle/input/workation-price-prediction-challengemachinehack/Train.csv\n/kaggle/input/workation-price-prediction-challengemachinehack/Test.csv\n","output_type":"stream"}]},{"cell_type":"code","source":"import numpy as np # linear algebra\nfrom sklearn.feature_selection import SelectKBest,f_regression,mutual_info_regression\nimport pandas as pd \nfrom geopy.geocoders import Nominatim \nfrom geopy import distance \nfrom xgboost import XGBRegressor\nfrom lightgbm import LGBMRegressor\nfrom catboost import CatBoostRegressor\nimport time\nfrom sklearn.model_selection import *\nfrom sklearn.metrics import *\nfrom sklearn.linear_model import *\nfrom wordcloud import WordCloud, STOPWORDS\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\nfrom nltk.tokenize import TreebankWordTokenizer\nimport matplotlib.pyplot as plt\nfrom geopy.geocoders import Nominatim \nfrom geopy import distance \ngeolocator = Nominatim(user_agent=\"geoapiExercises\") \n%matplotlib inline\nimport re\nfrom sklearn.metrics import mean_squared_log_error\ndef custom_metric(y_true, y_preds):\n    return np.sqrt(mean_squared_log_error(y_true, y_preds))\ndef rmse(y_true, y_pred):\n    return mean_squared_error(y_true, y_pred) ** 0.5","metadata":{"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"def distance_calc (x):\n    try :\n        p1 = geolocator.geocode(x[0]) \n        p2 = geolocator.geocode(x[1])\n\n        # Get latitude and longitude \n        lat1, lon1 = (p1.latitude), (p1.longitude) \n        lat2, lon2 = (p2.latitude), (p2.longitude) \n        \n        location1 = (lat1, lon1) \n        location2 = (lat2, lon2) \n        \n        dist = distance.distance(location1,location2).km \n        \n        return dist\n    except:\n        return np.nan\n    \ndef rating(x):\n    x =  x.lower()\n    x.replace(\"one\", '1') \n    x.replace(\"two\", '1') \n    x.replace(\"three\", '1') \n    x.replace(\"four\", '4') \n    x.replace(\"five\", '5') \n    lst = re.findall(r\"[-+]?\\d*\\.\\d+|\\d+\",x)\n    lst = [float(i) for i in lst]\n    if len(lst) == 0:\n        return 3\n    else:\n        return sum(lst)/len(lst)\n    \ndef tot_nit(x):\n    try :\n        \n        ls = x.split('.')\n        night = 0\n        for i in ls :\n            k1 = i.split(' ')\n\n            k2 = int([x for x in k1 if re.compile(r'^.N$').match(x)][0][0])\n            night = night + k2\n        return night\n    except :\n        return 0","metadata":{"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"code","source":"\ntrain = pd.read_csv('/kaggle/input/workation-price-prediction-challengemachinehack/Train.csv')\ntest = pd.read_csv('/kaggle/input/workation-price-prediction-challengemachinehack/Test.csv')\nsample_sub = pd.read_csv('/kaggle/input/workation-price-prediction-challengemachinehack/sample submission.csv')","metadata":{"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"code","source":"df = pd.concat([train, test], axis=0).reset_index(drop=True)\ndf.shape","metadata":{"trusted":true},"execution_count":32,"outputs":[{"execution_count":32,"output_type":"execute_result","data":{"text/plain":"(30000, 15)"},"metadata":{}}]},{"cell_type":"code","source":"df['d1'] = df['Destination'].apply(lambda x : x.split('|')[0])\ndf['d2'] = df['Destination'].apply(lambda x : x.split('|')[-1])\n\ndf['d1'] = np.where(df['d1']=='Tiruchirapally','Tiruchi',df['d1'])\ndf['d1'] = np.where(df['d1']=='Chikmangalur','Chikmagalur',df['d1'])\ndf['d2'] = np.where(df['d2']=='Tiruchirapally','Tiruchi',df['d2'])\ndf['d2'] = np.where(df['d2']=='Chikmangalur','Chikmagalur',df['d2'])\n\n\ntemp1 = df[['Start City','d1']].drop_duplicates(keep='first')\ntemp2 = df[['Start City','d2']].drop_duplicates(keep='first')\n\ntemp1['dist1'] = temp1.apply(lambda x : distance_calc (x) , axis=1)\ntemp2['dist2'] = temp2.apply(lambda x : distance_calc (x),axis=1)\n\n\ntemp1['country'] = temp1['d1'].apply(lambda x :geolocator.reverse(str(geolocator.geocode(x).latitude)+\",\"+str(geolocator.geocode(x).longitude) ).raw['address'].get('country_code', '') )\ntemp1 = pd.get_dummies(temp1, columns = ['country'])\n\ndf1 = df[['Uniq Id','Start City','d1','d2']]\n\ndf1 = pd.merge(df1,temp1,how = 'left' , on=['Start City','d1'])\ndf1 = pd.merge(df1,temp2,how = 'left' , on=['Start City','d2'])\n\n\ndf1.head(3)","metadata":{"trusted":true},"execution_count":33,"outputs":[{"execution_count":33,"output_type":"execute_result","data":{"text/plain":"                            Uniq Id Start City         d1           d2  \\\n0  e788ab76d9d8cf1e6ed2f139645ca5d1     Mumbai  New Delhi   Chandigarh   \n1  178f892630ce3e335a5a41d5d83937fd  New Delhi   Srinagar     Srinagar   \n2  f060f2954840503cc2fdaf495357b7df  New Delhi    Udaipur  Chittorgarh   \n\n         dist1  country_ae  country_bt  country_cn  country_cz  country_dk  \\\n0  1144.539256           0           0           0           0           0   \n1   646.667617           0           0           0           0           0   \n2   568.255010           0           0           0           0           0   \n\n   ...  country_lk  country_mu  country_my  country_np  country_nz  \\\n0  ...           0           0           0           0           0   \n1  ...           0           0           0           0           0   \n2  ...           0           0           0           0           0   \n\n   country_ru  country_sg  country_th  country_us        dist2  \n0           0           0           0           0  1349.851080  \n1           0           0           0           0   646.667617  \n2           0           0           0           0   510.392507  \n\n[3 rows x 24 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Uniq Id</th>\n      <th>Start City</th>\n      <th>d1</th>\n      <th>d2</th>\n      <th>dist1</th>\n      <th>country_ae</th>\n      <th>country_bt</th>\n      <th>country_cn</th>\n      <th>country_cz</th>\n      <th>country_dk</th>\n      <th>...</th>\n      <th>country_lk</th>\n      <th>country_mu</th>\n      <th>country_my</th>\n      <th>country_np</th>\n      <th>country_nz</th>\n      <th>country_ru</th>\n      <th>country_sg</th>\n      <th>country_th</th>\n      <th>country_us</th>\n      <th>dist2</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>e788ab76d9d8cf1e6ed2f139645ca5d1</td>\n      <td>Mumbai</td>\n      <td>New Delhi</td>\n      <td>Chandigarh</td>\n      <td>1144.539256</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1349.851080</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>178f892630ce3e335a5a41d5d83937fd</td>\n      <td>New Delhi</td>\n      <td>Srinagar</td>\n      <td>Srinagar</td>\n      <td>646.667617</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>646.667617</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>f060f2954840503cc2fdaf495357b7df</td>\n      <td>New Delhi</td>\n      <td>Udaipur</td>\n      <td>Chittorgarh</td>\n      <td>568.255010</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>510.392507</td>\n    </tr>\n  </tbody>\n</table>\n<p>3 rows Ã— 24 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"df1 = df1.drop(['Start City','d1','d2'],axis=1)\ndf = pd.merge(df,df1,how = 'left' , on=['Uniq Id'])\n\n","metadata":{"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"code","source":"df['Airline1'] = df['Airline'].apply(lambda x : x.split('|')[0])\ndf['rating'] = df['Hotel Details'].apply(lambda x : rating(x))\ndf['rating'] = np.where(df['rating'] > 5 , 3 ,df['rating'])\ndf['info'] = df[['Package Name','Destination','Itinerary','Places Covered','Hotel Details','Sightseeing Places Covered','Cancellation Rules','Airline']].agg('-'.join, axis=1)\ndf['ps'] = df['Package Type']+'-'+df['Start City']\n\n# datetimindex variables :- https://pandas.pydata.org/pandas-docs/version/0.23/generated/pandas.DatetimeIndex.html\n\ndf['Date'] = pd.to_datetime(df['Travel Date'])\ndf['Year'] = pd.to_datetime(df['Date']).dt.year\ndf['Month'] = pd.to_datetime(df['Date']).dt.month\n\ndf['Week'] = pd.to_datetime(df['Date']).dt.week \ndf['Quarter'] = pd.to_datetime(df['Date']).dt.quarter  \n\ndf['Is_quarter_end'] = pd.to_datetime(df['Date']).dt.is_quarter_end \ndf['Day'] = pd.to_datetime(df['Date']).dt.day\ndf['Dayofweek'] = pd.to_datetime(df['Date']).dt.dayofweek\ndf['DayOfyear'] = pd.to_datetime(df['Date']).dt.dayofyear\ndf['Is_year_start'] = pd.to_datetime(df['Date']).dt.is_year_start \ndf['Is_year_end'] = pd.to_datetime(df['Date']).dt.is_year_end\ndf['Semester'] = np.where(df['Quarter'].isin([1,2]),1,2)   \ndf['Is_weekday'] = np.where(df['Dayofweek'].isin([0,1,2,3,4]),1,0)\ndf['Is_month_start'] = pd.to_datetime(df['Date']).dt.is_month_start \ndf['Is_month_end'] = pd.to_datetime(df['Date']).dt.is_month_end \ndf['Is_quarter_start'] = pd.to_datetime(df['Date']).dt.is_quarter_start\ndf['Days_in_month'] = pd.to_datetime(df['Date']).dt.days_in_month \ndel df['Travel Date']\n\ndf['tot_night'] = df['Itinerary'].apply(lambda x : tot_nit(x) )\ndf['places_covered_ct']  = df['Destination'].apply(lambda x: len(x.split('|')) )\ndf['Sightseeing Places Covered ct']  = df['Sightseeing Places Covered'].apply(lambda x: len(x.split('|')) )\ndf['Cancellation Rules ct'] = df['Cancellation Rules'].apply(lambda x : len(x.split()))","metadata":{"trusted":true},"execution_count":35,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:13: FutureWarning: Series.dt.weekofyear and Series.dt.week have been deprecated.  Please use Series.dt.isocalendar().week instead.\n  del sys.path[0]\n","output_type":"stream"}]},{"cell_type":"code","source":"df['package_per_airline'] = df.groupby(['Package Type'])['Airline1'].transform('nunique')\n\ndf['avg_night_per_package'] = df.groupby(['Package Type'])['tot_night'].transform('mean')\ndf['avg_night_per_ps'] = df.groupby(['ps'])['tot_night'].transform('mean')\n\ndf['avg_places_per_package'] = df.groupby(['Package Type'])['places_covered_ct'].transform('mean')\ndf['avg_places_per_ps'] = df.groupby(['ps'])['places_covered_ct'].transform('mean')\n\ndf['avg_sight_per_package'] = df.groupby(['Package Type'])['Sightseeing Places Covered ct'].transform('mean')\ndf['avg_sight_per_ps'] = df.groupby(['ps'])['Sightseeing Places Covered ct'].transform('mean')\n\ndf['avg_can_per_package'] = df.groupby(['Package Type'])['Cancellation Rules ct'].transform('mean')\ndf['avg_can_per_ps'] = df.groupby(['ps'])['Cancellation Rules ct'].transform('mean')\n","metadata":{"trusted":true},"execution_count":36,"outputs":[]},{"cell_type":"code","source":"df = df.drop(['Package Name','d1','d2','Destination','Itinerary','Places Covered','Hotel Details','Sightseeing Places Covered','Cancellation Rules','Airline'],axis=1)\ndf.head(1)","metadata":{"trusted":true},"execution_count":37,"outputs":[{"execution_count":37,"output_type":"execute_result","data":{"text/plain":"                            Uniq Id Package Type Start City  Flight Stops  \\\n0  e788ab76d9d8cf1e6ed2f139645ca5d1     Standard     Mumbai             2   \n\n   Meals  Per Person Price        dist1  country_ae  country_bt  country_cn  \\\n0      3           11509.0  1144.539256           0           0           0   \n\n   ...  Cancellation Rules ct  package_per_airline  avg_night_per_package  \\\n0  ...                      2                   26               4.759148   \n\n   avg_night_per_ps  avg_places_per_package  avg_places_per_ps  \\\n0          4.919508                2.597234           2.691388   \n\n   avg_sight_per_package  avg_sight_per_ps  avg_can_per_package  \\\n0               7.509118           7.68717           124.714845   \n\n   avg_can_per_ps  \n0      116.724429  \n\n[1 rows x 60 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Uniq Id</th>\n      <th>Package Type</th>\n      <th>Start City</th>\n      <th>Flight Stops</th>\n      <th>Meals</th>\n      <th>Per Person Price</th>\n      <th>dist1</th>\n      <th>country_ae</th>\n      <th>country_bt</th>\n      <th>country_cn</th>\n      <th>...</th>\n      <th>Cancellation Rules ct</th>\n      <th>package_per_airline</th>\n      <th>avg_night_per_package</th>\n      <th>avg_night_per_ps</th>\n      <th>avg_places_per_package</th>\n      <th>avg_places_per_ps</th>\n      <th>avg_sight_per_package</th>\n      <th>avg_sight_per_ps</th>\n      <th>avg_can_per_package</th>\n      <th>avg_can_per_ps</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>e788ab76d9d8cf1e6ed2f139645ca5d1</td>\n      <td>Standard</td>\n      <td>Mumbai</td>\n      <td>2</td>\n      <td>3</td>\n      <td>11509.0</td>\n      <td>1144.539256</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>2</td>\n      <td>26</td>\n      <td>4.759148</td>\n      <td>4.919508</td>\n      <td>2.597234</td>\n      <td>2.691388</td>\n      <td>7.509118</td>\n      <td>7.68717</td>\n      <td>124.714845</td>\n      <td>116.724429</td>\n    </tr>\n  </tbody>\n</table>\n<p>1 rows Ã— 60 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"df['dist'] = np.abs(df['dist1']-df['dist2'])\ndf = pd.get_dummies(df, columns = ['Package Type', 'Start City','ps','Airline1'])\ndf.info()","metadata":{"trusted":true},"execution_count":38,"outputs":[{"name":"stdout","text":"<class 'pandas.core.frame.DataFrame'>\nInt64Index: 30000 entries, 0 to 29999\nColumns: 106 entries, Uniq Id to Airline1_Vistara\ndtypes: bool(6), datetime64[ns](1), float64(13), int64(17), object(2), uint8(67)\nmemory usage: 9.9+ MB\n","output_type":"stream"}]},{"cell_type":"code","source":"count_vec = CountVectorizer(stop_words=\"english\", analyzer='word', \n                            ngram_range=(1, 1), max_df=1.0, min_df=1, max_features=1100)\ndf_info = pd.DataFrame(count_vec.fit_transform(df['info']).todense())\ndf_info.columns = ['Model_Info_Top_' + str(c) for c in df_info.columns]\ndf = pd.concat([df, df_info], axis=1)","metadata":{"trusted":true},"execution_count":39,"outputs":[]},{"cell_type":"code","source":"train, test  = df[:train.shape[0]].reset_index(drop=True), df[train.shape[0]:].reset_index(drop=True)","metadata":{"trusted":true},"execution_count":40,"outputs":[]},{"cell_type":"code","source":"\nfeatures = [c for c in train.columns if c not in ['Uniq Id' ,'Per Person Price', 'info','Date']]\ntarget = train['Per Person Price']","metadata":{"trusted":true},"execution_count":42,"outputs":[]},{"cell_type":"code","source":"selector = SelectKBest(f_regression, k=900).fit(train[features], target)\ncols = selector.get_support(indices=True)\nfeatures1 = train[features].iloc[:,cols].columns","metadata":{"trusted":true},"execution_count":43,"outputs":[]},{"cell_type":"code","source":"\ntraining_start_time = time.time()\n\nfolds = StratifiedKFold(n_splits = 10)\ntrain_prediction = np.zeros(len(train))\ntest_prediction = np.zeros(len(test))\n\n\nfor fold_, (train_idx, val_idx) in enumerate(folds.split(train, pd.qcut(target, 10, labels=False))):\n    \n    print(f'\\n---- Fold {fold_} -----\\n')\n    \n    fold_start_time = time.time()\n    \n    X_train, y_train = train.iloc[train_idx][features1], target.iloc[train_idx]\n    X_val, y_val = train.iloc[val_idx][features1], target.iloc[val_idx]\n    X_test = test[features1]\n    print(X_train.shape[1], X_val.shape[1])\n\n    lgbm = LGBMRegressor(n_estimators=10000, num_leaves=70, max_depth=20\n                        ,min_child_samples=10, learning_rate=0.01, colsample_bytree=0.8,subsample=0.5, reg_alpha=0, reg_lambda=20)  \n\n    \n    _ = lgbm.fit(X_train, np.log(y_train), eval_set = [(X_val, np.log(y_val))], verbose=100, early_stopping_rounds=200, eval_metric='rmse')\n\n    train_prediction[val_idx] = np.exp(lgbm.predict(X_val))\n    current_test_pred = np.exp(lgbm.predict(X_test))\n    test_prediction += np.exp(lgbm.predict(X_test))/10\n    \n    \n    print(f'\\n Fold {rmse(np.log(y_val), np.log(train_prediction[val_idx]))}')\n    \n    fold_end_time = time.time()\n    total_fold_time = int(fold_end_time - fold_start_time)\n    \n    print(f\"\\n->-> Fold ran for {(total_fold_time)//60} minutes {(total_fold_time)%60} seconds\")\n    \n\nprint(f'\\ntrain score (validation fold) : {rmse(np.log(target), np.log(train_prediction))}')\ntraining_end_time = time.time()\ntotal_training_time = int(training_end_time - training_start_time)\n\nprint(f'\\n->-> Total  time: {(total_training_time)//60} minutes ')\n\nlgbm_prediction = test_prediction","metadata":{"trusted":true},"execution_count":45,"outputs":[{"name":"stdout","text":"\n---- Fold 0 -----\n\n900 900\nTraining until validation scores don't improve for 200 rounds\n[100]\tvalid_0's rmse: 0.313207\tvalid_0's l2: 0.0980989\n[200]\tvalid_0's rmse: 0.241892\tvalid_0's l2: 0.0585117\n[300]\tvalid_0's rmse: 0.216956\tvalid_0's l2: 0.0470697\n[400]\tvalid_0's rmse: 0.204491\tvalid_0's l2: 0.0418166\n[500]\tvalid_0's rmse: 0.196649\tvalid_0's l2: 0.0386709\n[600]\tvalid_0's rmse: 0.191783\tvalid_0's l2: 0.0367808\n[700]\tvalid_0's rmse: 0.188357\tvalid_0's l2: 0.0354783\n[800]\tvalid_0's rmse: 0.185577\tvalid_0's l2: 0.0344388\n[900]\tvalid_0's rmse: 0.183303\tvalid_0's l2: 0.0335998\n[1000]\tvalid_0's rmse: 0.181245\tvalid_0's l2: 0.0328499\n[1100]\tvalid_0's rmse: 0.179558\tvalid_0's l2: 0.032241\n[1200]\tvalid_0's rmse: 0.17822\tvalid_0's l2: 0.0317622\n[1300]\tvalid_0's rmse: 0.177174\tvalid_0's l2: 0.0313905\n[1400]\tvalid_0's rmse: 0.176221\tvalid_0's l2: 0.0310539\n[1500]\tvalid_0's rmse: 0.175299\tvalid_0's l2: 0.0307296\n[1600]\tvalid_0's rmse: 0.174454\tvalid_0's l2: 0.0304342\n[1700]\tvalid_0's rmse: 0.173811\tvalid_0's l2: 0.0302104\n[1800]\tvalid_0's rmse: 0.173194\tvalid_0's l2: 0.0299963\n[1900]\tvalid_0's rmse: 0.172689\tvalid_0's l2: 0.0298216\n[2000]\tvalid_0's rmse: 0.17211\tvalid_0's l2: 0.0296217\n[2100]\tvalid_0's rmse: 0.17162\tvalid_0's l2: 0.0294533\n[2200]\tvalid_0's rmse: 0.171183\tvalid_0's l2: 0.0293036\n[2300]\tvalid_0's rmse: 0.170862\tvalid_0's l2: 0.0291939\n[2400]\tvalid_0's rmse: 0.170576\tvalid_0's l2: 0.0290961\n[2500]\tvalid_0's rmse: 0.170294\tvalid_0's l2: 0.0289999\n[2600]\tvalid_0's rmse: 0.170002\tvalid_0's l2: 0.0289006\n[2700]\tvalid_0's rmse: 0.16972\tvalid_0's l2: 0.028805\n[2800]\tvalid_0's rmse: 0.169482\tvalid_0's l2: 0.0287241\n[2900]\tvalid_0's rmse: 0.169203\tvalid_0's l2: 0.0286296\n[3000]\tvalid_0's rmse: 0.168963\tvalid_0's l2: 0.0285484\n[3100]\tvalid_0's rmse: 0.168772\tvalid_0's l2: 0.0284839\n[3200]\tvalid_0's rmse: 0.168599\tvalid_0's l2: 0.0284258\n[3300]\tvalid_0's rmse: 0.168346\tvalid_0's l2: 0.0283405\n[3400]\tvalid_0's rmse: 0.168096\tvalid_0's l2: 0.0282561\n[3500]\tvalid_0's rmse: 0.167928\tvalid_0's l2: 0.0281998\n[3600]\tvalid_0's rmse: 0.167743\tvalid_0's l2: 0.0281377\n[3700]\tvalid_0's rmse: 0.16759\tvalid_0's l2: 0.0280864\n[3800]\tvalid_0's rmse: 0.167423\tvalid_0's l2: 0.0280305\n[3900]\tvalid_0's rmse: 0.167325\tvalid_0's l2: 0.0279977\n[4000]\tvalid_0's rmse: 0.167211\tvalid_0's l2: 0.0279594\n[4100]\tvalid_0's rmse: 0.167134\tvalid_0's l2: 0.0279339\n[4200]\tvalid_0's rmse: 0.167037\tvalid_0's l2: 0.0279013\n[4300]\tvalid_0's rmse: 0.166898\tvalid_0's l2: 0.0278549\n[4400]\tvalid_0's rmse: 0.166783\tvalid_0's l2: 0.0278166\n[4500]\tvalid_0's rmse: 0.166728\tvalid_0's l2: 0.0277984\n[4600]\tvalid_0's rmse: 0.16663\tvalid_0's l2: 0.0277655\n[4700]\tvalid_0's rmse: 0.166546\tvalid_0's l2: 0.0277376\n[4800]\tvalid_0's rmse: 0.166467\tvalid_0's l2: 0.0277114\n[4900]\tvalid_0's rmse: 0.166427\tvalid_0's l2: 0.027698\n[5000]\tvalid_0's rmse: 0.166351\tvalid_0's l2: 0.0276727\n[5100]\tvalid_0's rmse: 0.166301\tvalid_0's l2: 0.027656\n[5200]\tvalid_0's rmse: 0.166243\tvalid_0's l2: 0.0276366\n[5300]\tvalid_0's rmse: 0.166176\tvalid_0's l2: 0.0276144\n[5400]\tvalid_0's rmse: 0.16612\tvalid_0's l2: 0.0275957\n[5500]\tvalid_0's rmse: 0.166095\tvalid_0's l2: 0.0275875\n[5600]\tvalid_0's rmse: 0.166046\tvalid_0's l2: 0.0275712\n[5700]\tvalid_0's rmse: 0.165994\tvalid_0's l2: 0.0275541\n[5800]\tvalid_0's rmse: 0.165924\tvalid_0's l2: 0.0275308\n[5900]\tvalid_0's rmse: 0.165844\tvalid_0's l2: 0.0275043\n[6000]\tvalid_0's rmse: 0.165821\tvalid_0's l2: 0.0274967\n[6100]\tvalid_0's rmse: 0.165788\tvalid_0's l2: 0.0274855\n[6200]\tvalid_0's rmse: 0.165731\tvalid_0's l2: 0.0274669\n[6300]\tvalid_0's rmse: 0.165774\tvalid_0's l2: 0.027481\n[6400]\tvalid_0's rmse: 0.16578\tvalid_0's l2: 0.0274831\nEarly stopping, best iteration is:\n[6270]\tvalid_0's rmse: 0.165715\tvalid_0's l2: 0.0274614\n\n Fold 0.16571473832764003\n\n->-> Fold ran for 1 minutes 6 seconds\n\n---- Fold 1 -----\n\n900 900\nTraining until validation scores don't improve for 200 rounds\n[100]\tvalid_0's rmse: 0.307267\tvalid_0's l2: 0.0944128\n[200]\tvalid_0's rmse: 0.234498\tvalid_0's l2: 0.0549893\n[300]\tvalid_0's rmse: 0.207224\tvalid_0's l2: 0.0429417\n[400]\tvalid_0's rmse: 0.193526\tvalid_0's l2: 0.0374522\n[500]\tvalid_0's rmse: 0.18572\tvalid_0's l2: 0.0344917\n[600]\tvalid_0's rmse: 0.180578\tvalid_0's l2: 0.0326083\n[700]\tvalid_0's rmse: 0.176974\tvalid_0's l2: 0.0313199\n[800]\tvalid_0's rmse: 0.174213\tvalid_0's l2: 0.0303502\n[900]\tvalid_0's rmse: 0.172106\tvalid_0's l2: 0.0296206\n[1000]\tvalid_0's rmse: 0.170353\tvalid_0's l2: 0.0290201\n[1100]\tvalid_0's rmse: 0.168798\tvalid_0's l2: 0.0284926\n[1200]\tvalid_0's rmse: 0.167538\tvalid_0's l2: 0.0280691\n[1300]\tvalid_0's rmse: 0.166303\tvalid_0's l2: 0.0276566\n[1400]\tvalid_0's rmse: 0.165337\tvalid_0's l2: 0.0273365\n[1500]\tvalid_0's rmse: 0.164457\tvalid_0's l2: 0.0270462\n[1600]\tvalid_0's rmse: 0.163695\tvalid_0's l2: 0.0267961\n[1700]\tvalid_0's rmse: 0.163031\tvalid_0's l2: 0.0265791\n[1800]\tvalid_0's rmse: 0.162446\tvalid_0's l2: 0.0263888\n[1900]\tvalid_0's rmse: 0.161886\tvalid_0's l2: 0.026207\n[2000]\tvalid_0's rmse: 0.161392\tvalid_0's l2: 0.0260474\n[2100]\tvalid_0's rmse: 0.160925\tvalid_0's l2: 0.0258969\n[2200]\tvalid_0's rmse: 0.160551\tvalid_0's l2: 0.0257767\n[2300]\tvalid_0's rmse: 0.160171\tvalid_0's l2: 0.0256547\n[2400]\tvalid_0's rmse: 0.159842\tvalid_0's l2: 0.0255496\n[2500]\tvalid_0's rmse: 0.159498\tvalid_0's l2: 0.0254397\n[2600]\tvalid_0's rmse: 0.159159\tvalid_0's l2: 0.0253316\n[2700]\tvalid_0's rmse: 0.158896\tvalid_0's l2: 0.025248\n[2800]\tvalid_0's rmse: 0.158554\tvalid_0's l2: 0.0251392\n[2900]\tvalid_0's rmse: 0.158301\tvalid_0's l2: 0.0250591\n[3000]\tvalid_0's rmse: 0.158038\tvalid_0's l2: 0.0249762\n[3100]\tvalid_0's rmse: 0.157769\tvalid_0's l2: 0.0248911\n[3200]\tvalid_0's rmse: 0.157512\tvalid_0's l2: 0.0248101\n[3300]\tvalid_0's rmse: 0.157289\tvalid_0's l2: 0.0247399\n[3400]\tvalid_0's rmse: 0.157056\tvalid_0's l2: 0.0246666\n[3500]\tvalid_0's rmse: 0.156819\tvalid_0's l2: 0.0245921\n[3600]\tvalid_0's rmse: 0.156645\tvalid_0's l2: 0.0245378\n[3700]\tvalid_0's rmse: 0.156451\tvalid_0's l2: 0.024477\n[3800]\tvalid_0's rmse: 0.156305\tvalid_0's l2: 0.0244313\n[3900]\tvalid_0's rmse: 0.156164\tvalid_0's l2: 0.0243871\n[4000]\tvalid_0's rmse: 0.155986\tvalid_0's l2: 0.0243317\n[4100]\tvalid_0's rmse: 0.155862\tvalid_0's l2: 0.0242929\n[4200]\tvalid_0's rmse: 0.155724\tvalid_0's l2: 0.0242499\n[4300]\tvalid_0's rmse: 0.155579\tvalid_0's l2: 0.0242047\n[4400]\tvalid_0's rmse: 0.155484\tvalid_0's l2: 0.0241754\n[4500]\tvalid_0's rmse: 0.155402\tvalid_0's l2: 0.0241497\n[4600]\tvalid_0's rmse: 0.155295\tvalid_0's l2: 0.0241166\n[4700]\tvalid_0's rmse: 0.155206\tvalid_0's l2: 0.024089\n[4800]\tvalid_0's rmse: 0.155077\tvalid_0's l2: 0.0240489\n[4900]\tvalid_0's rmse: 0.15496\tvalid_0's l2: 0.0240125\n[5000]\tvalid_0's rmse: 0.154905\tvalid_0's l2: 0.0239956\n[5100]\tvalid_0's rmse: 0.154825\tvalid_0's l2: 0.0239706\n[5200]\tvalid_0's rmse: 0.154753\tvalid_0's l2: 0.0239484\n[5300]\tvalid_0's rmse: 0.154668\tvalid_0's l2: 0.0239222\n[5400]\tvalid_0's rmse: 0.15456\tvalid_0's l2: 0.0238888\n[5500]\tvalid_0's rmse: 0.154482\tvalid_0's l2: 0.0238646\n[5600]\tvalid_0's rmse: 0.154404\tvalid_0's l2: 0.0238407\n[5700]\tvalid_0's rmse: 0.154377\tvalid_0's l2: 0.0238321\n[5800]\tvalid_0's rmse: 0.154347\tvalid_0's l2: 0.0238229\n[5900]\tvalid_0's rmse: 0.154303\tvalid_0's l2: 0.0238093\n[6000]\tvalid_0's rmse: 0.154253\tvalid_0's l2: 0.0237939\n[6100]\tvalid_0's rmse: 0.154177\tvalid_0's l2: 0.0237707\n[6200]\tvalid_0's rmse: 0.154132\tvalid_0's l2: 0.0237566\n[6300]\tvalid_0's rmse: 0.154088\tvalid_0's l2: 0.023743\n[6400]\tvalid_0's rmse: 0.154047\tvalid_0's l2: 0.0237304\n[6500]\tvalid_0's rmse: 0.154007\tvalid_0's l2: 0.0237183\n[6600]\tvalid_0's rmse: 0.153991\tvalid_0's l2: 0.0237134\n[6700]\tvalid_0's rmse: 0.153982\tvalid_0's l2: 0.0237106\n[6800]\tvalid_0's rmse: 0.153947\tvalid_0's l2: 0.0236997\n[6900]\tvalid_0's rmse: 0.153938\tvalid_0's l2: 0.0236968\n[7000]\tvalid_0's rmse: 0.153912\tvalid_0's l2: 0.023689\n[7100]\tvalid_0's rmse: 0.15388\tvalid_0's l2: 0.0236791\n[7200]\tvalid_0's rmse: 0.15385\tvalid_0's l2: 0.0236699\n[7300]\tvalid_0's rmse: 0.153824\tvalid_0's l2: 0.023662\n[7400]\tvalid_0's rmse: 0.153824\tvalid_0's l2: 0.0236618\n[7500]\tvalid_0's rmse: 0.15382\tvalid_0's l2: 0.0236606\n[7600]\tvalid_0's rmse: 0.153794\tvalid_0's l2: 0.0236526\n[7700]\tvalid_0's rmse: 0.153776\tvalid_0's l2: 0.023647\n[7800]\tvalid_0's rmse: 0.153747\tvalid_0's l2: 0.0236382\n[7900]\tvalid_0's rmse: 0.15371\tvalid_0's l2: 0.0236267\n[8000]\tvalid_0's rmse: 0.153685\tvalid_0's l2: 0.023619\n[8100]\tvalid_0's rmse: 0.153709\tvalid_0's l2: 0.0236263\nEarly stopping, best iteration is:\n[7997]\tvalid_0's rmse: 0.153682\tvalid_0's l2: 0.0236183\n\n Fold 0.15368240955173854\n\n->-> Fold ran for 1 minutes 21 seconds\n\n---- Fold 2 -----\n\n900 900\nTraining until validation scores don't improve for 200 rounds\n[100]\tvalid_0's rmse: 0.298985\tvalid_0's l2: 0.0893917\n[200]\tvalid_0's rmse: 0.222339\tvalid_0's l2: 0.0494345\n[300]\tvalid_0's rmse: 0.194998\tvalid_0's l2: 0.0380242\n[400]\tvalid_0's rmse: 0.182254\tvalid_0's l2: 0.0332165\n[500]\tvalid_0's rmse: 0.174777\tvalid_0's l2: 0.0305471\n[600]\tvalid_0's rmse: 0.169905\tvalid_0's l2: 0.0288678\n[700]\tvalid_0's rmse: 0.166706\tvalid_0's l2: 0.0277908\n[800]\tvalid_0's rmse: 0.16443\tvalid_0's l2: 0.0270373\n[900]\tvalid_0's rmse: 0.162562\tvalid_0's l2: 0.0264266\n[1000]\tvalid_0's rmse: 0.160933\tvalid_0's l2: 0.0258994\n[1100]\tvalid_0's rmse: 0.159722\tvalid_0's l2: 0.0255113\n[1200]\tvalid_0's rmse: 0.158598\tvalid_0's l2: 0.0251532\n[1300]\tvalid_0's rmse: 0.157733\tvalid_0's l2: 0.0248798\n[1400]\tvalid_0's rmse: 0.157039\tvalid_0's l2: 0.0246614\n[1500]\tvalid_0's rmse: 0.156422\tvalid_0's l2: 0.0244679\n[1600]\tvalid_0's rmse: 0.155901\tvalid_0's l2: 0.0243051\n[1700]\tvalid_0's rmse: 0.15544\tvalid_0's l2: 0.0241617\n[1800]\tvalid_0's rmse: 0.154984\tvalid_0's l2: 0.0240199\n[1900]\tvalid_0's rmse: 0.154598\tvalid_0's l2: 0.0239004\n[2000]\tvalid_0's rmse: 0.154247\tvalid_0's l2: 0.0237921\n[2100]\tvalid_0's rmse: 0.154037\tvalid_0's l2: 0.0237273\n[2200]\tvalid_0's rmse: 0.153741\tvalid_0's l2: 0.0236363\n[2300]\tvalid_0's rmse: 0.153575\tvalid_0's l2: 0.0235854\n[2400]\tvalid_0's rmse: 0.153423\tvalid_0's l2: 0.0235387\n[2500]\tvalid_0's rmse: 0.153219\tvalid_0's l2: 0.0234761\n[2600]\tvalid_0's rmse: 0.152995\tvalid_0's l2: 0.0234075\n[2700]\tvalid_0's rmse: 0.152801\tvalid_0's l2: 0.0233481\n[2800]\tvalid_0's rmse: 0.152575\tvalid_0's l2: 0.0232792\n[2900]\tvalid_0's rmse: 0.15237\tvalid_0's l2: 0.0232165\n[3000]\tvalid_0's rmse: 0.152204\tvalid_0's l2: 0.023166\n[3100]\tvalid_0's rmse: 0.15203\tvalid_0's l2: 0.0231132\n[3200]\tvalid_0's rmse: 0.151889\tvalid_0's l2: 0.0230702\n[3300]\tvalid_0's rmse: 0.15173\tvalid_0's l2: 0.0230219\n[3400]\tvalid_0's rmse: 0.151614\tvalid_0's l2: 0.0229869\n[3500]\tvalid_0's rmse: 0.151528\tvalid_0's l2: 0.0229608\n[3600]\tvalid_0's rmse: 0.151376\tvalid_0's l2: 0.0229148\n[3700]\tvalid_0's rmse: 0.151269\tvalid_0's l2: 0.0228823\n[3800]\tvalid_0's rmse: 0.151151\tvalid_0's l2: 0.0228466\n[3900]\tvalid_0's rmse: 0.15107\tvalid_0's l2: 0.0228221\n[4000]\tvalid_0's rmse: 0.150968\tvalid_0's l2: 0.0227914\n[4100]\tvalid_0's rmse: 0.150918\tvalid_0's l2: 0.0227762\n[4200]\tvalid_0's rmse: 0.150863\tvalid_0's l2: 0.0227597\n[4300]\tvalid_0's rmse: 0.15079\tvalid_0's l2: 0.0227375\n[4400]\tvalid_0's rmse: 0.150756\tvalid_0's l2: 0.0227275\n[4500]\tvalid_0's rmse: 0.15068\tvalid_0's l2: 0.0227044\n[4600]\tvalid_0's rmse: 0.150616\tvalid_0's l2: 0.0226852\n[4700]\tvalid_0's rmse: 0.150583\tvalid_0's l2: 0.0226751\n[4800]\tvalid_0's rmse: 0.15055\tvalid_0's l2: 0.0226654\n[4900]\tvalid_0's rmse: 0.150538\tvalid_0's l2: 0.0226616\n[5000]\tvalid_0's rmse: 0.150501\tvalid_0's l2: 0.0226506\n[5100]\tvalid_0's rmse: 0.150467\tvalid_0's l2: 0.0226402\n[5200]\tvalid_0's rmse: 0.150441\tvalid_0's l2: 0.0226324\n[5300]\tvalid_0's rmse: 0.1505\tvalid_0's l2: 0.0226502\n[5400]\tvalid_0's rmse: 0.150538\tvalid_0's l2: 0.0226616\nEarly stopping, best iteration is:\n[5231]\tvalid_0's rmse: 0.150436\tvalid_0's l2: 0.0226309\n\n Fold 0.15043563851011843\n\n->-> Fold ran for 0 minutes 55 seconds\n\n---- Fold 3 -----\n\n900 900\nTraining until validation scores don't improve for 200 rounds\n[100]\tvalid_0's rmse: 0.312774\tvalid_0's l2: 0.0978276\n[200]\tvalid_0's rmse: 0.240683\tvalid_0's l2: 0.0579284\n[300]\tvalid_0's rmse: 0.214612\tvalid_0's l2: 0.0460581\n[400]\tvalid_0's rmse: 0.200843\tvalid_0's l2: 0.040338\n[500]\tvalid_0's rmse: 0.192603\tvalid_0's l2: 0.0370959\n[600]\tvalid_0's rmse: 0.187172\tvalid_0's l2: 0.0350335\n[700]\tvalid_0's rmse: 0.183628\tvalid_0's l2: 0.0337194\n[800]\tvalid_0's rmse: 0.180804\tvalid_0's l2: 0.0326901\n[900]\tvalid_0's rmse: 0.178685\tvalid_0's l2: 0.0319282\n[1000]\tvalid_0's rmse: 0.177001\tvalid_0's l2: 0.0313294\n[1100]\tvalid_0's rmse: 0.175436\tvalid_0's l2: 0.0307777\n[1200]\tvalid_0's rmse: 0.174171\tvalid_0's l2: 0.0303354\n[1300]\tvalid_0's rmse: 0.173092\tvalid_0's l2: 0.0299607\n[1400]\tvalid_0's rmse: 0.172138\tvalid_0's l2: 0.0296315\n[1500]\tvalid_0's rmse: 0.17135\tvalid_0's l2: 0.0293609\n[1600]\tvalid_0's rmse: 0.170536\tvalid_0's l2: 0.0290826\n[1700]\tvalid_0's rmse: 0.169935\tvalid_0's l2: 0.028878\n[1800]\tvalid_0's rmse: 0.16938\tvalid_0's l2: 0.0286894\n[1900]\tvalid_0's rmse: 0.168849\tvalid_0's l2: 0.02851\n[2000]\tvalid_0's rmse: 0.168379\tvalid_0's l2: 0.0283514\n[2100]\tvalid_0's rmse: 0.167938\tvalid_0's l2: 0.0282032\n[2200]\tvalid_0's rmse: 0.167532\tvalid_0's l2: 0.0280669\n[2300]\tvalid_0's rmse: 0.167178\tvalid_0's l2: 0.0279486\n[2400]\tvalid_0's rmse: 0.16683\tvalid_0's l2: 0.0278322\n[2500]\tvalid_0's rmse: 0.166525\tvalid_0's l2: 0.0277304\n[2600]\tvalid_0's rmse: 0.166226\tvalid_0's l2: 0.027631\n[2700]\tvalid_0's rmse: 0.16595\tvalid_0's l2: 0.0275393\n[2800]\tvalid_0's rmse: 0.165672\tvalid_0's l2: 0.0274474\n[2900]\tvalid_0's rmse: 0.165449\tvalid_0's l2: 0.0273735\n[3000]\tvalid_0's rmse: 0.165242\tvalid_0's l2: 0.0273049\n[3100]\tvalid_0's rmse: 0.165026\tvalid_0's l2: 0.0272336\n[3200]\tvalid_0's rmse: 0.164812\tvalid_0's l2: 0.027163\n[3300]\tvalid_0's rmse: 0.16463\tvalid_0's l2: 0.0271031\n[3400]\tvalid_0's rmse: 0.164457\tvalid_0's l2: 0.0270462\n[3500]\tvalid_0's rmse: 0.16431\tvalid_0's l2: 0.0269979\n[3600]\tvalid_0's rmse: 0.164145\tvalid_0's l2: 0.0269437\n[3700]\tvalid_0's rmse: 0.164003\tvalid_0's l2: 0.026897\n[3800]\tvalid_0's rmse: 0.163878\tvalid_0's l2: 0.0268559\n[3900]\tvalid_0's rmse: 0.163761\tvalid_0's l2: 0.0268175\n[4000]\tvalid_0's rmse: 0.163641\tvalid_0's l2: 0.0267782\n[4100]\tvalid_0's rmse: 0.1635\tvalid_0's l2: 0.0267324\n[4200]\tvalid_0's rmse: 0.163397\tvalid_0's l2: 0.0266986\n[4300]\tvalid_0's rmse: 0.163299\tvalid_0's l2: 0.0266665\n[4400]\tvalid_0's rmse: 0.163196\tvalid_0's l2: 0.0266329\n[4500]\tvalid_0's rmse: 0.163101\tvalid_0's l2: 0.0266018\n[4600]\tvalid_0's rmse: 0.163024\tvalid_0's l2: 0.0265767\n[4700]\tvalid_0's rmse: 0.162953\tvalid_0's l2: 0.0265536\n[4800]\tvalid_0's rmse: 0.162834\tvalid_0's l2: 0.026515\n[4900]\tvalid_0's rmse: 0.162751\tvalid_0's l2: 0.026488\n[5000]\tvalid_0's rmse: 0.162699\tvalid_0's l2: 0.0264709\n[5100]\tvalid_0's rmse: 0.162655\tvalid_0's l2: 0.0264568\n[5200]\tvalid_0's rmse: 0.162641\tvalid_0's l2: 0.0264522\n[5300]\tvalid_0's rmse: 0.16258\tvalid_0's l2: 0.0264322\n[5400]\tvalid_0's rmse: 0.162495\tvalid_0's l2: 0.0264047\n[5500]\tvalid_0's rmse: 0.162447\tvalid_0's l2: 0.0263891\n[5600]\tvalid_0's rmse: 0.162415\tvalid_0's l2: 0.0263787\n[5700]\tvalid_0's rmse: 0.162388\tvalid_0's l2: 0.02637\n[5800]\tvalid_0's rmse: 0.162383\tvalid_0's l2: 0.0263683\n[5900]\tvalid_0's rmse: 0.162362\tvalid_0's l2: 0.0263614\n[6000]\tvalid_0's rmse: 0.162355\tvalid_0's l2: 0.0263593\n[6100]\tvalid_0's rmse: 0.16233\tvalid_0's l2: 0.0263509\n[6200]\tvalid_0's rmse: 0.162294\tvalid_0's l2: 0.0263393\n[6300]\tvalid_0's rmse: 0.162262\tvalid_0's l2: 0.026329\n[6400]\tvalid_0's rmse: 0.162209\tvalid_0's l2: 0.0263119\n[6500]\tvalid_0's rmse: 0.162175\tvalid_0's l2: 0.0263006\n[6600]\tvalid_0's rmse: 0.162136\tvalid_0's l2: 0.026288\n[6700]\tvalid_0's rmse: 0.162075\tvalid_0's l2: 0.0262683\n[6800]\tvalid_0's rmse: 0.162046\tvalid_0's l2: 0.0262589\n[6900]\tvalid_0's rmse: 0.162012\tvalid_0's l2: 0.026248\n[7000]\tvalid_0's rmse: 0.161988\tvalid_0's l2: 0.0262401\n[7100]\tvalid_0's rmse: 0.161963\tvalid_0's l2: 0.026232\n[7200]\tvalid_0's rmse: 0.161927\tvalid_0's l2: 0.0262203\n[7300]\tvalid_0's rmse: 0.161928\tvalid_0's l2: 0.0262206\n[7400]\tvalid_0's rmse: 0.161943\tvalid_0's l2: 0.0262254\n[7500]\tvalid_0's rmse: 0.161942\tvalid_0's l2: 0.0262251\nEarly stopping, best iteration is:\n[7330]\tvalid_0's rmse: 0.161923\tvalid_0's l2: 0.0262191\n\n Fold 0.16192324729143762\n\n->-> Fold ran for 1 minutes 15 seconds\n\n---- Fold 4 -----\n\n900 900\nTraining until validation scores don't improve for 200 rounds\n[100]\tvalid_0's rmse: 0.31431\tvalid_0's l2: 0.0987907\n[200]\tvalid_0's rmse: 0.23604\tvalid_0's l2: 0.0557146\n[300]\tvalid_0's rmse: 0.206292\tvalid_0's l2: 0.0425565\n[400]\tvalid_0's rmse: 0.191899\tvalid_0's l2: 0.0368251\n[500]\tvalid_0's rmse: 0.183772\tvalid_0's l2: 0.033772\n[600]\tvalid_0's rmse: 0.178905\tvalid_0's l2: 0.0320069\n[700]\tvalid_0's rmse: 0.17553\tvalid_0's l2: 0.0308109\n[800]\tvalid_0's rmse: 0.172686\tvalid_0's l2: 0.0298206\n[900]\tvalid_0's rmse: 0.170546\tvalid_0's l2: 0.029086\n[1000]\tvalid_0's rmse: 0.168834\tvalid_0's l2: 0.0285048\n[1100]\tvalid_0's rmse: 0.167333\tvalid_0's l2: 0.0280002\n[1200]\tvalid_0's rmse: 0.166032\tvalid_0's l2: 0.0275666\n[1300]\tvalid_0's rmse: 0.164843\tvalid_0's l2: 0.0271734\n[1400]\tvalid_0's rmse: 0.163885\tvalid_0's l2: 0.0268583\n[1500]\tvalid_0's rmse: 0.162979\tvalid_0's l2: 0.026562\n[1600]\tvalid_0's rmse: 0.16222\tvalid_0's l2: 0.0263152\n[1700]\tvalid_0's rmse: 0.1615\tvalid_0's l2: 0.0260823\n[1800]\tvalid_0's rmse: 0.160886\tvalid_0's l2: 0.0258844\n[1900]\tvalid_0's rmse: 0.160408\tvalid_0's l2: 0.0257306\n[2000]\tvalid_0's rmse: 0.16003\tvalid_0's l2: 0.0256096\n[2100]\tvalid_0's rmse: 0.15963\tvalid_0's l2: 0.0254818\n[2200]\tvalid_0's rmse: 0.159309\tvalid_0's l2: 0.0253794\n[2300]\tvalid_0's rmse: 0.159014\tvalid_0's l2: 0.0252855\n[2400]\tvalid_0's rmse: 0.158736\tvalid_0's l2: 0.0251972\n[2500]\tvalid_0's rmse: 0.158469\tvalid_0's l2: 0.0251125\n[2600]\tvalid_0's rmse: 0.158196\tvalid_0's l2: 0.0250259\n[2700]\tvalid_0's rmse: 0.157989\tvalid_0's l2: 0.0249605\n[2800]\tvalid_0's rmse: 0.157751\tvalid_0's l2: 0.0248855\n[2900]\tvalid_0's rmse: 0.157549\tvalid_0's l2: 0.0248216\n[3000]\tvalid_0's rmse: 0.157363\tvalid_0's l2: 0.0247632\n[3100]\tvalid_0's rmse: 0.157195\tvalid_0's l2: 0.0247104\n[3200]\tvalid_0's rmse: 0.15703\tvalid_0's l2: 0.0246583\n[3300]\tvalid_0's rmse: 0.156873\tvalid_0's l2: 0.0246092\n[3400]\tvalid_0's rmse: 0.156697\tvalid_0's l2: 0.0245539\n[3500]\tvalid_0's rmse: 0.15656\tvalid_0's l2: 0.024511\n[3600]\tvalid_0's rmse: 0.156403\tvalid_0's l2: 0.024462\n[3700]\tvalid_0's rmse: 0.156299\tvalid_0's l2: 0.0244292\n[3800]\tvalid_0's rmse: 0.156168\tvalid_0's l2: 0.0243885\n[3900]\tvalid_0's rmse: 0.156058\tvalid_0's l2: 0.0243542\n[4000]\tvalid_0's rmse: 0.155928\tvalid_0's l2: 0.0243135\n[4100]\tvalid_0's rmse: 0.155833\tvalid_0's l2: 0.0242839\n[4200]\tvalid_0's rmse: 0.155736\tvalid_0's l2: 0.0242538\n[4300]\tvalid_0's rmse: 0.155652\tvalid_0's l2: 0.0242277\n[4400]\tvalid_0's rmse: 0.155574\tvalid_0's l2: 0.0242033\n[4500]\tvalid_0's rmse: 0.155498\tvalid_0's l2: 0.0241797\n[4600]\tvalid_0's rmse: 0.155449\tvalid_0's l2: 0.0241643\n[4700]\tvalid_0's rmse: 0.155389\tvalid_0's l2: 0.0241458\n[4800]\tvalid_0's rmse: 0.155328\tvalid_0's l2: 0.0241268\n[4900]\tvalid_0's rmse: 0.155237\tvalid_0's l2: 0.0240986\n[5000]\tvalid_0's rmse: 0.155177\tvalid_0's l2: 0.02408\n[5100]\tvalid_0's rmse: 0.155099\tvalid_0's l2: 0.0240556\n[5200]\tvalid_0's rmse: 0.15506\tvalid_0's l2: 0.0240436\n[5300]\tvalid_0's rmse: 0.155054\tvalid_0's l2: 0.0240418\n[5400]\tvalid_0's rmse: 0.15501\tvalid_0's l2: 0.0240282\n[5500]\tvalid_0's rmse: 0.154991\tvalid_0's l2: 0.0240222\n[5600]\tvalid_0's rmse: 0.154962\tvalid_0's l2: 0.0240132\n[5700]\tvalid_0's rmse: 0.154965\tvalid_0's l2: 0.024014\n[5800]\tvalid_0's rmse: 0.154974\tvalid_0's l2: 0.0240169\nEarly stopping, best iteration is:\n[5637]\tvalid_0's rmse: 0.154956\tvalid_0's l2: 0.0240114\n\n Fold 0.1549561407531377\n\n->-> Fold ran for 1 minutes 0 seconds\n\n---- Fold 5 -----\n\n900 900\nTraining until validation scores don't improve for 200 rounds\n[100]\tvalid_0's rmse: 0.306635\tvalid_0's l2: 0.0940249\n[200]\tvalid_0's rmse: 0.233894\tvalid_0's l2: 0.0547065\n[300]\tvalid_0's rmse: 0.206318\tvalid_0's l2: 0.042567\n[400]\tvalid_0's rmse: 0.192199\tvalid_0's l2: 0.0369406\n[500]\tvalid_0's rmse: 0.183946\tvalid_0's l2: 0.0338363\n[600]\tvalid_0's rmse: 0.178408\tvalid_0's l2: 0.0318294\n[700]\tvalid_0's rmse: 0.174713\tvalid_0's l2: 0.0305245\n[800]\tvalid_0's rmse: 0.17206\tvalid_0's l2: 0.0296046\n[900]\tvalid_0's rmse: 0.17005\tvalid_0's l2: 0.028917\n[1000]\tvalid_0's rmse: 0.168222\tvalid_0's l2: 0.0282988\n[1100]\tvalid_0's rmse: 0.16682\tvalid_0's l2: 0.0278291\n[1200]\tvalid_0's rmse: 0.16562\tvalid_0's l2: 0.0274299\n[1300]\tvalid_0's rmse: 0.164661\tvalid_0's l2: 0.0271132\n[1400]\tvalid_0's rmse: 0.163736\tvalid_0's l2: 0.0268096\n[1500]\tvalid_0's rmse: 0.162968\tvalid_0's l2: 0.0265585\n[1600]\tvalid_0's rmse: 0.162273\tvalid_0's l2: 0.0263325\n[1700]\tvalid_0's rmse: 0.161627\tvalid_0's l2: 0.0261234\n[1800]\tvalid_0's rmse: 0.161072\tvalid_0's l2: 0.0259442\n[1900]\tvalid_0's rmse: 0.160557\tvalid_0's l2: 0.0257785\n[2000]\tvalid_0's rmse: 0.160067\tvalid_0's l2: 0.0256214\n[2100]\tvalid_0's rmse: 0.159709\tvalid_0's l2: 0.0255068\n[2200]\tvalid_0's rmse: 0.159303\tvalid_0's l2: 0.0253774\n[2300]\tvalid_0's rmse: 0.15899\tvalid_0's l2: 0.0252777\n[2400]\tvalid_0's rmse: 0.158658\tvalid_0's l2: 0.0251723\n[2500]\tvalid_0's rmse: 0.158388\tvalid_0's l2: 0.0250867\n[2600]\tvalid_0's rmse: 0.158056\tvalid_0's l2: 0.0249816\n[2700]\tvalid_0's rmse: 0.157804\tvalid_0's l2: 0.0249021\n[2800]\tvalid_0's rmse: 0.157584\tvalid_0's l2: 0.0248326\n[2900]\tvalid_0's rmse: 0.157331\tvalid_0's l2: 0.024753\n[3000]\tvalid_0's rmse: 0.157102\tvalid_0's l2: 0.0246812\n[3100]\tvalid_0's rmse: 0.156867\tvalid_0's l2: 0.0246072\n[3200]\tvalid_0's rmse: 0.156697\tvalid_0's l2: 0.024554\n[3300]\tvalid_0's rmse: 0.156495\tvalid_0's l2: 0.0244906\n[3400]\tvalid_0's rmse: 0.156332\tvalid_0's l2: 0.0244396\n[3500]\tvalid_0's rmse: 0.156175\tvalid_0's l2: 0.0243907\n[3600]\tvalid_0's rmse: 0.156017\tvalid_0's l2: 0.0243413\n[3700]\tvalid_0's rmse: 0.155911\tvalid_0's l2: 0.0243083\n[3800]\tvalid_0's rmse: 0.155801\tvalid_0's l2: 0.0242739\n[3900]\tvalid_0's rmse: 0.155699\tvalid_0's l2: 0.024242\n[4000]\tvalid_0's rmse: 0.155545\tvalid_0's l2: 0.0241943\n[4100]\tvalid_0's rmse: 0.155404\tvalid_0's l2: 0.0241505\n[4200]\tvalid_0's rmse: 0.15529\tvalid_0's l2: 0.0241149\n[4300]\tvalid_0's rmse: 0.155229\tvalid_0's l2: 0.024096\n[4400]\tvalid_0's rmse: 0.155161\tvalid_0's l2: 0.024075\n[4500]\tvalid_0's rmse: 0.155063\tvalid_0's l2: 0.0240444\n[4600]\tvalid_0's rmse: 0.15502\tvalid_0's l2: 0.0240312\n[4700]\tvalid_0's rmse: 0.15495\tvalid_0's l2: 0.0240095\n[4800]\tvalid_0's rmse: 0.154885\tvalid_0's l2: 0.0239893\n[4900]\tvalid_0's rmse: 0.154803\tvalid_0's l2: 0.0239641\n[5000]\tvalid_0's rmse: 0.154748\tvalid_0's l2: 0.0239471\n[5100]\tvalid_0's rmse: 0.154679\tvalid_0's l2: 0.0239255\n[5200]\tvalid_0's rmse: 0.154639\tvalid_0's l2: 0.0239131\n[5300]\tvalid_0's rmse: 0.154587\tvalid_0's l2: 0.023897\n[5400]\tvalid_0's rmse: 0.154515\tvalid_0's l2: 0.023875\n[5500]\tvalid_0's rmse: 0.154457\tvalid_0's l2: 0.0238568\n[5600]\tvalid_0's rmse: 0.154414\tvalid_0's l2: 0.0238438\n[5700]\tvalid_0's rmse: 0.154364\tvalid_0's l2: 0.0238281\n[5800]\tvalid_0's rmse: 0.154317\tvalid_0's l2: 0.0238137\n[5900]\tvalid_0's rmse: 0.15428\tvalid_0's l2: 0.0238024\n[6000]\tvalid_0's rmse: 0.154245\tvalid_0's l2: 0.0237916\n[6100]\tvalid_0's rmse: 0.154158\tvalid_0's l2: 0.0237648\n[6200]\tvalid_0's rmse: 0.154142\tvalid_0's l2: 0.0237597\n[6300]\tvalid_0's rmse: 0.154124\tvalid_0's l2: 0.0237541\n[6400]\tvalid_0's rmse: 0.15414\tvalid_0's l2: 0.0237592\nEarly stopping, best iteration is:\n[6246]\tvalid_0's rmse: 0.154095\tvalid_0's l2: 0.0237453\n\n Fold 0.1540951479071215\n\n->-> Fold ran for 1 minutes 5 seconds\n\n---- Fold 6 -----\n\n900 900\nTraining until validation scores don't improve for 200 rounds\n[100]\tvalid_0's rmse: 0.318371\tvalid_0's l2: 0.10136\n[200]\tvalid_0's rmse: 0.24572\tvalid_0's l2: 0.0603782\n[300]\tvalid_0's rmse: 0.217815\tvalid_0's l2: 0.0474435\n[400]\tvalid_0's rmse: 0.20321\tvalid_0's l2: 0.0412945\n[500]\tvalid_0's rmse: 0.19449\tvalid_0's l2: 0.0378264\n[600]\tvalid_0's rmse: 0.188721\tvalid_0's l2: 0.0356157\n[700]\tvalid_0's rmse: 0.184876\tvalid_0's l2: 0.0341792\n[800]\tvalid_0's rmse: 0.182119\tvalid_0's l2: 0.0331674\n[900]\tvalid_0's rmse: 0.179814\tvalid_0's l2: 0.0323331\n[1000]\tvalid_0's rmse: 0.178099\tvalid_0's l2: 0.0317192\n[1100]\tvalid_0's rmse: 0.176512\tvalid_0's l2: 0.0311566\n[1200]\tvalid_0's rmse: 0.175255\tvalid_0's l2: 0.0307144\n[1300]\tvalid_0's rmse: 0.174208\tvalid_0's l2: 0.0303486\n[1400]\tvalid_0's rmse: 0.173233\tvalid_0's l2: 0.0300095\n[1500]\tvalid_0's rmse: 0.172286\tvalid_0's l2: 0.0296825\n[1600]\tvalid_0's rmse: 0.171461\tvalid_0's l2: 0.0293989\n[1700]\tvalid_0's rmse: 0.170766\tvalid_0's l2: 0.029161\n[1800]\tvalid_0's rmse: 0.170167\tvalid_0's l2: 0.0289569\n[1900]\tvalid_0's rmse: 0.169581\tvalid_0's l2: 0.0287578\n[2000]\tvalid_0's rmse: 0.169088\tvalid_0's l2: 0.0285908\n[2100]\tvalid_0's rmse: 0.168571\tvalid_0's l2: 0.0284163\n[2200]\tvalid_0's rmse: 0.168141\tvalid_0's l2: 0.0282715\n[2300]\tvalid_0's rmse: 0.167745\tvalid_0's l2: 0.0281384\n[2400]\tvalid_0's rmse: 0.167397\tvalid_0's l2: 0.0280219\n[2500]\tvalid_0's rmse: 0.167015\tvalid_0's l2: 0.027894\n[2600]\tvalid_0's rmse: 0.166738\tvalid_0's l2: 0.0278017\n[2700]\tvalid_0's rmse: 0.16646\tvalid_0's l2: 0.0277091\n[2800]\tvalid_0's rmse: 0.16616\tvalid_0's l2: 0.0276092\n[2900]\tvalid_0's rmse: 0.165901\tvalid_0's l2: 0.0275231\n[3000]\tvalid_0's rmse: 0.165625\tvalid_0's l2: 0.0274317\n[3100]\tvalid_0's rmse: 0.165393\tvalid_0's l2: 0.0273549\n[3200]\tvalid_0's rmse: 0.165138\tvalid_0's l2: 0.0272706\n[3300]\tvalid_0's rmse: 0.164916\tvalid_0's l2: 0.0271973\n[3400]\tvalid_0's rmse: 0.164745\tvalid_0's l2: 0.027141\n[3500]\tvalid_0's rmse: 0.164584\tvalid_0's l2: 0.0270878\n[3600]\tvalid_0's rmse: 0.164405\tvalid_0's l2: 0.0270292\n[3700]\tvalid_0's rmse: 0.164208\tvalid_0's l2: 0.0269643\n[3800]\tvalid_0's rmse: 0.163978\tvalid_0's l2: 0.0268888\n[3900]\tvalid_0's rmse: 0.163722\tvalid_0's l2: 0.0268049\n[4000]\tvalid_0's rmse: 0.163507\tvalid_0's l2: 0.0267344\n[4100]\tvalid_0's rmse: 0.163389\tvalid_0's l2: 0.0266961\n[4200]\tvalid_0's rmse: 0.163252\tvalid_0's l2: 0.0266513\n[4300]\tvalid_0's rmse: 0.163115\tvalid_0's l2: 0.0266065\n[4400]\tvalid_0's rmse: 0.162976\tvalid_0's l2: 0.0265613\n[4500]\tvalid_0's rmse: 0.162826\tvalid_0's l2: 0.0265124\n[4600]\tvalid_0's rmse: 0.162708\tvalid_0's l2: 0.0264738\n[4700]\tvalid_0's rmse: 0.162623\tvalid_0's l2: 0.0264463\n[4800]\tvalid_0's rmse: 0.162508\tvalid_0's l2: 0.0264088\n[4900]\tvalid_0's rmse: 0.162409\tvalid_0's l2: 0.0263767\n[5000]\tvalid_0's rmse: 0.162293\tvalid_0's l2: 0.0263389\n[5100]\tvalid_0's rmse: 0.162205\tvalid_0's l2: 0.0263105\n[5200]\tvalid_0's rmse: 0.162117\tvalid_0's l2: 0.0262821\n[5300]\tvalid_0's rmse: 0.162021\tvalid_0's l2: 0.0262509\n[5400]\tvalid_0's rmse: 0.161958\tvalid_0's l2: 0.0262306\n[5500]\tvalid_0's rmse: 0.161869\tvalid_0's l2: 0.0262014\n[5600]\tvalid_0's rmse: 0.161791\tvalid_0's l2: 0.0261763\n[5700]\tvalid_0's rmse: 0.161729\tvalid_0's l2: 0.0261564\n[5800]\tvalid_0's rmse: 0.161676\tvalid_0's l2: 0.026139\n[5900]\tvalid_0's rmse: 0.161616\tvalid_0's l2: 0.0261198\n[6000]\tvalid_0's rmse: 0.161526\tvalid_0's l2: 0.0260908\n[6100]\tvalid_0's rmse: 0.161484\tvalid_0's l2: 0.0260772\n[6200]\tvalid_0's rmse: 0.16143\tvalid_0's l2: 0.0260596\n[6300]\tvalid_0's rmse: 0.16139\tvalid_0's l2: 0.0260467\n[6400]\tvalid_0's rmse: 0.161343\tvalid_0's l2: 0.0260316\n[6500]\tvalid_0's rmse: 0.161263\tvalid_0's l2: 0.0260058\n[6600]\tvalid_0's rmse: 0.161229\tvalid_0's l2: 0.0259949\n[6700]\tvalid_0's rmse: 0.161217\tvalid_0's l2: 0.025991\n[6800]\tvalid_0's rmse: 0.161204\tvalid_0's l2: 0.0259867\n[6900]\tvalid_0's rmse: 0.161175\tvalid_0's l2: 0.0259775\n[7000]\tvalid_0's rmse: 0.161159\tvalid_0's l2: 0.0259724\n[7100]\tvalid_0's rmse: 0.161142\tvalid_0's l2: 0.0259668\n[7200]\tvalid_0's rmse: 0.161095\tvalid_0's l2: 0.0259517\n[7300]\tvalid_0's rmse: 0.161077\tvalid_0's l2: 0.0259459\n[7400]\tvalid_0's rmse: 0.161045\tvalid_0's l2: 0.0259355\n[7500]\tvalid_0's rmse: 0.161033\tvalid_0's l2: 0.0259317\n[7600]\tvalid_0's rmse: 0.161021\tvalid_0's l2: 0.0259279\n[7700]\tvalid_0's rmse: 0.160984\tvalid_0's l2: 0.0259159\n[7800]\tvalid_0's rmse: 0.160985\tvalid_0's l2: 0.0259161\n[7900]\tvalid_0's rmse: 0.160983\tvalid_0's l2: 0.0259154\n[8000]\tvalid_0's rmse: 0.160959\tvalid_0's l2: 0.025908\n[8100]\tvalid_0's rmse: 0.160965\tvalid_0's l2: 0.0259098\n[8200]\tvalid_0's rmse: 0.160957\tvalid_0's l2: 0.0259073\n[8300]\tvalid_0's rmse: 0.160944\tvalid_0's l2: 0.025903\n[8400]\tvalid_0's rmse: 0.1609\tvalid_0's l2: 0.0258889\n[8500]\tvalid_0's rmse: 0.160891\tvalid_0's l2: 0.025886\n[8600]\tvalid_0's rmse: 0.160885\tvalid_0's l2: 0.025884\n[8700]\tvalid_0's rmse: 0.160878\tvalid_0's l2: 0.0258816\n[8800]\tvalid_0's rmse: 0.160885\tvalid_0's l2: 0.0258839\n[8900]\tvalid_0's rmse: 0.160903\tvalid_0's l2: 0.0258898\nEarly stopping, best iteration is:\n[8764]\tvalid_0's rmse: 0.160863\tvalid_0's l2: 0.0258769\n\n Fold 0.16086308451139886\n\n->-> Fold ran for 1 minutes 31 seconds\n\n---- Fold 7 -----\n\n900 900\nTraining until validation scores don't improve for 200 rounds\n[100]\tvalid_0's rmse: 0.315264\tvalid_0's l2: 0.0993915\n[200]\tvalid_0's rmse: 0.240362\tvalid_0's l2: 0.0577738\n[300]\tvalid_0's rmse: 0.210729\tvalid_0's l2: 0.0444067\n[400]\tvalid_0's rmse: 0.195427\tvalid_0's l2: 0.0381919\n[500]\tvalid_0's rmse: 0.186452\tvalid_0's l2: 0.0347642\n[600]\tvalid_0's rmse: 0.180832\tvalid_0's l2: 0.0327003\n[700]\tvalid_0's rmse: 0.177071\tvalid_0's l2: 0.0313541\n[800]\tvalid_0's rmse: 0.1743\tvalid_0's l2: 0.0303804\n[900]\tvalid_0's rmse: 0.171987\tvalid_0's l2: 0.0295794\n[1000]\tvalid_0's rmse: 0.170139\tvalid_0's l2: 0.0289472\n[1100]\tvalid_0's rmse: 0.168587\tvalid_0's l2: 0.0284216\n[1200]\tvalid_0's rmse: 0.167281\tvalid_0's l2: 0.0279828\n[1300]\tvalid_0's rmse: 0.16622\tvalid_0's l2: 0.0276292\n[1400]\tvalid_0's rmse: 0.165295\tvalid_0's l2: 0.0273224\n[1500]\tvalid_0's rmse: 0.164497\tvalid_0's l2: 0.0270593\n[1600]\tvalid_0's rmse: 0.163793\tvalid_0's l2: 0.0268282\n[1700]\tvalid_0's rmse: 0.163135\tvalid_0's l2: 0.0266132\n[1800]\tvalid_0's rmse: 0.162593\tvalid_0's l2: 0.0264366\n[1900]\tvalid_0's rmse: 0.162115\tvalid_0's l2: 0.0262814\n[2000]\tvalid_0's rmse: 0.161693\tvalid_0's l2: 0.0261445\n[2100]\tvalid_0's rmse: 0.161291\tvalid_0's l2: 0.0260149\n[2200]\tvalid_0's rmse: 0.16091\tvalid_0's l2: 0.025892\n[2300]\tvalid_0's rmse: 0.160635\tvalid_0's l2: 0.0258035\n[2400]\tvalid_0's rmse: 0.160296\tvalid_0's l2: 0.0256949\n[2500]\tvalid_0's rmse: 0.160033\tvalid_0's l2: 0.0256104\n[2600]\tvalid_0's rmse: 0.159753\tvalid_0's l2: 0.025521\n[2700]\tvalid_0's rmse: 0.15951\tvalid_0's l2: 0.0254434\n[2800]\tvalid_0's rmse: 0.159223\tvalid_0's l2: 0.025352\n[2900]\tvalid_0's rmse: 0.159015\tvalid_0's l2: 0.0252857\n[3000]\tvalid_0's rmse: 0.158843\tvalid_0's l2: 0.025231\n[3100]\tvalid_0's rmse: 0.158643\tvalid_0's l2: 0.0251677\n[3200]\tvalid_0's rmse: 0.158463\tvalid_0's l2: 0.0251106\n[3300]\tvalid_0's rmse: 0.158349\tvalid_0's l2: 0.0250745\n[3400]\tvalid_0's rmse: 0.158226\tvalid_0's l2: 0.0250354\n[3500]\tvalid_0's rmse: 0.158084\tvalid_0's l2: 0.0249907\n[3600]\tvalid_0's rmse: 0.157933\tvalid_0's l2: 0.0249428\n[3700]\tvalid_0's rmse: 0.157794\tvalid_0's l2: 0.024899\n[3800]\tvalid_0's rmse: 0.15762\tvalid_0's l2: 0.0248441\n[3900]\tvalid_0's rmse: 0.157463\tvalid_0's l2: 0.0247945\n[4000]\tvalid_0's rmse: 0.157326\tvalid_0's l2: 0.0247514\n[4100]\tvalid_0's rmse: 0.157207\tvalid_0's l2: 0.0247142\n[4200]\tvalid_0's rmse: 0.157079\tvalid_0's l2: 0.0246737\n[4300]\tvalid_0's rmse: 0.156975\tvalid_0's l2: 0.0246412\n[4400]\tvalid_0's rmse: 0.156871\tvalid_0's l2: 0.0246084\n[4500]\tvalid_0's rmse: 0.15679\tvalid_0's l2: 0.024583\n[4600]\tvalid_0's rmse: 0.15671\tvalid_0's l2: 0.0245581\n[4700]\tvalid_0's rmse: 0.156595\tvalid_0's l2: 0.024522\n[4800]\tvalid_0's rmse: 0.156531\tvalid_0's l2: 0.0245018\n[4900]\tvalid_0's rmse: 0.156477\tvalid_0's l2: 0.024485\n[5000]\tvalid_0's rmse: 0.156431\tvalid_0's l2: 0.0244705\n[5100]\tvalid_0's rmse: 0.156361\tvalid_0's l2: 0.0244488\n[5200]\tvalid_0's rmse: 0.156314\tvalid_0's l2: 0.0244342\n[5300]\tvalid_0's rmse: 0.156236\tvalid_0's l2: 0.0244095\n[5400]\tvalid_0's rmse: 0.156172\tvalid_0's l2: 0.0243897\n[5500]\tvalid_0's rmse: 0.156105\tvalid_0's l2: 0.0243687\n[5600]\tvalid_0's rmse: 0.156064\tvalid_0's l2: 0.024356\n[5700]\tvalid_0's rmse: 0.155999\tvalid_0's l2: 0.0243356\n[5800]\tvalid_0's rmse: 0.155973\tvalid_0's l2: 0.0243276\n[5900]\tvalid_0's rmse: 0.155965\tvalid_0's l2: 0.0243251\n[6000]\tvalid_0's rmse: 0.155941\tvalid_0's l2: 0.0243177\n[6100]\tvalid_0's rmse: 0.155921\tvalid_0's l2: 0.0243113\n[6200]\tvalid_0's rmse: 0.155894\tvalid_0's l2: 0.0243029\n[6300]\tvalid_0's rmse: 0.155857\tvalid_0's l2: 0.0242913\n[6400]\tvalid_0's rmse: 0.155834\tvalid_0's l2: 0.0242843\n[6500]\tvalid_0's rmse: 0.155817\tvalid_0's l2: 0.024279\n[6600]\tvalid_0's rmse: 0.155808\tvalid_0's l2: 0.0242763\n[6700]\tvalid_0's rmse: 0.155795\tvalid_0's l2: 0.0242721\n[6800]\tvalid_0's rmse: 0.15577\tvalid_0's l2: 0.0242642\n[6900]\tvalid_0's rmse: 0.155786\tvalid_0's l2: 0.0242692\n[7000]\tvalid_0's rmse: 0.155769\tvalid_0's l2: 0.0242639\n[7100]\tvalid_0's rmse: 0.155736\tvalid_0's l2: 0.0242537\n[7200]\tvalid_0's rmse: 0.155717\tvalid_0's l2: 0.0242477\n[7300]\tvalid_0's rmse: 0.155697\tvalid_0's l2: 0.0242415\n[7400]\tvalid_0's rmse: 0.155673\tvalid_0's l2: 0.024234\n[7500]\tvalid_0's rmse: 0.155665\tvalid_0's l2: 0.0242315\n[7600]\tvalid_0's rmse: 0.155638\tvalid_0's l2: 0.0242232\n[7700]\tvalid_0's rmse: 0.155608\tvalid_0's l2: 0.0242137\n[7800]\tvalid_0's rmse: 0.155588\tvalid_0's l2: 0.0242075\n[7900]\tvalid_0's rmse: 0.155589\tvalid_0's l2: 0.0242078\n[8000]\tvalid_0's rmse: 0.155604\tvalid_0's l2: 0.0242126\nEarly stopping, best iteration is:\n[7878]\tvalid_0's rmse: 0.155578\tvalid_0's l2: 0.0242046\n\n Fold 0.15557837913897227\n\n->-> Fold ran for 1 minutes 25 seconds\n\n---- Fold 8 -----\n\n900 900\nTraining until validation scores don't improve for 200 rounds\n[100]\tvalid_0's rmse: 0.315284\tvalid_0's l2: 0.0994043\n[200]\tvalid_0's rmse: 0.242969\tvalid_0's l2: 0.0590341\n[300]\tvalid_0's rmse: 0.217294\tvalid_0's l2: 0.0472169\n[400]\tvalid_0's rmse: 0.205312\tvalid_0's l2: 0.0421532\n[500]\tvalid_0's rmse: 0.198117\tvalid_0's l2: 0.0392502\n[600]\tvalid_0's rmse: 0.193312\tvalid_0's l2: 0.0373695\n[700]\tvalid_0's rmse: 0.190005\tvalid_0's l2: 0.0361021\n[800]\tvalid_0's rmse: 0.187456\tvalid_0's l2: 0.0351397\n[900]\tvalid_0's rmse: 0.185196\tvalid_0's l2: 0.0342974\n[1000]\tvalid_0's rmse: 0.183391\tvalid_0's l2: 0.0336323\n[1100]\tvalid_0's rmse: 0.181746\tvalid_0's l2: 0.0330317\n[1200]\tvalid_0's rmse: 0.180487\tvalid_0's l2: 0.0325756\n[1300]\tvalid_0's rmse: 0.179289\tvalid_0's l2: 0.0321446\n[1400]\tvalid_0's rmse: 0.178262\tvalid_0's l2: 0.0317774\n[1500]\tvalid_0's rmse: 0.177337\tvalid_0's l2: 0.0314485\n[1600]\tvalid_0's rmse: 0.176524\tvalid_0's l2: 0.0311608\n[1700]\tvalid_0's rmse: 0.175815\tvalid_0's l2: 0.0309109\n[1800]\tvalid_0's rmse: 0.175188\tvalid_0's l2: 0.0306907\n[1900]\tvalid_0's rmse: 0.174646\tvalid_0's l2: 0.0305013\n[2000]\tvalid_0's rmse: 0.174132\tvalid_0's l2: 0.0303221\n[2100]\tvalid_0's rmse: 0.17367\tvalid_0's l2: 0.0301611\n[2200]\tvalid_0's rmse: 0.173289\tvalid_0's l2: 0.0300292\n[2300]\tvalid_0's rmse: 0.172971\tvalid_0's l2: 0.029919\n[2400]\tvalid_0's rmse: 0.17264\tvalid_0's l2: 0.0298047\n[2500]\tvalid_0's rmse: 0.172321\tvalid_0's l2: 0.0296944\n[2600]\tvalid_0's rmse: 0.172008\tvalid_0's l2: 0.0295868\n[2700]\tvalid_0's rmse: 0.171724\tvalid_0's l2: 0.0294893\n[2800]\tvalid_0's rmse: 0.171439\tvalid_0's l2: 0.0293913\n[2900]\tvalid_0's rmse: 0.17123\tvalid_0's l2: 0.0293197\n[3000]\tvalid_0's rmse: 0.171002\tvalid_0's l2: 0.0292418\n[3100]\tvalid_0's rmse: 0.17081\tvalid_0's l2: 0.0291761\n[3200]\tvalid_0's rmse: 0.170619\tvalid_0's l2: 0.0291109\n[3300]\tvalid_0's rmse: 0.170435\tvalid_0's l2: 0.029048\n[3400]\tvalid_0's rmse: 0.170256\tvalid_0's l2: 0.0289872\n[3500]\tvalid_0's rmse: 0.170118\tvalid_0's l2: 0.02894\n[3600]\tvalid_0's rmse: 0.169987\tvalid_0's l2: 0.0288954\n[3700]\tvalid_0's rmse: 0.169838\tvalid_0's l2: 0.0288451\n[3800]\tvalid_0's rmse: 0.169749\tvalid_0's l2: 0.0288148\n[3900]\tvalid_0's rmse: 0.169609\tvalid_0's l2: 0.0287671\n[4000]\tvalid_0's rmse: 0.169457\tvalid_0's l2: 0.0287157\n[4100]\tvalid_0's rmse: 0.169377\tvalid_0's l2: 0.0286887\n[4200]\tvalid_0's rmse: 0.169272\tvalid_0's l2: 0.0286529\n[4300]\tvalid_0's rmse: 0.16919\tvalid_0's l2: 0.0286252\n[4400]\tvalid_0's rmse: 0.169061\tvalid_0's l2: 0.0285817\n[4500]\tvalid_0's rmse: 0.168951\tvalid_0's l2: 0.0285444\n[4600]\tvalid_0's rmse: 0.168887\tvalid_0's l2: 0.0285229\n[4700]\tvalid_0's rmse: 0.168817\tvalid_0's l2: 0.0284993\n[4800]\tvalid_0's rmse: 0.16874\tvalid_0's l2: 0.0284733\n[4900]\tvalid_0's rmse: 0.16866\tvalid_0's l2: 0.028446\n[5000]\tvalid_0's rmse: 0.16863\tvalid_0's l2: 0.028436\n[5100]\tvalid_0's rmse: 0.168582\tvalid_0's l2: 0.02842\n[5200]\tvalid_0's rmse: 0.168515\tvalid_0's l2: 0.0283972\n[5300]\tvalid_0's rmse: 0.168456\tvalid_0's l2: 0.0283773\n[5400]\tvalid_0's rmse: 0.168426\tvalid_0's l2: 0.0283674\n[5500]\tvalid_0's rmse: 0.168372\tvalid_0's l2: 0.028349\n[5600]\tvalid_0's rmse: 0.168337\tvalid_0's l2: 0.0283372\n[5700]\tvalid_0's rmse: 0.16833\tvalid_0's l2: 0.028335\n[5800]\tvalid_0's rmse: 0.168283\tvalid_0's l2: 0.0283192\n[5900]\tvalid_0's rmse: 0.16824\tvalid_0's l2: 0.0283048\n[6000]\tvalid_0's rmse: 0.168222\tvalid_0's l2: 0.0282985\n[6100]\tvalid_0's rmse: 0.168192\tvalid_0's l2: 0.0282886\n[6200]\tvalid_0's rmse: 0.168189\tvalid_0's l2: 0.0282875\n[6300]\tvalid_0's rmse: 0.168232\tvalid_0's l2: 0.0283019\nEarly stopping, best iteration is:\n[6151]\tvalid_0's rmse: 0.168182\tvalid_0's l2: 0.0282853\n\n Fold 0.16818222027556784\n\n->-> Fold ran for 1 minutes 7 seconds\n\n---- Fold 9 -----\n\n900 900\nTraining until validation scores don't improve for 200 rounds\n[100]\tvalid_0's rmse: 0.319957\tvalid_0's l2: 0.102372\n[200]\tvalid_0's rmse: 0.239254\tvalid_0's l2: 0.0572425\n[300]\tvalid_0's rmse: 0.207233\tvalid_0's l2: 0.0429456\n[400]\tvalid_0's rmse: 0.191369\tvalid_0's l2: 0.036622\n[500]\tvalid_0's rmse: 0.182229\tvalid_0's l2: 0.0332073\n[600]\tvalid_0's rmse: 0.176343\tvalid_0's l2: 0.0310968\n[700]\tvalid_0's rmse: 0.172421\tvalid_0's l2: 0.029729\n[800]\tvalid_0's rmse: 0.169399\tvalid_0's l2: 0.028696\n[900]\tvalid_0's rmse: 0.167081\tvalid_0's l2: 0.0279161\n[1000]\tvalid_0's rmse: 0.165135\tvalid_0's l2: 0.0272695\n[1100]\tvalid_0's rmse: 0.163548\tvalid_0's l2: 0.026748\n[1200]\tvalid_0's rmse: 0.162251\tvalid_0's l2: 0.0263254\n[1300]\tvalid_0's rmse: 0.161028\tvalid_0's l2: 0.02593\n[1400]\tvalid_0's rmse: 0.16005\tvalid_0's l2: 0.0256161\n[1500]\tvalid_0's rmse: 0.159262\tvalid_0's l2: 0.0253643\n[1600]\tvalid_0's rmse: 0.158527\tvalid_0's l2: 0.0251308\n[1700]\tvalid_0's rmse: 0.157937\tvalid_0's l2: 0.0249441\n[1800]\tvalid_0's rmse: 0.157391\tvalid_0's l2: 0.0247718\n[1900]\tvalid_0's rmse: 0.156923\tvalid_0's l2: 0.0246248\n[2000]\tvalid_0's rmse: 0.1565\tvalid_0's l2: 0.0244924\n[2100]\tvalid_0's rmse: 0.156112\tvalid_0's l2: 0.024371\n[2200]\tvalid_0's rmse: 0.155724\tvalid_0's l2: 0.0242501\n[2300]\tvalid_0's rmse: 0.155377\tvalid_0's l2: 0.0241419\n[2400]\tvalid_0's rmse: 0.155088\tvalid_0's l2: 0.0240524\n[2500]\tvalid_0's rmse: 0.154768\tvalid_0's l2: 0.0239533\n[2600]\tvalid_0's rmse: 0.154418\tvalid_0's l2: 0.0238449\n[2700]\tvalid_0's rmse: 0.154157\tvalid_0's l2: 0.0237644\n[2800]\tvalid_0's rmse: 0.153879\tvalid_0's l2: 0.0236787\n[2900]\tvalid_0's rmse: 0.153623\tvalid_0's l2: 0.0236\n[3000]\tvalid_0's rmse: 0.153375\tvalid_0's l2: 0.0235238\n[3100]\tvalid_0's rmse: 0.153156\tvalid_0's l2: 0.0234568\n[3200]\tvalid_0's rmse: 0.152906\tvalid_0's l2: 0.0233804\n[3300]\tvalid_0's rmse: 0.152754\tvalid_0's l2: 0.0233338\n[3400]\tvalid_0's rmse: 0.152567\tvalid_0's l2: 0.0232767\n[3500]\tvalid_0's rmse: 0.152345\tvalid_0's l2: 0.023209\n[3600]\tvalid_0's rmse: 0.152155\tvalid_0's l2: 0.023151\n[3700]\tvalid_0's rmse: 0.152003\tvalid_0's l2: 0.0231048\n[3800]\tvalid_0's rmse: 0.151836\tvalid_0's l2: 0.0230542\n[3900]\tvalid_0's rmse: 0.1516\tvalid_0's l2: 0.0229824\n[4000]\tvalid_0's rmse: 0.151389\tvalid_0's l2: 0.0229187\n[4100]\tvalid_0's rmse: 0.151233\tvalid_0's l2: 0.0228714\n[4200]\tvalid_0's rmse: 0.151119\tvalid_0's l2: 0.0228369\n[4300]\tvalid_0's rmse: 0.150994\tvalid_0's l2: 0.0227991\n[4400]\tvalid_0's rmse: 0.150886\tvalid_0's l2: 0.0227666\n[4500]\tvalid_0's rmse: 0.150768\tvalid_0's l2: 0.0227309\n[4600]\tvalid_0's rmse: 0.150647\tvalid_0's l2: 0.0226945\n[4700]\tvalid_0's rmse: 0.150545\tvalid_0's l2: 0.0226639\n[4800]\tvalid_0's rmse: 0.150467\tvalid_0's l2: 0.0226404\n[4900]\tvalid_0's rmse: 0.15032\tvalid_0's l2: 0.0225961\n[5000]\tvalid_0's rmse: 0.150232\tvalid_0's l2: 0.0225696\n[5100]\tvalid_0's rmse: 0.150141\tvalid_0's l2: 0.0225422\n[5200]\tvalid_0's rmse: 0.150062\tvalid_0's l2: 0.0225186\n[5300]\tvalid_0's rmse: 0.149957\tvalid_0's l2: 0.0224871\n[5400]\tvalid_0's rmse: 0.149869\tvalid_0's l2: 0.0224608\n[5500]\tvalid_0's rmse: 0.149796\tvalid_0's l2: 0.0224387\n[5600]\tvalid_0's rmse: 0.149729\tvalid_0's l2: 0.0224186\n[5700]\tvalid_0's rmse: 0.149658\tvalid_0's l2: 0.0223976\n[5800]\tvalid_0's rmse: 0.14958\tvalid_0's l2: 0.0223741\n[5900]\tvalid_0's rmse: 0.149527\tvalid_0's l2: 0.0223584\n[6000]\tvalid_0's rmse: 0.149454\tvalid_0's l2: 0.0223366\n[6100]\tvalid_0's rmse: 0.149375\tvalid_0's l2: 0.022313\n[6200]\tvalid_0's rmse: 0.149304\tvalid_0's l2: 0.0222918\n[6300]\tvalid_0's rmse: 0.149296\tvalid_0's l2: 0.0222892\n[6400]\tvalid_0's rmse: 0.149268\tvalid_0's l2: 0.0222811\n[6500]\tvalid_0's rmse: 0.149233\tvalid_0's l2: 0.0222706\n[6600]\tvalid_0's rmse: 0.149215\tvalid_0's l2: 0.0222651\n[6700]\tvalid_0's rmse: 0.149214\tvalid_0's l2: 0.0222649\n[6800]\tvalid_0's rmse: 0.149205\tvalid_0's l2: 0.0222623\n[6900]\tvalid_0's rmse: 0.149206\tvalid_0's l2: 0.0222625\n[7000]\tvalid_0's rmse: 0.149258\tvalid_0's l2: 0.0222779\nEarly stopping, best iteration is:\n[6857]\tvalid_0's rmse: 0.149202\tvalid_0's l2: 0.0222613\n\n Fold 0.14920217447338022\n\n->-> Fold ran for 1 minutes 10 seconds\n\ntrain score (validation fold) : 0.15757998258536435\n\n->-> Total  time: 11 minutes \n","output_type":"stream"}]},{"cell_type":"code","source":"\ntraining_start_time = time.time()\n\nfolds = StratifiedKFold(n_splits = 10)\ntrain_prediction = np.zeros(len(train))\ntest_prediction = np.zeros(len(test))\n\n\nfor fold_, (train_idx, val_idx) in enumerate(folds.split(train, pd.qcut(target, 10, labels=False))):\n    \n    print(f'\\n---- Fold {fold_} -----\\n')\n    \n    fold_start_time = time.time()\n    \n    X_train, y_train = train.iloc[train_idx][features1], target.iloc[train_idx]\n    X_val, y_val = train.iloc[val_idx][features1], target.iloc[val_idx]\n    X_test = test[features1]\n    print(X_train.shape[1], X_val.shape[1])\n\n    xgb = XGBRegressor(n_estimators=1000, max_depth=12, learning_rate=0.05, colsample_bytree=0.45)\n\n    \n    _ = xgb.fit(X_train, np.log(y_train), eval_set = [(X_val, np.log(y_val))], verbose=100, early_stopping_rounds=200, eval_metric='rmse')\n\n    train_prediction[val_idx] = np.exp(xgb.predict(X_val))\n    current_test_pred = np.exp(xgb.predict(X_test))\n    test_prediction += np.exp(xgb.predict(X_test))/10\n    \n    \n    print(f'\\n Fold {rmse(np.log(y_val), np.log(train_prediction[val_idx]))}')\n    \n    fold_end_time = time.time()\n    total_fold_time = int(fold_end_time - fold_start_time)\n    \n    print(f\"\\n->-> Fold ran for {(total_fold_time)//60} minutes {(total_fold_time)%60} seconds\")\n    \n\nprint(f'\\ntrain score (validation fold) : {rmse(np.log(target), np.log(train_prediction))}')\ntraining_end_time = time.time()\ntotal_training_time = int(training_end_time - training_start_time)\n\nprint(f'\\n->-> Total  time: {(total_training_time)//60} minutes ')\n\nxgb_prediction = test_prediction","metadata":{"trusted":true},"execution_count":46,"outputs":[{"name":"stdout","text":"\n---- Fold 0 -----\n\n900 900\n[0]\tvalidation_0-rmse:8.81060\n[100]\tvalidation_0-rmse:0.19627\n[200]\tvalidation_0-rmse:0.17425\n[300]\tvalidation_0-rmse:0.17067\n[400]\tvalidation_0-rmse:0.16942\n[500]\tvalidation_0-rmse:0.16880\n[600]\tvalidation_0-rmse:0.16863\n[700]\tvalidation_0-rmse:0.16862\n[778]\tvalidation_0-rmse:0.16865\n\n Fold 0.16853677726261548\n\n->-> Fold ran for 3 minutes 11 seconds\n\n---- Fold 1 -----\n\n900 900\n[0]\tvalidation_0-rmse:8.81889\n[100]\tvalidation_0-rmse:0.18686\n[200]\tvalidation_0-rmse:0.16454\n[300]\tvalidation_0-rmse:0.16057\n[400]\tvalidation_0-rmse:0.15881\n[500]\tvalidation_0-rmse:0.15783\n[600]\tvalidation_0-rmse:0.15724\n[700]\tvalidation_0-rmse:0.15716\n[800]\tvalidation_0-rmse:0.15715\n[900]\tvalidation_0-rmse:0.15721\n[953]\tvalidation_0-rmse:0.15733\n\n Fold 0.15707460135577\n\n->-> Fold ran for 3 minutes 49 seconds\n\n---- Fold 2 -----\n\n900 900\n[0]\tvalidation_0-rmse:8.81494\n[100]\tvalidation_0-rmse:0.17573\n[200]\tvalidation_0-rmse:0.15518\n[300]\tvalidation_0-rmse:0.15201\n[400]\tvalidation_0-rmse:0.15062\n[500]\tvalidation_0-rmse:0.14996\n[600]\tvalidation_0-rmse:0.14990\n[700]\tvalidation_0-rmse:0.14996\n[757]\tvalidation_0-rmse:0.14991\n\n Fold 0.14970499701085135\n\n->-> Fold ran for 3 minutes 2 seconds\n\n---- Fold 3 -----\n\n900 900\n[0]\tvalidation_0-rmse:8.81806\n[100]\tvalidation_0-rmse:0.19497\n[200]\tvalidation_0-rmse:0.17279\n[300]\tvalidation_0-rmse:0.16847\n[400]\tvalidation_0-rmse:0.16655\n[500]\tvalidation_0-rmse:0.16529\n[600]\tvalidation_0-rmse:0.16457\n[700]\tvalidation_0-rmse:0.16429\n[800]\tvalidation_0-rmse:0.16428\n[900]\tvalidation_0-rmse:0.16444\n[963]\tvalidation_0-rmse:0.16447\n\n Fold 0.16418341696228145\n\n->-> Fold ran for 3 minutes 52 seconds\n\n---- Fold 4 -----\n\n900 900\n[0]\tvalidation_0-rmse:8.82298\n[100]\tvalidation_0-rmse:0.18519\n[200]\tvalidation_0-rmse:0.16398\n[300]\tvalidation_0-rmse:0.15963\n[400]\tvalidation_0-rmse:0.15760\n[500]\tvalidation_0-rmse:0.15667\n[600]\tvalidation_0-rmse:0.15626\n[700]\tvalidation_0-rmse:0.15598\n[800]\tvalidation_0-rmse:0.15610\n[900]\tvalidation_0-rmse:0.15631\n[906]\tvalidation_0-rmse:0.15634\n\n Fold 0.1559574376566752\n\n->-> Fold ran for 3 minutes 38 seconds\n\n---- Fold 5 -----\n\n900 900\n[0]\tvalidation_0-rmse:8.81401\n[100]\tvalidation_0-rmse:0.17931\n[200]\tvalidation_0-rmse:0.16046\n[300]\tvalidation_0-rmse:0.15639\n[400]\tvalidation_0-rmse:0.15455\n[500]\tvalidation_0-rmse:0.15313\n[600]\tvalidation_0-rmse:0.15232\n[700]\tvalidation_0-rmse:0.15201\n[800]\tvalidation_0-rmse:0.15191\n[900]\tvalidation_0-rmse:0.15198\n[999]\tvalidation_0-rmse:0.15215\n\n Fold 0.15179733582276148\n\n->-> Fold ran for 4 minutes 2 seconds\n\n---- Fold 6 -----\n\n900 900\n[0]\tvalidation_0-rmse:8.81159\n[100]\tvalidation_0-rmse:0.19557\n[200]\tvalidation_0-rmse:0.17162\n[300]\tvalidation_0-rmse:0.16778\n[400]\tvalidation_0-rmse:0.16558\n[500]\tvalidation_0-rmse:0.16445\n[600]\tvalidation_0-rmse:0.16408\n[700]\tvalidation_0-rmse:0.16361\n[800]\tvalidation_0-rmse:0.16347\n[900]\tvalidation_0-rmse:0.16364\n[959]\tvalidation_0-rmse:0.16356\n\n Fold 0.16337927478438033\n\n->-> Fold ran for 3 minutes 50 seconds\n\n---- Fold 7 -----\n\n900 900\n[0]\tvalidation_0-rmse:8.81981\n[100]\tvalidation_0-rmse:0.18535\n[200]\tvalidation_0-rmse:0.16224\n[300]\tvalidation_0-rmse:0.15804\n[400]\tvalidation_0-rmse:0.15606\n[500]\tvalidation_0-rmse:0.15480\n[600]\tvalidation_0-rmse:0.15363\n[700]\tvalidation_0-rmse:0.15297\n[800]\tvalidation_0-rmse:0.15296\n[900]\tvalidation_0-rmse:0.15301\n[964]\tvalidation_0-rmse:0.15311\n\n Fold 0.15286314606426832\n\n->-> Fold ran for 3 minutes 51 seconds\n\n---- Fold 8 -----\n\n900 900\n[0]\tvalidation_0-rmse:8.81571\n[100]\tvalidation_0-rmse:0.19644\n[200]\tvalidation_0-rmse:0.17808\n[300]\tvalidation_0-rmse:0.17491\n[400]\tvalidation_0-rmse:0.17336\n[500]\tvalidation_0-rmse:0.17261\n[600]\tvalidation_0-rmse:0.17244\n[700]\tvalidation_0-rmse:0.17248\n[771]\tvalidation_0-rmse:0.17266\n\n Fold 0.17226497331490104\n\n->-> Fold ran for 3 minutes 4 seconds\n\n---- Fold 9 -----\n\n900 900\n[0]\tvalidation_0-rmse:8.81357\n[100]\tvalidation_0-rmse:0.17813\n[200]\tvalidation_0-rmse:0.15774\n[300]\tvalidation_0-rmse:0.15421\n[400]\tvalidation_0-rmse:0.15263\n[500]\tvalidation_0-rmse:0.15146\n[600]\tvalidation_0-rmse:0.15086\n[700]\tvalidation_0-rmse:0.15097\n[800]\tvalidation_0-rmse:0.15112\n[818]\tvalidation_0-rmse:0.15118\n\n Fold 0.15079209551769365\n\n->-> Fold ran for 3 minutes 19 seconds\n\ntrain score (validation fold) : 0.15883432113225168\n\n->-> Total  time: 35 minutes \n","output_type":"stream"}]},{"cell_type":"code","source":"\ntraining_start_time = time.time()\n\nfolds = StratifiedKFold(n_splits = 10)\ntrain_prediction = np.zeros(len(train))\ntest_prediction = np.zeros(len(test))\n\n\nfor fold_, (train_idx, val_idx) in enumerate(folds.split(train, pd.qcut(target, 10, labels=False))):\n    \n    print(f'\\n---- Fold {fold_} -----\\n')\n    \n    fold_start_time = time.time()\n    \n    X_train, y_train = train.iloc[train_idx][features1], target.iloc[train_idx]\n    X_val, y_val = train.iloc[val_idx][features1], target.iloc[val_idx]\n    X_test = test[features1]\n    print(X_train.shape[1], X_val.shape[1])\n\n    cat = CatBoostRegressor(n_estimators=2000, learning_rate=0.05, max_depth=9, rsm=0.5)\n\n    \n    _ = cat.fit(X_train, np.log(y_train), eval_set = [(X_val, np.log(y_val))], verbose=100, early_stopping_rounds=200)\n\n    train_prediction[val_idx] = np.exp(cat.predict(X_val))\n    current_test_pred = np.exp(cat.predict(X_test))\n    test_prediction += np.exp(cat.predict(X_test))/10\n    \n    \n    print(f'\\n Fold {rmse(np.log(y_val), np.log(train_prediction[val_idx]))}')\n    \n    fold_end_time = time.time()\n    total_fold_time = int(fold_end_time - fold_start_time)\n    \n    print(f\"\\n->-> Fold ran for {(total_fold_time)//60} minutes {(total_fold_time)%60} seconds\")\n    \n\nprint(f'\\ntrain score (validation fold) : {rmse(np.log(target), np.log(train_prediction))}')\ntraining_end_time = time.time()\ntotal_training_time = int(training_end_time - training_start_time)\n\nprint(f'\\n->-> Total  time: {(total_training_time)//60} minutes ')\n\ncat_prediction = test_prediction","metadata":{"trusted":true},"execution_count":49,"outputs":[{"name":"stdout","text":"\n---- Fold 0 -----\n\n900 900\n0:\tlearn: 0.5253567\ttest: 0.5258151\tbest: 0.5258151 (0)\ttotal: 106ms\tremaining: 3m 32s\n100:\tlearn: 0.2145369\ttest: 0.2265797\tbest: 0.2265797 (100)\ttotal: 3.4s\tremaining: 1m 4s\n200:\tlearn: 0.1927714\ttest: 0.2086190\tbest: 0.2086190 (200)\ttotal: 6.58s\tremaining: 58.9s\n300:\tlearn: 0.1776580\ttest: 0.1970527\tbest: 0.1970527 (300)\ttotal: 9.64s\tremaining: 54.4s\n400:\tlearn: 0.1672044\ttest: 0.1898582\tbest: 0.1898582 (400)\ttotal: 12.7s\tremaining: 50.7s\n500:\tlearn: 0.1591165\ttest: 0.1845939\tbest: 0.1845939 (500)\ttotal: 15.8s\tremaining: 47.4s\n600:\tlearn: 0.1527201\ttest: 0.1809913\tbest: 0.1809913 (600)\ttotal: 19s\tremaining: 44.3s\n700:\tlearn: 0.1471349\ttest: 0.1782502\tbest: 0.1782502 (700)\ttotal: 22.2s\tremaining: 41.1s\n800:\tlearn: 0.1426138\ttest: 0.1762083\tbest: 0.1762083 (800)\ttotal: 25.2s\tremaining: 37.7s\n900:\tlearn: 0.1385538\ttest: 0.1746820\tbest: 0.1746655 (897)\ttotal: 28.6s\tremaining: 34.9s\n1000:\tlearn: 0.1348338\ttest: 0.1735669\tbest: 0.1735669 (1000)\ttotal: 31.9s\tremaining: 31.8s\n1100:\tlearn: 0.1315153\ttest: 0.1723608\tbest: 0.1723608 (1100)\ttotal: 35s\tremaining: 28.6s\n1200:\tlearn: 0.1285009\ttest: 0.1714875\tbest: 0.1714875 (1200)\ttotal: 38s\tremaining: 25.3s\n1300:\tlearn: 0.1259525\ttest: 0.1706306\tbest: 0.1706183 (1299)\ttotal: 41.2s\tremaining: 22.1s\n1400:\tlearn: 0.1235591\ttest: 0.1699763\tbest: 0.1699763 (1400)\ttotal: 44.2s\tremaining: 18.9s\n1500:\tlearn: 0.1210684\ttest: 0.1693526\tbest: 0.1693526 (1500)\ttotal: 47.3s\tremaining: 15.7s\n1600:\tlearn: 0.1190449\ttest: 0.1688226\tbest: 0.1688222 (1598)\ttotal: 50.4s\tremaining: 12.6s\n1700:\tlearn: 0.1170574\ttest: 0.1684940\tbest: 0.1684939 (1699)\ttotal: 53.5s\tremaining: 9.4s\n1800:\tlearn: 0.1150085\ttest: 0.1680807\tbest: 0.1680807 (1800)\ttotal: 56.6s\tremaining: 6.25s\n1900:\tlearn: 0.1132986\ttest: 0.1676928\tbest: 0.1676904 (1899)\ttotal: 59.9s\tremaining: 3.12s\n1999:\tlearn: 0.1115645\ttest: 0.1674646\tbest: 0.1674158 (1988)\ttotal: 1m 3s\tremaining: 0us\n\nbestTest = 0.1674158164\nbestIteration = 1988\n\nShrink model to first 1989 iterations.\n\n Fold 0.16741580958866084\n\n->-> Fold ran for 1 minutes 4 seconds\n\n---- Fold 1 -----\n\n900 900\n0:\tlearn: 0.5250608\ttest: 0.5156106\tbest: 0.5156106 (0)\ttotal: 32.6ms\tremaining: 1m 5s\n100:\tlearn: 0.2160452\ttest: 0.2224418\tbest: 0.2224418 (100)\ttotal: 2.93s\tremaining: 55.1s\n200:\tlearn: 0.1941899\ttest: 0.2032992\tbest: 0.2032992 (200)\ttotal: 5.67s\tremaining: 50.7s\n300:\tlearn: 0.1791362\ttest: 0.1915707\tbest: 0.1915707 (300)\ttotal: 8.46s\tremaining: 47.8s\n400:\tlearn: 0.1679517\ttest: 0.1828761\tbest: 0.1828761 (400)\ttotal: 11.3s\tremaining: 45s\n500:\tlearn: 0.1598570\ttest: 0.1773759\tbest: 0.1773759 (500)\ttotal: 14.1s\tremaining: 42.2s\n600:\tlearn: 0.1533801\ttest: 0.1737834\tbest: 0.1737834 (600)\ttotal: 16.9s\tremaining: 39.3s\n700:\tlearn: 0.1479928\ttest: 0.1708447\tbest: 0.1708447 (700)\ttotal: 19.7s\tremaining: 36.6s\n800:\tlearn: 0.1435416\ttest: 0.1685009\tbest: 0.1685009 (800)\ttotal: 22.6s\tremaining: 33.8s\n900:\tlearn: 0.1398318\ttest: 0.1667428\tbest: 0.1667428 (900)\ttotal: 25.3s\tremaining: 30.9s\n1000:\tlearn: 0.1362715\ttest: 0.1650121\tbest: 0.1650109 (999)\ttotal: 28.4s\tremaining: 28.3s\n1100:\tlearn: 0.1331495\ttest: 0.1638926\tbest: 0.1638926 (1100)\ttotal: 31.2s\tremaining: 25.5s\n1200:\tlearn: 0.1301393\ttest: 0.1628208\tbest: 0.1628208 (1200)\ttotal: 34s\tremaining: 22.6s\n1300:\tlearn: 0.1273833\ttest: 0.1618236\tbest: 0.1618236 (1300)\ttotal: 36.9s\tremaining: 19.8s\n1400:\tlearn: 0.1248723\ttest: 0.1609122\tbest: 0.1609122 (1400)\ttotal: 39.7s\tremaining: 17s\n1500:\tlearn: 0.1223904\ttest: 0.1601664\tbest: 0.1601588 (1499)\ttotal: 42.6s\tremaining: 14.2s\n1600:\tlearn: 0.1200405\ttest: 0.1593997\tbest: 0.1593997 (1600)\ttotal: 45.5s\tremaining: 11.3s\n1700:\tlearn: 0.1178445\ttest: 0.1587707\tbest: 0.1587565 (1698)\ttotal: 48.3s\tremaining: 8.49s\n1800:\tlearn: 0.1158111\ttest: 0.1582721\tbest: 0.1582628 (1799)\ttotal: 51.3s\tremaining: 5.67s\n1900:\tlearn: 0.1140363\ttest: 0.1578464\tbest: 0.1578402 (1894)\ttotal: 54.1s\tremaining: 2.82s\n1999:\tlearn: 0.1122849\ttest: 0.1574338\tbest: 0.1574338 (1999)\ttotal: 57s\tremaining: 0us\n\nbestTest = 0.1574337581\nbestIteration = 1999\n\n\n Fold 0.15743376555481567\n\n->-> Fold ran for 0 minutes 58 seconds\n\n---- Fold 2 -----\n\n900 900\n0:\tlearn: 0.5282447\ttest: 0.5170253\tbest: 0.5170253 (0)\ttotal: 34.6ms\tremaining: 1m 9s\n100:\tlearn: 0.2181950\ttest: 0.2074452\tbest: 0.2074452 (100)\ttotal: 3.52s\tremaining: 1m 6s\n200:\tlearn: 0.1955033\ttest: 0.1885938\tbest: 0.1885938 (200)\ttotal: 6.61s\tremaining: 59.2s\n300:\tlearn: 0.1794989\ttest: 0.1773589\tbest: 0.1773589 (300)\ttotal: 9.66s\tremaining: 54.5s\n400:\tlearn: 0.1689184\ttest: 0.1705322\tbest: 0.1705322 (400)\ttotal: 12.8s\tremaining: 51s\n500:\tlearn: 0.1609863\ttest: 0.1664628\tbest: 0.1664628 (500)\ttotal: 15.9s\tremaining: 47.7s\n600:\tlearn: 0.1545243\ttest: 0.1630910\tbest: 0.1630910 (600)\ttotal: 19s\tremaining: 44.3s\n700:\tlearn: 0.1486088\ttest: 0.1603406\tbest: 0.1603406 (700)\ttotal: 22.2s\tremaining: 41.1s\n800:\tlearn: 0.1439143\ttest: 0.1585474\tbest: 0.1585474 (800)\ttotal: 25.3s\tremaining: 37.8s\n900:\tlearn: 0.1397423\ttest: 0.1569502\tbest: 0.1569502 (900)\ttotal: 28.4s\tremaining: 34.6s\n1000:\tlearn: 0.1360682\ttest: 0.1557312\tbest: 0.1557312 (1000)\ttotal: 31.5s\tremaining: 31.4s\n1100:\tlearn: 0.1326456\ttest: 0.1545088\tbest: 0.1544874 (1099)\ttotal: 34.8s\tremaining: 28.4s\n1200:\tlearn: 0.1297355\ttest: 0.1537960\tbest: 0.1537960 (1200)\ttotal: 38.1s\tremaining: 25.3s\n1300:\tlearn: 0.1269966\ttest: 0.1530719\tbest: 0.1530719 (1300)\ttotal: 41.2s\tremaining: 22.1s\n1400:\tlearn: 0.1246018\ttest: 0.1524103\tbest: 0.1524086 (1399)\ttotal: 44.2s\tremaining: 18.9s\n1500:\tlearn: 0.1222219\ttest: 0.1518793\tbest: 0.1518464 (1496)\ttotal: 47.3s\tremaining: 15.7s\n1600:\tlearn: 0.1199765\ttest: 0.1515905\tbest: 0.1515892 (1599)\ttotal: 50.4s\tremaining: 12.6s\n1700:\tlearn: 0.1178559\ttest: 0.1511118\tbest: 0.1511097 (1699)\ttotal: 53.4s\tremaining: 9.39s\n1800:\tlearn: 0.1159503\ttest: 0.1508096\tbest: 0.1508076 (1799)\ttotal: 56.4s\tremaining: 6.23s\n1900:\tlearn: 0.1139987\ttest: 0.1503734\tbest: 0.1503734 (1900)\ttotal: 59.5s\tremaining: 3.1s\n1999:\tlearn: 0.1121712\ttest: 0.1499271\tbest: 0.1499271 (1999)\ttotal: 1m 2s\tremaining: 0us\n\nbestTest = 0.1499271329\nbestIteration = 1999\n\n\n Fold 0.14992714081071942\n\n->-> Fold ran for 1 minutes 3 seconds\n\n---- Fold 3 -----\n\n900 900\n0:\tlearn: 0.5245693\ttest: 0.5252911\tbest: 0.5252911 (0)\ttotal: 32.2ms\tremaining: 1m 4s\n100:\tlearn: 0.2157314\ttest: 0.2266488\tbest: 0.2266488 (100)\ttotal: 3.38s\tremaining: 1m 3s\n200:\tlearn: 0.1931927\ttest: 0.2070569\tbest: 0.2070569 (200)\ttotal: 6.43s\tremaining: 57.6s\n300:\tlearn: 0.1771950\ttest: 0.1945842\tbest: 0.1945842 (300)\ttotal: 9.48s\tremaining: 53.5s\n400:\tlearn: 0.1658791\ttest: 0.1867924\tbest: 0.1867924 (400)\ttotal: 12.5s\tremaining: 49.9s\n500:\tlearn: 0.1580567\ttest: 0.1816407\tbest: 0.1816407 (500)\ttotal: 15.5s\tremaining: 46.5s\n600:\tlearn: 0.1520555\ttest: 0.1777971\tbest: 0.1777971 (600)\ttotal: 18.6s\tremaining: 43.3s\n700:\tlearn: 0.1467069\ttest: 0.1749869\tbest: 0.1749869 (700)\ttotal: 21.6s\tremaining: 40s\n800:\tlearn: 0.1420258\ttest: 0.1726197\tbest: 0.1726197 (800)\ttotal: 24.6s\tremaining: 36.8s\n900:\tlearn: 0.1379957\ttest: 0.1710427\tbest: 0.1710427 (900)\ttotal: 27.6s\tremaining: 33.7s\n1000:\tlearn: 0.1345027\ttest: 0.1694471\tbest: 0.1694471 (1000)\ttotal: 30.8s\tremaining: 30.7s\n1100:\tlearn: 0.1314689\ttest: 0.1680307\tbest: 0.1680249 (1098)\ttotal: 33.9s\tremaining: 27.7s\n1200:\tlearn: 0.1285898\ttest: 0.1669669\tbest: 0.1669669 (1200)\ttotal: 37.2s\tremaining: 24.7s\n1300:\tlearn: 0.1258091\ttest: 0.1659081\tbest: 0.1659081 (1300)\ttotal: 40.3s\tremaining: 21.7s\n1400:\tlearn: 0.1232626\ttest: 0.1651913\tbest: 0.1651913 (1400)\ttotal: 43.3s\tremaining: 18.5s\n1500:\tlearn: 0.1210670\ttest: 0.1646675\tbest: 0.1646675 (1500)\ttotal: 46.3s\tremaining: 15.4s\n1600:\tlearn: 0.1188055\ttest: 0.1640771\tbest: 0.1640771 (1600)\ttotal: 49.4s\tremaining: 12.3s\n1700:\tlearn: 0.1168000\ttest: 0.1636510\tbest: 0.1636510 (1700)\ttotal: 52.5s\tremaining: 9.22s\n1800:\tlearn: 0.1148836\ttest: 0.1631376\tbest: 0.1631263 (1790)\ttotal: 55.5s\tremaining: 6.14s\n1900:\tlearn: 0.1131020\ttest: 0.1627620\tbest: 0.1627615 (1899)\ttotal: 58.6s\tremaining: 3.05s\n1999:\tlearn: 0.1113483\ttest: 0.1624953\tbest: 0.1624953 (1999)\ttotal: 1m 1s\tremaining: 0us\n\nbestTest = 0.1624953154\nbestIteration = 1999\n\n\n Fold 0.16249530836265427\n\n->-> Fold ran for 1 minutes 2 seconds\n\n---- Fold 4 -----\n\n900 900\n0:\tlearn: 0.5255758\ttest: 0.5275286\tbest: 0.5275286 (0)\ttotal: 33.1ms\tremaining: 1m 6s\n100:\tlearn: 0.2161378\ttest: 0.2175824\tbest: 0.2175824 (100)\ttotal: 3.27s\tremaining: 1m 1s\n200:\tlearn: 0.1927436\ttest: 0.1983702\tbest: 0.1983702 (200)\ttotal: 6.74s\tremaining: 1m\n300:\tlearn: 0.1780982\ttest: 0.1873629\tbest: 0.1873629 (300)\ttotal: 10s\tremaining: 56.7s\n400:\tlearn: 0.1679792\ttest: 0.1805461\tbest: 0.1805461 (400)\ttotal: 13.2s\tremaining: 52.8s\n500:\tlearn: 0.1598852\ttest: 0.1753381\tbest: 0.1753381 (500)\ttotal: 16.4s\tremaining: 49.1s\n600:\tlearn: 0.1535423\ttest: 0.1718997\tbest: 0.1718997 (600)\ttotal: 19.5s\tremaining: 45.5s\n700:\tlearn: 0.1483126\ttest: 0.1692638\tbest: 0.1692618 (699)\ttotal: 22.7s\tremaining: 42.1s\n800:\tlearn: 0.1435785\ttest: 0.1672123\tbest: 0.1672123 (800)\ttotal: 25.9s\tremaining: 38.8s\n900:\tlearn: 0.1394712\ttest: 0.1653549\tbest: 0.1653549 (900)\ttotal: 29s\tremaining: 35.4s\n1000:\tlearn: 0.1357420\ttest: 0.1637648\tbest: 0.1637648 (1000)\ttotal: 32.2s\tremaining: 32.1s\n1100:\tlearn: 0.1325052\ttest: 0.1625297\tbest: 0.1625297 (1100)\ttotal: 35.3s\tremaining: 28.9s\n1200:\tlearn: 0.1294803\ttest: 0.1614517\tbest: 0.1614517 (1200)\ttotal: 38.7s\tremaining: 25.7s\n1300:\tlearn: 0.1266994\ttest: 0.1607379\tbest: 0.1607379 (1300)\ttotal: 41.8s\tremaining: 22.5s\n1400:\tlearn: 0.1243094\ttest: 0.1599433\tbest: 0.1599433 (1400)\ttotal: 45s\tremaining: 19.2s\n1500:\tlearn: 0.1218908\ttest: 0.1591876\tbest: 0.1591876 (1500)\ttotal: 48.1s\tremaining: 16s\n1600:\tlearn: 0.1197983\ttest: 0.1584086\tbest: 0.1584086 (1600)\ttotal: 51.3s\tremaining: 12.8s\n1700:\tlearn: 0.1177377\ttest: 0.1580023\tbest: 0.1579975 (1696)\ttotal: 54.5s\tremaining: 9.57s\n1800:\tlearn: 0.1158079\ttest: 0.1576470\tbest: 0.1576470 (1800)\ttotal: 57.7s\tremaining: 6.37s\n1900:\tlearn: 0.1138761\ttest: 0.1571295\tbest: 0.1571216 (1896)\ttotal: 1m\tremaining: 3.17s\n1999:\tlearn: 0.1121984\ttest: 0.1567654\tbest: 0.1567486 (1993)\ttotal: 1m 4s\tremaining: 0us\n\nbestTest = 0.1567485547\nbestIteration = 1993\n\nShrink model to first 1994 iterations.\n\n Fold 0.15674855181281303\n\n->-> Fold ran for 1 minutes 5 seconds\n\n---- Fold 5 -----\n\n900 900\n0:\tlearn: 0.5258595\ttest: 0.5179509\tbest: 0.5179509 (0)\ttotal: 37.4ms\tremaining: 1m 14s\n100:\tlearn: 0.2167806\ttest: 0.2169117\tbest: 0.2169117 (100)\ttotal: 3.48s\tremaining: 1m 5s\n200:\tlearn: 0.1945571\ttest: 0.1977963\tbest: 0.1977963 (200)\ttotal: 6.51s\tremaining: 58.2s\n300:\tlearn: 0.1791241\ttest: 0.1857990\tbest: 0.1857990 (300)\ttotal: 9.59s\tremaining: 54.1s\n400:\tlearn: 0.1689847\ttest: 0.1788446\tbest: 0.1788446 (400)\ttotal: 12.7s\tremaining: 50.6s\n500:\tlearn: 0.1609387\ttest: 0.1736867\tbest: 0.1736867 (500)\ttotal: 15.8s\tremaining: 47.3s\n600:\tlearn: 0.1541338\ttest: 0.1693444\tbest: 0.1693444 (600)\ttotal: 18.9s\tremaining: 44s\n700:\tlearn: 0.1487011\ttest: 0.1662777\tbest: 0.1662777 (700)\ttotal: 22.1s\tremaining: 41s\n800:\tlearn: 0.1439414\ttest: 0.1636674\tbest: 0.1636674 (800)\ttotal: 25.2s\tremaining: 37.8s\n900:\tlearn: 0.1399549\ttest: 0.1618182\tbest: 0.1618182 (900)\ttotal: 28.3s\tremaining: 34.5s\n1000:\tlearn: 0.1364547\ttest: 0.1600544\tbest: 0.1600544 (1000)\ttotal: 31.4s\tremaining: 31.3s\n1100:\tlearn: 0.1331791\ttest: 0.1587705\tbest: 0.1587705 (1100)\ttotal: 34.6s\tremaining: 28.3s\n1200:\tlearn: 0.1303097\ttest: 0.1575664\tbest: 0.1575657 (1192)\ttotal: 37.9s\tremaining: 25.2s\n1300:\tlearn: 0.1275787\ttest: 0.1565890\tbest: 0.1565890 (1300)\ttotal: 41s\tremaining: 22s\n1400:\tlearn: 0.1249969\ttest: 0.1554812\tbest: 0.1554812 (1400)\ttotal: 44.1s\tremaining: 18.9s\n1500:\tlearn: 0.1228226\ttest: 0.1548094\tbest: 0.1548094 (1500)\ttotal: 47.2s\tremaining: 15.7s\n1600:\tlearn: 0.1207815\ttest: 0.1541106\tbest: 0.1541106 (1600)\ttotal: 50.3s\tremaining: 12.5s\n1700:\tlearn: 0.1186325\ttest: 0.1534099\tbest: 0.1534099 (1700)\ttotal: 53.5s\tremaining: 9.41s\n1800:\tlearn: 0.1167559\ttest: 0.1528935\tbest: 0.1528858 (1799)\ttotal: 56.7s\tremaining: 6.27s\n1900:\tlearn: 0.1149329\ttest: 0.1522555\tbest: 0.1522555 (1900)\ttotal: 59.9s\tremaining: 3.12s\n1999:\tlearn: 0.1132330\ttest: 0.1517613\tbest: 0.1517613 (1999)\ttotal: 1m 2s\tremaining: 0us\n\nbestTest = 0.1517612972\nbestIteration = 1999\n\n\n Fold 0.15176129292099233\n\n->-> Fold ran for 1 minutes 4 seconds\n\n---- Fold 6 -----\n\n900 900\n0:\tlearn: 0.5237791\ttest: 0.5294957\tbest: 0.5294957 (0)\ttotal: 35ms\tremaining: 1m 9s\n100:\tlearn: 0.2160088\ttest: 0.2315908\tbest: 0.2315908 (100)\ttotal: 3.56s\tremaining: 1m 6s\n200:\tlearn: 0.1935324\ttest: 0.2095553\tbest: 0.2095553 (200)\ttotal: 6.73s\tremaining: 1m\n300:\tlearn: 0.1781433\ttest: 0.1966470\tbest: 0.1966470 (300)\ttotal: 9.83s\tremaining: 55.5s\n400:\tlearn: 0.1676139\ttest: 0.1883834\tbest: 0.1883834 (400)\ttotal: 13s\tremaining: 51.9s\n500:\tlearn: 0.1594349\ttest: 0.1827974\tbest: 0.1827974 (500)\ttotal: 16.1s\tremaining: 48.2s\n600:\tlearn: 0.1525950\ttest: 0.1785059\tbest: 0.1785059 (600)\ttotal: 19.2s\tremaining: 44.8s\n700:\tlearn: 0.1470092\ttest: 0.1753386\tbest: 0.1753386 (700)\ttotal: 22.4s\tremaining: 41.5s\n800:\tlearn: 0.1425355\ttest: 0.1730532\tbest: 0.1730532 (800)\ttotal: 25.5s\tremaining: 38.2s\n900:\tlearn: 0.1385713\ttest: 0.1709257\tbest: 0.1709257 (900)\ttotal: 28.7s\tremaining: 34.9s\n1000:\tlearn: 0.1352036\ttest: 0.1695164\tbest: 0.1695164 (1000)\ttotal: 31.8s\tremaining: 31.7s\n1100:\tlearn: 0.1321100\ttest: 0.1680651\tbest: 0.1680651 (1100)\ttotal: 35.2s\tremaining: 28.7s\n1200:\tlearn: 0.1292220\ttest: 0.1669239\tbest: 0.1669236 (1199)\ttotal: 38.4s\tremaining: 25.5s\n1300:\tlearn: 0.1266536\ttest: 0.1659134\tbest: 0.1659114 (1299)\ttotal: 41.5s\tremaining: 22.3s\n1400:\tlearn: 0.1242297\ttest: 0.1651944\tbest: 0.1651802 (1399)\ttotal: 44.7s\tremaining: 19.1s\n1500:\tlearn: 0.1218749\ttest: 0.1644992\tbest: 0.1644992 (1500)\ttotal: 47.9s\tremaining: 15.9s\n1600:\tlearn: 0.1197260\ttest: 0.1640005\tbest: 0.1640005 (1600)\ttotal: 51.1s\tremaining: 12.7s\n1700:\tlearn: 0.1177524\ttest: 0.1635412\tbest: 0.1635412 (1700)\ttotal: 54.3s\tremaining: 9.54s\n1800:\tlearn: 0.1160325\ttest: 0.1629677\tbest: 0.1629677 (1800)\ttotal: 57.5s\tremaining: 6.35s\n1900:\tlearn: 0.1142103\ttest: 0.1626022\tbest: 0.1625924 (1898)\ttotal: 1m\tremaining: 3.16s\n1999:\tlearn: 0.1126050\ttest: 0.1622519\tbest: 0.1622379 (1996)\ttotal: 1m 3s\tremaining: 0us\n\nbestTest = 0.1622378994\nbestIteration = 1996\n\nShrink model to first 1997 iterations.\n\n Fold 0.16223790482815167\n\n->-> Fold ran for 1 minutes 5 seconds\n\n---- Fold 7 -----\n\n900 900\n0:\tlearn: 0.5250639\ttest: 0.5285415\tbest: 0.5285415 (0)\ttotal: 33.7ms\tremaining: 1m 7s\n100:\tlearn: 0.2162856\ttest: 0.2226643\tbest: 0.2226643 (100)\ttotal: 3.58s\tremaining: 1m 7s\n200:\tlearn: 0.1932419\ttest: 0.2009259\tbest: 0.2009259 (200)\ttotal: 6.71s\tremaining: 1m\n300:\tlearn: 0.1779156\ttest: 0.1885410\tbest: 0.1885410 (300)\ttotal: 9.84s\tremaining: 55.5s\n400:\tlearn: 0.1676754\ttest: 0.1817924\tbest: 0.1817924 (400)\ttotal: 13s\tremaining: 51.9s\n500:\tlearn: 0.1593859\ttest: 0.1767227\tbest: 0.1767227 (500)\ttotal: 16.2s\tremaining: 48.5s\n600:\tlearn: 0.1530092\ttest: 0.1730564\tbest: 0.1730564 (600)\ttotal: 19.3s\tremaining: 45s\n700:\tlearn: 0.1474843\ttest: 0.1702631\tbest: 0.1702631 (700)\ttotal: 22.5s\tremaining: 41.6s\n800:\tlearn: 0.1429921\ttest: 0.1681089\tbest: 0.1681089 (800)\ttotal: 25.7s\tremaining: 38.4s\n900:\tlearn: 0.1391520\ttest: 0.1662986\tbest: 0.1662916 (899)\ttotal: 28.8s\tremaining: 35.1s\n1000:\tlearn: 0.1354500\ttest: 0.1647761\tbest: 0.1647761 (1000)\ttotal: 31.9s\tremaining: 31.9s\n1100:\tlearn: 0.1322390\ttest: 0.1637275\tbest: 0.1637275 (1100)\ttotal: 35.3s\tremaining: 28.8s\n1200:\tlearn: 0.1293162\ttest: 0.1626716\tbest: 0.1626716 (1200)\ttotal: 38.5s\tremaining: 25.6s\n1300:\tlearn: 0.1263550\ttest: 0.1617227\tbest: 0.1617171 (1299)\ttotal: 41.6s\tremaining: 22.4s\n1400:\tlearn: 0.1237398\ttest: 0.1609067\tbest: 0.1609067 (1400)\ttotal: 44.7s\tremaining: 19.1s\n1500:\tlearn: 0.1213656\ttest: 0.1603191\tbest: 0.1603191 (1500)\ttotal: 47.9s\tremaining: 15.9s\n1600:\tlearn: 0.1188903\ttest: 0.1596487\tbest: 0.1596362 (1599)\ttotal: 51s\tremaining: 12.7s\n1700:\tlearn: 0.1167394\ttest: 0.1592886\tbest: 0.1592839 (1698)\ttotal: 54.2s\tremaining: 9.53s\n1800:\tlearn: 0.1149212\ttest: 0.1588663\tbest: 0.1588663 (1800)\ttotal: 57.3s\tremaining: 6.34s\n1900:\tlearn: 0.1130918\ttest: 0.1585613\tbest: 0.1585580 (1899)\ttotal: 1m\tremaining: 3.15s\n1999:\tlearn: 0.1112720\ttest: 0.1583756\tbest: 0.1583756 (1999)\ttotal: 1m 3s\tremaining: 0us\n\nbestTest = 0.1583756093\nbestIteration = 1999\n\n\n Fold 0.15837560482452717\n\n->-> Fold ran for 1 minutes 4 seconds\n\n---- Fold 8 -----\n\n900 900\n0:\tlearn: 0.5250075\ttest: 0.5276298\tbest: 0.5276298 (0)\ttotal: 37.8ms\tremaining: 1m 15s\n100:\tlearn: 0.2154477\ttest: 0.2282846\tbest: 0.2282846 (100)\ttotal: 3.56s\tremaining: 1m 6s\n200:\tlearn: 0.1931475\ttest: 0.2101944\tbest: 0.2101944 (200)\ttotal: 6.74s\tremaining: 1m\n300:\tlearn: 0.1780633\ttest: 0.1989244\tbest: 0.1989244 (300)\ttotal: 9.93s\tremaining: 56s\n400:\tlearn: 0.1673997\ttest: 0.1919714\tbest: 0.1919714 (400)\ttotal: 13.2s\tremaining: 52.5s\n500:\tlearn: 0.1594800\ttest: 0.1870749\tbest: 0.1870749 (500)\ttotal: 16.4s\tremaining: 49s\n600:\tlearn: 0.1532766\ttest: 0.1834233\tbest: 0.1834214 (599)\ttotal: 19.5s\tremaining: 45.4s\n700:\tlearn: 0.1478130\ttest: 0.1808076\tbest: 0.1808076 (700)\ttotal: 22.7s\tremaining: 42.1s\n800:\tlearn: 0.1429678\ttest: 0.1786372\tbest: 0.1786372 (800)\ttotal: 26.1s\tremaining: 39s\n900:\tlearn: 0.1388542\ttest: 0.1770539\tbest: 0.1770539 (900)\ttotal: 29.4s\tremaining: 35.9s\n1000:\tlearn: 0.1353368\ttest: 0.1755381\tbest: 0.1755381 (1000)\ttotal: 33s\tremaining: 32.9s\n1100:\tlearn: 0.1320528\ttest: 0.1740719\tbest: 0.1740719 (1100)\ttotal: 36.2s\tremaining: 29.6s\n1200:\tlearn: 0.1291518\ttest: 0.1731100\tbest: 0.1731100 (1200)\ttotal: 39.4s\tremaining: 26.2s\n1300:\tlearn: 0.1262393\ttest: 0.1721737\tbest: 0.1721737 (1300)\ttotal: 42.7s\tremaining: 22.9s\n1400:\tlearn: 0.1236856\ttest: 0.1715465\tbest: 0.1715439 (1396)\ttotal: 45.9s\tremaining: 19.6s\n1500:\tlearn: 0.1212314\ttest: 0.1710151\tbest: 0.1710151 (1500)\ttotal: 49.2s\tremaining: 16.3s\n1600:\tlearn: 0.1190805\ttest: 0.1703335\tbest: 0.1703335 (1600)\ttotal: 52.3s\tremaining: 13s\n1700:\tlearn: 0.1169468\ttest: 0.1698538\tbest: 0.1698538 (1700)\ttotal: 55.6s\tremaining: 9.77s\n1800:\tlearn: 0.1149355\ttest: 0.1694661\tbest: 0.1694463 (1796)\ttotal: 58.8s\tremaining: 6.5s\n1900:\tlearn: 0.1131548\ttest: 0.1689797\tbest: 0.1689603 (1894)\ttotal: 1m 2s\tremaining: 3.23s\n1999:\tlearn: 0.1114430\ttest: 0.1686429\tbest: 0.1686223 (1993)\ttotal: 1m 5s\tremaining: 0us\n\nbestTest = 0.1686222846\nbestIteration = 1993\n\nShrink model to first 1994 iterations.\n\n Fold 0.16862228191036827\n\n->-> Fold ran for 1 minutes 6 seconds\n\n---- Fold 9 -----\n\n900 900\n0:\tlearn: 0.5232833\ttest: 0.5373114\tbest: 0.5373114 (0)\ttotal: 35.4ms\tremaining: 1m 10s\n100:\tlearn: 0.2170800\ttest: 0.2165434\tbest: 0.2165434 (100)\ttotal: 3.47s\tremaining: 1m 5s\n200:\tlearn: 0.1947604\ttest: 0.1946723\tbest: 0.1946723 (200)\ttotal: 6.68s\tremaining: 59.8s\n300:\tlearn: 0.1786965\ttest: 0.1812020\tbest: 0.1812020 (300)\ttotal: 9.89s\tremaining: 55.8s\n400:\tlearn: 0.1681849\ttest: 0.1734461\tbest: 0.1734461 (400)\ttotal: 13.2s\tremaining: 52.4s\n500:\tlearn: 0.1599687\ttest: 0.1683054\tbest: 0.1683054 (500)\ttotal: 16.4s\tremaining: 49s\n600:\tlearn: 0.1542069\ttest: 0.1645455\tbest: 0.1645455 (600)\ttotal: 19.7s\tremaining: 45.8s\n700:\tlearn: 0.1486719\ttest: 0.1611937\tbest: 0.1611937 (700)\ttotal: 22.9s\tremaining: 42.5s\n800:\tlearn: 0.1440171\ttest: 0.1590338\tbest: 0.1590338 (800)\ttotal: 26.3s\tremaining: 39.3s\n900:\tlearn: 0.1397078\ttest: 0.1571051\tbest: 0.1571051 (900)\ttotal: 29.8s\tremaining: 36.4s\n1000:\tlearn: 0.1361506\ttest: 0.1555662\tbest: 0.1555662 (1000)\ttotal: 33.3s\tremaining: 33.3s\n1100:\tlearn: 0.1325860\ttest: 0.1540745\tbest: 0.1540745 (1100)\ttotal: 36.8s\tremaining: 30s\n1200:\tlearn: 0.1296673\ttest: 0.1528421\tbest: 0.1528421 (1200)\ttotal: 40.2s\tremaining: 26.8s\n1300:\tlearn: 0.1270812\ttest: 0.1519021\tbest: 0.1518781 (1299)\ttotal: 43.6s\tremaining: 23.4s\n1400:\tlearn: 0.1247972\ttest: 0.1511613\tbest: 0.1511613 (1400)\ttotal: 47s\tremaining: 20.1s\n1500:\tlearn: 0.1224699\ttest: 0.1504961\tbest: 0.1504961 (1500)\ttotal: 50.5s\tremaining: 16.8s\n1600:\tlearn: 0.1200721\ttest: 0.1500278\tbest: 0.1500278 (1600)\ttotal: 53.9s\tremaining: 13.4s\n1700:\tlearn: 0.1179576\ttest: 0.1495352\tbest: 0.1495267 (1693)\ttotal: 57.2s\tremaining: 10.1s\n1800:\tlearn: 0.1159731\ttest: 0.1491128\tbest: 0.1491097 (1799)\ttotal: 1m\tremaining: 6.72s\n1900:\tlearn: 0.1142299\ttest: 0.1486518\tbest: 0.1486518 (1900)\ttotal: 1m 4s\tremaining: 3.34s\n1999:\tlearn: 0.1124550\ttest: 0.1482536\tbest: 0.1482452 (1997)\ttotal: 1m 7s\tremaining: 0us\n\nbestTest = 0.1482451689\nbestIteration = 1997\n\nShrink model to first 1998 iterations.\n\n Fold 0.14824517614637275\n\n->-> Fold ran for 1 minutes 8 seconds\n\ntrain score (validation fold) : 0.15846513538532825\n\n->-> Total  time: 10 minutes \n","output_type":"stream"}]},{"cell_type":"code","source":"preds = lgbm_prediction*0.6 + xgb_prediction*0.3 + cat_prediction*0.1\nsub = pd.DataFrame({'Per Person Price': preds})\nsub.to_csv('cvec14_2.csv', index=False)","metadata":{"trusted":true},"execution_count":52,"outputs":[]},{"cell_type":"code","source":"pd.DataFrame(preds)[0].describe()","metadata":{"trusted":true},"execution_count":51,"outputs":[{"execution_count":51,"output_type":"execute_result","data":{"text/plain":"count      9000.000000\nmean      19645.620020\nstd       10418.672995\nmin        1835.631351\n25%       12651.115034\n50%       17581.734434\n75%       24661.301808\nmax      122409.211346\nName: 0, dtype: float64"},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}