{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O4jyeOxLhrW5",
        "outputId": "a84b05ff-d098-4b9a-8ceb-8ec685f81017"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fri Jan 14 05:50:19 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 495.46       Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   43C    P0    28W / 250W |      0MiB / 16280MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Select the Runtime > \"Change runtime type\" menu to enable a GPU accelerator, ')\n",
        "  print('and then re-execute this cell.')\n",
        "else:\n",
        "  print(gpu_info)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "GzE7BZwOzdCz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "61947b44-a2f5-4a5c-867c-4cb9fb195edf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# from google.colab import drive\n",
        "# drive._mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "pr1c6jPGho3z",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "dcb3dcd8-f1d8-4b0c-dd3a-c1338e09e0a5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting simpletransformers\n",
            "  Downloading simpletransformers-0.63.3-py3-none-any.whl (247 kB)\n",
            "\u001b[?25l\r\u001b[K     |█▎                              | 10 kB 22.0 MB/s eta 0:00:01\r\u001b[K     |██▋                             | 20 kB 22.2 MB/s eta 0:00:01\r\u001b[K     |████                            | 30 kB 21.5 MB/s eta 0:00:01\r\u001b[K     |█████▎                          | 40 kB 16.6 MB/s eta 0:00:01\r\u001b[K     |██████▋                         | 51 kB 14.6 MB/s eta 0:00:01\r\u001b[K     |████████                        | 61 kB 13.2 MB/s eta 0:00:01\r\u001b[K     |█████████▎                      | 71 kB 10.6 MB/s eta 0:00:01\r\u001b[K     |██████████▋                     | 81 kB 11.7 MB/s eta 0:00:01\r\u001b[K     |████████████                    | 92 kB 12.8 MB/s eta 0:00:01\r\u001b[K     |█████████████▎                  | 102 kB 12.0 MB/s eta 0:00:01\r\u001b[K     |██████████████▌                 | 112 kB 12.0 MB/s eta 0:00:01\r\u001b[K     |███████████████▉                | 122 kB 12.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████▏              | 133 kB 12.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████▌             | 143 kB 12.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████▉            | 153 kB 12.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▏          | 163 kB 12.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▌         | 174 kB 12.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▉        | 184 kB 12.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▏      | 194 kB 12.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▌     | 204 kB 12.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▊    | 215 kB 12.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 225 kB 12.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▍ | 235 kB 12.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▊| 245 kB 12.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 247 kB 12.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from simpletransformers) (1.1.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from simpletransformers) (2.23.0)\n",
            "Collecting wandb>=0.10.32\n",
            "  Downloading wandb-0.12.9-py2.py3-none-any.whl (1.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.7 MB 60.0 MB/s \n",
            "\u001b[?25hCollecting seqeval\n",
            "  Downloading seqeval-1.2.2.tar.gz (43 kB)\n",
            "\u001b[K     |████████████████████████████████| 43 kB 2.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from simpletransformers) (1.19.5)\n",
            "Collecting tokenizers\n",
            "  Downloading tokenizers-0.11.2-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.8 MB 27.3 MB/s \n",
            "\u001b[?25hCollecting datasets\n",
            "  Downloading datasets-1.17.0-py3-none-any.whl (306 kB)\n",
            "\u001b[K     |████████████████████████████████| 306 kB 54.8 MB/s \n",
            "\u001b[?25hCollecting transformers>=4.6.0\n",
            "  Downloading transformers-4.15.0-py3-none-any.whl (3.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.4 MB 49.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from simpletransformers) (1.0.2)\n",
            "Collecting streamlit\n",
            "  Downloading streamlit-1.4.0-py2.py3-none-any.whl (9.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 9.3 MB 38.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tensorboard in /usr/local/lib/python3.7/dist-packages (from simpletransformers) (2.7.0)\n",
            "Requirement already satisfied: tqdm>=4.47.0 in /usr/local/lib/python3.7/dist-packages (from simpletransformers) (4.62.3)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from simpletransformers) (1.4.1)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from simpletransformers) (2019.12.20)\n",
            "Collecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.96-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2 MB 59.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers>=4.6.0->simpletransformers) (3.4.2)\n",
            "Collecting sacremoses\n",
            "  Downloading sacremoses-0.0.47-py2.py3-none-any.whl (895 kB)\n",
            "\u001b[K     |████████████████████████████████| 895 kB 59.4 MB/s \n",
            "\u001b[?25hCollecting tokenizers\n",
            "  Downloading tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3 MB 49.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers>=4.6.0->simpletransformers) (21.3)\n",
            "Collecting huggingface-hub<1.0,>=0.1.0\n",
            "  Downloading huggingface_hub-0.4.0-py3-none-any.whl (67 kB)\n",
            "\u001b[K     |████████████████████████████████| 67 kB 5.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers>=4.6.0->simpletransformers) (4.10.0)\n",
            "Collecting pyyaml>=5.1\n",
            "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
            "\u001b[K     |████████████████████████████████| 596 kB 62.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers>=4.6.0->simpletransformers) (3.10.0.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers>=4.6.0->simpletransformers) (3.0.6)\n",
            "Collecting yaspin>=1.0.0\n",
            "  Downloading yaspin-2.1.0-py3-none-any.whl (18 kB)\n",
            "Collecting pathtools\n",
            "  Downloading pathtools-0.1.2.tar.gz (11 kB)\n",
            "Collecting sentry-sdk>=1.0.0\n",
            "  Downloading sentry_sdk-1.5.2-py2.py3-none-any.whl (142 kB)\n",
            "\u001b[K     |████████████████████████████████| 142 kB 62.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: protobuf>=3.12.0 in /usr/local/lib/python3.7/dist-packages (from wandb>=0.10.32->simpletransformers) (3.17.3)\n",
            "Collecting configparser>=3.8.1\n",
            "  Downloading configparser-5.2.0-py3-none-any.whl (19 kB)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.7/dist-packages (from wandb>=0.10.32->simpletransformers) (2.8.2)\n",
            "Requirement already satisfied: six>=1.13.0 in /usr/local/lib/python3.7/dist-packages (from wandb>=0.10.32->simpletransformers) (1.15.0)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb>=0.10.32->simpletransformers) (5.4.8)\n",
            "Requirement already satisfied: Click!=8.0.0,>=7.0 in /usr/local/lib/python3.7/dist-packages (from wandb>=0.10.32->simpletransformers) (7.1.2)\n",
            "Collecting shortuuid>=0.5.0\n",
            "  Downloading shortuuid-1.0.8-py3-none-any.whl (9.5 kB)\n",
            "Collecting GitPython>=1.0.0\n",
            "  Downloading GitPython-3.1.26-py3-none-any.whl (180 kB)\n",
            "\u001b[K     |████████████████████████████████| 180 kB 67.9 MB/s \n",
            "\u001b[?25hCollecting docker-pycreds>=0.4.0\n",
            "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
            "Collecting subprocess32>=3.5.3\n",
            "  Downloading subprocess32-3.5.4.tar.gz (97 kB)\n",
            "\u001b[K     |████████████████████████████████| 97 kB 7.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: promise<3,>=2.0 in /usr/local/lib/python3.7/dist-packages (from wandb>=0.10.32->simpletransformers) (2.3)\n",
            "Collecting gitdb<5,>=4.0.1\n",
            "  Downloading gitdb-4.0.9-py3-none-any.whl (63 kB)\n",
            "\u001b[K     |████████████████████████████████| 63 kB 1.7 MB/s \n",
            "\u001b[?25hCollecting smmap<6,>=3.0.1\n",
            "  Downloading smmap-5.0.0-py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->simpletransformers) (2021.10.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->simpletransformers) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->simpletransformers) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->simpletransformers) (3.0.4)\n",
            "Requirement already satisfied: termcolor<2.0.0,>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from yaspin>=1.0.0->wandb>=0.10.32->simpletransformers) (1.1.0)\n",
            "Collecting fsspec[http]>=2021.05.0\n",
            "  Downloading fsspec-2022.1.0-py3-none-any.whl (133 kB)\n",
            "\u001b[K     |████████████████████████████████| 133 kB 62.7 MB/s \n",
            "\u001b[?25hCollecting xxhash\n",
            "  Downloading xxhash-2.0.2-cp37-cp37m-manylinux2010_x86_64.whl (243 kB)\n",
            "\u001b[K     |████████████████████████████████| 243 kB 63.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyarrow!=4.0.0,>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from datasets->simpletransformers) (3.0.0)\n",
            "Collecting aiohttp\n",
            "  Downloading aiohttp-3.8.1-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 50.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from datasets->simpletransformers) (0.3.4)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.7/dist-packages (from datasets->simpletransformers) (0.70.12.2)\n",
            "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets->simpletransformers) (2.0.10)\n",
            "Collecting multidict<7.0,>=4.5\n",
            "  Downloading multidict-5.2.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (160 kB)\n",
            "\u001b[K     |████████████████████████████████| 160 kB 54.9 MB/s \n",
            "\u001b[?25hCollecting yarl<2.0,>=1.0\n",
            "  Downloading yarl-1.7.2-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (271 kB)\n",
            "\u001b[K     |████████████████████████████████| 271 kB 55.4 MB/s \n",
            "\u001b[?25hCollecting frozenlist>=1.1.1\n",
            "  Downloading frozenlist-1.2.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (192 kB)\n",
            "\u001b[K     |████████████████████████████████| 192 kB 52.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets->simpletransformers) (21.4.0)\n",
            "Collecting async-timeout<5.0,>=4.0.0a3\n",
            "  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n",
            "Collecting aiosignal>=1.1.2\n",
            "  Downloading aiosignal-1.2.0-py3-none-any.whl (8.2 kB)\n",
            "Collecting asynctest==0.13.0\n",
            "  Downloading asynctest-0.13.0-py3-none-any.whl (26 kB)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers>=4.6.0->simpletransformers) (3.7.0)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->simpletransformers) (2018.9)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers>=4.6.0->simpletransformers) (1.1.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->simpletransformers) (3.0.0)\n",
            "Requirement already satisfied: altair>=3.2.0 in /usr/local/lib/python3.7/dist-packages (from streamlit->simpletransformers) (4.2.0)\n",
            "Requirement already satisfied: tzlocal in /usr/local/lib/python3.7/dist-packages (from streamlit->simpletransformers) (1.5.1)\n",
            "Collecting validators\n",
            "  Downloading validators-0.18.2-py3-none-any.whl (19 kB)\n",
            "Requirement already satisfied: tornado>=5.0 in /usr/local/lib/python3.7/dist-packages (from streamlit->simpletransformers) (5.1.1)\n",
            "Collecting pympler>=0.9\n",
            "  Downloading Pympler-1.0.1-py3-none-any.whl (164 kB)\n",
            "\u001b[K     |████████████████████████████████| 164 kB 51.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: cachetools>=4.0 in /usr/local/lib/python3.7/dist-packages (from streamlit->simpletransformers) (4.2.4)\n",
            "Requirement already satisfied: toml in /usr/local/lib/python3.7/dist-packages (from streamlit->simpletransformers) (0.10.2)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.7/dist-packages (from streamlit->simpletransformers) (7.1.2)\n",
            "Collecting blinker\n",
            "  Downloading blinker-1.4.tar.gz (111 kB)\n",
            "\u001b[K     |████████████████████████████████| 111 kB 57.5 MB/s \n",
            "\u001b[?25hCollecting base58\n",
            "  Downloading base58-2.1.1-py3-none-any.whl (5.6 kB)\n",
            "Collecting watchdog\n",
            "  Downloading watchdog-2.1.6-py3-none-manylinux2014_x86_64.whl (76 kB)\n",
            "\u001b[K     |████████████████████████████████| 76 kB 5.6 MB/s \n",
            "\u001b[?25hCollecting pydeck>=0.1.dev5\n",
            "  Downloading pydeck-0.7.1-py2.py3-none-any.whl (4.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.3 MB 48.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: astor in /usr/local/lib/python3.7/dist-packages (from streamlit->simpletransformers) (0.8.1)\n",
            "Requirement already satisfied: entrypoints in /usr/local/lib/python3.7/dist-packages (from altair>=3.2.0->streamlit->simpletransformers) (0.3)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.7/dist-packages (from altair>=3.2.0->streamlit->simpletransformers) (4.3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from altair>=3.2.0->streamlit->simpletransformers) (2.11.3)\n",
            "Requirement already satisfied: toolz in /usr/local/lib/python3.7/dist-packages (from altair>=3.2.0->streamlit->simpletransformers) (0.11.2)\n",
            "Requirement already satisfied: importlib-resources>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema>=3.0->altair>=3.2.0->streamlit->simpletransformers) (5.4.0)\n",
            "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema>=3.0->altair>=3.2.0->streamlit->simpletransformers) (0.18.0)\n",
            "Collecting ipykernel>=5.1.2\n",
            "  Downloading ipykernel-6.7.0-py3-none-any.whl (127 kB)\n",
            "\u001b[K     |████████████████████████████████| 127 kB 55.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: traitlets>=4.3.2 in /usr/local/lib/python3.7/dist-packages (from pydeck>=0.1.dev5->streamlit->simpletransformers) (5.1.1)\n",
            "Requirement already satisfied: ipywidgets>=7.0.0 in /usr/local/lib/python3.7/dist-packages (from pydeck>=0.1.dev5->streamlit->simpletransformers) (7.6.5)\n",
            "Collecting ipython>=7.23.1\n",
            "  Downloading ipython-7.31.0-py3-none-any.whl (792 kB)\n",
            "\u001b[K     |████████████████████████████████| 792 kB 51.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: jupyter-client<8.0 in /usr/local/lib/python3.7/dist-packages (from ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit->simpletransformers) (5.3.5)\n",
            "Requirement already satisfied: nest-asyncio in /usr/local/lib/python3.7/dist-packages (from ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit->simpletransformers) (1.5.4)\n",
            "Requirement already satisfied: matplotlib-inline<0.2.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit->simpletransformers) (0.1.3)\n",
            "Requirement already satisfied: debugpy<2.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit->simpletransformers) (1.0.0)\n",
            "Collecting prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0\n",
            "  Downloading prompt_toolkit-3.0.24-py3-none-any.whl (374 kB)\n",
            "\u001b[K     |████████████████████████████████| 374 kB 69.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pygments in /usr/local/lib/python3.7/dist-packages (from ipython>=7.23.1->ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit->simpletransformers) (2.6.1)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.7/dist-packages (from ipython>=7.23.1->ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit->simpletransformers) (57.4.0)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.7/dist-packages (from ipython>=7.23.1->ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit->simpletransformers) (0.7.5)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.7/dist-packages (from ipython>=7.23.1->ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit->simpletransformers) (4.8.0)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.7/dist-packages (from ipython>=7.23.1->ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit->simpletransformers) (0.2.0)\n",
            "Requirement already satisfied: jedi>=0.16 in /usr/local/lib/python3.7/dist-packages (from ipython>=7.23.1->ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit->simpletransformers) (0.18.1)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from ipython>=7.23.1->ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit->simpletransformers) (4.4.2)\n",
            "Requirement already satisfied: nbformat>=4.2.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit->simpletransformers) (5.1.3)\n",
            "Requirement already satisfied: ipython-genutils~=0.2.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit->simpletransformers) (0.2.0)\n",
            "Requirement already satisfied: jupyterlab-widgets>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit->simpletransformers) (1.0.2)\n",
            "Requirement already satisfied: widgetsnbextension~=3.5.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit->simpletransformers) (3.5.2)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /usr/local/lib/python3.7/dist-packages (from jedi>=0.16->ipython>=7.23.1->ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit->simpletransformers) (0.8.3)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->altair>=3.2.0->streamlit->simpletransformers) (2.0.1)\n",
            "Requirement already satisfied: jupyter-core>=4.6.0 in /usr/local/lib/python3.7/dist-packages (from jupyter-client<8.0->ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit->simpletransformers) (4.9.1)\n",
            "Requirement already satisfied: pyzmq>=13 in /usr/local/lib/python3.7/dist-packages (from jupyter-client<8.0->ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit->simpletransformers) (22.3.0)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.7/dist-packages (from pexpect>4.3->ipython>=7.23.1->ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit->simpletransformers) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=7.23.1->ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit->simpletransformers) (0.2.5)\n",
            "Requirement already satisfied: notebook>=4.4.1 in /usr/local/lib/python3.7/dist-packages (from widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit->simpletransformers) (5.3.1)\n",
            "Requirement already satisfied: terminado>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit->simpletransformers) (0.12.1)\n",
            "Requirement already satisfied: Send2Trash in /usr/local/lib/python3.7/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit->simpletransformers) (1.8.0)\n",
            "Requirement already satisfied: nbconvert in /usr/local/lib/python3.7/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit->simpletransformers) (5.6.1)\n",
            "Requirement already satisfied: testpath in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit->simpletransformers) (0.5.0)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit->simpletransformers) (4.1.0)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit->simpletransformers) (0.7.1)\n",
            "Requirement already satisfied: mistune<2,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit->simpletransformers) (0.8.4)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit->simpletransformers) (1.5.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.7/dist-packages (from bleach->nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit->simpletransformers) (0.5.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard->simpletransformers) (3.3.6)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard->simpletransformers) (1.8.1)\n",
            "Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard->simpletransformers) (1.43.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard->simpletransformers) (0.6.1)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorboard->simpletransformers) (0.37.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard->simpletransformers) (1.0.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard->simpletransformers) (0.4.6)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard->simpletransformers) (1.35.0)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.7/dist-packages (from tensorboard->simpletransformers) (0.12.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard->simpletransformers) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard->simpletransformers) (4.8)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard->simpletransformers) (1.3.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard->simpletransformers) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard->simpletransformers) (3.1.1)\n",
            "Building wheels for collected packages: subprocess32, pathtools, seqeval, blinker\n",
            "  Building wheel for subprocess32 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for subprocess32: filename=subprocess32-3.5.4-py3-none-any.whl size=6502 sha256=c5caa57ebb174cc915d346ca6c48915be3b7763b797ac442145eb7b3791b079a\n",
            "  Stored in directory: /root/.cache/pip/wheels/50/ca/fa/8fca8d246e64f19488d07567547ddec8eb084e8c0d7a59226a\n",
            "  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pathtools: filename=pathtools-0.1.2-py3-none-any.whl size=8806 sha256=4efe47407dd74f0c6ae10703590f0d358ccaa96975ebfc00cb8abb69a579ea81\n",
            "  Stored in directory: /root/.cache/pip/wheels/3e/31/09/fa59cef12cdcfecc627b3d24273699f390e71828921b2cbba2\n",
            "  Building wheel for seqeval (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for seqeval: filename=seqeval-1.2.2-py3-none-any.whl size=16180 sha256=95da01e0031c2270405100655525eb6d0f14121b285d784ca6a5036a6ea40e39\n",
            "  Stored in directory: /root/.cache/pip/wheels/05/96/ee/7cac4e74f3b19e3158dce26a20a1c86b3533c43ec72a549fd7\n",
            "  Building wheel for blinker (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for blinker: filename=blinker-1.4-py3-none-any.whl size=13478 sha256=7911d129e0779a11685569fd521977969d6d4821aae0975691ebe0aabc809bd6\n",
            "  Stored in directory: /root/.cache/pip/wheels/22/f5/18/df711b66eb25b21325c132757d4314db9ac5e8dabeaf196eab\n",
            "Successfully built subprocess32 pathtools seqeval blinker\n",
            "Installing collected packages: prompt-toolkit, ipython, ipykernel, multidict, frozenlist, yarl, smmap, asynctest, async-timeout, aiosignal, pyyaml, gitdb, fsspec, aiohttp, yaspin, xxhash, watchdog, validators, tokenizers, subprocess32, shortuuid, sentry-sdk, sacremoses, pympler, pydeck, pathtools, huggingface-hub, GitPython, docker-pycreds, configparser, blinker, base58, wandb, transformers, streamlit, seqeval, sentencepiece, datasets, simpletransformers\n",
            "  Attempting uninstall: prompt-toolkit\n",
            "    Found existing installation: prompt-toolkit 1.0.18\n",
            "    Uninstalling prompt-toolkit-1.0.18:\n",
            "      Successfully uninstalled prompt-toolkit-1.0.18\n",
            "  Attempting uninstall: ipython\n",
            "    Found existing installation: ipython 5.5.0\n",
            "    Uninstalling ipython-5.5.0:\n",
            "      Successfully uninstalled ipython-5.5.0\n",
            "  Attempting uninstall: ipykernel\n",
            "    Found existing installation: ipykernel 4.10.1\n",
            "    Uninstalling ipykernel-4.10.1:\n",
            "      Successfully uninstalled ipykernel-4.10.1\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "jupyter-console 5.2.0 requires prompt-toolkit<2.0.0,>=1.0.0, but you have prompt-toolkit 3.0.24 which is incompatible.\n",
            "google-colab 1.0.0 requires ipykernel~=4.10, but you have ipykernel 6.7.0 which is incompatible.\n",
            "google-colab 1.0.0 requires ipython~=5.5.0, but you have ipython 7.31.0 which is incompatible.\u001b[0m\n",
            "Successfully installed GitPython-3.1.26 aiohttp-3.8.1 aiosignal-1.2.0 async-timeout-4.0.2 asynctest-0.13.0 base58-2.1.1 blinker-1.4 configparser-5.2.0 datasets-1.17.0 docker-pycreds-0.4.0 frozenlist-1.2.0 fsspec-2022.1.0 gitdb-4.0.9 huggingface-hub-0.4.0 ipykernel-6.7.0 ipython-7.31.0 multidict-5.2.0 pathtools-0.1.2 prompt-toolkit-3.0.24 pydeck-0.7.1 pympler-1.0.1 pyyaml-6.0 sacremoses-0.0.47 sentencepiece-0.1.96 sentry-sdk-1.5.2 seqeval-1.2.2 shortuuid-1.0.8 simpletransformers-0.63.3 smmap-5.0.0 streamlit-1.4.0 subprocess32-3.5.4 tokenizers-0.10.3 transformers-4.15.0 validators-0.18.2 wandb-0.12.9 watchdog-2.1.6 xxhash-2.0.2 yarl-1.7.2 yaspin-2.1.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "IPython",
                  "ipykernel",
                  "prompt_toolkit"
                ]
              }
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (1.10.0+cu111)\n",
            "Collecting torch\n",
            "  Downloading torch-1.10.1-cp37-cp37m-manylinux1_x86_64.whl (881.9 MB)\n",
            "\u001b[K     |██████████████████████████████▎ | 834.1 MB 41.4 MB/s eta 0:00:02tcmalloc: large alloc 1147494400 bytes == 0x55e08b3e2000 @  0x7f5bdbca5615 0x55e0257a04cc 0x55e02588047a 0x55e0257a32ed 0x55e025894e1d 0x55e025816e99 0x55e0258119ee 0x55e0257a4bda 0x55e025816d00 0x55e0258119ee 0x55e0257a4bda 0x55e025813737 0x55e025895c66 0x55e025812daf 0x55e025895c66 0x55e025812daf 0x55e025895c66 0x55e025812daf 0x55e0257a5039 0x55e0257e8409 0x55e0257a3c52 0x55e025816c25 0x55e0258119ee 0x55e0257a4bda 0x55e025813737 0x55e0258119ee 0x55e0257a4bda 0x55e025812915 0x55e0257a4afa 0x55e025812c0d 0x55e0258119ee\n",
            "\u001b[K     |████████████████████████████████| 881.9 MB 14 kB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch) (3.10.0.2)\n",
            "Installing collected packages: torch\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 1.10.0+cu111\n",
            "    Uninstalling torch-1.10.0+cu111:\n",
            "      Successfully uninstalled torch-1.10.0+cu111\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchvision 0.11.1+cu111 requires torch==1.10.0, but you have torch 1.10.1 which is incompatible.\n",
            "torchtext 0.11.0 requires torch==1.10.0, but you have torch 1.10.1 which is incompatible.\n",
            "torchaudio 0.10.0+cu111 requires torch==1.10.0, but you have torch 1.10.1 which is incompatible.\u001b[0m\n",
            "Successfully installed torch-1.10.1\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.15.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.47)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.4.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.10.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.4.0)\n",
            "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.10.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.62.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.10.0.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.6)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.7.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.1.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install --upgrade simpletransformers\n",
        "!pip install --upgrade torch\n",
        "!pip install --upgrade transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Restart the notebook after installing above packages**"
      ],
      "metadata": {
        "id": "lWN0dgCMI7g5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "-pLl9WcSh6s8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d011f6e7-37f1-4412-bded-22b6ffd213b6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  /content/Participants_Data_DCW.zip\n",
            "  inflating: test.csv                \n",
            "  inflating: __MACOSX/._test.csv     \n",
            "  inflating: train.csv               \n",
            "  inflating: __MACOSX/._train.csv    \n",
            "  inflating: submission.csv          \n",
            "  inflating: __MACOSX/._submission.csv  \n"
          ]
        }
      ],
      "source": [
        "!unzip '/content/Participants_Data_DCW.zip'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "dfUec4cuxZ7r"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "from transformers import (AutoModel,AutoModelForMaskedLM, \n",
        "                          AutoTokenizer, LineByLineTextDataset,\n",
        "                          DataCollatorForLanguageModeling,\n",
        "                          Trainer, TrainingArguments)\n",
        "\n",
        "from collections import Counter\n",
        "from tqdm import tqdm\n",
        "import gc\n",
        "from scipy.special import softmax\n",
        "from simpletransformers.classification import ClassificationModel\n",
        "from sklearn.model_selection import train_test_split, StratifiedKFold, KFold\n",
        "import sklearn\n",
        "from sklearn.metrics import log_loss\n",
        "from sklearn.metrics import *\n",
        "from sklearn.model_selection import *\n",
        "import re\n",
        "import random\n",
        "import torch\n",
        "pd.options.display.max_colwidth = 200\n",
        "\n",
        "def seed_all(seed_value):\n",
        "    random.seed(seed_value) # Python\n",
        "    np.random.seed(seed_value) # cpu vars\n",
        "    torch.manual_seed(seed_value) # cpu  vars\n",
        "    \n",
        "    if torch.cuda.is_available(): \n",
        "        torch.cuda.manual_seed(seed_value)\n",
        "        torch.cuda.manual_seed_all(seed_value) # gpu vars\n",
        "        torch.backends.cudnn.deterministic = True  #needed\n",
        "        torch.backends.cudnn.benchmark = False\n",
        "seed_all(13)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "89aZnRGaxd2S"
      },
      "outputs": [],
      "source": [
        "train_data=pd.read_csv(\"train.csv\")\n",
        "test_data=pd.read_csv(\"test.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_text(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r\"what's\", \"what is \", text)\n",
        "    text = re.sub(r\"\\'s\", \" \", text)\n",
        "    text = re.sub(r\"\\'ve\", \" have \", text)\n",
        "    text = re.sub(r\"can't\", \"cannot \", text)\n",
        "    text = re.sub(r\"n't\", \" not \", text)\n",
        "    text = re.sub(r\"i'm\", \"i am \", text)\n",
        "    text = re.sub(r\"\\'re\", \" are \", text)\n",
        "    text = re.sub(r\"\\'d\", \" would \", text)\n",
        "    text = re.sub(r\"\\'ll\", \" will \", text)\n",
        "    text = re.sub(r\"\\'scuse\", \" excuse \", text)\n",
        "    text = re.sub('\\W', ' ', text)\n",
        "    text = re.sub('\\s+', ' ', text)\n",
        "    text = text.strip(' ')\n",
        "    return text\n",
        "\n",
        "train_data['Review'] = train_data['Review'].map(lambda com : clean_text(com))\n",
        "train_data[\"Review\"] = train_data['Review'].str.replace('[^\\w\\s]','')\n",
        "\n",
        "test_data['Review'] = test_data['Review'].map(lambda com : clean_text(com))\n",
        "test_data[\"Review\"] = test_data['Review'].str.replace('[^\\w\\s]','')\n"
      ],
      "metadata": {
        "id": "qNEfoObdVmRk"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "1M_-r2E1xh3W"
      },
      "outputs": [],
      "source": [
        "data = pd.concat([train_data,test_data])\n",
        "data['Review'] = data['Review'].apply(lambda x: x.replace('\\n',''))\n",
        "\n",
        "text  = '\\n'.join(data.Review.str.lower().tolist())\n",
        "\n",
        "with open('text.txt','w') as f:\n",
        "    f.write(text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 264,
          "referenced_widgets": [
            "79c669321bb44061950e37bd94e4a259",
            "c8b81f14afd742c481feb776f784d373",
            "63ea255d6b8e4344843cbd202199ef39",
            "55cb5bb5132a4561a26495837b42cb53",
            "d182d9b06ce64c37ade3155e757d7b54",
            "4339806721fb4226b95ef36b143b8abf",
            "8ee569af346e4bfeb9e33ade8bc4cc32",
            "f8c1a50e12864335a1bc1a87dd2217a7",
            "1de3eaca2d5748508882b63a9d22da41",
            "59ae600e3bea4fc6aa3e1b493055b0a4",
            "a5fc9e2be1aa478eb74fb36d09f2b5b4",
            "604bf0e27fb54ba796606a90d2495f7f",
            "dfa246c09a03470ab44d0a3d03dd34f5",
            "a40d411dcd6c400d8c8722fb6b40a56b",
            "dea6ed6a3db34d0397abc3ab2e593d01",
            "d677ec818ea945a7a7049908fc89f4ac",
            "25ba258e241842c094451cf60ae9077c",
            "80440ad1112642df9f96fad9e95c4f59",
            "12b2d39d4bcc47a4b313686942a102fc",
            "f76b892816384499b0f99c450f13f89f",
            "344fff8984f24981945aa2444ac700be",
            "db8edb44026645d18c7ea7ba32582ade",
            "d5c9c39f7b2145c1932eef10ef1053b5",
            "0552864714f843a7ba676b7d81ee78ee",
            "f2f551636d1c4a0f85b732a006e86de6",
            "eb42b0ed7503427cbf611e6c10e0b18e",
            "8a9d265c97564e13ada9606e634e2dfa",
            "b74c1406476749ab93303c73657a257a",
            "30a64747440144a6b120d65c905eacbe",
            "04459085433a49659fa7398178831b42",
            "dc0a6e3129fb47b99fddb315095f0c5f",
            "0ffe54217a5747a08547564fda3066f2",
            "742448ecae5649109b7874e5175f7622",
            "304903f465214ea2a58292b87205b7e2",
            "f5679a4c21084b1b9489ae5bd55ff649",
            "7a38201ffa5645c3a71ecfe1080526cf",
            "19d079cfea414db3b8c4ffbfa3bc09ae",
            "e5949ad8849f483ca5922b88e9ac6337",
            "ed35b2367710430aa803f48dd4e9f654",
            "93fcb41580b44df7baf9cf30be332866",
            "999db374173d4672b7cb0eaedc20bcd4",
            "cb47b246cb7c4140b9c7f07b20aa6ce0",
            "db953f5f0489452d873b251c0a47f29e",
            "9bb623193d7345efbf503ec8268e9f2a",
            "dd2efccff6ae46f080a2a95ee1507420",
            "a5fbcadc429c496395cc9a26666cacb1",
            "91675cf4f0ae4ef09cc84fc1f74f9d24",
            "64477bfc60994688a1382c4573476345",
            "7969c221547e4bf4b1b5dc4a0bf2b6d9",
            "1693983495f7446aa45cb7091dad1d10",
            "66c21ec35db344ec8807df3b92e99f85",
            "80e1a188ddc7419f9cd58e3410606e65",
            "e62c65f4e53d4ad7a29bd1fb7e52b7c6",
            "307ff0069df244a5b027cbf6188b9c17",
            "b09a5a3fb7444536bf9ac2575a8723bf"
          ]
        },
        "id": "NAm9Od4jxlNP",
        "outputId": "0650bf0b-d1d1-43f2-d90a-81dcea47cc63"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "79c669321bb44061950e37bd94e4a259",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/1.00k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "604bf0e27fb54ba796606a90d2495f7f",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/510M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d5c9c39f7b2145c1932eef10ef1053b5",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/39.0 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "304903f465214ea2a58292b87205b7e2",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/226k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "dd2efccff6ae46f080a2a95ee1507420",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/112 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('/content/activebus_bert_final/tokenizer_config.json',\n",
              " '/content/activebus_bert_final/special_tokens_map.json',\n",
              " '/content/activebus_bert_final/vocab.txt',\n",
              " '/content/activebus_bert_final/added_tokens.json',\n",
              " '/content/activebus_bert_final/tokenizer.json')"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "model_name = 'activebus/BERT-XD_Review'\n",
        "model = AutoModelForMaskedLM.from_pretrained(model_name)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "tokenizer.save_pretrained('/content/activebus_bert_final')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v0hIgVAzx1au",
        "outputId": "85d919e9-6e02-4035-e086-fc6d8f68e46f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "count    6136.000000\n",
              "mean       32.192308\n",
              "std        35.578546\n",
              "min         2.000000\n",
              "50%        20.000000\n",
              "80%        47.000000\n",
              "90%        73.000000\n",
              "95%        99.250000\n",
              "99%       172.650000\n",
              "max       407.000000\n",
              "Name: Review, dtype: float64"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "train_data['Review'].apply(lambda x:tokenizer.tokenize(x)).str.len().describe([0.8,0.9,0.95,0.99])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LRFbEpeIPuZu",
        "outputId": "689402d0-7b6c-4436-f38a-c3ef60ad0797"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Token indices sequence length is longer than the specified maximum sequence length for this model (730 > 512). Running this sequence through the model will result in indexing errors\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "count    2631.000000\n",
              "mean       32.530597\n",
              "std        39.368906\n",
              "min         2.000000\n",
              "50%        20.000000\n",
              "80%        48.000000\n",
              "90%        73.000000\n",
              "95%        97.000000\n",
              "99%       162.100000\n",
              "max       730.000000\n",
              "Name: Review, dtype: float64"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "test_data['Review'].apply(lambda x:tokenizer.tokenize(x)).str.len().describe([0.8,0.9,0.95,0.99])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "ZlkzVTf5x1XN",
        "outputId": "3952f626-491c-4e46-b384-9f76aa9b2cfd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "***** Running training *****\n",
            "  Num examples = 8767\n",
            "  Num Epochs = 25\n",
            "  Instantaneous batch size per device = 16\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
            "  Gradient Accumulation steps = 1\n",
            "  Total optimization steps = 13700\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='13700' max='13700' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [13700/13700 1:18:16, Epoch 25/25]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>2.333600</td>\n",
              "      <td>1.996112</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1000</td>\n",
              "      <td>2.178800</td>\n",
              "      <td>1.895074</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1500</td>\n",
              "      <td>2.114500</td>\n",
              "      <td>1.818793</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2000</td>\n",
              "      <td>2.034000</td>\n",
              "      <td>1.783606</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2500</td>\n",
              "      <td>1.966600</td>\n",
              "      <td>1.706522</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3000</td>\n",
              "      <td>1.924200</td>\n",
              "      <td>1.655064</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3500</td>\n",
              "      <td>1.870200</td>\n",
              "      <td>1.610308</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4000</td>\n",
              "      <td>1.816200</td>\n",
              "      <td>1.554853</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4500</td>\n",
              "      <td>1.757100</td>\n",
              "      <td>1.504841</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5000</td>\n",
              "      <td>1.729100</td>\n",
              "      <td>1.492055</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5500</td>\n",
              "      <td>1.693500</td>\n",
              "      <td>1.436760</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6000</td>\n",
              "      <td>1.656800</td>\n",
              "      <td>1.393798</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6500</td>\n",
              "      <td>1.626100</td>\n",
              "      <td>1.377941</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7000</td>\n",
              "      <td>1.558800</td>\n",
              "      <td>1.335421</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7500</td>\n",
              "      <td>1.531200</td>\n",
              "      <td>1.307318</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8000</td>\n",
              "      <td>1.500800</td>\n",
              "      <td>1.313441</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8500</td>\n",
              "      <td>1.460800</td>\n",
              "      <td>1.267447</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9000</td>\n",
              "      <td>1.442500</td>\n",
              "      <td>1.230588</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9500</td>\n",
              "      <td>1.420500</td>\n",
              "      <td>1.188880</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10000</td>\n",
              "      <td>1.402700</td>\n",
              "      <td>1.189335</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10500</td>\n",
              "      <td>1.361500</td>\n",
              "      <td>1.151274</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11000</td>\n",
              "      <td>1.365700</td>\n",
              "      <td>1.116038</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11500</td>\n",
              "      <td>1.337500</td>\n",
              "      <td>1.100800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12000</td>\n",
              "      <td>1.284800</td>\n",
              "      <td>1.102411</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12500</td>\n",
              "      <td>1.312100</td>\n",
              "      <td>1.091678</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>13000</td>\n",
              "      <td>1.286600</td>\n",
              "      <td>1.087640</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>13500</td>\n",
              "      <td>1.256700</td>\n",
              "      <td>1.080495</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 8767\n",
            "  Batch size = 16\n",
            "Saving model checkpoint to uhack_sentiments/checkpoint-500\n",
            "Configuration saved in uhack_sentiments/checkpoint-500/config.json\n",
            "Model weights saved in uhack_sentiments/checkpoint-500/pytorch_model.bin\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 8767\n",
            "  Batch size = 16\n",
            "Saving model checkpoint to uhack_sentiments/checkpoint-1000\n",
            "Configuration saved in uhack_sentiments/checkpoint-1000/config.json\n",
            "Model weights saved in uhack_sentiments/checkpoint-1000/pytorch_model.bin\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 8767\n",
            "  Batch size = 16\n",
            "Saving model checkpoint to uhack_sentiments/checkpoint-1500\n",
            "Configuration saved in uhack_sentiments/checkpoint-1500/config.json\n",
            "Model weights saved in uhack_sentiments/checkpoint-1500/pytorch_model.bin\n",
            "Deleting older checkpoint [uhack_sentiments/checkpoint-500] due to args.save_total_limit\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 8767\n",
            "  Batch size = 16\n",
            "Saving model checkpoint to uhack_sentiments/checkpoint-2000\n",
            "Configuration saved in uhack_sentiments/checkpoint-2000/config.json\n",
            "Model weights saved in uhack_sentiments/checkpoint-2000/pytorch_model.bin\n",
            "Deleting older checkpoint [uhack_sentiments/checkpoint-1000] due to args.save_total_limit\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 8767\n",
            "  Batch size = 16\n",
            "Saving model checkpoint to uhack_sentiments/checkpoint-2500\n",
            "Configuration saved in uhack_sentiments/checkpoint-2500/config.json\n",
            "Model weights saved in uhack_sentiments/checkpoint-2500/pytorch_model.bin\n",
            "Deleting older checkpoint [uhack_sentiments/checkpoint-1500] due to args.save_total_limit\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 8767\n",
            "  Batch size = 16\n",
            "Saving model checkpoint to uhack_sentiments/checkpoint-3000\n",
            "Configuration saved in uhack_sentiments/checkpoint-3000/config.json\n",
            "Model weights saved in uhack_sentiments/checkpoint-3000/pytorch_model.bin\n",
            "Deleting older checkpoint [uhack_sentiments/checkpoint-2000] due to args.save_total_limit\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 8767\n",
            "  Batch size = 16\n",
            "Saving model checkpoint to uhack_sentiments/checkpoint-3500\n",
            "Configuration saved in uhack_sentiments/checkpoint-3500/config.json\n",
            "Model weights saved in uhack_sentiments/checkpoint-3500/pytorch_model.bin\n",
            "Deleting older checkpoint [uhack_sentiments/checkpoint-2500] due to args.save_total_limit\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 8767\n",
            "  Batch size = 16\n",
            "Saving model checkpoint to uhack_sentiments/checkpoint-4000\n",
            "Configuration saved in uhack_sentiments/checkpoint-4000/config.json\n",
            "Model weights saved in uhack_sentiments/checkpoint-4000/pytorch_model.bin\n",
            "Deleting older checkpoint [uhack_sentiments/checkpoint-3000] due to args.save_total_limit\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 8767\n",
            "  Batch size = 16\n",
            "Saving model checkpoint to uhack_sentiments/checkpoint-4500\n",
            "Configuration saved in uhack_sentiments/checkpoint-4500/config.json\n",
            "Model weights saved in uhack_sentiments/checkpoint-4500/pytorch_model.bin\n",
            "Deleting older checkpoint [uhack_sentiments/checkpoint-3500] due to args.save_total_limit\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 8767\n",
            "  Batch size = 16\n",
            "Saving model checkpoint to uhack_sentiments/checkpoint-5000\n",
            "Configuration saved in uhack_sentiments/checkpoint-5000/config.json\n",
            "Model weights saved in uhack_sentiments/checkpoint-5000/pytorch_model.bin\n",
            "Deleting older checkpoint [uhack_sentiments/checkpoint-4000] due to args.save_total_limit\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 8767\n",
            "  Batch size = 16\n",
            "Saving model checkpoint to uhack_sentiments/checkpoint-5500\n",
            "Configuration saved in uhack_sentiments/checkpoint-5500/config.json\n",
            "Model weights saved in uhack_sentiments/checkpoint-5500/pytorch_model.bin\n",
            "Deleting older checkpoint [uhack_sentiments/checkpoint-4500] due to args.save_total_limit\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 8767\n",
            "  Batch size = 16\n",
            "Saving model checkpoint to uhack_sentiments/checkpoint-6000\n",
            "Configuration saved in uhack_sentiments/checkpoint-6000/config.json\n",
            "Model weights saved in uhack_sentiments/checkpoint-6000/pytorch_model.bin\n",
            "Deleting older checkpoint [uhack_sentiments/checkpoint-5000] due to args.save_total_limit\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 8767\n",
            "  Batch size = 16\n",
            "Saving model checkpoint to uhack_sentiments/checkpoint-6500\n",
            "Configuration saved in uhack_sentiments/checkpoint-6500/config.json\n",
            "Model weights saved in uhack_sentiments/checkpoint-6500/pytorch_model.bin\n",
            "Deleting older checkpoint [uhack_sentiments/checkpoint-5500] due to args.save_total_limit\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 8767\n",
            "  Batch size = 16\n",
            "Saving model checkpoint to uhack_sentiments/checkpoint-7000\n",
            "Configuration saved in uhack_sentiments/checkpoint-7000/config.json\n",
            "Model weights saved in uhack_sentiments/checkpoint-7000/pytorch_model.bin\n",
            "Deleting older checkpoint [uhack_sentiments/checkpoint-6000] due to args.save_total_limit\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 8767\n",
            "  Batch size = 16\n",
            "Saving model checkpoint to uhack_sentiments/checkpoint-7500\n",
            "Configuration saved in uhack_sentiments/checkpoint-7500/config.json\n",
            "Model weights saved in uhack_sentiments/checkpoint-7500/pytorch_model.bin\n",
            "Deleting older checkpoint [uhack_sentiments/checkpoint-6500] due to args.save_total_limit\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 8767\n",
            "  Batch size = 16\n",
            "Saving model checkpoint to uhack_sentiments/checkpoint-8000\n",
            "Configuration saved in uhack_sentiments/checkpoint-8000/config.json\n",
            "Model weights saved in uhack_sentiments/checkpoint-8000/pytorch_model.bin\n",
            "Deleting older checkpoint [uhack_sentiments/checkpoint-7000] due to args.save_total_limit\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 8767\n",
            "  Batch size = 16\n",
            "Saving model checkpoint to uhack_sentiments/checkpoint-8500\n",
            "Configuration saved in uhack_sentiments/checkpoint-8500/config.json\n",
            "Model weights saved in uhack_sentiments/checkpoint-8500/pytorch_model.bin\n",
            "Deleting older checkpoint [uhack_sentiments/checkpoint-7500] due to args.save_total_limit\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 8767\n",
            "  Batch size = 16\n",
            "Saving model checkpoint to uhack_sentiments/checkpoint-9000\n",
            "Configuration saved in uhack_sentiments/checkpoint-9000/config.json\n",
            "Model weights saved in uhack_sentiments/checkpoint-9000/pytorch_model.bin\n",
            "Deleting older checkpoint [uhack_sentiments/checkpoint-8000] due to args.save_total_limit\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 8767\n",
            "  Batch size = 16\n",
            "Saving model checkpoint to uhack_sentiments/checkpoint-9500\n",
            "Configuration saved in uhack_sentiments/checkpoint-9500/config.json\n",
            "Model weights saved in uhack_sentiments/checkpoint-9500/pytorch_model.bin\n",
            "Deleting older checkpoint [uhack_sentiments/checkpoint-8500] due to args.save_total_limit\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 8767\n",
            "  Batch size = 16\n",
            "Saving model checkpoint to uhack_sentiments/checkpoint-10000\n",
            "Configuration saved in uhack_sentiments/checkpoint-10000/config.json\n",
            "Model weights saved in uhack_sentiments/checkpoint-10000/pytorch_model.bin\n",
            "Deleting older checkpoint [uhack_sentiments/checkpoint-9000] due to args.save_total_limit\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 8767\n",
            "  Batch size = 16\n",
            "Saving model checkpoint to uhack_sentiments/checkpoint-10500\n",
            "Configuration saved in uhack_sentiments/checkpoint-10500/config.json\n",
            "Model weights saved in uhack_sentiments/checkpoint-10500/pytorch_model.bin\n",
            "Deleting older checkpoint [uhack_sentiments/checkpoint-9500] due to args.save_total_limit\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 8767\n",
            "  Batch size = 16\n",
            "Saving model checkpoint to uhack_sentiments/checkpoint-11000\n",
            "Configuration saved in uhack_sentiments/checkpoint-11000/config.json\n",
            "Model weights saved in uhack_sentiments/checkpoint-11000/pytorch_model.bin\n",
            "Deleting older checkpoint [uhack_sentiments/checkpoint-10000] due to args.save_total_limit\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 8767\n",
            "  Batch size = 16\n",
            "Saving model checkpoint to uhack_sentiments/checkpoint-11500\n",
            "Configuration saved in uhack_sentiments/checkpoint-11500/config.json\n",
            "Model weights saved in uhack_sentiments/checkpoint-11500/pytorch_model.bin\n",
            "Deleting older checkpoint [uhack_sentiments/checkpoint-10500] due to args.save_total_limit\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 8767\n",
            "  Batch size = 16\n",
            "Saving model checkpoint to uhack_sentiments/checkpoint-12000\n",
            "Configuration saved in uhack_sentiments/checkpoint-12000/config.json\n",
            "Model weights saved in uhack_sentiments/checkpoint-12000/pytorch_model.bin\n",
            "Deleting older checkpoint [uhack_sentiments/checkpoint-11000] due to args.save_total_limit\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 8767\n",
            "  Batch size = 16\n",
            "Saving model checkpoint to uhack_sentiments/checkpoint-12500\n",
            "Configuration saved in uhack_sentiments/checkpoint-12500/config.json\n",
            "Model weights saved in uhack_sentiments/checkpoint-12500/pytorch_model.bin\n",
            "Deleting older checkpoint [uhack_sentiments/checkpoint-11500] due to args.save_total_limit\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 8767\n",
            "  Batch size = 16\n",
            "Saving model checkpoint to uhack_sentiments/checkpoint-13000\n",
            "Configuration saved in uhack_sentiments/checkpoint-13000/config.json\n",
            "Model weights saved in uhack_sentiments/checkpoint-13000/pytorch_model.bin\n",
            "Deleting older checkpoint [uhack_sentiments/checkpoint-12000] due to args.save_total_limit\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 8767\n",
            "  Batch size = 16\n",
            "Saving model checkpoint to uhack_sentiments/checkpoint-13500\n",
            "Configuration saved in uhack_sentiments/checkpoint-13500/config.json\n",
            "Model weights saved in uhack_sentiments/checkpoint-13500/pytorch_model.bin\n",
            "Deleting older checkpoint [uhack_sentiments/checkpoint-12500] due to args.save_total_limit\n",
            "\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "Loading best model from uhack_sentiments/checkpoint-13500 (score: 1.0804953575134277).\n",
            "Saving model checkpoint to /content/activebus_bert_final\n",
            "Configuration saved in /content/activebus_bert_final/config.json\n",
            "Model weights saved in /content/activebus_bert_final/pytorch_model.bin\n"
          ]
        }
      ],
      "source": [
        "train_dataset = LineByLineTextDataset(\n",
        "    tokenizer=tokenizer,\n",
        "    file_path=\"text.txt\", #mention train text file here\n",
        "    block_size=160)\n",
        "\n",
        "valid_dataset = LineByLineTextDataset(\n",
        "    tokenizer=tokenizer,\n",
        "    file_path=\"text.txt\", #mention valid text file here\n",
        "    block_size=160)\n",
        "\n",
        "data_collator = DataCollatorForLanguageModeling(\n",
        "    tokenizer=tokenizer, mlm=True, mlm_probability=0.15)\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"uhack_sentiments\", #select model path for checkpoint\n",
        "    overwrite_output_dir=True,\n",
        "    num_train_epochs=25,\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=16,\n",
        "    evaluation_strategy= 'steps',\n",
        "    save_total_limit=2,\n",
        "    # eval_steps=200,\n",
        "    metric_for_best_model='eval_loss',\n",
        "    greater_is_better=False,\n",
        "    load_best_model_at_end =True,\n",
        "    prediction_loss_only=True,\n",
        "    report_to='none')\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    data_collator=data_collator,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=valid_dataset)\n",
        "trainer.train()\n",
        "trainer.save_model('/content/activebus_bert_final')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UJbuGHikx1UV"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jnzjI29_fTGg",
        "outputId": "87d6a70e-e4be-47a9-bca5-6934f7f70aee"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "import os\n",
        "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
        "\n",
        "import sys\n",
        "\n",
        "import re\n",
        "import math\n",
        "import random\n",
        "import time\n",
        "import warnings\n",
        "\n",
        "from torch.optim.swa_utils import AveragedModel, SWALR\n",
        "    \n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import transformers as T\n",
        "from sklearn.metrics import fbeta_score\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from tqdm.notebook import tqdm\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "from transformers import AutoTokenizer, AutoModel, AdamW,AutoModelForSequenceClassification\n",
        "from torch.optim import AdamW\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "from tqdm import tqdm\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "device"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "d_S7ZF6QVDls"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "-Yc-oXOQfbxP"
      },
      "outputs": [],
      "source": [
        "DATA_DIR = \"/content/\"\n",
        "OUTPUT_DIR = \"/content/\"\n",
        "MODEL_NAME=\"/content/activebus_bert_final\"\n",
        "TARGETS=['Delivery and Customer Support',\t'Dimensions','Functionality','Price',\t'Quality','Polarity']"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for Target in tqdm(TARGETS):\n",
        "    def init_logger(log_file=OUTPUT_DIR + \"train.log\"):\n",
        "        from logging import INFO, FileHandler, Formatter, StreamHandler, getLogger\n",
        "\n",
        "        logger = getLogger(__name__)\n",
        "        logger.setLevel(INFO)\n",
        "        handler1 = StreamHandler()\n",
        "        handler1.setFormatter(Formatter(\"%(message)s\"))\n",
        "        handler2 = FileHandler(filename=log_file)\n",
        "        handler2.setFormatter(Formatter(\"%(message)s\"))\n",
        "        logger.addHandler(handler1)\n",
        "        logger.addHandler(handler2)\n",
        "        return logger\n",
        "\n",
        "    LOGGER = init_logger()\n",
        "\n",
        "    def seed_torch(seed=13):\n",
        "        random.seed(seed)\n",
        "        os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
        "        np.random.seed(seed)\n",
        "        torch.manual_seed(seed)\n",
        "        torch.cuda.manual_seed(seed)\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "        torch.backends.cudnn.deterministic = True\n",
        "\n",
        "    seed = 13\n",
        "    seed_torch(seed)\n",
        "\n",
        "    \n",
        "    train = pd.read_csv(DATA_DIR + \"train.csv\",usecols=['Review',Target])\n",
        "    test = pd.read_csv(DATA_DIR + \"test.csv\",usecols=['Review',Target])\n",
        "    train['Review']=train['Review'].str.lower()\n",
        "    test['Review']=test['Review'].str.lower()\n",
        "    sub = pd.read_csv(\"/content/submission.csv\")\n",
        "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "    \n",
        "    def get_train_data(train):\n",
        "\n",
        "        Fold = StratifiedKFold(n_splits=5, shuffle=True, random_state=seed)\n",
        "        for n, (train_index, val_index) in enumerate(Fold.split(train, train[Target])):\n",
        "            train.loc[val_index, \"fold\"] = int(n)\n",
        "        train[\"fold\"] = train[\"fold\"].astype(np.uint8)\n",
        "\n",
        "        return train\n",
        "\n",
        "    def get_test_data(test):\n",
        "        return test\n",
        "\n",
        "    train = get_train_data(train)\n",
        "\n",
        "    class BaseDataset(Dataset):\n",
        "        def __init__(self, df, model_name, include_labels=True):\n",
        "            tokenizer = T.BertTokenizer.from_pretrained(model_name)\n",
        "\n",
        "            self.df = df\n",
        "            self.include_labels = include_labels\n",
        "\n",
        "            self.title = df[\"Review\"].tolist()\n",
        "            self.encoded = tokenizer.batch_encode_plus(\n",
        "                self.title,\n",
        "                padding = 'max_length',            \n",
        "                max_length = 200,\n",
        "                truncation = True,\n",
        "                return_attention_mask=True\n",
        "            )\n",
        "            \n",
        "            if self.include_labels:\n",
        "                self.labels = df[Target].values\n",
        "\n",
        "        def __len__(self):\n",
        "            return len(self.df)\n",
        "\n",
        "        def __getitem__(self, idx):\n",
        "            input_ids = torch.tensor(self.encoded['input_ids'][idx])\n",
        "            attention_mask = torch.tensor(self.encoded['attention_mask'][idx])\n",
        "\n",
        "            if self.include_labels:\n",
        "                label = torch.tensor(self.labels[idx]).float()\n",
        "                return input_ids, attention_mask, label\n",
        "\n",
        "            return input_ids, attention_mask\n",
        "\n",
        "    class BaseModel(nn.Module):\n",
        "        def __init__(self, model_name):\n",
        "            super().__init__()\n",
        "\n",
        "            self.model = T.BertForSequenceClassification.from_pretrained(model_name, num_labels=1)\n",
        "            self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "        def forward(self, input_ids, attention_mask):\n",
        "            out = self.model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "            out = self.sigmoid(out.logits).squeeze()\n",
        "\n",
        "            return out\n",
        "\n",
        "    class AverageMeter(object):\n",
        "        \"\"\"Computes and stores the average and current value\"\"\"\n",
        "\n",
        "        def __init__(self):\n",
        "            self.reset()\n",
        "\n",
        "        def reset(self):\n",
        "            self.val = 0\n",
        "            self.avg = 0\n",
        "            self.sum = 0\n",
        "            self.count = 0\n",
        "\n",
        "        def update(self, val, n=1):\n",
        "            self.val = val\n",
        "            self.sum += val * n\n",
        "            self.count += n\n",
        "            self.avg = self.sum / self.count\n",
        "\n",
        "\n",
        "    def asMinutes(s):\n",
        "        m = math.floor(s / 60)\n",
        "        s -= m * 60\n",
        "        return \"%dm %ds\" % (m, s)\n",
        "\n",
        "\n",
        "    def timeSince(since, percent):\n",
        "        now = time.time()\n",
        "        s = now - since\n",
        "        es = s / (percent)\n",
        "        rs = es - s\n",
        "        return \"%s (remain %s)\" % (asMinutes(s), asMinutes(rs))\n",
        "\n",
        "    def train_fn(train_loader, model, criterion, optimizer, epoch, device):\n",
        "        start = end = time.time()\n",
        "        losses = AverageMeter()\n",
        "\n",
        "        # switch to train mode\n",
        "        model.train()\n",
        "\n",
        "        for step, (input_ids, attention_mask, labels) in enumerate(train_loader):\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            input_ids = input_ids.to(device)\n",
        "            attention_mask = attention_mask.to(device)\n",
        "            labels = labels.to(device)\n",
        "            batch_size = labels.size(0)\n",
        "\n",
        "            y_preds = model(input_ids, attention_mask)\n",
        "\n",
        "            loss = criterion(y_preds, labels)\n",
        "\n",
        "            # record loss\n",
        "            losses.update(loss.item(), batch_size)\n",
        "            loss.backward()\n",
        "\n",
        "            optimizer.step()\n",
        "\n",
        "            if step % 100 == 0 or step == (len(train_loader) - 1):\n",
        "                print(\n",
        "                    f\"Epoch: [{epoch + 1}][{step}/{len(train_loader)}] \"\n",
        "                    f\"Elapsed {timeSince(start, float(step + 1) / len(train_loader)):s} \"\n",
        "                    f\"Loss: {losses.avg:.4f} \"\n",
        "                )\n",
        "\n",
        "        return losses.avg\n",
        "\n",
        "    def valid_fn(valid_loader, model, criterion, device):\n",
        "        start = end = time.time()\n",
        "        losses = AverageMeter()\n",
        "\n",
        "        # switch to evaluation mode\n",
        "        model.eval()\n",
        "        preds = []\n",
        "\n",
        "        for step, (input_ids, attention_mask, labels) in enumerate(valid_loader):\n",
        "            input_ids = input_ids.to(device)\n",
        "            attention_mask = attention_mask.to(device)\n",
        "            labels = labels.to(device)\n",
        "            batch_size = labels.size(0)\n",
        "            # compute loss\n",
        "            with torch.no_grad():\n",
        "                y_preds = model(input_ids, attention_mask)\n",
        "            \n",
        "            loss = criterion(y_preds, labels)\n",
        "            losses.update(loss.item(), batch_size)\n",
        "\n",
        "            # record score\n",
        "            preds.append(y_preds.to(\"cpu\").numpy())\n",
        "\n",
        "            if step % 100 == 0 or step == (len(valid_loader) - 1):\n",
        "                print(\n",
        "                    f\"EVAL: [{step}/{len(valid_loader)}] \"\n",
        "                    f\"Elapsed {timeSince(start, float(step + 1) / len(valid_loader)):s} \"\n",
        "                    f\"Loss: {losses.avg:.4f} \"\n",
        "                )\n",
        "\n",
        "        predictions = np.concatenate(preds)\n",
        "        return losses.avg, predictions\n",
        "\n",
        "    def inference():\n",
        "        predictions = []\n",
        "\n",
        "        test_dataset = BaseDataset(test, MODEL_NAME, include_labels=False)\n",
        "        test_loader = DataLoader(\n",
        "            test_dataset, batch_size=16, shuffle=False, num_workers=4, pin_memory=True\n",
        "        )\n",
        "\n",
        "        for fold in range(5):\n",
        "            LOGGER.info(f\"========== model: bert-base-uncased fold: {fold} inference ==========\")\n",
        "            model = BaseModel(MODEL_NAME)\n",
        "            model.to(device)\n",
        "            model.load_state_dict(torch.load(OUTPUT_DIR + f\"bert-base-uncased_fold{fold}_best.pth\")[\"model\"])\n",
        "            model.eval()\n",
        "            preds = []\n",
        "            for i, (input_ids, attention_mask) in tqdm(enumerate(test_loader), total=len(test_loader)):\n",
        "                input_ids = input_ids.to(device)\n",
        "                attention_mask = attention_mask.to(device)\n",
        "                with torch.no_grad():\n",
        "                    y_preds = model(input_ids, attention_mask)\n",
        "                preds.append(y_preds.to(\"cpu\").numpy())\n",
        "            preds = np.concatenate(preds)\n",
        "            predictions.append(preds)\n",
        "        predictions = np.mean(predictions, axis=0)\n",
        "\n",
        "        return predictions\n",
        "\n",
        "    def train_loop(train, fold):\n",
        "\n",
        "        LOGGER.info(f\"========== fold: {fold} training ==========\")\n",
        "\n",
        "        # ====================================================\n",
        "        # Data Loader\n",
        "        # ====================================================\n",
        "        trn_idx = train[train[\"fold\"] != fold].index\n",
        "        val_idx = train[train[\"fold\"] == fold].index\n",
        "\n",
        "        train_folds = train.loc[trn_idx].reset_index(drop=True)\n",
        "        valid_folds = train.loc[val_idx].reset_index(drop=True)\n",
        "\n",
        "        train_dataset = BaseDataset(train_folds,MODEL_NAME)\n",
        "        valid_dataset = BaseDataset(valid_folds, MODEL_NAME)\n",
        "\n",
        "        train_loader = DataLoader(\n",
        "            train_dataset,\n",
        "            batch_size=16,\n",
        "            shuffle=True,\n",
        "            num_workers=4,\n",
        "            pin_memory=True,\n",
        "            drop_last=True,\n",
        "        )\n",
        "        valid_loader = DataLoader(\n",
        "            valid_dataset,\n",
        "            batch_size=16,\n",
        "            shuffle=False,\n",
        "            num_workers=4,\n",
        "            pin_memory=True,\n",
        "            drop_last=False,\n",
        "        )\n",
        "\n",
        "        # ====================================================\n",
        "        # Model\n",
        "        # ====================================================\n",
        "        model = BaseModel(MODEL_NAME)\n",
        "        model.to(device)\n",
        "\n",
        "        optimizer = T.AdamW(model.parameters(),lr=2e-5)\n",
        "\n",
        "        criterion = nn.BCELoss()\n",
        "\n",
        "        # ====================================================\n",
        "        # Loop\n",
        "        # ====================================================\n",
        "        best_score = 100\n",
        "        best_loss = np.inf\n",
        "\n",
        "        for epoch in range(5):\n",
        "            start_time = time.time()\n",
        "            \n",
        "            # train\n",
        "            avg_loss = train_fn(train_loader, model, criterion, optimizer, epoch, device)\n",
        "\n",
        "            # eval\n",
        "            avg_val_loss, preds = valid_fn(valid_loader, model, criterion, device)\n",
        "            valid_labels = valid_folds[Target].values\n",
        "\n",
        "            # scoring\n",
        "            score = log_loss(valid_labels, preds)\n",
        "            print(f\"Log loss for {epoch} is {score}\")\n",
        "            elapsed = time.time() - start_time\n",
        "            LOGGER.info(\n",
        "                f\"Epoch {epoch+1} - avg_train_loss: {avg_loss:.4f}  avg_val_loss: {avg_val_loss:.4f}  time: {elapsed:.0f}s\"\n",
        "            )\n",
        "            LOGGER.info(f\"Epoch {epoch+1} - Score: {score}\")\n",
        "\n",
        "            if score < best_score:\n",
        "                best_score = score\n",
        "                LOGGER.info(f\"Epoch {epoch+1} - Save Best Score: {best_score:.4f} Model\")\n",
        "                torch.save(\n",
        "                    {\"model\": model.state_dict(), \"preds\": preds}, OUTPUT_DIR + f\"bert-base-uncased_fold{fold}_best.pth\"\n",
        "                )\n",
        "\n",
        "        check_point = torch.load(OUTPUT_DIR + f\"bert-base-uncased_fold{fold}_best.pth\")\n",
        "\n",
        "        valid_folds[\"preds\"] = check_point[\"preds\"]\n",
        "\n",
        "        return valid_folds\n",
        "\n",
        "    from sklearn.metrics import log_loss\n",
        "    def get_result(result_df):\n",
        "        preds = result_df[\"preds\"].values\n",
        "        labels = result_df[Target].values\n",
        "        score = log_loss(labels, preds)\n",
        "        LOGGER.info(f\"Score: {score:<.5f}\")\n",
        "\n",
        "    def main():\n",
        "        # Training\n",
        "        oof_df = pd.DataFrame()\n",
        "        for fold in range(5):\n",
        "            _oof_df = train_loop(train, fold)\n",
        "            oof_df = pd.concat([oof_df, _oof_df])\n",
        "            LOGGER.info(f\"========== fold: {fold} result ==========\")\n",
        "            get_result(_oof_df)\n",
        "            \n",
        "        # CV result\n",
        "        LOGGER.info(f\"========== CV ==========\")\n",
        "        get_result(oof_df)\n",
        "        \n",
        "        # Save OOF result\n",
        "        oof_df.to_csv(OUTPUT_DIR + \"oof_df.csv\", index=False)\n",
        "\n",
        "        # Inference\n",
        "        predictions = inference()\n",
        "        \n",
        "        # submission\n",
        "        sub[Target] = predictions\n",
        "        sub.to_csv('/content/submission.csv', index=False)\n",
        "\n",
        "    if __name__ == \"__main__\":\n",
        "        main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y7ej79LNKtGS",
        "outputId": "c17d5e85-eb91-4bbb-a34f-600b83c91483"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/6 [00:00<?, ?it/s]Didn't find file /content/activebus_bert_final/added_tokens.json. We won't load it.\n",
            "loading file /content/activebus_bert_final/vocab.txt\n",
            "loading file /content/activebus_bert_final/tokenizer.json\n",
            "loading file None\n",
            "loading file /content/activebus_bert_final/special_tokens_map.json\n",
            "loading file /content/activebus_bert_final/tokenizer_config.json\n",
            "========== fold: 0 training ==========\n",
            "Didn't find file /content/activebus_bert_final/added_tokens.json. We won't load it.\n",
            "loading file /content/activebus_bert_final/vocab.txt\n",
            "loading file None\n",
            "loading file /content/activebus_bert_final/special_tokens_map.json\n",
            "loading file /content/activebus_bert_final/tokenizer_config.json\n",
            "loading file /content/activebus_bert_final/tokenizer.json\n",
            "Didn't find file /content/activebus_bert_final/added_tokens.json. We won't load it.\n",
            "loading file /content/activebus_bert_final/vocab.txt\n",
            "loading file None\n",
            "loading file /content/activebus_bert_final/special_tokens_map.json\n",
            "loading file /content/activebus_bert_final/tokenizer_config.json\n",
            "loading file /content/activebus_bert_final/tokenizer.json\n",
            "loading configuration file /content/activebus_bert_final/config.json\n",
            "Model config BertConfig {\n",
            "  \"_name_or_path\": \"activebus/BERT-XD_Review\",\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_ids\": 0,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.15.0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "loading weights file /content/activebus_bert_final/pytorch_model.bin\n",
            "Some weights of the model checkpoint at /content/activebus_bert_final were not used when initializing BertForSequenceClassification: ['cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /content/activebus_bert_final and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: [1][0/306] Elapsed 0m 0s (remain 3m 4s) Loss: 0.6982 \n",
            "Epoch: [1][100/306] Elapsed 0m 33s (remain 1m 7s) Loss: 0.1538 \n",
            "Epoch: [1][200/306] Elapsed 1m 5s (remain 0m 34s) Loss: 0.0983 \n",
            "Epoch: [1][300/306] Elapsed 1m 37s (remain 0m 1s) Loss: 0.0811 \n",
            "Epoch: [1][305/306] Elapsed 1m 39s (remain 0m 0s) Loss: 0.0808 \n",
            "EVAL: [0/77] Elapsed 0m 0s (remain 0m 23s) Loss: 0.0190 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1 - avg_train_loss: 0.0808  avg_val_loss: 0.0433  time: 108s\n",
            "Epoch 1 - Score: 0.043295294722900134\n",
            "Epoch 1 - Save Best Score: 0.0433 Model\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EVAL: [76/77] Elapsed 0m 8s (remain 0m 0s) Loss: 0.0433 \n",
            "Log loss for 0 is 0.043295294722900134\n",
            "Epoch: [2][0/306] Elapsed 0m 0s (remain 2m 52s) Loss: 0.0268 \n",
            "Epoch: [2][100/306] Elapsed 0m 33s (remain 1m 7s) Loss: 0.0256 \n",
            "Epoch: [2][200/306] Elapsed 1m 5s (remain 0m 34s) Loss: 0.0253 \n",
            "Epoch: [2][300/306] Elapsed 1m 37s (remain 0m 1s) Loss: 0.0304 \n",
            "Epoch: [2][305/306] Elapsed 1m 39s (remain 0m 0s) Loss: 0.0300 \n",
            "EVAL: [0/77] Elapsed 0m 0s (remain 0m 24s) Loss: 0.0025 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2 - avg_train_loss: 0.0300  avg_val_loss: 0.0256  time: 108s\n",
            "Epoch 2 - Score: 0.02563582293416983\n",
            "Epoch 2 - Save Best Score: 0.0256 Model\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EVAL: [76/77] Elapsed 0m 8s (remain 0m 0s) Loss: 0.0256 \n",
            "Log loss for 1 is 0.02563582293416983\n",
            "Epoch: [3][0/306] Elapsed 0m 0s (remain 2m 44s) Loss: 0.0077 \n",
            "Epoch: [3][100/306] Elapsed 0m 33s (remain 1m 7s) Loss: 0.0067 \n",
            "Epoch: [3][200/306] Elapsed 1m 5s (remain 0m 34s) Loss: 0.0117 \n",
            "Epoch: [3][300/306] Elapsed 1m 37s (remain 0m 1s) Loss: 0.0153 \n",
            "Epoch: [3][305/306] Elapsed 1m 39s (remain 0m 0s) Loss: 0.0158 \n",
            "EVAL: [0/77] Elapsed 0m 0s (remain 0m 24s) Loss: 0.0016 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3 - avg_train_loss: 0.0158  avg_val_loss: 0.0398  time: 108s\n",
            "Epoch 3 - Score: 0.03976276774710789\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EVAL: [76/77] Elapsed 0m 8s (remain 0m 0s) Loss: 0.0398 \n",
            "Log loss for 2 is 0.03976276774710789\n",
            "Epoch: [4][0/306] Elapsed 0m 0s (remain 2m 45s) Loss: 0.0011 \n",
            "Epoch: [4][100/306] Elapsed 0m 32s (remain 1m 6s) Loss: 0.0098 \n",
            "Epoch: [4][200/306] Elapsed 1m 5s (remain 0m 34s) Loss: 0.0096 \n",
            "Epoch: [4][300/306] Elapsed 1m 37s (remain 0m 1s) Loss: 0.0086 \n",
            "Epoch: [4][305/306] Elapsed 1m 39s (remain 0m 0s) Loss: 0.0085 \n",
            "EVAL: [0/77] Elapsed 0m 0s (remain 0m 23s) Loss: 0.0005 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4 - avg_train_loss: 0.0085  avg_val_loss: 0.0364  time: 108s\n",
            "Epoch 4 - Score: 0.036358015847845336\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EVAL: [76/77] Elapsed 0m 8s (remain 0m 0s) Loss: 0.0364 \n",
            "Log loss for 3 is 0.036358015847845336\n",
            "Epoch: [5][0/306] Elapsed 0m 0s (remain 2m 44s) Loss: 0.0005 \n",
            "Epoch: [5][100/306] Elapsed 0m 33s (remain 1m 7s) Loss: 0.0022 \n",
            "Epoch: [5][200/306] Elapsed 1m 5s (remain 0m 34s) Loss: 0.0028 \n",
            "Epoch: [5][300/306] Elapsed 1m 37s (remain 0m 1s) Loss: 0.0024 \n",
            "Epoch: [5][305/306] Elapsed 1m 39s (remain 0m 0s) Loss: 0.0023 \n",
            "EVAL: [0/77] Elapsed 0m 0s (remain 0m 23s) Loss: 0.0002 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5 - avg_train_loss: 0.0023  avg_val_loss: 0.0499  time: 108s\n",
            "Epoch 5 - Score: 0.049926365225112575\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EVAL: [76/77] Elapsed 0m 8s (remain 0m 0s) Loss: 0.0499 \n",
            "Log loss for 4 is 0.049926365225112575\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "========== fold: 0 result ==========\n",
            "Score: 0.02564\n",
            "========== fold: 1 training ==========\n",
            "Didn't find file /content/activebus_bert_final/added_tokens.json. We won't load it.\n",
            "loading file /content/activebus_bert_final/vocab.txt\n",
            "loading file None\n",
            "loading file /content/activebus_bert_final/special_tokens_map.json\n",
            "loading file /content/activebus_bert_final/tokenizer_config.json\n",
            "loading file /content/activebus_bert_final/tokenizer.json\n",
            "Didn't find file /content/activebus_bert_final/added_tokens.json. We won't load it.\n",
            "loading file /content/activebus_bert_final/vocab.txt\n",
            "loading file None\n",
            "loading file /content/activebus_bert_final/special_tokens_map.json\n",
            "loading file /content/activebus_bert_final/tokenizer_config.json\n",
            "loading file /content/activebus_bert_final/tokenizer.json\n",
            "loading configuration file /content/activebus_bert_final/config.json\n",
            "Model config BertConfig {\n",
            "  \"_name_or_path\": \"activebus/BERT-XD_Review\",\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_ids\": 0,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.15.0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "loading weights file /content/activebus_bert_final/pytorch_model.bin\n",
            "Some weights of the model checkpoint at /content/activebus_bert_final were not used when initializing BertForSequenceClassification: ['cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /content/activebus_bert_final and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: [1][0/306] Elapsed 0m 0s (remain 2m 47s) Loss: 0.6000 \n",
            "Epoch: [1][100/306] Elapsed 0m 33s (remain 1m 7s) Loss: 0.1590 \n",
            "Epoch: [1][200/306] Elapsed 1m 5s (remain 0m 34s) Loss: 0.1119 \n",
            "Epoch: [1][300/306] Elapsed 1m 38s (remain 0m 1s) Loss: 0.0873 \n",
            "Epoch: [1][305/306] Elapsed 1m 39s (remain 0m 0s) Loss: 0.0862 \n",
            "EVAL: [0/77] Elapsed 0m 0s (remain 0m 23s) Loss: 0.0030 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1 - avg_train_loss: 0.0862  avg_val_loss: 0.0512  time: 108s\n",
            "Epoch 1 - Score: 0.05117835826335358\n",
            "Epoch 1 - Save Best Score: 0.0512 Model\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EVAL: [76/77] Elapsed 0m 8s (remain 0m 0s) Loss: 0.0512 \n",
            "Log loss for 0 is 0.05117835826335358\n",
            "Epoch: [2][0/306] Elapsed 0m 0s (remain 2m 45s) Loss: 0.0067 \n",
            "Epoch: [2][100/306] Elapsed 0m 33s (remain 1m 7s) Loss: 0.0296 \n",
            "Epoch: [2][200/306] Elapsed 1m 5s (remain 0m 34s) Loss: 0.0309 \n",
            "Epoch: [2][300/306] Elapsed 1m 38s (remain 0m 1s) Loss: 0.0320 \n",
            "Epoch: [2][305/306] Elapsed 1m 39s (remain 0m 0s) Loss: 0.0318 \n",
            "EVAL: [0/77] Elapsed 0m 0s (remain 0m 23s) Loss: 0.0012 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2 - avg_train_loss: 0.0318  avg_val_loss: 0.0354  time: 108s\n",
            "Epoch 2 - Score: 0.03541250387899283\n",
            "Epoch 2 - Save Best Score: 0.0354 Model\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EVAL: [76/77] Elapsed 0m 8s (remain 0m 0s) Loss: 0.0354 \n",
            "Log loss for 1 is 0.03541250387899283\n",
            "Epoch: [3][0/306] Elapsed 0m 0s (remain 2m 44s) Loss: 0.0019 \n",
            "Epoch: [3][100/306] Elapsed 0m 32s (remain 1m 6s) Loss: 0.0071 \n",
            "Epoch: [3][200/306] Elapsed 1m 5s (remain 0m 34s) Loss: 0.0114 \n",
            "Epoch: [3][300/306] Elapsed 1m 37s (remain 0m 1s) Loss: 0.0151 \n",
            "Epoch: [3][305/306] Elapsed 1m 39s (remain 0m 0s) Loss: 0.0150 \n",
            "EVAL: [0/77] Elapsed 0m 0s (remain 0m 23s) Loss: 0.0012 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3 - avg_train_loss: 0.0150  avg_val_loss: 0.0337  time: 108s\n",
            "Epoch 3 - Score: 0.033682545978528884\n",
            "Epoch 3 - Save Best Score: 0.0337 Model\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EVAL: [76/77] Elapsed 0m 8s (remain 0m 0s) Loss: 0.0337 \n",
            "Log loss for 2 is 0.033682545978528884\n",
            "Epoch: [4][0/306] Elapsed 0m 0s (remain 2m 48s) Loss: 0.0490 \n",
            "Epoch: [4][100/306] Elapsed 0m 33s (remain 1m 7s) Loss: 0.0071 \n",
            "Epoch: [4][200/306] Elapsed 1m 5s (remain 0m 34s) Loss: 0.0098 \n",
            "Epoch: [4][300/306] Elapsed 1m 37s (remain 0m 1s) Loss: 0.0088 \n",
            "Epoch: [4][305/306] Elapsed 1m 39s (remain 0m 0s) Loss: 0.0095 \n",
            "EVAL: [0/77] Elapsed 0m 0s (remain 0m 24s) Loss: 0.0003 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4 - avg_train_loss: 0.0095  avg_val_loss: 0.0788  time: 108s\n",
            "Epoch 4 - Score: 0.07880858332815313\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EVAL: [76/77] Elapsed 0m 8s (remain 0m 0s) Loss: 0.0788 \n",
            "Log loss for 3 is 0.07880858332815313\n",
            "Epoch: [5][0/306] Elapsed 0m 0s (remain 2m 45s) Loss: 0.0013 \n",
            "Epoch: [5][100/306] Elapsed 0m 33s (remain 1m 7s) Loss: 0.0121 \n",
            "Epoch: [5][200/306] Elapsed 1m 5s (remain 0m 34s) Loss: 0.0089 \n",
            "Epoch: [5][300/306] Elapsed 1m 37s (remain 0m 1s) Loss: 0.0085 \n",
            "Epoch: [5][305/306] Elapsed 1m 39s (remain 0m 0s) Loss: 0.0084 \n",
            "EVAL: [0/77] Elapsed 0m 0s (remain 0m 24s) Loss: 0.0003 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5 - avg_train_loss: 0.0084  avg_val_loss: 0.0680  time: 108s\n",
            "Epoch 5 - Score: 0.06800685708945249\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EVAL: [76/77] Elapsed 0m 8s (remain 0m 0s) Loss: 0.0680 \n",
            "Log loss for 4 is 0.06800685708945249\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "========== fold: 1 result ==========\n",
            "Score: 0.03368\n",
            "========== fold: 2 training ==========\n",
            "Didn't find file /content/activebus_bert_final/added_tokens.json. We won't load it.\n",
            "loading file /content/activebus_bert_final/vocab.txt\n",
            "loading file None\n",
            "loading file /content/activebus_bert_final/special_tokens_map.json\n",
            "loading file /content/activebus_bert_final/tokenizer_config.json\n",
            "loading file /content/activebus_bert_final/tokenizer.json\n",
            "Didn't find file /content/activebus_bert_final/added_tokens.json. We won't load it.\n",
            "loading file /content/activebus_bert_final/vocab.txt\n",
            "loading file None\n",
            "loading file /content/activebus_bert_final/special_tokens_map.json\n",
            "loading file /content/activebus_bert_final/tokenizer_config.json\n",
            "loading file /content/activebus_bert_final/tokenizer.json\n",
            "loading configuration file /content/activebus_bert_final/config.json\n",
            "Model config BertConfig {\n",
            "  \"_name_or_path\": \"activebus/BERT-XD_Review\",\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_ids\": 0,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.15.0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "loading weights file /content/activebus_bert_final/pytorch_model.bin\n",
            "Some weights of the model checkpoint at /content/activebus_bert_final were not used when initializing BertForSequenceClassification: ['cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /content/activebus_bert_final and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: [1][0/306] Elapsed 0m 0s (remain 2m 54s) Loss: 0.6789 \n",
            "Epoch: [1][100/306] Elapsed 0m 33s (remain 1m 7s) Loss: 0.1668 \n",
            "Epoch: [1][200/306] Elapsed 1m 5s (remain 0m 34s) Loss: 0.1088 \n",
            "Epoch: [1][300/306] Elapsed 1m 38s (remain 0m 1s) Loss: 0.0853 \n",
            "Epoch: [1][305/306] Elapsed 1m 39s (remain 0m 0s) Loss: 0.0841 \n",
            "EVAL: [0/77] Elapsed 0m 0s (remain 0m 23s) Loss: 0.0033 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1 - avg_train_loss: 0.0841  avg_val_loss: 0.0548  time: 108s\n",
            "Epoch 1 - Score: 0.05481156500763417\n",
            "Epoch 1 - Save Best Score: 0.0548 Model\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EVAL: [76/77] Elapsed 0m 8s (remain 0m 0s) Loss: 0.0548 \n",
            "Log loss for 0 is 0.05481156500763417\n",
            "Epoch: [2][0/306] Elapsed 0m 0s (remain 2m 44s) Loss: 0.0041 \n",
            "Epoch: [2][100/306] Elapsed 0m 33s (remain 1m 7s) Loss: 0.0295 \n",
            "Epoch: [2][200/306] Elapsed 1m 5s (remain 0m 34s) Loss: 0.0307 \n",
            "Epoch: [2][300/306] Elapsed 1m 38s (remain 0m 1s) Loss: 0.0280 \n",
            "Epoch: [2][305/306] Elapsed 1m 39s (remain 0m 0s) Loss: 0.0286 \n",
            "EVAL: [0/77] Elapsed 0m 0s (remain 0m 22s) Loss: 0.0008 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2 - avg_train_loss: 0.0286  avg_val_loss: 0.0494  time: 108s\n",
            "Epoch 2 - Score: 0.04940332912050754\n",
            "Epoch 2 - Save Best Score: 0.0494 Model\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EVAL: [76/77] Elapsed 0m 8s (remain 0m 0s) Loss: 0.0494 \n",
            "Log loss for 1 is 0.04940332912050754\n",
            "Epoch: [3][0/306] Elapsed 0m 0s (remain 2m 50s) Loss: 0.0015 \n",
            "Epoch: [3][100/306] Elapsed 0m 33s (remain 1m 7s) Loss: 0.0165 \n",
            "Epoch: [3][200/306] Elapsed 1m 5s (remain 0m 34s) Loss: 0.0118 \n",
            "Epoch: [3][300/306] Elapsed 1m 37s (remain 0m 1s) Loss: 0.0136 \n",
            "Epoch: [3][305/306] Elapsed 1m 39s (remain 0m 0s) Loss: 0.0135 \n",
            "EVAL: [0/77] Elapsed 0m 0s (remain 0m 23s) Loss: 0.0154 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3 - avg_train_loss: 0.0135  avg_val_loss: 0.0498  time: 108s\n",
            "Epoch 3 - Score: 0.04980219794728944\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EVAL: [76/77] Elapsed 0m 8s (remain 0m 0s) Loss: 0.0498 \n",
            "Log loss for 2 is 0.04980219794728944\n",
            "Epoch: [4][0/306] Elapsed 0m 0s (remain 2m 50s) Loss: 0.0037 \n",
            "Epoch: [4][100/306] Elapsed 0m 33s (remain 1m 7s) Loss: 0.0065 \n",
            "Epoch: [4][200/306] Elapsed 1m 5s (remain 0m 34s) Loss: 0.0115 \n",
            "Epoch: [4][300/306] Elapsed 1m 37s (remain 0m 1s) Loss: 0.0100 \n",
            "Epoch: [4][305/306] Elapsed 1m 39s (remain 0m 0s) Loss: 0.0099 \n",
            "EVAL: [0/77] Elapsed 0m 0s (remain 0m 23s) Loss: 0.0070 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4 - avg_train_loss: 0.0099  avg_val_loss: 0.0637  time: 108s\n",
            "Epoch 4 - Score: 0.06365019967335962\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EVAL: [76/77] Elapsed 0m 8s (remain 0m 0s) Loss: 0.0637 \n",
            "Log loss for 3 is 0.06365019967335962\n",
            "Epoch: [5][0/306] Elapsed 0m 0s (remain 2m 57s) Loss: 0.0014 \n",
            "Epoch: [5][100/306] Elapsed 0m 33s (remain 1m 7s) Loss: 0.0016 \n",
            "Epoch: [5][200/306] Elapsed 1m 5s (remain 0m 34s) Loss: 0.0011 \n",
            "Epoch: [5][300/306] Elapsed 1m 38s (remain 0m 1s) Loss: 0.0008 \n",
            "Epoch: [5][305/306] Elapsed 1m 39s (remain 0m 0s) Loss: 0.0008 \n",
            "EVAL: [0/77] Elapsed 0m 0s (remain 0m 24s) Loss: 0.0002 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5 - avg_train_loss: 0.0008  avg_val_loss: 0.0739  time: 108s\n",
            "Epoch 5 - Score: 0.07393353370021577\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EVAL: [76/77] Elapsed 0m 8s (remain 0m 0s) Loss: 0.0739 \n",
            "Log loss for 4 is 0.07393353370021577\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "========== fold: 2 result ==========\n",
            "Score: 0.04940\n",
            "========== fold: 3 training ==========\n",
            "Didn't find file /content/activebus_bert_final/added_tokens.json. We won't load it.\n",
            "loading file /content/activebus_bert_final/vocab.txt\n",
            "loading file None\n",
            "loading file /content/activebus_bert_final/special_tokens_map.json\n",
            "loading file /content/activebus_bert_final/tokenizer_config.json\n",
            "loading file /content/activebus_bert_final/tokenizer.json\n",
            "Didn't find file /content/activebus_bert_final/added_tokens.json. We won't load it.\n",
            "loading file /content/activebus_bert_final/vocab.txt\n",
            "loading file None\n",
            "loading file /content/activebus_bert_final/special_tokens_map.json\n",
            "loading file /content/activebus_bert_final/tokenizer_config.json\n",
            "loading file /content/activebus_bert_final/tokenizer.json\n",
            "loading configuration file /content/activebus_bert_final/config.json\n",
            "Model config BertConfig {\n",
            "  \"_name_or_path\": \"activebus/BERT-XD_Review\",\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_ids\": 0,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.15.0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "loading weights file /content/activebus_bert_final/pytorch_model.bin\n",
            "Some weights of the model checkpoint at /content/activebus_bert_final were not used when initializing BertForSequenceClassification: ['cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /content/activebus_bert_final and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: [1][0/306] Elapsed 0m 0s (remain 2m 50s) Loss: 0.7076 \n",
            "Epoch: [1][100/306] Elapsed 0m 33s (remain 1m 7s) Loss: 0.1428 \n",
            "Epoch: [1][200/306] Elapsed 1m 5s (remain 0m 34s) Loss: 0.0983 \n",
            "Epoch: [1][300/306] Elapsed 1m 38s (remain 0m 1s) Loss: 0.0806 \n",
            "Epoch: [1][305/306] Elapsed 1m 39s (remain 0m 0s) Loss: 0.0798 \n",
            "EVAL: [0/77] Elapsed 0m 0s (remain 0m 23s) Loss: 0.0150 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1 - avg_train_loss: 0.0798  avg_val_loss: 0.0560  time: 108s\n",
            "Epoch 1 - Score: 0.05597383007612125\n",
            "Epoch 1 - Save Best Score: 0.0560 Model\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EVAL: [76/77] Elapsed 0m 8s (remain 0m 0s) Loss: 0.0560 \n",
            "Log loss for 0 is 0.05597383007612125\n",
            "Epoch: [2][0/306] Elapsed 0m 0s (remain 2m 43s) Loss: 0.1913 \n",
            "Epoch: [2][100/306] Elapsed 0m 32s (remain 1m 6s) Loss: 0.0254 \n",
            "Epoch: [2][200/306] Elapsed 1m 5s (remain 0m 34s) Loss: 0.0267 \n",
            "Epoch: [2][300/306] Elapsed 1m 37s (remain 0m 1s) Loss: 0.0242 \n",
            "Epoch: [2][305/306] Elapsed 1m 39s (remain 0m 0s) Loss: 0.0239 \n",
            "EVAL: [0/77] Elapsed 0m 0s (remain 0m 25s) Loss: 0.0059 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2 - avg_train_loss: 0.0239  avg_val_loss: 0.0531  time: 108s\n",
            "Epoch 2 - Score: 0.05307562703857071\n",
            "Epoch 2 - Save Best Score: 0.0531 Model\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EVAL: [76/77] Elapsed 0m 8s (remain 0m 0s) Loss: 0.0531 \n",
            "Log loss for 1 is 0.05307562703857071\n",
            "Epoch: [3][0/306] Elapsed 0m 0s (remain 2m 47s) Loss: 0.0014 \n",
            "Epoch: [3][100/306] Elapsed 0m 32s (remain 1m 6s) Loss: 0.0219 \n",
            "Epoch: [3][200/306] Elapsed 1m 5s (remain 0m 34s) Loss: 0.0137 \n",
            "Epoch: [3][300/306] Elapsed 1m 37s (remain 0m 1s) Loss: 0.0119 \n",
            "Epoch: [3][305/306] Elapsed 1m 39s (remain 0m 0s) Loss: 0.0118 \n",
            "EVAL: [0/77] Elapsed 0m 0s (remain 0m 23s) Loss: 0.0007 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3 - avg_train_loss: 0.0118  avg_val_loss: 0.0813  time: 108s\n",
            "Epoch 3 - Score: 0.0813394023913673\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EVAL: [76/77] Elapsed 0m 8s (remain 0m 0s) Loss: 0.0813 \n",
            "Log loss for 2 is 0.0813394023913673\n",
            "Epoch: [4][0/306] Elapsed 0m 0s (remain 2m 53s) Loss: 0.0011 \n",
            "Epoch: [4][100/306] Elapsed 0m 33s (remain 1m 7s) Loss: 0.0110 \n",
            "Epoch: [4][200/306] Elapsed 1m 5s (remain 0m 34s) Loss: 0.0108 \n",
            "Epoch: [4][300/306] Elapsed 1m 37s (remain 0m 1s) Loss: 0.0097 \n",
            "Epoch: [4][305/306] Elapsed 1m 39s (remain 0m 0s) Loss: 0.0096 \n",
            "EVAL: [0/77] Elapsed 0m 0s (remain 0m 24s) Loss: 0.0022 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4 - avg_train_loss: 0.0096  avg_val_loss: 0.1056  time: 108s\n",
            "Epoch 4 - Score: 0.10563033838624881\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EVAL: [76/77] Elapsed 0m 8s (remain 0m 0s) Loss: 0.1056 \n",
            "Log loss for 3 is 0.10563033838624881\n",
            "Epoch: [5][0/306] Elapsed 0m 0s (remain 2m 45s) Loss: 0.0043 \n",
            "Epoch: [5][100/306] Elapsed 0m 33s (remain 1m 7s) Loss: 0.0064 \n",
            "Epoch: [5][200/306] Elapsed 1m 5s (remain 0m 34s) Loss: 0.0060 \n",
            "Epoch: [5][300/306] Elapsed 1m 37s (remain 0m 1s) Loss: 0.0069 \n",
            "Epoch: [5][305/306] Elapsed 1m 39s (remain 0m 0s) Loss: 0.0068 \n",
            "EVAL: [0/77] Elapsed 0m 0s (remain 0m 23s) Loss: 0.0008 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5 - avg_train_loss: 0.0068  avg_val_loss: 0.0771  time: 108s\n",
            "Epoch 5 - Score: 0.07708081979241553\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EVAL: [76/77] Elapsed 0m 8s (remain 0m 0s) Loss: 0.0771 \n",
            "Log loss for 4 is 0.07708081979241553\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "========== fold: 3 result ==========\n",
            "Score: 0.05308\n",
            "========== fold: 4 training ==========\n",
            "Didn't find file /content/activebus_bert_final/added_tokens.json. We won't load it.\n",
            "loading file /content/activebus_bert_final/vocab.txt\n",
            "loading file None\n",
            "loading file /content/activebus_bert_final/special_tokens_map.json\n",
            "loading file /content/activebus_bert_final/tokenizer_config.json\n",
            "loading file /content/activebus_bert_final/tokenizer.json\n",
            "Didn't find file /content/activebus_bert_final/added_tokens.json. We won't load it.\n",
            "loading file /content/activebus_bert_final/vocab.txt\n",
            "loading file None\n",
            "loading file /content/activebus_bert_final/special_tokens_map.json\n",
            "loading file /content/activebus_bert_final/tokenizer_config.json\n",
            "loading file /content/activebus_bert_final/tokenizer.json\n",
            "loading configuration file /content/activebus_bert_final/config.json\n",
            "Model config BertConfig {\n",
            "  \"_name_or_path\": \"activebus/BERT-XD_Review\",\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_ids\": 0,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.15.0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "loading weights file /content/activebus_bert_final/pytorch_model.bin\n",
            "Some weights of the model checkpoint at /content/activebus_bert_final were not used when initializing BertForSequenceClassification: ['cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /content/activebus_bert_final and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: [1][0/306] Elapsed 0m 0s (remain 2m 50s) Loss: 0.6563 \n",
            "Epoch: [1][100/306] Elapsed 0m 33s (remain 1m 7s) Loss: 0.1588 \n",
            "Epoch: [1][200/306] Elapsed 1m 5s (remain 0m 34s) Loss: 0.1094 \n",
            "Epoch: [1][300/306] Elapsed 1m 37s (remain 0m 1s) Loss: 0.0909 \n",
            "Epoch: [1][305/306] Elapsed 1m 39s (remain 0m 0s) Loss: 0.0913 \n",
            "EVAL: [0/77] Elapsed 0m 0s (remain 0m 23s) Loss: 0.0224 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1 - avg_train_loss: 0.0913  avg_val_loss: 0.0296  time: 108s\n",
            "Epoch 1 - Score: 0.029577971748991003\n",
            "Epoch 1 - Save Best Score: 0.0296 Model\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EVAL: [76/77] Elapsed 0m 8s (remain 0m 0s) Loss: 0.0296 \n",
            "Log loss for 0 is 0.029577971748991003\n",
            "Epoch: [2][0/306] Elapsed 0m 0s (remain 2m 48s) Loss: 0.0178 \n",
            "Epoch: [2][100/306] Elapsed 0m 33s (remain 1m 7s) Loss: 0.0307 \n",
            "Epoch: [2][200/306] Elapsed 1m 5s (remain 0m 34s) Loss: 0.0340 \n",
            "Epoch: [2][300/306] Elapsed 1m 37s (remain 0m 1s) Loss: 0.0319 \n",
            "Epoch: [2][305/306] Elapsed 1m 39s (remain 0m 0s) Loss: 0.0314 \n",
            "EVAL: [0/77] Elapsed 0m 0s (remain 0m 23s) Loss: 0.0074 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2 - avg_train_loss: 0.0314  avg_val_loss: 0.0331  time: 108s\n",
            "Epoch 2 - Score: 0.033146646636367044\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EVAL: [76/77] Elapsed 0m 8s (remain 0m 0s) Loss: 0.0331 \n",
            "Log loss for 1 is 0.033146646636367044\n",
            "Epoch: [3][0/306] Elapsed 0m 0s (remain 2m 49s) Loss: 0.0167 \n",
            "Epoch: [3][100/306] Elapsed 0m 33s (remain 1m 7s) Loss: 0.0082 \n",
            "Epoch: [3][200/306] Elapsed 1m 5s (remain 0m 34s) Loss: 0.0152 \n",
            "Epoch: [3][300/306] Elapsed 1m 38s (remain 0m 1s) Loss: 0.0162 \n",
            "Epoch: [3][305/306] Elapsed 1m 39s (remain 0m 0s) Loss: 0.0160 \n",
            "EVAL: [0/77] Elapsed 0m 0s (remain 0m 24s) Loss: 0.0029 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3 - avg_train_loss: 0.0160  avg_val_loss: 0.0257  time: 108s\n",
            "Epoch 3 - Score: 0.02571444661646266\n",
            "Epoch 3 - Save Best Score: 0.0257 Model\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EVAL: [76/77] Elapsed 0m 8s (remain 0m 0s) Loss: 0.0257 \n",
            "Log loss for 2 is 0.02571444661646266\n",
            "Epoch: [4][0/306] Elapsed 0m 0s (remain 2m 46s) Loss: 0.0013 \n",
            "Epoch: [4][100/306] Elapsed 0m 33s (remain 1m 7s) Loss: 0.0067 \n",
            "Epoch: [4][200/306] Elapsed 1m 5s (remain 0m 34s) Loss: 0.0082 \n",
            "Epoch: [4][300/306] Elapsed 1m 38s (remain 0m 1s) Loss: 0.0074 \n",
            "Epoch: [4][305/306] Elapsed 1m 39s (remain 0m 0s) Loss: 0.0073 \n",
            "EVAL: [0/77] Elapsed 0m 0s (remain 0m 23s) Loss: 0.0011 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4 - avg_train_loss: 0.0073  avg_val_loss: 0.0242  time: 108s\n",
            "Epoch 4 - Score: 0.024172613901540997\n",
            "Epoch 4 - Save Best Score: 0.0242 Model\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EVAL: [76/77] Elapsed 0m 8s (remain 0m 0s) Loss: 0.0242 \n",
            "Log loss for 3 is 0.024172613901540997\n",
            "Epoch: [5][0/306] Elapsed 0m 0s (remain 2m 47s) Loss: 0.0019 \n",
            "Epoch: [5][100/306] Elapsed 0m 33s (remain 1m 7s) Loss: 0.0051 \n",
            "Epoch: [5][200/306] Elapsed 1m 5s (remain 0m 34s) Loss: 0.0059 \n",
            "Epoch: [5][300/306] Elapsed 1m 37s (remain 0m 1s) Loss: 0.0044 \n",
            "Epoch: [5][305/306] Elapsed 1m 39s (remain 0m 0s) Loss: 0.0043 \n",
            "EVAL: [0/77] Elapsed 0m 0s (remain 0m 24s) Loss: 0.0005 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5 - avg_train_loss: 0.0043  avg_val_loss: 0.0269  time: 108s\n",
            "Epoch 5 - Score: 0.026931892604588552\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EVAL: [76/77] Elapsed 0m 8s (remain 0m 0s) Loss: 0.0269 \n",
            "Log loss for 4 is 0.026931892604588552\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "========== fold: 4 result ==========\n",
            "Score: 0.02417\n",
            "========== CV ==========\n",
            "Score: 0.03719\n",
            "Didn't find file /content/activebus_bert_final/added_tokens.json. We won't load it.\n",
            "loading file /content/activebus_bert_final/vocab.txt\n",
            "loading file None\n",
            "loading file /content/activebus_bert_final/special_tokens_map.json\n",
            "loading file /content/activebus_bert_final/tokenizer_config.json\n",
            "loading file /content/activebus_bert_final/tokenizer.json\n",
            "========== model: bert-base-uncased fold: 0 inference ==========\n",
            "loading configuration file /content/activebus_bert_final/config.json\n",
            "Model config BertConfig {\n",
            "  \"_name_or_path\": \"activebus/BERT-XD_Review\",\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_ids\": 0,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.15.0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "loading weights file /content/activebus_bert_final/pytorch_model.bin\n",
            "Some weights of the model checkpoint at /content/activebus_bert_final were not used when initializing BertForSequenceClassification: ['cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /content/activebus_bert_final and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "\n",
            "  0%|          | 0/165 [00:00<?, ?it/s]\u001b[A\n",
            "  1%|          | 1/165 [00:00<00:21,  7.66it/s]\u001b[A\n",
            "  1%|          | 2/165 [00:00<00:20,  8.08it/s]\u001b[A\n",
            "  2%|▏         | 3/165 [00:00<00:19,  8.45it/s]\u001b[A\n",
            "  2%|▏         | 4/165 [00:00<00:18,  8.67it/s]\u001b[A\n",
            "  3%|▎         | 5/165 [00:00<00:18,  8.83it/s]\u001b[A\n",
            "  4%|▎         | 6/165 [00:00<00:17,  8.89it/s]\u001b[A\n",
            "  4%|▍         | 7/165 [00:00<00:17,  8.95it/s]\u001b[A\n",
            "  5%|▍         | 8/165 [00:00<00:17,  8.99it/s]\u001b[A\n",
            "  5%|▌         | 9/165 [00:01<00:17,  9.06it/s]\u001b[A\n",
            "  6%|▌         | 10/165 [00:01<00:17,  9.08it/s]\u001b[A\n",
            "  7%|▋         | 11/165 [00:01<00:17,  9.05it/s]\u001b[A\n",
            "  7%|▋         | 12/165 [00:01<00:16,  9.07it/s]\u001b[A\n",
            "  8%|▊         | 13/165 [00:01<00:16,  9.08it/s]\u001b[A\n",
            "  8%|▊         | 14/165 [00:01<00:16,  9.11it/s]\u001b[A\n",
            "  9%|▉         | 15/165 [00:01<00:16,  9.12it/s]\u001b[A\n",
            " 10%|▉         | 16/165 [00:01<00:16,  9.11it/s]\u001b[A\n",
            " 10%|█         | 17/165 [00:01<00:16,  9.11it/s]\u001b[A\n",
            " 11%|█         | 18/165 [00:02<00:16,  9.12it/s]\u001b[A\n",
            " 12%|█▏        | 19/165 [00:02<00:16,  9.10it/s]\u001b[A\n",
            " 12%|█▏        | 20/165 [00:02<00:15,  9.10it/s]\u001b[A\n",
            " 13%|█▎        | 21/165 [00:02<00:15,  9.09it/s]\u001b[A\n",
            " 13%|█▎        | 22/165 [00:02<00:15,  9.11it/s]\u001b[A\n",
            " 14%|█▍        | 23/165 [00:02<00:15,  9.12it/s]\u001b[A\n",
            " 15%|█▍        | 24/165 [00:02<00:15,  9.09it/s]\u001b[A\n",
            " 15%|█▌        | 25/165 [00:02<00:15,  9.09it/s]\u001b[A\n",
            " 16%|█▌        | 26/165 [00:02<00:15,  9.09it/s]\u001b[A\n",
            " 16%|█▋        | 27/165 [00:03<00:15,  9.09it/s]\u001b[A\n",
            " 17%|█▋        | 28/165 [00:03<00:15,  9.11it/s]\u001b[A\n",
            " 18%|█▊        | 29/165 [00:03<00:14,  9.12it/s]\u001b[A\n",
            " 18%|█▊        | 30/165 [00:03<00:14,  9.12it/s]\u001b[A\n",
            " 19%|█▉        | 31/165 [00:03<00:14,  9.09it/s]\u001b[A\n",
            " 19%|█▉        | 32/165 [00:03<00:14,  9.12it/s]\u001b[A\n",
            " 20%|██        | 33/165 [00:03<00:14,  9.11it/s]\u001b[A\n",
            " 21%|██        | 34/165 [00:03<00:14,  9.09it/s]\u001b[A\n",
            " 21%|██        | 35/165 [00:03<00:14,  9.10it/s]\u001b[A\n",
            " 22%|██▏       | 36/165 [00:03<00:14,  9.11it/s]\u001b[A\n",
            " 22%|██▏       | 37/165 [00:04<00:14,  9.09it/s]\u001b[A\n",
            " 23%|██▎       | 38/165 [00:04<00:13,  9.12it/s]\u001b[A\n",
            " 24%|██▎       | 39/165 [00:04<00:13,  9.11it/s]\u001b[A\n",
            " 24%|██▍       | 40/165 [00:04<00:13,  9.09it/s]\u001b[A\n",
            " 25%|██▍       | 41/165 [00:04<00:13,  9.10it/s]\u001b[A\n",
            " 25%|██▌       | 42/165 [00:04<00:13,  9.09it/s]\u001b[A\n",
            " 26%|██▌       | 43/165 [00:04<00:13,  9.11it/s]\u001b[A\n",
            " 27%|██▋       | 44/165 [00:04<00:13,  9.09it/s]\u001b[A\n",
            " 27%|██▋       | 45/165 [00:04<00:13,  9.09it/s]\u001b[A\n",
            " 28%|██▊       | 46/165 [00:05<00:13,  9.11it/s]\u001b[A\n",
            " 28%|██▊       | 47/165 [00:05<00:12,  9.10it/s]\u001b[A\n",
            " 29%|██▉       | 48/165 [00:05<00:12,  9.08it/s]\u001b[A\n",
            " 30%|██▉       | 49/165 [00:05<00:12,  9.07it/s]\u001b[A\n",
            " 30%|███       | 50/165 [00:05<00:12,  9.07it/s]\u001b[A\n",
            " 31%|███       | 51/165 [00:05<00:12,  9.09it/s]\u001b[A\n",
            " 32%|███▏      | 52/165 [00:05<00:12,  9.11it/s]\u001b[A\n",
            " 32%|███▏      | 53/165 [00:05<00:12,  9.12it/s]\u001b[A\n",
            " 33%|███▎      | 54/165 [00:05<00:12,  9.11it/s]\u001b[A\n",
            " 33%|███▎      | 55/165 [00:06<00:12,  9.12it/s]\u001b[A\n",
            " 34%|███▍      | 56/165 [00:06<00:11,  9.13it/s]\u001b[A\n",
            " 35%|███▍      | 57/165 [00:06<00:11,  9.13it/s]\u001b[A\n",
            " 35%|███▌      | 58/165 [00:06<00:11,  9.12it/s]\u001b[A\n",
            " 36%|███▌      | 59/165 [00:06<00:11,  9.11it/s]\u001b[A\n",
            " 36%|███▋      | 60/165 [00:06<00:11,  9.11it/s]\u001b[A\n",
            " 37%|███▋      | 61/165 [00:06<00:11,  9.13it/s]\u001b[A\n",
            " 38%|███▊      | 62/165 [00:06<00:11,  9.11it/s]\u001b[A\n",
            " 38%|███▊      | 63/165 [00:06<00:11,  9.12it/s]\u001b[A\n",
            " 39%|███▉      | 64/165 [00:07<00:11,  9.11it/s]\u001b[A\n",
            " 39%|███▉      | 65/165 [00:07<00:10,  9.13it/s]\u001b[A\n",
            " 40%|████      | 66/165 [00:07<00:10,  9.12it/s]\u001b[A\n",
            " 41%|████      | 67/165 [00:07<00:10,  9.10it/s]\u001b[A\n",
            " 41%|████      | 68/165 [00:07<00:10,  9.11it/s]\u001b[A\n",
            " 42%|████▏     | 69/165 [00:07<00:10,  9.12it/s]\u001b[A\n",
            " 42%|████▏     | 70/165 [00:07<00:10,  9.10it/s]\u001b[A\n",
            " 43%|████▎     | 71/165 [00:07<00:10,  9.12it/s]\u001b[A\n",
            " 44%|████▎     | 72/165 [00:07<00:10,  9.13it/s]\u001b[A\n",
            " 44%|████▍     | 73/165 [00:08<00:10,  9.09it/s]\u001b[A\n",
            " 45%|████▍     | 74/165 [00:08<00:10,  9.09it/s]\u001b[A\n",
            " 45%|████▌     | 75/165 [00:08<00:09,  9.10it/s]\u001b[A\n",
            " 46%|████▌     | 76/165 [00:08<00:09,  9.09it/s]\u001b[A\n",
            " 47%|████▋     | 77/165 [00:08<00:09,  9.08it/s]\u001b[A\n",
            " 47%|████▋     | 78/165 [00:08<00:09,  9.08it/s]\u001b[A\n",
            " 48%|████▊     | 79/165 [00:08<00:09,  9.09it/s]\u001b[A\n",
            " 48%|████▊     | 80/165 [00:08<00:09,  9.10it/s]\u001b[A\n",
            " 49%|████▉     | 81/165 [00:08<00:09,  9.11it/s]\u001b[A\n",
            " 50%|████▉     | 82/165 [00:09<00:09,  9.13it/s]\u001b[A\n",
            " 50%|█████     | 83/165 [00:09<00:08,  9.13it/s]\u001b[A\n",
            " 51%|█████     | 84/165 [00:09<00:08,  9.14it/s]\u001b[A\n",
            " 52%|█████▏    | 85/165 [00:09<00:08,  9.10it/s]\u001b[A\n",
            " 52%|█████▏    | 86/165 [00:09<00:08,  9.11it/s]\u001b[A\n",
            " 53%|█████▎    | 87/165 [00:09<00:08,  9.12it/s]\u001b[A\n",
            " 53%|█████▎    | 88/165 [00:09<00:08,  9.13it/s]\u001b[A\n",
            " 54%|█████▍    | 89/165 [00:09<00:08,  9.13it/s]\u001b[A\n",
            " 55%|█████▍    | 90/165 [00:09<00:08,  9.11it/s]\u001b[A\n",
            " 55%|█████▌    | 91/165 [00:10<00:08,  9.11it/s]\u001b[A\n",
            " 56%|█████▌    | 92/165 [00:10<00:08,  9.09it/s]\u001b[A\n",
            " 56%|█████▋    | 93/165 [00:10<00:07,  9.10it/s]\u001b[A\n",
            " 57%|█████▋    | 94/165 [00:10<00:07,  9.10it/s]\u001b[A\n",
            " 58%|█████▊    | 95/165 [00:10<00:07,  9.10it/s]\u001b[A\n",
            " 58%|█████▊    | 96/165 [00:10<00:07,  9.09it/s]\u001b[A\n",
            " 59%|█████▉    | 97/165 [00:10<00:07,  9.09it/s]\u001b[A\n",
            " 59%|█████▉    | 98/165 [00:10<00:07,  9.10it/s]\u001b[A\n",
            " 60%|██████    | 99/165 [00:10<00:07,  9.09it/s]\u001b[A\n",
            " 61%|██████    | 100/165 [00:11<00:07,  9.09it/s]\u001b[A\n",
            " 61%|██████    | 101/165 [00:11<00:07,  9.10it/s]\u001b[A\n",
            " 62%|██████▏   | 102/165 [00:11<00:06,  9.11it/s]\u001b[A\n",
            " 62%|██████▏   | 103/165 [00:11<00:06,  9.11it/s]\u001b[A\n",
            " 63%|██████▎   | 104/165 [00:11<00:06,  9.12it/s]\u001b[A\n",
            " 64%|██████▎   | 105/165 [00:11<00:06,  9.11it/s]\u001b[A\n",
            " 64%|██████▍   | 106/165 [00:11<00:06,  9.12it/s]\u001b[A\n",
            " 65%|██████▍   | 107/165 [00:11<00:06,  9.12it/s]\u001b[A\n",
            " 65%|██████▌   | 108/165 [00:11<00:06,  9.10it/s]\u001b[A\n",
            " 66%|██████▌   | 109/165 [00:12<00:06,  9.07it/s]\u001b[A\n",
            " 67%|██████▋   | 110/165 [00:12<00:06,  9.10it/s]\u001b[A\n",
            " 67%|██████▋   | 111/165 [00:12<00:05,  9.05it/s]\u001b[A\n",
            " 68%|██████▊   | 112/165 [00:12<00:05,  9.07it/s]\u001b[A\n",
            " 68%|██████▊   | 113/165 [00:12<00:05,  9.10it/s]\u001b[A\n",
            " 69%|██████▉   | 114/165 [00:12<00:05,  9.09it/s]\u001b[A\n",
            " 70%|██████▉   | 115/165 [00:12<00:05,  9.11it/s]\u001b[A\n",
            " 70%|███████   | 116/165 [00:12<00:05,  9.11it/s]\u001b[A\n",
            " 71%|███████   | 117/165 [00:12<00:05,  9.12it/s]\u001b[A\n",
            " 72%|███████▏  | 118/165 [00:12<00:05,  9.12it/s]\u001b[A\n",
            " 72%|███████▏  | 119/165 [00:13<00:05,  9.11it/s]\u001b[A\n",
            " 73%|███████▎  | 120/165 [00:13<00:04,  9.10it/s]\u001b[A\n",
            " 73%|███████▎  | 121/165 [00:13<00:04,  9.05it/s]\u001b[A\n",
            " 74%|███████▍  | 122/165 [00:13<00:04,  9.09it/s]\u001b[A\n",
            " 75%|███████▍  | 123/165 [00:13<00:04,  9.10it/s]\u001b[A\n",
            " 75%|███████▌  | 124/165 [00:13<00:04,  9.08it/s]\u001b[A\n",
            " 76%|███████▌  | 125/165 [00:13<00:04,  9.09it/s]\u001b[A\n",
            " 76%|███████▋  | 126/165 [00:13<00:04,  9.11it/s]\u001b[A\n",
            " 77%|███████▋  | 127/165 [00:13<00:04,  9.13it/s]\u001b[A\n",
            " 78%|███████▊  | 128/165 [00:14<00:04,  9.13it/s]\u001b[A\n",
            " 78%|███████▊  | 129/165 [00:14<00:03,  9.14it/s]\u001b[A\n",
            " 79%|███████▉  | 130/165 [00:14<00:03,  9.15it/s]\u001b[A\n",
            " 79%|███████▉  | 131/165 [00:14<00:03,  9.14it/s]\u001b[A\n",
            " 80%|████████  | 132/165 [00:14<00:03,  9.16it/s]\u001b[A\n",
            " 81%|████████  | 133/165 [00:14<00:03,  9.12it/s]\u001b[A\n",
            " 81%|████████  | 134/165 [00:14<00:03,  9.12it/s]\u001b[A\n",
            " 82%|████████▏ | 135/165 [00:14<00:03,  9.11it/s]\u001b[A\n",
            " 82%|████████▏ | 136/165 [00:14<00:03,  9.10it/s]\u001b[A\n",
            " 83%|████████▎ | 137/165 [00:15<00:03,  9.11it/s]\u001b[A\n",
            " 84%|████████▎ | 138/165 [00:15<00:02,  9.12it/s]\u001b[A\n",
            " 84%|████████▍ | 139/165 [00:15<00:02,  9.12it/s]\u001b[A\n",
            " 85%|████████▍ | 140/165 [00:15<00:02,  9.11it/s]\u001b[A\n",
            " 85%|████████▌ | 141/165 [00:15<00:02,  9.12it/s]\u001b[A\n",
            " 86%|████████▌ | 142/165 [00:15<00:02,  9.14it/s]\u001b[A\n",
            " 87%|████████▋ | 143/165 [00:15<00:02,  9.12it/s]\u001b[A\n",
            " 87%|████████▋ | 144/165 [00:15<00:02,  9.12it/s]\u001b[A\n",
            " 88%|████████▊ | 145/165 [00:15<00:02,  9.12it/s]\u001b[A\n",
            " 88%|████████▊ | 146/165 [00:16<00:02,  9.13it/s]\u001b[A\n",
            " 89%|████████▉ | 147/165 [00:16<00:01,  9.14it/s]\u001b[A\n",
            " 90%|████████▉ | 148/165 [00:16<00:01,  9.15it/s]\u001b[A\n",
            " 90%|█████████ | 149/165 [00:16<00:01,  9.15it/s]\u001b[A\n",
            " 91%|█████████ | 150/165 [00:16<00:01,  9.12it/s]\u001b[A\n",
            " 92%|█████████▏| 151/165 [00:16<00:01,  9.11it/s]\u001b[A\n",
            " 92%|█████████▏| 152/165 [00:16<00:01,  9.12it/s]\u001b[A\n",
            " 93%|█████████▎| 153/165 [00:16<00:01,  9.13it/s]\u001b[A\n",
            " 93%|█████████▎| 154/165 [00:16<00:01,  9.13it/s]\u001b[A\n",
            " 94%|█████████▍| 155/165 [00:17<00:01,  9.14it/s]\u001b[A\n",
            " 95%|█████████▍| 156/165 [00:17<00:00,  9.14it/s]\u001b[A\n",
            " 95%|█████████▌| 157/165 [00:17<00:00,  9.14it/s]\u001b[A\n",
            " 96%|█████████▌| 158/165 [00:17<00:00,  9.14it/s]\u001b[A\n",
            " 96%|█████████▋| 159/165 [00:17<00:00,  9.15it/s]\u001b[A\n",
            " 97%|█████████▋| 160/165 [00:17<00:00,  9.16it/s]\u001b[A\n",
            " 98%|█████████▊| 161/165 [00:17<00:00,  9.14it/s]\u001b[A\n",
            " 98%|█████████▊| 162/165 [00:17<00:00,  9.14it/s]\u001b[A\n",
            " 99%|█████████▉| 163/165 [00:17<00:00,  9.15it/s]\u001b[A\n",
            "100%|██████████| 165/165 [00:18<00:00,  9.09it/s]\n",
            "========== model: bert-base-uncased fold: 1 inference ==========\n",
            "loading configuration file /content/activebus_bert_final/config.json\n",
            "Model config BertConfig {\n",
            "  \"_name_or_path\": \"activebus/BERT-XD_Review\",\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_ids\": 0,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.15.0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "loading weights file /content/activebus_bert_final/pytorch_model.bin\n",
            "Some weights of the model checkpoint at /content/activebus_bert_final were not used when initializing BertForSequenceClassification: ['cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /content/activebus_bert_final and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "\n",
            "  0%|          | 0/165 [00:00<?, ?it/s]\u001b[A\n",
            "  1%|          | 1/165 [00:00<00:19,  8.40it/s]\u001b[A\n",
            "  1%|          | 2/165 [00:00<00:18,  8.79it/s]\u001b[A\n",
            "  2%|▏         | 3/165 [00:00<00:18,  8.94it/s]\u001b[A\n",
            "  2%|▏         | 4/165 [00:00<00:17,  8.97it/s]\u001b[A\n",
            "  3%|▎         | 5/165 [00:00<00:17,  9.00it/s]\u001b[A\n",
            "  4%|▎         | 6/165 [00:00<00:17,  9.03it/s]\u001b[A\n",
            "  4%|▍         | 7/165 [00:00<00:17,  9.06it/s]\u001b[A\n",
            "  5%|▍         | 8/165 [00:00<00:17,  9.09it/s]\u001b[A\n",
            "  5%|▌         | 9/165 [00:00<00:17,  9.10it/s]\u001b[A\n",
            "  6%|▌         | 10/165 [00:01<00:17,  9.10it/s]\u001b[A\n",
            "  7%|▋         | 11/165 [00:01<00:16,  9.09it/s]\u001b[A\n",
            "  7%|▋         | 12/165 [00:01<00:16,  9.09it/s]\u001b[A\n",
            "  8%|▊         | 13/165 [00:01<00:16,  9.10it/s]\u001b[A\n",
            "  8%|▊         | 14/165 [00:01<00:16,  9.09it/s]\u001b[A\n",
            "  9%|▉         | 15/165 [00:01<00:16,  9.10it/s]\u001b[A\n",
            " 10%|▉         | 16/165 [00:01<00:16,  9.09it/s]\u001b[A\n",
            " 10%|█         | 17/165 [00:01<00:16,  9.09it/s]\u001b[A\n",
            " 11%|█         | 18/165 [00:01<00:16,  9.10it/s]\u001b[A\n",
            " 12%|█▏        | 19/165 [00:02<00:16,  9.09it/s]\u001b[A\n",
            " 12%|█▏        | 20/165 [00:02<00:15,  9.08it/s]\u001b[A\n",
            " 13%|█▎        | 21/165 [00:02<00:15,  9.08it/s]\u001b[A\n",
            " 13%|█▎        | 22/165 [00:02<00:15,  9.07it/s]\u001b[A\n",
            " 14%|█▍        | 23/165 [00:02<00:15,  9.06it/s]\u001b[A\n",
            " 15%|█▍        | 24/165 [00:02<00:15,  9.08it/s]\u001b[A\n",
            " 15%|█▌        | 25/165 [00:02<00:15,  9.10it/s]\u001b[A\n",
            " 16%|█▌        | 26/165 [00:02<00:15,  9.11it/s]\u001b[A\n",
            " 16%|█▋        | 27/165 [00:02<00:15,  9.12it/s]\u001b[A\n",
            " 17%|█▋        | 28/165 [00:03<00:15,  9.12it/s]\u001b[A\n",
            " 18%|█▊        | 29/165 [00:03<00:14,  9.12it/s]\u001b[A\n",
            " 18%|█▊        | 30/165 [00:03<00:14,  9.12it/s]\u001b[A\n",
            " 19%|█▉        | 31/165 [00:03<00:14,  9.14it/s]\u001b[A\n",
            " 19%|█▉        | 32/165 [00:03<00:14,  9.15it/s]\u001b[A\n",
            " 20%|██        | 33/165 [00:03<00:14,  9.14it/s]\u001b[A\n",
            " 21%|██        | 34/165 [00:03<00:14,  9.13it/s]\u001b[A\n",
            " 21%|██        | 35/165 [00:03<00:14,  9.13it/s]\u001b[A\n",
            " 22%|██▏       | 36/165 [00:03<00:14,  9.06it/s]\u001b[A\n",
            " 22%|██▏       | 37/165 [00:04<00:14,  9.07it/s]\u001b[A\n",
            " 23%|██▎       | 38/165 [00:04<00:14,  9.07it/s]\u001b[A\n",
            " 24%|██▎       | 39/165 [00:04<00:13,  9.12it/s]\u001b[A\n",
            " 24%|██▍       | 40/165 [00:04<00:13,  9.12it/s]\u001b[A\n",
            " 25%|██▍       | 41/165 [00:04<00:13,  9.14it/s]\u001b[A\n",
            " 25%|██▌       | 42/165 [00:04<00:13,  9.13it/s]\u001b[A\n",
            " 26%|██▌       | 43/165 [00:04<00:13,  9.12it/s]\u001b[A\n",
            " 27%|██▋       | 44/165 [00:04<00:13,  9.06it/s]\u001b[A\n",
            " 27%|██▋       | 45/165 [00:04<00:13,  9.07it/s]\u001b[A\n",
            " 28%|██▊       | 46/165 [00:05<00:13,  9.08it/s]\u001b[A\n",
            " 28%|██▊       | 47/165 [00:05<00:12,  9.09it/s]\u001b[A\n",
            " 29%|██▉       | 48/165 [00:05<00:12,  9.11it/s]\u001b[A\n",
            " 30%|██▉       | 49/165 [00:05<00:12,  9.13it/s]\u001b[A\n",
            " 30%|███       | 50/165 [00:05<00:12,  9.13it/s]\u001b[A\n",
            " 31%|███       | 51/165 [00:05<00:12,  9.12it/s]\u001b[A\n",
            " 32%|███▏      | 52/165 [00:05<00:12,  9.12it/s]\u001b[A\n",
            " 32%|███▏      | 53/165 [00:05<00:12,  9.13it/s]\u001b[A\n",
            " 33%|███▎      | 54/165 [00:05<00:12,  9.13it/s]\u001b[A\n",
            " 33%|███▎      | 55/165 [00:06<00:12,  9.10it/s]\u001b[A\n",
            " 34%|███▍      | 56/165 [00:06<00:11,  9.10it/s]\u001b[A\n",
            " 35%|███▍      | 57/165 [00:06<00:11,  9.09it/s]\u001b[A\n",
            " 35%|███▌      | 58/165 [00:06<00:11,  9.08it/s]\u001b[A\n",
            " 36%|███▌      | 59/165 [00:06<00:11,  9.09it/s]\u001b[A\n",
            " 36%|███▋      | 60/165 [00:06<00:11,  9.09it/s]\u001b[A\n",
            " 37%|███▋      | 61/165 [00:06<00:11,  9.08it/s]\u001b[A\n",
            " 38%|███▊      | 62/165 [00:06<00:11,  9.09it/s]\u001b[A\n",
            " 38%|███▊      | 63/165 [00:06<00:11,  9.08it/s]\u001b[A\n",
            " 39%|███▉      | 64/165 [00:07<00:11,  9.10it/s]\u001b[A\n",
            " 39%|███▉      | 65/165 [00:07<00:10,  9.09it/s]\u001b[A\n",
            " 40%|████      | 66/165 [00:07<00:10,  9.09it/s]\u001b[A\n",
            " 41%|████      | 67/165 [00:07<00:10,  9.09it/s]\u001b[A\n",
            " 41%|████      | 68/165 [00:07<00:10,  9.11it/s]\u001b[A\n",
            " 42%|████▏     | 69/165 [00:07<00:10,  9.10it/s]\u001b[A\n",
            " 42%|████▏     | 70/165 [00:07<00:10,  9.10it/s]\u001b[A\n",
            " 43%|████▎     | 71/165 [00:07<00:10,  9.10it/s]\u001b[A\n",
            " 44%|████▎     | 72/165 [00:07<00:10,  9.10it/s]\u001b[A\n",
            " 44%|████▍     | 73/165 [00:08<00:10,  9.10it/s]\u001b[A\n",
            " 45%|████▍     | 74/165 [00:08<00:09,  9.11it/s]\u001b[A\n",
            " 45%|████▌     | 75/165 [00:08<00:09,  9.12it/s]\u001b[A\n",
            " 46%|████▌     | 76/165 [00:08<00:09,  9.12it/s]\u001b[A\n",
            " 47%|████▋     | 77/165 [00:08<00:09,  9.10it/s]\u001b[A\n",
            " 47%|████▋     | 78/165 [00:08<00:09,  9.10it/s]\u001b[A\n",
            " 48%|████▊     | 79/165 [00:08<00:09,  9.10it/s]\u001b[A\n",
            " 48%|████▊     | 80/165 [00:08<00:09,  9.11it/s]\u001b[A\n",
            " 49%|████▉     | 81/165 [00:08<00:09,  9.12it/s]\u001b[A\n",
            " 50%|████▉     | 82/165 [00:09<00:09,  9.11it/s]\u001b[A\n",
            " 50%|█████     | 83/165 [00:09<00:09,  9.10it/s]\u001b[A\n",
            " 51%|█████     | 84/165 [00:09<00:08,  9.10it/s]\u001b[A\n",
            " 52%|█████▏    | 85/165 [00:09<00:08,  9.08it/s]\u001b[A\n",
            " 52%|█████▏    | 86/165 [00:09<00:08,  9.08it/s]\u001b[A\n",
            " 53%|█████▎    | 87/165 [00:09<00:08,  9.06it/s]\u001b[A\n",
            " 53%|█████▎    | 88/165 [00:09<00:08,  9.06it/s]\u001b[A\n",
            " 54%|█████▍    | 89/165 [00:09<00:08,  9.07it/s]\u001b[A\n",
            " 55%|█████▍    | 90/165 [00:09<00:08,  9.09it/s]\u001b[A\n",
            " 55%|█████▌    | 91/165 [00:10<00:08,  9.10it/s]\u001b[A\n",
            " 56%|█████▌    | 92/165 [00:10<00:08,  9.10it/s]\u001b[A\n",
            " 56%|█████▋    | 93/165 [00:10<00:07,  9.12it/s]\u001b[A\n",
            " 57%|█████▋    | 94/165 [00:10<00:07,  9.12it/s]\u001b[A\n",
            " 58%|█████▊    | 95/165 [00:10<00:07,  9.11it/s]\u001b[A\n",
            " 58%|█████▊    | 96/165 [00:10<00:07,  9.11it/s]\u001b[A\n",
            " 59%|█████▉    | 97/165 [00:10<00:07,  9.13it/s]\u001b[A\n",
            " 59%|█████▉    | 98/165 [00:10<00:07,  9.13it/s]\u001b[A\n",
            " 60%|██████    | 99/165 [00:10<00:07,  9.13it/s]\u001b[A\n",
            " 61%|██████    | 100/165 [00:10<00:07,  9.14it/s]\u001b[A\n",
            " 61%|██████    | 101/165 [00:11<00:07,  9.12it/s]\u001b[A\n",
            " 62%|██████▏   | 102/165 [00:11<00:06,  9.12it/s]\u001b[A\n",
            " 62%|██████▏   | 103/165 [00:11<00:06,  9.13it/s]\u001b[A\n",
            " 63%|██████▎   | 104/165 [00:11<00:06,  9.11it/s]\u001b[A\n",
            " 64%|██████▎   | 105/165 [00:11<00:06,  9.10it/s]\u001b[A\n",
            " 64%|██████▍   | 106/165 [00:11<00:06,  9.11it/s]\u001b[A\n",
            " 65%|██████▍   | 107/165 [00:11<00:06,  9.11it/s]\u001b[A\n",
            " 65%|██████▌   | 108/165 [00:11<00:06,  9.11it/s]\u001b[A\n",
            " 66%|██████▌   | 109/165 [00:11<00:06,  9.12it/s]\u001b[A\n",
            " 67%|██████▋   | 110/165 [00:12<00:06,  9.09it/s]\u001b[A\n",
            " 67%|██████▋   | 111/165 [00:12<00:05,  9.05it/s]\u001b[A\n",
            " 68%|██████▊   | 112/165 [00:12<00:05,  9.08it/s]\u001b[A\n",
            " 68%|██████▊   | 113/165 [00:12<00:05,  9.10it/s]\u001b[A\n",
            " 69%|██████▉   | 114/165 [00:12<00:05,  9.12it/s]\u001b[A\n",
            " 70%|██████▉   | 115/165 [00:12<00:05,  9.14it/s]\u001b[A\n",
            " 70%|███████   | 116/165 [00:12<00:05,  9.12it/s]\u001b[A\n",
            " 71%|███████   | 117/165 [00:12<00:05,  9.12it/s]\u001b[A\n",
            " 72%|███████▏  | 118/165 [00:12<00:05,  9.10it/s]\u001b[A\n",
            " 72%|███████▏  | 119/165 [00:13<00:05,  9.10it/s]\u001b[A\n",
            " 73%|███████▎  | 120/165 [00:13<00:04,  9.08it/s]\u001b[A\n",
            " 73%|███████▎  | 121/165 [00:13<00:04,  9.07it/s]\u001b[A\n",
            " 74%|███████▍  | 122/165 [00:13<00:04,  9.08it/s]\u001b[A\n",
            " 75%|███████▍  | 123/165 [00:13<00:04,  9.09it/s]\u001b[A\n",
            " 75%|███████▌  | 124/165 [00:13<00:04,  9.08it/s]\u001b[A\n",
            " 76%|███████▌  | 125/165 [00:13<00:04,  9.10it/s]\u001b[A\n",
            " 76%|███████▋  | 126/165 [00:13<00:04,  9.11it/s]\u001b[A\n",
            " 77%|███████▋  | 127/165 [00:13<00:04,  9.11it/s]\u001b[A\n",
            " 78%|███████▊  | 128/165 [00:14<00:04,  9.11it/s]\u001b[A\n",
            " 78%|███████▊  | 129/165 [00:14<00:03,  9.11it/s]\u001b[A\n",
            " 79%|███████▉  | 130/165 [00:14<00:03,  9.12it/s]\u001b[A\n",
            " 79%|███████▉  | 131/165 [00:14<00:03,  9.11it/s]\u001b[A\n",
            " 80%|████████  | 132/165 [00:14<00:03,  9.11it/s]\u001b[A\n",
            " 81%|████████  | 133/165 [00:14<00:03,  9.09it/s]\u001b[A\n",
            " 81%|████████  | 134/165 [00:14<00:03,  8.99it/s]\u001b[A\n",
            " 82%|████████▏ | 135/165 [00:14<00:03,  9.02it/s]\u001b[A\n",
            " 82%|████████▏ | 136/165 [00:14<00:03,  9.05it/s]\u001b[A\n",
            " 83%|████████▎ | 137/165 [00:15<00:03,  9.07it/s]\u001b[A\n",
            " 84%|████████▎ | 138/165 [00:15<00:02,  9.08it/s]\u001b[A\n",
            " 84%|████████▍ | 139/165 [00:15<00:02,  9.07it/s]\u001b[A\n",
            " 85%|████████▍ | 140/165 [00:15<00:02,  9.05it/s]\u001b[A\n",
            " 85%|████████▌ | 141/165 [00:15<00:02,  9.07it/s]\u001b[A\n",
            " 86%|████████▌ | 142/165 [00:15<00:02,  9.02it/s]\u001b[A\n",
            " 87%|████████▋ | 143/165 [00:15<00:02,  9.05it/s]\u001b[A\n",
            " 87%|████████▋ | 144/165 [00:15<00:02,  9.08it/s]\u001b[A\n",
            " 88%|████████▊ | 145/165 [00:15<00:02,  9.09it/s]\u001b[A\n",
            " 88%|████████▊ | 146/165 [00:16<00:02,  9.09it/s]\u001b[A\n",
            " 89%|████████▉ | 147/165 [00:16<00:01,  9.12it/s]\u001b[A\n",
            " 90%|████████▉ | 148/165 [00:16<00:01,  9.13it/s]\u001b[A\n",
            " 90%|█████████ | 149/165 [00:16<00:01,  9.13it/s]\u001b[A\n",
            " 91%|█████████ | 150/165 [00:16<00:01,  9.14it/s]\u001b[A\n",
            " 92%|█████████▏| 151/165 [00:16<00:01,  9.14it/s]\u001b[A\n",
            " 92%|█████████▏| 152/165 [00:16<00:01,  9.14it/s]\u001b[A\n",
            " 93%|█████████▎| 153/165 [00:16<00:01,  9.13it/s]\u001b[A\n",
            " 93%|█████████▎| 154/165 [00:16<00:01,  9.14it/s]\u001b[A\n",
            " 94%|█████████▍| 155/165 [00:17<00:01,  9.16it/s]\u001b[A\n",
            " 95%|█████████▍| 156/165 [00:17<00:00,  9.16it/s]\u001b[A\n",
            " 95%|█████████▌| 157/165 [00:17<00:00,  9.16it/s]\u001b[A\n",
            " 96%|█████████▌| 158/165 [00:17<00:00,  9.16it/s]\u001b[A\n",
            " 96%|█████████▋| 159/165 [00:17<00:00,  9.16it/s]\u001b[A\n",
            " 97%|█████████▋| 160/165 [00:17<00:00,  9.16it/s]\u001b[A\n",
            " 98%|█████████▊| 161/165 [00:17<00:00,  9.14it/s]\u001b[A\n",
            " 98%|█████████▊| 162/165 [00:17<00:00,  9.14it/s]\u001b[A\n",
            " 99%|█████████▉| 163/165 [00:17<00:00,  9.15it/s]\u001b[A\n",
            "100%|██████████| 165/165 [00:18<00:00,  9.09it/s]\n",
            "========== model: bert-base-uncased fold: 2 inference ==========\n",
            "loading configuration file /content/activebus_bert_final/config.json\n",
            "Model config BertConfig {\n",
            "  \"_name_or_path\": \"activebus/BERT-XD_Review\",\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_ids\": 0,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.15.0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "loading weights file /content/activebus_bert_final/pytorch_model.bin\n",
            "Some weights of the model checkpoint at /content/activebus_bert_final were not used when initializing BertForSequenceClassification: ['cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /content/activebus_bert_final and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "\n",
            "  0%|          | 0/165 [00:00<?, ?it/s]\u001b[A\n",
            "  1%|          | 1/165 [00:00<00:18,  8.66it/s]\u001b[A\n",
            "  1%|          | 2/165 [00:00<00:18,  8.90it/s]\u001b[A\n",
            "  2%|▏         | 3/165 [00:00<00:18,  8.99it/s]\u001b[A\n",
            "  2%|▏         | 4/165 [00:00<00:17,  9.01it/s]\u001b[A\n",
            "  3%|▎         | 5/165 [00:00<00:17,  9.05it/s]\u001b[A\n",
            "  4%|▎         | 6/165 [00:00<00:17,  9.07it/s]\u001b[A\n",
            "  4%|▍         | 7/165 [00:00<00:17,  9.09it/s]\u001b[A\n",
            "  5%|▍         | 8/165 [00:00<00:17,  9.10it/s]\u001b[A\n",
            "  5%|▌         | 9/165 [00:00<00:17,  9.10it/s]\u001b[A\n",
            "  6%|▌         | 10/165 [00:01<00:17,  9.11it/s]\u001b[A\n",
            "  7%|▋         | 11/165 [00:01<00:16,  9.10it/s]\u001b[A\n",
            "  7%|▋         | 12/165 [00:01<00:16,  9.10it/s]\u001b[A\n",
            "  8%|▊         | 13/165 [00:01<00:16,  9.12it/s]\u001b[A\n",
            "  8%|▊         | 14/165 [00:01<00:16,  9.13it/s]\u001b[A\n",
            "  9%|▉         | 15/165 [00:01<00:16,  9.12it/s]\u001b[A\n",
            " 10%|▉         | 16/165 [00:01<00:16,  9.12it/s]\u001b[A\n",
            " 10%|█         | 17/165 [00:01<00:16,  9.12it/s]\u001b[A\n",
            " 11%|█         | 18/165 [00:01<00:16,  9.11it/s]\u001b[A\n",
            " 12%|█▏        | 19/165 [00:02<00:16,  9.10it/s]\u001b[A\n",
            " 12%|█▏        | 20/165 [00:02<00:15,  9.10it/s]\u001b[A\n",
            " 13%|█▎        | 21/165 [00:02<00:15,  9.10it/s]\u001b[A\n",
            " 13%|█▎        | 22/165 [00:02<00:15,  9.09it/s]\u001b[A\n",
            " 14%|█▍        | 23/165 [00:02<00:15,  9.10it/s]\u001b[A\n",
            " 15%|█▍        | 24/165 [00:02<00:15,  9.10it/s]\u001b[A\n",
            " 15%|█▌        | 25/165 [00:02<00:15,  9.09it/s]\u001b[A\n",
            " 16%|█▌        | 26/165 [00:02<00:15,  9.11it/s]\u001b[A\n",
            " 16%|█▋        | 27/165 [00:02<00:15,  9.12it/s]\u001b[A\n",
            " 17%|█▋        | 28/165 [00:03<00:15,  9.13it/s]\u001b[A\n",
            " 18%|█▊        | 29/165 [00:03<00:14,  9.13it/s]\u001b[A\n",
            " 18%|█▊        | 30/165 [00:03<00:14,  9.13it/s]\u001b[A\n",
            " 19%|█▉        | 31/165 [00:03<00:14,  9.06it/s]\u001b[A\n",
            " 19%|█▉        | 32/165 [00:03<00:14,  9.07it/s]\u001b[A\n",
            " 20%|██        | 33/165 [00:03<00:14,  9.08it/s]\u001b[A\n",
            " 21%|██        | 34/165 [00:03<00:14,  9.10it/s]\u001b[A\n",
            " 21%|██        | 35/165 [00:03<00:14,  9.10it/s]\u001b[A\n",
            " 22%|██▏       | 36/165 [00:03<00:14,  9.10it/s]\u001b[A\n",
            " 22%|██▏       | 37/165 [00:04<00:14,  9.07it/s]\u001b[A\n",
            " 23%|██▎       | 38/165 [00:04<00:13,  9.08it/s]\u001b[A\n",
            " 24%|██▎       | 39/165 [00:04<00:13,  9.09it/s]\u001b[A\n",
            " 24%|██▍       | 40/165 [00:04<00:13,  9.10it/s]\u001b[A\n",
            " 25%|██▍       | 41/165 [00:04<00:13,  9.03it/s]\u001b[A\n",
            " 25%|██▌       | 42/165 [00:04<00:13,  9.06it/s]\u001b[A\n",
            " 26%|██▌       | 43/165 [00:04<00:13,  9.08it/s]\u001b[A\n",
            " 27%|██▋       | 44/165 [00:04<00:13,  9.09it/s]\u001b[A\n",
            " 27%|██▋       | 45/165 [00:04<00:13,  9.11it/s]\u001b[A\n",
            " 28%|██▊       | 46/165 [00:05<00:13,  9.10it/s]\u001b[A\n",
            " 28%|██▊       | 47/165 [00:05<00:12,  9.11it/s]\u001b[A\n",
            " 29%|██▉       | 48/165 [00:05<00:12,  9.12it/s]\u001b[A\n",
            " 30%|██▉       | 49/165 [00:05<00:12,  9.12it/s]\u001b[A\n",
            " 30%|███       | 50/165 [00:05<00:12,  9.14it/s]\u001b[A\n",
            " 31%|███       | 51/165 [00:05<00:12,  9.12it/s]\u001b[A\n",
            " 32%|███▏      | 52/165 [00:05<00:12,  9.11it/s]\u001b[A\n",
            " 32%|███▏      | 53/165 [00:05<00:12,  9.13it/s]\u001b[A\n",
            " 33%|███▎      | 54/165 [00:05<00:12,  9.13it/s]\u001b[A\n",
            " 33%|███▎      | 55/165 [00:06<00:12,  9.08it/s]\u001b[A\n",
            " 34%|███▍      | 56/165 [00:06<00:11,  9.10it/s]\u001b[A\n",
            " 35%|███▍      | 57/165 [00:06<00:11,  9.10it/s]\u001b[A\n",
            " 35%|███▌      | 58/165 [00:06<00:11,  9.11it/s]\u001b[A\n",
            " 36%|███▌      | 59/165 [00:06<00:11,  9.09it/s]\u001b[A\n",
            " 36%|███▋      | 60/165 [00:06<00:11,  9.11it/s]\u001b[A\n",
            " 37%|███▋      | 61/165 [00:06<00:11,  9.10it/s]\u001b[A\n",
            " 38%|███▊      | 62/165 [00:06<00:11,  9.10it/s]\u001b[A\n",
            " 38%|███▊      | 63/165 [00:06<00:11,  9.12it/s]\u001b[A\n",
            " 39%|███▉      | 64/165 [00:07<00:11,  9.11it/s]\u001b[A\n",
            " 39%|███▉      | 65/165 [00:07<00:10,  9.12it/s]\u001b[A\n",
            " 40%|████      | 66/165 [00:07<00:10,  9.09it/s]\u001b[A\n",
            " 41%|████      | 67/165 [00:07<00:10,  9.08it/s]\u001b[A\n",
            " 41%|████      | 68/165 [00:07<00:10,  9.10it/s]\u001b[A\n",
            " 42%|████▏     | 69/165 [00:07<00:10,  9.11it/s]\u001b[A\n",
            " 42%|████▏     | 70/165 [00:07<00:10,  9.13it/s]\u001b[A\n",
            " 43%|████▎     | 71/165 [00:07<00:10,  9.09it/s]\u001b[A\n",
            " 44%|████▎     | 72/165 [00:07<00:10,  9.11it/s]\u001b[A\n",
            " 44%|████▍     | 73/165 [00:08<00:10,  9.12it/s]\u001b[A\n",
            " 45%|████▍     | 74/165 [00:08<00:09,  9.12it/s]\u001b[A\n",
            " 45%|████▌     | 75/165 [00:08<00:09,  9.10it/s]\u001b[A\n",
            " 46%|████▌     | 76/165 [00:08<00:09,  9.13it/s]\u001b[A\n",
            " 47%|████▋     | 77/165 [00:08<00:09,  9.14it/s]\u001b[A\n",
            " 47%|████▋     | 78/165 [00:08<00:09,  9.11it/s]\u001b[A\n",
            " 48%|████▊     | 79/165 [00:08<00:09,  9.12it/s]\u001b[A\n",
            " 48%|████▊     | 80/165 [00:08<00:09,  9.12it/s]\u001b[A\n",
            " 49%|████▉     | 81/165 [00:08<00:09,  9.12it/s]\u001b[A\n",
            " 50%|████▉     | 82/165 [00:09<00:09,  9.10it/s]\u001b[A\n",
            " 50%|█████     | 83/165 [00:09<00:09,  9.09it/s]\u001b[A\n",
            " 51%|█████     | 84/165 [00:09<00:08,  9.09it/s]\u001b[A\n",
            " 52%|█████▏    | 85/165 [00:09<00:08,  9.10it/s]\u001b[A\n",
            " 52%|█████▏    | 86/165 [00:09<00:08,  9.11it/s]\u001b[A\n",
            " 53%|█████▎    | 87/165 [00:09<00:08,  9.12it/s]\u001b[A\n",
            " 53%|█████▎    | 88/165 [00:09<00:08,  9.14it/s]\u001b[A\n",
            " 54%|█████▍    | 89/165 [00:09<00:08,  9.14it/s]\u001b[A\n",
            " 55%|█████▍    | 90/165 [00:09<00:08,  9.11it/s]\u001b[A\n",
            " 55%|█████▌    | 91/165 [00:10<00:08,  9.09it/s]\u001b[A\n",
            " 56%|█████▌    | 92/165 [00:10<00:08,  9.09it/s]\u001b[A\n",
            " 56%|█████▋    | 93/165 [00:10<00:07,  9.10it/s]\u001b[A\n",
            " 57%|█████▋    | 94/165 [00:10<00:07,  9.10it/s]\u001b[A\n",
            " 58%|█████▊    | 95/165 [00:10<00:07,  9.11it/s]\u001b[A\n",
            " 58%|█████▊    | 96/165 [00:10<00:07,  9.11it/s]\u001b[A\n",
            " 59%|█████▉    | 97/165 [00:10<00:07,  9.13it/s]\u001b[A\n",
            " 59%|█████▉    | 98/165 [00:10<00:07,  9.13it/s]\u001b[A\n",
            " 60%|██████    | 99/165 [00:10<00:07,  9.12it/s]\u001b[A\n",
            " 61%|██████    | 100/165 [00:10<00:07,  9.12it/s]\u001b[A\n",
            " 61%|██████    | 101/165 [00:11<00:07,  9.13it/s]\u001b[A\n",
            " 62%|██████▏   | 102/165 [00:11<00:06,  9.14it/s]\u001b[A\n",
            " 62%|██████▏   | 103/165 [00:11<00:06,  9.15it/s]\u001b[A\n",
            " 63%|██████▎   | 104/165 [00:11<00:06,  9.15it/s]\u001b[A\n",
            " 64%|██████▎   | 105/165 [00:11<00:06,  9.15it/s]\u001b[A\n",
            " 64%|██████▍   | 106/165 [00:11<00:06,  9.14it/s]\u001b[A\n",
            " 65%|██████▍   | 107/165 [00:11<00:06,  9.14it/s]\u001b[A\n",
            " 65%|██████▌   | 108/165 [00:11<00:06,  9.14it/s]\u001b[A\n",
            " 66%|██████▌   | 109/165 [00:11<00:06,  9.13it/s]\u001b[A\n",
            " 67%|██████▋   | 110/165 [00:12<00:06,  9.13it/s]\u001b[A\n",
            " 67%|██████▋   | 111/165 [00:12<00:05,  9.13it/s]\u001b[A\n",
            " 68%|██████▊   | 112/165 [00:12<00:05,  9.11it/s]\u001b[A\n",
            " 68%|██████▊   | 113/165 [00:12<00:05,  9.10it/s]\u001b[A\n",
            " 69%|██████▉   | 114/165 [00:12<00:05,  9.11it/s]\u001b[A\n",
            " 70%|██████▉   | 115/165 [00:12<00:05,  9.11it/s]\u001b[A\n",
            " 70%|███████   | 116/165 [00:12<00:05,  9.12it/s]\u001b[A\n",
            " 71%|███████   | 117/165 [00:12<00:05,  9.13it/s]\u001b[A\n",
            " 72%|███████▏  | 118/165 [00:12<00:05,  9.12it/s]\u001b[A\n",
            " 72%|███████▏  | 119/165 [00:13<00:05,  9.11it/s]\u001b[A\n",
            " 73%|███████▎  | 120/165 [00:13<00:04,  9.12it/s]\u001b[A\n",
            " 73%|███████▎  | 121/165 [00:13<00:04,  9.13it/s]\u001b[A\n",
            " 74%|███████▍  | 122/165 [00:13<00:04,  9.09it/s]\u001b[A\n",
            " 75%|███████▍  | 123/165 [00:13<00:04,  9.09it/s]\u001b[A\n",
            " 75%|███████▌  | 124/165 [00:13<00:04,  9.09it/s]\u001b[A\n",
            " 76%|███████▌  | 125/165 [00:13<00:04,  9.10it/s]\u001b[A\n",
            " 76%|███████▋  | 126/165 [00:13<00:04,  9.12it/s]\u001b[A\n",
            " 77%|███████▋  | 127/165 [00:13<00:04,  9.11it/s]\u001b[A\n",
            " 78%|███████▊  | 128/165 [00:14<00:04,  9.11it/s]\u001b[A\n",
            " 78%|███████▊  | 129/165 [00:14<00:03,  9.13it/s]\u001b[A\n",
            " 79%|███████▉  | 130/165 [00:14<00:03,  9.14it/s]\u001b[A\n",
            " 79%|███████▉  | 131/165 [00:14<00:03,  9.13it/s]\u001b[A\n",
            " 80%|████████  | 132/165 [00:14<00:03,  9.13it/s]\u001b[A\n",
            " 81%|████████  | 133/165 [00:14<00:03,  9.12it/s]\u001b[A\n",
            " 81%|████████  | 134/165 [00:14<00:03,  9.14it/s]\u001b[A\n",
            " 82%|████████▏ | 135/165 [00:14<00:03,  9.12it/s]\u001b[A\n",
            " 82%|████████▏ | 136/165 [00:14<00:03,  9.13it/s]\u001b[A\n",
            " 83%|████████▎ | 137/165 [00:15<00:03,  9.10it/s]\u001b[A\n",
            " 84%|████████▎ | 138/165 [00:15<00:02,  9.08it/s]\u001b[A\n",
            " 84%|████████▍ | 139/165 [00:15<00:02,  9.12it/s]\u001b[A\n",
            " 85%|████████▍ | 140/165 [00:15<00:02,  9.12it/s]\u001b[A\n",
            " 85%|████████▌ | 141/165 [00:15<00:02,  9.13it/s]\u001b[A\n",
            " 86%|████████▌ | 142/165 [00:15<00:02,  9.12it/s]\u001b[A\n",
            " 87%|████████▋ | 143/165 [00:15<00:02,  9.14it/s]\u001b[A\n",
            " 87%|████████▋ | 144/165 [00:15<00:02,  9.15it/s]\u001b[A\n",
            " 88%|████████▊ | 145/165 [00:15<00:02,  9.14it/s]\u001b[A\n",
            " 88%|████████▊ | 146/165 [00:16<00:02,  9.13it/s]\u001b[A\n",
            " 89%|████████▉ | 147/165 [00:16<00:01,  9.13it/s]\u001b[A\n",
            " 90%|████████▉ | 148/165 [00:16<00:01,  9.14it/s]\u001b[A\n",
            " 90%|█████████ | 149/165 [00:16<00:01,  9.14it/s]\u001b[A\n",
            " 91%|█████████ | 150/165 [00:16<00:01,  9.12it/s]\u001b[A\n",
            " 92%|█████████▏| 151/165 [00:16<00:01,  9.12it/s]\u001b[A\n",
            " 92%|█████████▏| 152/165 [00:16<00:01,  9.13it/s]\u001b[A\n",
            " 93%|█████████▎| 153/165 [00:16<00:01,  9.13it/s]\u001b[A\n",
            " 93%|█████████▎| 154/165 [00:16<00:01,  9.14it/s]\u001b[A\n",
            " 94%|█████████▍| 155/165 [00:17<00:01,  9.13it/s]\u001b[A\n",
            " 95%|█████████▍| 156/165 [00:17<00:00,  9.15it/s]\u001b[A\n",
            " 95%|█████████▌| 157/165 [00:17<00:00,  9.13it/s]\u001b[A\n",
            " 96%|█████████▌| 158/165 [00:17<00:00,  9.14it/s]\u001b[A\n",
            " 96%|█████████▋| 159/165 [00:17<00:00,  9.14it/s]\u001b[A\n",
            " 97%|█████████▋| 160/165 [00:17<00:00,  9.15it/s]\u001b[A\n",
            " 98%|█████████▊| 161/165 [00:17<00:00,  9.15it/s]\u001b[A\n",
            " 98%|█████████▊| 162/165 [00:17<00:00,  9.15it/s]\u001b[A\n",
            " 99%|█████████▉| 163/165 [00:17<00:00,  9.14it/s]\u001b[A\n",
            "100%|██████████| 165/165 [00:18<00:00,  9.11it/s]\n",
            "========== model: bert-base-uncased fold: 3 inference ==========\n",
            "loading configuration file /content/activebus_bert_final/config.json\n",
            "Model config BertConfig {\n",
            "  \"_name_or_path\": \"activebus/BERT-XD_Review\",\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_ids\": 0,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.15.0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "loading weights file /content/activebus_bert_final/pytorch_model.bin\n",
            "Some weights of the model checkpoint at /content/activebus_bert_final were not used when initializing BertForSequenceClassification: ['cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /content/activebus_bert_final and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "\n",
            "  0%|          | 0/165 [00:00<?, ?it/s]\u001b[A\n",
            "  1%|          | 1/165 [00:00<00:18,  8.70it/s]\u001b[A\n",
            "  1%|          | 2/165 [00:00<00:18,  8.86it/s]\u001b[A\n",
            "  2%|▏         | 3/165 [00:00<00:18,  8.99it/s]\u001b[A\n",
            "  2%|▏         | 4/165 [00:00<00:17,  9.02it/s]\u001b[A\n",
            "  3%|▎         | 5/165 [00:00<00:17,  9.06it/s]\u001b[A\n",
            "  4%|▎         | 6/165 [00:00<00:17,  9.06it/s]\u001b[A\n",
            "  4%|▍         | 7/165 [00:00<00:17,  9.08it/s]\u001b[A\n",
            "  5%|▍         | 8/165 [00:00<00:17,  9.08it/s]\u001b[A\n",
            "  5%|▌         | 9/165 [00:00<00:17,  9.10it/s]\u001b[A\n",
            "  6%|▌         | 10/165 [00:01<00:17,  9.11it/s]\u001b[A\n",
            "  7%|▋         | 11/165 [00:01<00:16,  9.12it/s]\u001b[A\n",
            "  7%|▋         | 12/165 [00:01<00:16,  9.13it/s]\u001b[A\n",
            "  8%|▊         | 13/165 [00:01<00:16,  9.12it/s]\u001b[A\n",
            "  8%|▊         | 14/165 [00:01<00:16,  9.12it/s]\u001b[A\n",
            "  9%|▉         | 15/165 [00:01<00:16,  9.11it/s]\u001b[A\n",
            " 10%|▉         | 16/165 [00:01<00:16,  9.10it/s]\u001b[A\n",
            " 10%|█         | 17/165 [00:01<00:16,  9.10it/s]\u001b[A\n",
            " 11%|█         | 18/165 [00:01<00:16,  9.10it/s]\u001b[A\n",
            " 12%|█▏        | 19/165 [00:02<00:16,  9.11it/s]\u001b[A\n",
            " 12%|█▏        | 20/165 [00:02<00:15,  9.14it/s]\u001b[A\n",
            " 13%|█▎        | 21/165 [00:02<00:15,  9.14it/s]\u001b[A\n",
            " 13%|█▎        | 22/165 [00:02<00:15,  9.12it/s]\u001b[A\n",
            " 14%|█▍        | 23/165 [00:02<00:15,  9.10it/s]\u001b[A\n",
            " 15%|█▍        | 24/165 [00:02<00:15,  9.10it/s]\u001b[A\n",
            " 15%|█▌        | 25/165 [00:02<00:15,  9.11it/s]\u001b[A\n",
            " 16%|█▌        | 26/165 [00:02<00:15,  9.12it/s]\u001b[A\n",
            " 16%|█▋        | 27/165 [00:02<00:15,  9.09it/s]\u001b[A\n",
            " 17%|█▋        | 28/165 [00:03<00:15,  9.10it/s]\u001b[A\n",
            " 18%|█▊        | 29/165 [00:03<00:14,  9.10it/s]\u001b[A\n",
            " 18%|█▊        | 30/165 [00:03<00:14,  9.09it/s]\u001b[A\n",
            " 19%|█▉        | 31/165 [00:03<00:14,  9.11it/s]\u001b[A\n",
            " 19%|█▉        | 32/165 [00:03<00:14,  9.09it/s]\u001b[A\n",
            " 20%|██        | 33/165 [00:03<00:14,  9.07it/s]\u001b[A\n",
            " 21%|██        | 34/165 [00:03<00:14,  9.08it/s]\u001b[A\n",
            " 21%|██        | 35/165 [00:03<00:14,  9.09it/s]\u001b[A\n",
            " 22%|██▏       | 36/165 [00:03<00:14,  9.06it/s]\u001b[A\n",
            " 22%|██▏       | 37/165 [00:04<00:14,  9.05it/s]\u001b[A\n",
            " 23%|██▎       | 38/165 [00:04<00:14,  9.06it/s]\u001b[A\n",
            " 24%|██▎       | 39/165 [00:04<00:13,  9.05it/s]\u001b[A\n",
            " 24%|██▍       | 40/165 [00:04<00:13,  9.07it/s]\u001b[A\n",
            " 25%|██▍       | 41/165 [00:04<00:13,  9.09it/s]\u001b[A\n",
            " 25%|██▌       | 42/165 [00:04<00:13,  9.09it/s]\u001b[A\n",
            " 26%|██▌       | 43/165 [00:04<00:13,  9.08it/s]\u001b[A\n",
            " 27%|██▋       | 44/165 [00:04<00:13,  9.09it/s]\u001b[A\n",
            " 27%|██▋       | 45/165 [00:04<00:13,  9.08it/s]\u001b[A\n",
            " 28%|██▊       | 46/165 [00:05<00:13,  9.06it/s]\u001b[A\n",
            " 28%|██▊       | 47/165 [00:05<00:13,  9.07it/s]\u001b[A\n",
            " 29%|██▉       | 48/165 [00:05<00:12,  9.09it/s]\u001b[A\n",
            " 30%|██▉       | 49/165 [00:05<00:12,  9.09it/s]\u001b[A\n",
            " 30%|███       | 50/165 [00:05<00:12,  9.11it/s]\u001b[A\n",
            " 31%|███       | 51/165 [00:05<00:12,  9.11it/s]\u001b[A\n",
            " 32%|███▏      | 52/165 [00:05<00:12,  9.11it/s]\u001b[A\n",
            " 32%|███▏      | 53/165 [00:05<00:12,  9.10it/s]\u001b[A\n",
            " 33%|███▎      | 54/165 [00:05<00:12,  9.06it/s]\u001b[A\n",
            " 33%|███▎      | 55/165 [00:06<00:12,  9.10it/s]\u001b[A\n",
            " 34%|███▍      | 56/165 [00:06<00:11,  9.10it/s]\u001b[A\n",
            " 35%|███▍      | 57/165 [00:06<00:11,  9.12it/s]\u001b[A\n",
            " 35%|███▌      | 58/165 [00:06<00:11,  9.13it/s]\u001b[A\n",
            " 36%|███▌      | 59/165 [00:06<00:11,  9.13it/s]\u001b[A\n",
            " 36%|███▋      | 60/165 [00:06<00:11,  9.10it/s]\u001b[A\n",
            " 37%|███▋      | 61/165 [00:06<00:11,  9.11it/s]\u001b[A\n",
            " 38%|███▊      | 62/165 [00:06<00:11,  9.09it/s]\u001b[A\n",
            " 38%|███▊      | 63/165 [00:06<00:11,  9.11it/s]\u001b[A\n",
            " 39%|███▉      | 64/165 [00:07<00:11,  9.11it/s]\u001b[A\n",
            " 39%|███▉      | 65/165 [00:07<00:11,  9.09it/s]\u001b[A\n",
            " 40%|████      | 66/165 [00:07<00:10,  9.10it/s]\u001b[A\n",
            " 41%|████      | 67/165 [00:07<00:10,  9.11it/s]\u001b[A\n",
            " 41%|████      | 68/165 [00:07<00:10,  9.10it/s]\u001b[A\n",
            " 42%|████▏     | 69/165 [00:07<00:10,  9.08it/s]\u001b[A\n",
            " 42%|████▏     | 70/165 [00:07<00:10,  9.09it/s]\u001b[A\n",
            " 43%|████▎     | 71/165 [00:07<00:10,  9.09it/s]\u001b[A\n",
            " 44%|████▎     | 72/165 [00:07<00:10,  9.09it/s]\u001b[A\n",
            " 44%|████▍     | 73/165 [00:08<00:10,  9.11it/s]\u001b[A\n",
            " 45%|████▍     | 74/165 [00:08<00:10,  9.09it/s]\u001b[A\n",
            " 45%|████▌     | 75/165 [00:08<00:09,  9.10it/s]\u001b[A\n",
            " 46%|████▌     | 76/165 [00:08<00:09,  9.09it/s]\u001b[A\n",
            " 47%|████▋     | 77/165 [00:08<00:09,  9.01it/s]\u001b[A\n",
            " 47%|████▋     | 78/165 [00:08<00:09,  9.02it/s]\u001b[A\n",
            " 48%|████▊     | 79/165 [00:08<00:09,  9.05it/s]\u001b[A\n",
            " 48%|████▊     | 80/165 [00:08<00:09,  9.04it/s]\u001b[A\n",
            " 49%|████▉     | 81/165 [00:08<00:09,  9.03it/s]\u001b[A\n",
            " 50%|████▉     | 82/165 [00:09<00:09,  9.06it/s]\u001b[A\n",
            " 50%|█████     | 83/165 [00:09<00:09,  9.08it/s]\u001b[A\n",
            " 51%|█████     | 84/165 [00:09<00:08,  9.09it/s]\u001b[A\n",
            " 52%|█████▏    | 85/165 [00:09<00:08,  9.09it/s]\u001b[A\n",
            " 52%|█████▏    | 86/165 [00:09<00:08,  9.08it/s]\u001b[A\n",
            " 53%|█████▎    | 87/165 [00:09<00:08,  9.12it/s]\u001b[A\n",
            " 53%|█████▎    | 88/165 [00:09<00:08,  9.07it/s]\u001b[A\n",
            " 54%|█████▍    | 89/165 [00:09<00:08,  9.08it/s]\u001b[A\n",
            " 55%|█████▍    | 90/165 [00:09<00:08,  9.10it/s]\u001b[A\n",
            " 55%|█████▌    | 91/165 [00:10<00:08,  9.09it/s]\u001b[A\n",
            " 56%|█████▌    | 92/165 [00:10<00:08,  9.09it/s]\u001b[A\n",
            " 56%|█████▋    | 93/165 [00:10<00:07,  9.12it/s]\u001b[A\n",
            " 57%|█████▋    | 94/165 [00:10<00:07,  9.08it/s]\u001b[A\n",
            " 58%|█████▊    | 95/165 [00:10<00:07,  9.10it/s]\u001b[A\n",
            " 58%|█████▊    | 96/165 [00:10<00:07,  9.09it/s]\u001b[A\n",
            " 59%|█████▉    | 97/165 [00:10<00:07,  9.08it/s]\u001b[A\n",
            " 59%|█████▉    | 98/165 [00:10<00:07,  9.09it/s]\u001b[A\n",
            " 60%|██████    | 99/165 [00:10<00:07,  9.10it/s]\u001b[A\n",
            " 61%|██████    | 100/165 [00:11<00:07,  9.10it/s]\u001b[A\n",
            " 61%|██████    | 101/165 [00:11<00:07,  9.10it/s]\u001b[A\n",
            " 62%|██████▏   | 102/165 [00:11<00:06,  9.11it/s]\u001b[A\n",
            " 62%|██████▏   | 103/165 [00:11<00:06,  9.13it/s]\u001b[A\n",
            " 63%|██████▎   | 104/165 [00:11<00:06,  9.12it/s]\u001b[A\n",
            " 64%|██████▎   | 105/165 [00:11<00:06,  9.10it/s]\u001b[A\n",
            " 64%|██████▍   | 106/165 [00:11<00:06,  9.09it/s]\u001b[A\n",
            " 65%|██████▍   | 107/165 [00:11<00:06,  9.07it/s]\u001b[A\n",
            " 65%|██████▌   | 108/165 [00:11<00:06,  9.08it/s]\u001b[A\n",
            " 66%|██████▌   | 109/165 [00:11<00:06,  9.11it/s]\u001b[A\n",
            " 67%|██████▋   | 110/165 [00:12<00:06,  9.13it/s]\u001b[A\n",
            " 67%|██████▋   | 111/165 [00:12<00:05,  9.13it/s]\u001b[A\n",
            " 68%|██████▊   | 112/165 [00:12<00:05,  9.13it/s]\u001b[A\n",
            " 68%|██████▊   | 113/165 [00:12<00:05,  9.13it/s]\u001b[A\n",
            " 69%|██████▉   | 114/165 [00:12<00:05,  9.12it/s]\u001b[A\n",
            " 70%|██████▉   | 115/165 [00:12<00:05,  9.13it/s]\u001b[A\n",
            " 70%|███████   | 116/165 [00:12<00:05,  9.14it/s]\u001b[A\n",
            " 71%|███████   | 117/165 [00:12<00:05,  9.14it/s]\u001b[A\n",
            " 72%|███████▏  | 118/165 [00:12<00:05,  9.14it/s]\u001b[A\n",
            " 72%|███████▏  | 119/165 [00:13<00:05,  9.13it/s]\u001b[A\n",
            " 73%|███████▎  | 120/165 [00:13<00:04,  9.13it/s]\u001b[A\n",
            " 73%|███████▎  | 121/165 [00:13<00:04,  9.13it/s]\u001b[A\n",
            " 74%|███████▍  | 122/165 [00:13<00:04,  9.12it/s]\u001b[A\n",
            " 75%|███████▍  | 123/165 [00:13<00:04,  9.12it/s]\u001b[A\n",
            " 75%|███████▌  | 124/165 [00:13<00:04,  9.13it/s]\u001b[A\n",
            " 76%|███████▌  | 125/165 [00:13<00:04,  9.13it/s]\u001b[A\n",
            " 76%|███████▋  | 126/165 [00:13<00:04,  9.13it/s]\u001b[A\n",
            " 77%|███████▋  | 127/165 [00:13<00:04,  9.12it/s]\u001b[A\n",
            " 78%|███████▊  | 128/165 [00:14<00:04,  9.12it/s]\u001b[A\n",
            " 78%|███████▊  | 129/165 [00:14<00:03,  9.12it/s]\u001b[A\n",
            " 79%|███████▉  | 130/165 [00:14<00:03,  9.11it/s]\u001b[A\n",
            " 79%|███████▉  | 131/165 [00:14<00:03,  9.08it/s]\u001b[A\n",
            " 80%|████████  | 132/165 [00:14<00:03,  9.07it/s]\u001b[A\n",
            " 81%|████████  | 133/165 [00:14<00:03,  9.08it/s]\u001b[A\n",
            " 81%|████████  | 134/165 [00:14<00:03,  9.09it/s]\u001b[A\n",
            " 82%|████████▏ | 135/165 [00:14<00:03,  9.07it/s]\u001b[A\n",
            " 82%|████████▏ | 136/165 [00:14<00:03,  9.08it/s]\u001b[A\n",
            " 83%|████████▎ | 137/165 [00:15<00:03,  9.10it/s]\u001b[A\n",
            " 84%|████████▎ | 138/165 [00:15<00:02,  9.12it/s]\u001b[A\n",
            " 84%|████████▍ | 139/165 [00:15<00:02,  9.11it/s]\u001b[A\n",
            " 85%|████████▍ | 140/165 [00:15<00:02,  9.11it/s]\u001b[A\n",
            " 85%|████████▌ | 141/165 [00:15<00:02,  9.06it/s]\u001b[A\n",
            " 86%|████████▌ | 142/165 [00:15<00:02,  9.09it/s]\u001b[A\n",
            " 87%|████████▋ | 143/165 [00:15<00:02,  9.12it/s]\u001b[A\n",
            " 87%|████████▋ | 144/165 [00:15<00:02,  9.13it/s]\u001b[A\n",
            " 88%|████████▊ | 145/165 [00:15<00:02,  9.14it/s]\u001b[A\n",
            " 88%|████████▊ | 146/165 [00:16<00:02,  9.13it/s]\u001b[A\n",
            " 89%|████████▉ | 147/165 [00:16<00:01,  9.13it/s]\u001b[A\n",
            " 90%|████████▉ | 148/165 [00:16<00:01,  9.13it/s]\u001b[A\n",
            " 90%|█████████ | 149/165 [00:16<00:01,  9.14it/s]\u001b[A\n",
            " 91%|█████████ | 150/165 [00:16<00:01,  9.12it/s]\u001b[A\n",
            " 92%|█████████▏| 151/165 [00:16<00:01,  9.12it/s]\u001b[A\n",
            " 92%|█████████▏| 152/165 [00:16<00:01,  9.13it/s]\u001b[A\n",
            " 93%|█████████▎| 153/165 [00:16<00:01,  9.14it/s]\u001b[A\n",
            " 93%|█████████▎| 154/165 [00:16<00:01,  9.14it/s]\u001b[A\n",
            " 94%|█████████▍| 155/165 [00:17<00:01,  9.12it/s]\u001b[A\n",
            " 95%|█████████▍| 156/165 [00:17<00:00,  9.08it/s]\u001b[A\n",
            " 95%|█████████▌| 157/165 [00:17<00:00,  9.07it/s]\u001b[A\n",
            " 96%|█████████▌| 158/165 [00:17<00:00,  9.10it/s]\u001b[A\n",
            " 96%|█████████▋| 159/165 [00:17<00:00,  9.12it/s]\u001b[A\n",
            " 97%|█████████▋| 160/165 [00:17<00:00,  9.12it/s]\u001b[A\n",
            " 98%|█████████▊| 161/165 [00:17<00:00,  9.14it/s]\u001b[A\n",
            " 98%|█████████▊| 162/165 [00:17<00:00,  9.14it/s]\u001b[A\n",
            " 99%|█████████▉| 163/165 [00:17<00:00,  9.16it/s]\u001b[A\n",
            "100%|██████████| 165/165 [00:18<00:00,  9.09it/s]\n",
            "========== model: bert-base-uncased fold: 4 inference ==========\n",
            "loading configuration file /content/activebus_bert_final/config.json\n",
            "Model config BertConfig {\n",
            "  \"_name_or_path\": \"activebus/BERT-XD_Review\",\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_ids\": 0,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.15.0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "loading weights file /content/activebus_bert_final/pytorch_model.bin\n",
            "Some weights of the model checkpoint at /content/activebus_bert_final were not used when initializing BertForSequenceClassification: ['cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /content/activebus_bert_final and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "\n",
            "  0%|          | 0/165 [00:00<?, ?it/s]\u001b[A\n",
            "  1%|          | 1/165 [00:00<00:18,  8.75it/s]\u001b[A\n",
            "  1%|          | 2/165 [00:00<00:18,  8.95it/s]\u001b[A\n",
            "  2%|▏         | 3/165 [00:00<00:18,  9.00it/s]\u001b[A\n",
            "  2%|▏         | 4/165 [00:00<00:17,  9.04it/s]\u001b[A\n",
            "  3%|▎         | 5/165 [00:00<00:17,  9.03it/s]\u001b[A\n",
            "  4%|▎         | 6/165 [00:00<00:17,  9.06it/s]\u001b[A\n",
            "  4%|▍         | 7/165 [00:00<00:17,  9.09it/s]\u001b[A\n",
            "  5%|▍         | 8/165 [00:00<00:17,  9.11it/s]\u001b[A\n",
            "  5%|▌         | 9/165 [00:00<00:17,  9.07it/s]\u001b[A\n",
            "  6%|▌         | 10/165 [00:01<00:17,  9.09it/s]\u001b[A\n",
            "  7%|▋         | 11/165 [00:01<00:16,  9.11it/s]\u001b[A\n",
            "  7%|▋         | 12/165 [00:01<00:16,  9.10it/s]\u001b[A\n",
            "  8%|▊         | 13/165 [00:01<00:16,  9.13it/s]\u001b[A\n",
            "  8%|▊         | 14/165 [00:01<00:16,  9.14it/s]\u001b[A\n",
            "  9%|▉         | 15/165 [00:01<00:16,  9.13it/s]\u001b[A\n",
            " 10%|▉         | 16/165 [00:01<00:16,  9.14it/s]\u001b[A\n",
            " 10%|█         | 17/165 [00:01<00:16,  9.15it/s]\u001b[A\n",
            " 11%|█         | 18/165 [00:01<00:16,  9.15it/s]\u001b[A\n",
            " 12%|█▏        | 19/165 [00:02<00:16,  9.12it/s]\u001b[A\n",
            " 12%|█▏        | 20/165 [00:02<00:15,  9.12it/s]\u001b[A\n",
            " 13%|█▎        | 21/165 [00:02<00:15,  9.12it/s]\u001b[A\n",
            " 13%|█▎        | 22/165 [00:02<00:15,  9.12it/s]\u001b[A\n",
            " 14%|█▍        | 23/165 [00:02<00:15,  9.13it/s]\u001b[A\n",
            " 15%|█▍        | 24/165 [00:02<00:15,  9.12it/s]\u001b[A\n",
            " 15%|█▌        | 25/165 [00:02<00:15,  9.10it/s]\u001b[A\n",
            " 16%|█▌        | 26/165 [00:02<00:15,  9.09it/s]\u001b[A\n",
            " 16%|█▋        | 27/165 [00:02<00:15,  9.09it/s]\u001b[A\n",
            " 17%|█▋        | 28/165 [00:03<00:15,  9.10it/s]\u001b[A\n",
            " 18%|█▊        | 29/165 [00:03<00:14,  9.11it/s]\u001b[A\n",
            " 18%|█▊        | 30/165 [00:03<00:14,  9.08it/s]\u001b[A\n",
            " 19%|█▉        | 31/165 [00:03<00:14,  9.09it/s]\u001b[A\n",
            " 19%|█▉        | 32/165 [00:03<00:14,  9.11it/s]\u001b[A\n",
            " 20%|██        | 33/165 [00:03<00:14,  9.12it/s]\u001b[A\n",
            " 21%|██        | 34/165 [00:03<00:14,  9.10it/s]\u001b[A\n",
            " 21%|██        | 35/165 [00:03<00:14,  9.11it/s]\u001b[A\n",
            " 22%|██▏       | 36/165 [00:03<00:14,  9.05it/s]\u001b[A\n",
            " 22%|██▏       | 37/165 [00:04<00:14,  9.07it/s]\u001b[A\n",
            " 23%|██▎       | 38/165 [00:04<00:13,  9.08it/s]\u001b[A\n",
            " 24%|██▎       | 39/165 [00:04<00:13,  9.07it/s]\u001b[A\n",
            " 24%|██▍       | 40/165 [00:04<00:13,  9.09it/s]\u001b[A\n",
            " 25%|██▍       | 41/165 [00:04<00:13,  9.10it/s]\u001b[A\n",
            " 25%|██▌       | 42/165 [00:04<00:13,  9.11it/s]\u001b[A\n",
            " 26%|██▌       | 43/165 [00:04<00:13,  9.06it/s]\u001b[A\n",
            " 27%|██▋       | 44/165 [00:04<00:13,  9.08it/s]\u001b[A\n",
            " 27%|██▋       | 45/165 [00:04<00:13,  9.07it/s]\u001b[A\n",
            " 28%|██▊       | 46/165 [00:05<00:13,  9.08it/s]\u001b[A\n",
            " 28%|██▊       | 47/165 [00:05<00:13,  9.07it/s]\u001b[A\n",
            " 29%|██▉       | 48/165 [00:05<00:12,  9.08it/s]\u001b[A\n",
            " 30%|██▉       | 49/165 [00:05<00:12,  9.09it/s]\u001b[A\n",
            " 30%|███       | 50/165 [00:05<00:12,  9.12it/s]\u001b[A\n",
            " 31%|███       | 51/165 [00:05<00:12,  9.12it/s]\u001b[A\n",
            " 32%|███▏      | 52/165 [00:05<00:12,  9.13it/s]\u001b[A\n",
            " 32%|███▏      | 53/165 [00:05<00:12,  9.13it/s]\u001b[A\n",
            " 33%|███▎      | 54/165 [00:05<00:12,  9.11it/s]\u001b[A\n",
            " 33%|███▎      | 55/165 [00:06<00:12,  9.09it/s]\u001b[A\n",
            " 34%|███▍      | 56/165 [00:06<00:11,  9.10it/s]\u001b[A\n",
            " 35%|███▍      | 57/165 [00:06<00:11,  9.11it/s]\u001b[A\n",
            " 35%|███▌      | 58/165 [00:06<00:11,  9.12it/s]\u001b[A\n",
            " 36%|███▌      | 59/165 [00:06<00:11,  9.12it/s]\u001b[A\n",
            " 36%|███▋      | 60/165 [00:06<00:11,  9.12it/s]\u001b[A\n",
            " 37%|███▋      | 61/165 [00:06<00:11,  9.13it/s]\u001b[A\n",
            " 38%|███▊      | 62/165 [00:06<00:11,  9.12it/s]\u001b[A\n",
            " 38%|███▊      | 63/165 [00:06<00:11,  9.13it/s]\u001b[A\n",
            " 39%|███▉      | 64/165 [00:07<00:11,  9.12it/s]\u001b[A\n",
            " 39%|███▉      | 65/165 [00:07<00:10,  9.12it/s]\u001b[A\n",
            " 40%|████      | 66/165 [00:07<00:10,  9.10it/s]\u001b[A\n",
            " 41%|████      | 67/165 [00:07<00:10,  9.12it/s]\u001b[A\n",
            " 41%|████      | 68/165 [00:07<00:10,  9.11it/s]\u001b[A\n",
            " 42%|████▏     | 69/165 [00:07<00:10,  9.11it/s]\u001b[A\n",
            " 42%|████▏     | 70/165 [00:07<00:10,  9.11it/s]\u001b[A\n",
            " 43%|████▎     | 71/165 [00:07<00:10,  9.11it/s]\u001b[A\n",
            " 44%|████▎     | 72/165 [00:07<00:10,  9.13it/s]\u001b[A\n",
            " 44%|████▍     | 73/165 [00:08<00:10,  9.12it/s]\u001b[A\n",
            " 45%|████▍     | 74/165 [00:08<00:09,  9.11it/s]\u001b[A\n",
            " 45%|████▌     | 75/165 [00:08<00:09,  9.12it/s]\u001b[A\n",
            " 46%|████▌     | 76/165 [00:08<00:09,  9.12it/s]\u001b[A\n",
            " 47%|████▋     | 77/165 [00:08<00:09,  9.11it/s]\u001b[A\n",
            " 47%|████▋     | 78/165 [00:08<00:09,  9.09it/s]\u001b[A\n",
            " 48%|████▊     | 79/165 [00:08<00:09,  9.09it/s]\u001b[A\n",
            " 48%|████▊     | 80/165 [00:08<00:09,  9.10it/s]\u001b[A\n",
            " 49%|████▉     | 81/165 [00:08<00:09,  9.11it/s]\u001b[A\n",
            " 50%|████▉     | 82/165 [00:09<00:09,  9.12it/s]\u001b[A\n",
            " 50%|█████     | 83/165 [00:09<00:08,  9.13it/s]\u001b[A\n",
            " 51%|█████     | 84/165 [00:09<00:08,  9.14it/s]\u001b[A\n",
            " 52%|█████▏    | 85/165 [00:09<00:08,  9.11it/s]\u001b[A\n",
            " 52%|█████▏    | 86/165 [00:09<00:08,  9.12it/s]\u001b[A\n",
            " 53%|█████▎    | 87/165 [00:09<00:08,  9.12it/s]\u001b[A\n",
            " 53%|█████▎    | 88/165 [00:09<00:08,  9.12it/s]\u001b[A\n",
            " 54%|█████▍    | 89/165 [00:09<00:08,  9.12it/s]\u001b[A\n",
            " 55%|█████▍    | 90/165 [00:09<00:08,  9.13it/s]\u001b[A\n",
            " 55%|█████▌    | 91/165 [00:09<00:08,  9.14it/s]\u001b[A\n",
            " 56%|█████▌    | 92/165 [00:10<00:07,  9.14it/s]\u001b[A\n",
            " 56%|█████▋    | 93/165 [00:10<00:07,  9.15it/s]\u001b[A\n",
            " 57%|█████▋    | 94/165 [00:10<00:07,  9.15it/s]\u001b[A\n",
            " 58%|█████▊    | 95/165 [00:10<00:07,  9.16it/s]\u001b[A\n",
            " 58%|█████▊    | 96/165 [00:10<00:07,  9.15it/s]\u001b[A\n",
            " 59%|█████▉    | 97/165 [00:10<00:07,  9.15it/s]\u001b[A\n",
            " 59%|█████▉    | 98/165 [00:10<00:07,  9.15it/s]\u001b[A\n",
            " 60%|██████    | 99/165 [00:10<00:07,  9.13it/s]\u001b[A\n",
            " 61%|██████    | 100/165 [00:10<00:07,  9.13it/s]\u001b[A\n",
            " 61%|██████    | 101/165 [00:11<00:07,  9.09it/s]\u001b[A\n",
            " 62%|██████▏   | 102/165 [00:11<00:06,  9.09it/s]\u001b[A\n",
            " 62%|██████▏   | 103/165 [00:11<00:06,  9.11it/s]\u001b[A\n",
            " 63%|██████▎   | 104/165 [00:11<00:06,  9.12it/s]\u001b[A\n",
            " 64%|██████▎   | 105/165 [00:11<00:06,  9.12it/s]\u001b[A\n",
            " 64%|██████▍   | 106/165 [00:11<00:06,  9.11it/s]\u001b[A\n",
            " 65%|██████▍   | 107/165 [00:11<00:06,  9.10it/s]\u001b[A\n",
            " 65%|██████▌   | 108/165 [00:11<00:06,  9.08it/s]\u001b[A\n",
            " 66%|██████▌   | 109/165 [00:11<00:06,  9.09it/s]\u001b[A\n",
            " 67%|██████▋   | 110/165 [00:12<00:06,  8.99it/s]\u001b[A\n",
            " 67%|██████▋   | 111/165 [00:12<00:05,  9.04it/s]\u001b[A\n",
            " 68%|██████▊   | 112/165 [00:12<00:05,  9.05it/s]\u001b[A\n",
            " 68%|██████▊   | 113/165 [00:12<00:05,  9.07it/s]\u001b[A\n",
            " 69%|██████▉   | 114/165 [00:12<00:05,  9.08it/s]\u001b[A\n",
            " 70%|██████▉   | 115/165 [00:12<00:05,  9.08it/s]\u001b[A\n",
            " 70%|███████   | 116/165 [00:12<00:05,  9.09it/s]\u001b[A\n",
            " 71%|███████   | 117/165 [00:12<00:05,  9.11it/s]\u001b[A\n",
            " 72%|███████▏  | 118/165 [00:12<00:05,  9.13it/s]\u001b[A\n",
            " 72%|███████▏  | 119/165 [00:13<00:05,  9.11it/s]\u001b[A\n",
            " 73%|███████▎  | 120/165 [00:13<00:04,  9.12it/s]\u001b[A\n",
            " 73%|███████▎  | 121/165 [00:13<00:04,  9.13it/s]\u001b[A\n",
            " 74%|███████▍  | 122/165 [00:13<00:04,  9.13it/s]\u001b[A\n",
            " 75%|███████▍  | 123/165 [00:13<00:04,  9.12it/s]\u001b[A\n",
            " 75%|███████▌  | 124/165 [00:13<00:04,  9.11it/s]\u001b[A\n",
            " 76%|███████▌  | 125/165 [00:13<00:04,  9.12it/s]\u001b[A\n",
            " 76%|███████▋  | 126/165 [00:13<00:04,  9.11it/s]\u001b[A\n",
            " 77%|███████▋  | 127/165 [00:13<00:04,  9.12it/s]\u001b[A\n",
            " 78%|███████▊  | 128/165 [00:14<00:04,  9.11it/s]\u001b[A\n",
            " 78%|███████▊  | 129/165 [00:14<00:03,  9.11it/s]\u001b[A\n",
            " 79%|███████▉  | 130/165 [00:14<00:03,  9.11it/s]\u001b[A\n",
            " 79%|███████▉  | 131/165 [00:14<00:03,  9.11it/s]\u001b[A\n",
            " 80%|████████  | 132/165 [00:14<00:03,  9.12it/s]\u001b[A\n",
            " 81%|████████  | 133/165 [00:14<00:03,  9.12it/s]\u001b[A\n",
            " 81%|████████  | 134/165 [00:14<00:03,  9.11it/s]\u001b[A\n",
            " 82%|████████▏ | 135/165 [00:14<00:03,  9.11it/s]\u001b[A\n",
            " 82%|████████▏ | 136/165 [00:14<00:03,  9.14it/s]\u001b[A\n",
            " 83%|████████▎ | 137/165 [00:15<00:03,  9.13it/s]\u001b[A\n",
            " 84%|████████▎ | 138/165 [00:15<00:02,  9.14it/s]\u001b[A\n",
            " 84%|████████▍ | 139/165 [00:15<00:02,  9.15it/s]\u001b[A\n",
            " 85%|████████▍ | 140/165 [00:15<00:02,  9.14it/s]\u001b[A\n",
            " 85%|████████▌ | 141/165 [00:15<00:02,  9.12it/s]\u001b[A\n",
            " 86%|████████▌ | 142/165 [00:15<00:02,  9.14it/s]\u001b[A\n",
            " 87%|████████▋ | 143/165 [00:15<00:02,  9.11it/s]\u001b[A\n",
            " 87%|████████▋ | 144/165 [00:15<00:02,  9.11it/s]\u001b[A\n",
            " 88%|████████▊ | 145/165 [00:15<00:02,  9.11it/s]\u001b[A\n",
            " 88%|████████▊ | 146/165 [00:16<00:02,  9.13it/s]\u001b[A\n",
            " 89%|████████▉ | 147/165 [00:16<00:01,  9.09it/s]\u001b[A\n",
            " 90%|████████▉ | 148/165 [00:16<00:01,  9.09it/s]\u001b[A\n",
            " 90%|█████████ | 149/165 [00:16<00:01,  9.10it/s]\u001b[A\n",
            " 91%|█████████ | 150/165 [00:16<00:01,  9.09it/s]\u001b[A\n",
            " 92%|█████████▏| 151/165 [00:16<00:01,  9.10it/s]\u001b[A\n",
            " 92%|█████████▏| 152/165 [00:16<00:01,  9.08it/s]\u001b[A\n",
            " 93%|█████████▎| 153/165 [00:16<00:01,  9.08it/s]\u001b[A\n",
            " 93%|█████████▎| 154/165 [00:16<00:01,  9.09it/s]\u001b[A\n",
            " 94%|█████████▍| 155/165 [00:17<00:01,  9.09it/s]\u001b[A\n",
            " 95%|█████████▍| 156/165 [00:17<00:00,  9.09it/s]\u001b[A\n",
            " 95%|█████████▌| 157/165 [00:17<00:00,  9.10it/s]\u001b[A\n",
            " 96%|█████████▌| 158/165 [00:17<00:00,  9.10it/s]\u001b[A\n",
            " 96%|█████████▋| 159/165 [00:17<00:00,  9.08it/s]\u001b[A\n",
            " 97%|█████████▋| 160/165 [00:17<00:00,  9.08it/s]\u001b[A\n",
            " 98%|█████████▊| 161/165 [00:17<00:00,  9.12it/s]\u001b[A\n",
            " 98%|█████████▊| 162/165 [00:17<00:00,  9.12it/s]\u001b[A\n",
            " 99%|█████████▉| 163/165 [00:17<00:00,  9.13it/s]\u001b[A\n",
            "100%|██████████| 165/165 [00:18<00:00,  9.10it/s]\n",
            " 17%|█▋        | 1/6 [47:37<3:58:09, 2858.00s/it]Didn't find file /content/activebus_bert_final/added_tokens.json. We won't load it.\n",
            "loading file /content/activebus_bert_final/vocab.txt\n",
            "loading file /content/activebus_bert_final/tokenizer.json\n",
            "loading file None\n",
            "loading file /content/activebus_bert_final/special_tokens_map.json\n",
            "loading file /content/activebus_bert_final/tokenizer_config.json\n",
            "========== fold: 0 training ==========\n",
            "========== fold: 0 training ==========\n",
            "Didn't find file /content/activebus_bert_final/added_tokens.json. We won't load it.\n",
            "loading file /content/activebus_bert_final/vocab.txt\n",
            "loading file None\n",
            "loading file /content/activebus_bert_final/special_tokens_map.json\n",
            "loading file /content/activebus_bert_final/tokenizer_config.json\n",
            "loading file /content/activebus_bert_final/tokenizer.json\n",
            "Didn't find file /content/activebus_bert_final/added_tokens.json. We won't load it.\n",
            "loading file /content/activebus_bert_final/vocab.txt\n",
            "loading file None\n",
            "loading file /content/activebus_bert_final/special_tokens_map.json\n",
            "loading file /content/activebus_bert_final/tokenizer_config.json\n",
            "loading file /content/activebus_bert_final/tokenizer.json\n",
            "loading configuration file /content/activebus_bert_final/config.json\n",
            "Model config BertConfig {\n",
            "  \"_name_or_path\": \"activebus/BERT-XD_Review\",\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_ids\": 0,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.15.0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "loading weights file /content/activebus_bert_final/pytorch_model.bin\n",
            "Some weights of the model checkpoint at /content/activebus_bert_final were not used when initializing BertForSequenceClassification: ['cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /content/activebus_bert_final and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: [1][0/306] Elapsed 0m 0s (remain 2m 49s) Loss: 0.6944 \n",
            "Epoch: [1][100/306] Elapsed 0m 33s (remain 1m 6s) Loss: 0.3019 \n",
            "Epoch: [1][200/306] Elapsed 1m 5s (remain 0m 34s) Loss: 0.2254 \n",
            "Epoch: [1][300/306] Elapsed 1m 37s (remain 0m 1s) Loss: 0.1901 \n",
            "Epoch: [1][305/306] Elapsed 1m 39s (remain 0m 0s) Loss: 0.1890 \n",
            "EVAL: [0/77] Elapsed 0m 0s (remain 0m 25s) Loss: 0.0157 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1 - avg_train_loss: 0.1890  avg_val_loss: 0.1205  time: 108s\n",
            "Epoch 1 - avg_train_loss: 0.1890  avg_val_loss: 0.1205  time: 108s\n",
            "Epoch 1 - Score: 0.12050927933650576\n",
            "Epoch 1 - Score: 0.12050927933650576\n",
            "Epoch 1 - Save Best Score: 0.1205 Model\n",
            "Epoch 1 - Save Best Score: 0.1205 Model\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EVAL: [76/77] Elapsed 0m 8s (remain 0m 0s) Loss: 0.1205 \n",
            "Log loss for 0 is 0.12050927933650576\n",
            "Epoch: [2][0/306] Elapsed 0m 0s (remain 2m 51s) Loss: 0.1656 \n",
            "Epoch: [2][100/306] Elapsed 0m 33s (remain 1m 7s) Loss: 0.0733 \n",
            "Epoch: [2][200/306] Elapsed 1m 5s (remain 0m 34s) Loss: 0.0783 \n",
            "Epoch: [2][300/306] Elapsed 1m 38s (remain 0m 1s) Loss: 0.0815 \n",
            "Epoch: [2][305/306] Elapsed 1m 39s (remain 0m 0s) Loss: 0.0817 \n",
            "EVAL: [0/77] Elapsed 0m 0s (remain 0m 24s) Loss: 0.0511 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2 - avg_train_loss: 0.0817  avg_val_loss: 0.1250  time: 108s\n",
            "Epoch 2 - avg_train_loss: 0.0817  avg_val_loss: 0.1250  time: 108s\n",
            "Epoch 2 - Score: 0.1250107887398007\n",
            "Epoch 2 - Score: 0.1250107887398007\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EVAL: [76/77] Elapsed 0m 8s (remain 0m 0s) Loss: 0.1250 \n",
            "Log loss for 1 is 0.1250107887398007\n",
            "Epoch: [3][0/306] Elapsed 0m 0s (remain 2m 55s) Loss: 0.0075 \n",
            "Epoch: [3][100/306] Elapsed 0m 33s (remain 1m 7s) Loss: 0.0486 \n",
            "Epoch: [3][200/306] Elapsed 1m 5s (remain 0m 34s) Loss: 0.0379 \n",
            "Epoch: [3][300/306] Elapsed 1m 38s (remain 0m 1s) Loss: 0.0379 \n",
            "Epoch: [3][305/306] Elapsed 1m 39s (remain 0m 0s) Loss: 0.0388 \n",
            "EVAL: [0/77] Elapsed 0m 0s (remain 0m 24s) Loss: 0.1816 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3 - avg_train_loss: 0.0388  avg_val_loss: 0.1400  time: 108s\n",
            "Epoch 3 - avg_train_loss: 0.0388  avg_val_loss: 0.1400  time: 108s\n",
            "Epoch 3 - Score: 0.13995817050482648\n",
            "Epoch 3 - Score: 0.13995817050482648\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EVAL: [76/77] Elapsed 0m 8s (remain 0m 0s) Loss: 0.1400 \n",
            "Log loss for 2 is 0.13995817050482648\n",
            "Epoch: [4][0/306] Elapsed 0m 0s (remain 2m 57s) Loss: 0.0017 \n",
            "Epoch: [4][100/306] Elapsed 0m 33s (remain 1m 7s) Loss: 0.0235 \n",
            "Epoch: [4][200/306] Elapsed 1m 5s (remain 0m 34s) Loss: 0.0242 \n",
            "Epoch: [4][300/306] Elapsed 1m 38s (remain 0m 1s) Loss: 0.0261 \n",
            "Epoch: [4][305/306] Elapsed 1m 39s (remain 0m 0s) Loss: 0.0258 \n",
            "EVAL: [0/77] Elapsed 0m 0s (remain 0m 25s) Loss: 0.1008 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4 - avg_train_loss: 0.0258  avg_val_loss: 0.1733  time: 108s\n",
            "Epoch 4 - avg_train_loss: 0.0258  avg_val_loss: 0.1733  time: 108s\n",
            "Epoch 4 - Score: 0.17325411650777078\n",
            "Epoch 4 - Score: 0.17325411650777078\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EVAL: [76/77] Elapsed 0m 8s (remain 0m 0s) Loss: 0.1733 \n",
            "Log loss for 3 is 0.17325411650777078\n",
            "Epoch: [5][0/306] Elapsed 0m 0s (remain 2m 52s) Loss: 0.0027 \n",
            "Epoch: [5][100/306] Elapsed 0m 33s (remain 1m 7s) Loss: 0.0098 \n",
            "Epoch: [5][200/306] Elapsed 1m 5s (remain 0m 34s) Loss: 0.0082 \n",
            "Epoch: [5][300/306] Elapsed 1m 38s (remain 0m 1s) Loss: 0.0090 \n",
            "Epoch: [5][305/306] Elapsed 1m 39s (remain 0m 0s) Loss: 0.0089 \n",
            "EVAL: [0/77] Elapsed 0m 0s (remain 0m 24s) Loss: 0.2549 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5 - avg_train_loss: 0.0089  avg_val_loss: 0.2100  time: 108s\n",
            "Epoch 5 - avg_train_loss: 0.0089  avg_val_loss: 0.2100  time: 108s\n",
            "Epoch 5 - Score: 0.20996811355769704\n",
            "Epoch 5 - Score: 0.20996811355769704\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EVAL: [76/77] Elapsed 0m 8s (remain 0m 0s) Loss: 0.2100 \n",
            "Log loss for 4 is 0.20996811355769704\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "========== fold: 0 result ==========\n",
            "========== fold: 0 result ==========\n",
            "Score: 0.12051\n",
            "Score: 0.12051\n",
            "========== fold: 1 training ==========\n",
            "========== fold: 1 training ==========\n",
            "Didn't find file /content/activebus_bert_final/added_tokens.json. We won't load it.\n",
            "loading file /content/activebus_bert_final/vocab.txt\n",
            "loading file None\n",
            "loading file /content/activebus_bert_final/special_tokens_map.json\n",
            "loading file /content/activebus_bert_final/tokenizer_config.json\n",
            "loading file /content/activebus_bert_final/tokenizer.json\n",
            "Didn't find file /content/activebus_bert_final/added_tokens.json. We won't load it.\n",
            "loading file /content/activebus_bert_final/vocab.txt\n",
            "loading file None\n",
            "loading file /content/activebus_bert_final/special_tokens_map.json\n",
            "loading file /content/activebus_bert_final/tokenizer_config.json\n",
            "loading file /content/activebus_bert_final/tokenizer.json\n",
            "loading configuration file /content/activebus_bert_final/config.json\n",
            "Model config BertConfig {\n",
            "  \"_name_or_path\": \"activebus/BERT-XD_Review\",\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_ids\": 0,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.15.0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "loading weights file /content/activebus_bert_final/pytorch_model.bin\n",
            "Some weights of the model checkpoint at /content/activebus_bert_final were not used when initializing BertForSequenceClassification: ['cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /content/activebus_bert_final and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: [1][0/306] Elapsed 0m 0s (remain 3m 1s) Loss: 0.6431 \n",
            "Epoch: [1][100/306] Elapsed 0m 33s (remain 1m 7s) Loss: 0.2999 \n",
            "Epoch: [1][200/306] Elapsed 1m 5s (remain 0m 34s) Loss: 0.2248 \n",
            "Epoch: [1][300/306] Elapsed 1m 38s (remain 0m 1s) Loss: 0.1899 \n",
            "Epoch: [1][305/306] Elapsed 1m 39s (remain 0m 0s) Loss: 0.1880 \n",
            "EVAL: [0/77] Elapsed 0m 0s (remain 0m 25s) Loss: 0.1030 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1 - avg_train_loss: 0.1880  avg_val_loss: 0.0879  time: 108s\n",
            "Epoch 1 - avg_train_loss: 0.1880  avg_val_loss: 0.0879  time: 108s\n",
            "Epoch 1 - Score: 0.08787755728334935\n",
            "Epoch 1 - Score: 0.08787755728334935\n",
            "Epoch 1 - Save Best Score: 0.0879 Model\n",
            "Epoch 1 - Save Best Score: 0.0879 Model\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EVAL: [76/77] Elapsed 0m 8s (remain 0m 0s) Loss: 0.0879 \n",
            "Log loss for 0 is 0.08787755728334935\n",
            "Epoch: [2][0/306] Elapsed 0m 0s (remain 2m 52s) Loss: 0.4724 \n",
            "Epoch: [2][100/306] Elapsed 0m 33s (remain 1m 7s) Loss: 0.0818 \n",
            "Epoch: [2][200/306] Elapsed 1m 5s (remain 0m 34s) Loss: 0.0820 \n",
            "Epoch: [2][300/306] Elapsed 1m 38s (remain 0m 1s) Loss: 0.0864 \n",
            "Epoch: [2][305/306] Elapsed 1m 39s (remain 0m 0s) Loss: 0.0857 \n",
            "EVAL: [0/77] Elapsed 0m 0s (remain 0m 24s) Loss: 0.0797 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2 - avg_train_loss: 0.0857  avg_val_loss: 0.0906  time: 108s\n",
            "Epoch 2 - avg_train_loss: 0.0857  avg_val_loss: 0.0906  time: 108s\n",
            "Epoch 2 - Score: 0.09060475260701285\n",
            "Epoch 2 - Score: 0.09060475260701285\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EVAL: [76/77] Elapsed 0m 8s (remain 0m 0s) Loss: 0.0906 \n",
            "Log loss for 1 is 0.09060475260701285\n",
            "Epoch: [3][0/306] Elapsed 0m 0s (remain 2m 55s) Loss: 0.0047 \n",
            "Epoch: [3][100/306] Elapsed 0m 33s (remain 1m 7s) Loss: 0.0415 \n",
            "Epoch: [3][200/306] Elapsed 1m 5s (remain 0m 34s) Loss: 0.0487 \n",
            "Epoch: [3][300/306] Elapsed 1m 38s (remain 0m 1s) Loss: 0.0484 \n",
            "Epoch: [3][305/306] Elapsed 1m 39s (remain 0m 0s) Loss: 0.0480 \n",
            "EVAL: [0/77] Elapsed 0m 0s (remain 0m 23s) Loss: 0.3741 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3 - avg_train_loss: 0.0480  avg_val_loss: 0.1432  time: 108s\n",
            "Epoch 3 - avg_train_loss: 0.0480  avg_val_loss: 0.1432  time: 108s\n",
            "Epoch 3 - Score: 0.1432114256989551\n",
            "Epoch 3 - Score: 0.1432114256989551\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EVAL: [76/77] Elapsed 0m 8s (remain 0m 0s) Loss: 0.1432 \n",
            "Log loss for 2 is 0.1432114256989551\n",
            "Epoch: [4][0/306] Elapsed 0m 0s (remain 2m 50s) Loss: 0.1582 \n",
            "Epoch: [4][100/306] Elapsed 0m 32s (remain 1m 6s) Loss: 0.0261 \n",
            "Epoch: [4][200/306] Elapsed 1m 5s (remain 0m 34s) Loss: 0.0233 \n",
            "Epoch: [4][300/306] Elapsed 1m 37s (remain 0m 1s) Loss: 0.0291 \n",
            "Epoch: [4][305/306] Elapsed 1m 39s (remain 0m 0s) Loss: 0.0292 \n",
            "EVAL: [0/77] Elapsed 0m 0s (remain 0m 25s) Loss: 0.1692 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4 - avg_train_loss: 0.0292  avg_val_loss: 0.1246  time: 108s\n",
            "Epoch 4 - avg_train_loss: 0.0292  avg_val_loss: 0.1246  time: 108s\n",
            "Epoch 4 - Score: 0.12459482887260556\n",
            "Epoch 4 - Score: 0.12459482887260556\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EVAL: [76/77] Elapsed 0m 8s (remain 0m 0s) Loss: 0.1246 \n",
            "Log loss for 3 is 0.12459482887260556\n",
            "Epoch: [5][0/306] Elapsed 0m 0s (remain 2m 57s) Loss: 0.0176 \n",
            "Epoch: [5][100/306] Elapsed 0m 33s (remain 1m 7s) Loss: 0.0141 \n",
            "Epoch: [5][200/306] Elapsed 1m 5s (remain 0m 34s) Loss: 0.0169 \n",
            "Epoch: [5][300/306] Elapsed 1m 38s (remain 0m 1s) Loss: 0.0166 \n",
            "Epoch: [5][305/306] Elapsed 1m 39s (remain 0m 0s) Loss: 0.0163 \n",
            "EVAL: [0/77] Elapsed 0m 0s (remain 0m 24s) Loss: 0.2642 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5 - avg_train_loss: 0.0163  avg_val_loss: 0.1597  time: 108s\n",
            "Epoch 5 - avg_train_loss: 0.0163  avg_val_loss: 0.1597  time: 108s\n",
            "Epoch 5 - Score: 0.15968367117401588\n",
            "Epoch 5 - Score: 0.15968367117401588\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EVAL: [76/77] Elapsed 0m 8s (remain 0m 0s) Loss: 0.1597 \n",
            "Log loss for 4 is 0.15968367117401588\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "========== fold: 1 result ==========\n",
            "========== fold: 1 result ==========\n",
            "Score: 0.08788\n",
            "Score: 0.08788\n",
            "========== fold: 2 training ==========\n",
            "========== fold: 2 training ==========\n",
            "Didn't find file /content/activebus_bert_final/added_tokens.json. We won't load it.\n",
            "loading file /content/activebus_bert_final/vocab.txt\n",
            "loading file None\n",
            "loading file /content/activebus_bert_final/special_tokens_map.json\n",
            "loading file /content/activebus_bert_final/tokenizer_config.json\n",
            "loading file /content/activebus_bert_final/tokenizer.json\n",
            "Didn't find file /content/activebus_bert_final/added_tokens.json. We won't load it.\n",
            "loading file /content/activebus_bert_final/vocab.txt\n",
            "loading file None\n",
            "loading file /content/activebus_bert_final/special_tokens_map.json\n",
            "loading file /content/activebus_bert_final/tokenizer_config.json\n",
            "loading file /content/activebus_bert_final/tokenizer.json\n",
            "loading configuration file /content/activebus_bert_final/config.json\n",
            "Model config BertConfig {\n",
            "  \"_name_or_path\": \"activebus/BERT-XD_Review\",\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_ids\": 0,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.15.0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "loading weights file /content/activebus_bert_final/pytorch_model.bin\n",
            "Some weights of the model checkpoint at /content/activebus_bert_final were not used when initializing BertForSequenceClassification: ['cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /content/activebus_bert_final and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: [1][0/306] Elapsed 0m 0s (remain 2m 58s) Loss: 0.6898 \n",
            "Epoch: [1][100/306] Elapsed 0m 33s (remain 1m 6s) Loss: 0.3072 \n",
            "Epoch: [1][200/306] Elapsed 1m 5s (remain 0m 34s) Loss: 0.2312 \n",
            "Epoch: [1][300/306] Elapsed 1m 37s (remain 0m 1s) Loss: 0.1934 \n",
            "Epoch: [1][305/306] Elapsed 1m 39s (remain 0m 0s) Loss: 0.1937 \n",
            "EVAL: [0/77] Elapsed 0m 0s (remain 0m 25s) Loss: 0.0289 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1 - avg_train_loss: 0.1937  avg_val_loss: 0.1136  time: 108s\n",
            "Epoch 1 - avg_train_loss: 0.1937  avg_val_loss: 0.1136  time: 108s\n",
            "Epoch 1 - Score: 0.11360801353074532\n",
            "Epoch 1 - Score: 0.11360801353074532\n",
            "Epoch 1 - Save Best Score: 0.1136 Model\n",
            "Epoch 1 - Save Best Score: 0.1136 Model\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EVAL: [76/77] Elapsed 0m 8s (remain 0m 0s) Loss: 0.1136 \n",
            "Log loss for 0 is 0.11360801353074532\n",
            "Epoch: [2][0/306] Elapsed 0m 0s (remain 2m 44s) Loss: 0.0142 \n",
            "Epoch: [2][100/306] Elapsed 0m 32s (remain 1m 6s) Loss: 0.0675 \n",
            "Epoch: [2][200/306] Elapsed 1m 5s (remain 0m 34s) Loss: 0.0778 \n",
            "Epoch: [2][300/306] Elapsed 1m 37s (remain 0m 1s) Loss: 0.0763 \n",
            "Epoch: [2][305/306] Elapsed 1m 39s (remain 0m 0s) Loss: 0.0755 \n",
            "EVAL: [0/77] Elapsed 0m 0s (remain 0m 25s) Loss: 0.0075 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2 - avg_train_loss: 0.0755  avg_val_loss: 0.1622  time: 108s\n",
            "Epoch 2 - avg_train_loss: 0.0755  avg_val_loss: 0.1622  time: 108s\n",
            "Epoch 2 - Score: 0.1621917772821068\n",
            "Epoch 2 - Score: 0.1621917772821068\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EVAL: [76/77] Elapsed 0m 8s (remain 0m 0s) Loss: 0.1622 \n",
            "Log loss for 1 is 0.1621917772821068\n",
            "Epoch: [3][0/306] Elapsed 0m 0s (remain 2m 53s) Loss: 0.0667 \n",
            "Epoch: [3][100/306] Elapsed 0m 32s (remain 1m 6s) Loss: 0.0328 \n",
            "Epoch: [3][200/306] Elapsed 1m 5s (remain 0m 34s) Loss: 0.0431 \n",
            "Epoch: [3][300/306] Elapsed 1m 37s (remain 0m 1s) Loss: 0.0432 \n",
            "Epoch: [3][305/306] Elapsed 1m 39s (remain 0m 0s) Loss: 0.0430 \n",
            "EVAL: [0/77] Elapsed 0m 0s (remain 0m 25s) Loss: 0.0181 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3 - avg_train_loss: 0.0430  avg_val_loss: 0.1587  time: 108s\n",
            "Epoch 3 - avg_train_loss: 0.0430  avg_val_loss: 0.1587  time: 108s\n",
            "Epoch 3 - Score: 0.15874584227355962\n",
            "Epoch 3 - Score: 0.15874584227355962\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EVAL: [76/77] Elapsed 0m 8s (remain 0m 0s) Loss: 0.1587 \n",
            "Log loss for 2 is 0.15874584227355962\n",
            "Epoch: [4][0/306] Elapsed 0m 0s (remain 2m 53s) Loss: 0.0130 \n",
            "Epoch: [4][100/306] Elapsed 0m 33s (remain 1m 7s) Loss: 0.0153 \n",
            "Epoch: [4][200/306] Elapsed 1m 5s (remain 0m 34s) Loss: 0.0288 \n",
            "Epoch: [4][300/306] Elapsed 1m 37s (remain 0m 1s) Loss: 0.0247 \n",
            "Epoch: [4][305/306] Elapsed 1m 39s (remain 0m 0s) Loss: 0.0244 \n",
            "EVAL: [0/77] Elapsed 0m 0s (remain 0m 25s) Loss: 0.0059 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4 - avg_train_loss: 0.0244  avg_val_loss: 0.2119  time: 108s\n",
            "Epoch 4 - avg_train_loss: 0.0244  avg_val_loss: 0.2119  time: 108s\n",
            "Epoch 4 - Score: 0.21193618030612663\n",
            "Epoch 4 - Score: 0.21193618030612663\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EVAL: [76/77] Elapsed 0m 8s (remain 0m 0s) Loss: 0.2119 \n",
            "Log loss for 3 is 0.21193618030612663\n",
            "Epoch: [5][0/306] Elapsed 0m 0s (remain 2m 49s) Loss: 0.0021 \n",
            "Epoch: [5][100/306] Elapsed 0m 33s (remain 1m 7s) Loss: 0.0049 \n",
            "Epoch: [5][200/306] Elapsed 1m 5s (remain 0m 34s) Loss: 0.0062 \n",
            "Epoch: [5][300/306] Elapsed 1m 37s (remain 0m 1s) Loss: 0.0085 \n",
            "Epoch: [5][305/306] Elapsed 1m 39s (remain 0m 0s) Loss: 0.0084 \n",
            "EVAL: [0/77] Elapsed 0m 0s (remain 0m 24s) Loss: 0.0911 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5 - avg_train_loss: 0.0084  avg_val_loss: 0.2892  time: 108s\n",
            "Epoch 5 - avg_train_loss: 0.0084  avg_val_loss: 0.2892  time: 108s\n",
            "Epoch 5 - Score: 0.2892105824582041\n",
            "Epoch 5 - Score: 0.2892105824582041\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EVAL: [76/77] Elapsed 0m 8s (remain 0m 0s) Loss: 0.2892 \n",
            "Log loss for 4 is 0.2892105824582041\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "========== fold: 2 result ==========\n",
            "========== fold: 2 result ==========\n",
            "Score: 0.11361\n",
            "Score: 0.11361\n",
            "========== fold: 3 training ==========\n",
            "========== fold: 3 training ==========\n",
            "Didn't find file /content/activebus_bert_final/added_tokens.json. We won't load it.\n",
            "loading file /content/activebus_bert_final/vocab.txt\n",
            "loading file None\n",
            "loading file /content/activebus_bert_final/special_tokens_map.json\n",
            "loading file /content/activebus_bert_final/tokenizer_config.json\n",
            "loading file /content/activebus_bert_final/tokenizer.json\n",
            "Didn't find file /content/activebus_bert_final/added_tokens.json. We won't load it.\n",
            "loading file /content/activebus_bert_final/vocab.txt\n",
            "loading file None\n",
            "loading file /content/activebus_bert_final/special_tokens_map.json\n",
            "loading file /content/activebus_bert_final/tokenizer_config.json\n",
            "loading file /content/activebus_bert_final/tokenizer.json\n",
            "loading configuration file /content/activebus_bert_final/config.json\n",
            "Model config BertConfig {\n",
            "  \"_name_or_path\": \"activebus/BERT-XD_Review\",\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_ids\": 0,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.15.0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "loading weights file /content/activebus_bert_final/pytorch_model.bin\n",
            "Some weights of the model checkpoint at /content/activebus_bert_final were not used when initializing BertForSequenceClassification: ['cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /content/activebus_bert_final and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: [1][0/306] Elapsed 0m 0s (remain 3m 2s) Loss: 0.7218 \n",
            "Epoch: [1][100/306] Elapsed 0m 33s (remain 1m 7s) Loss: 0.2949 \n",
            "Epoch: [1][200/306] Elapsed 1m 5s (remain 0m 34s) Loss: 0.2197 \n",
            "Epoch: [1][300/306] Elapsed 1m 38s (remain 0m 1s) Loss: 0.1845 \n",
            "Epoch: [1][305/306] Elapsed 1m 39s (remain 0m 0s) Loss: 0.1828 \n",
            "EVAL: [0/77] Elapsed 0m 0s (remain 0m 24s) Loss: 0.0563 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1 - avg_train_loss: 0.1828  avg_val_loss: 0.1174  time: 108s\n",
            "Epoch 1 - avg_train_loss: 0.1828  avg_val_loss: 0.1174  time: 108s\n",
            "Epoch 1 - Score: 0.11736573013662989\n",
            "Epoch 1 - Score: 0.11736573013662989\n",
            "Epoch 1 - Save Best Score: 0.1174 Model\n",
            "Epoch 1 - Save Best Score: 0.1174 Model\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EVAL: [76/77] Elapsed 0m 8s (remain 0m 0s) Loss: 0.1174 \n",
            "Log loss for 0 is 0.11736573013662989\n",
            "Epoch: [2][0/306] Elapsed 0m 0s (remain 2m 55s) Loss: 0.0557 \n",
            "Epoch: [2][100/306] Elapsed 0m 33s (remain 1m 7s) Loss: 0.0729 \n",
            "Epoch: [2][200/306] Elapsed 1m 5s (remain 0m 34s) Loss: 0.0738 \n",
            "Epoch: [2][300/306] Elapsed 1m 38s (remain 0m 1s) Loss: 0.0797 \n",
            "Epoch: [2][305/306] Elapsed 1m 39s (remain 0m 0s) Loss: 0.0795 \n",
            "EVAL: [0/77] Elapsed 0m 0s (remain 0m 25s) Loss: 0.0157 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2 - avg_train_loss: 0.0795  avg_val_loss: 0.1084  time: 108s\n",
            "Epoch 2 - avg_train_loss: 0.0795  avg_val_loss: 0.1084  time: 108s\n",
            "Epoch 2 - Score: 0.10838262027963698\n",
            "Epoch 2 - Score: 0.10838262027963698\n",
            "Epoch 2 - Save Best Score: 0.1084 Model\n",
            "Epoch 2 - Save Best Score: 0.1084 Model\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EVAL: [76/77] Elapsed 0m 8s (remain 0m 0s) Loss: 0.1084 \n",
            "Log loss for 1 is 0.10838262027963698\n",
            "Epoch: [3][0/306] Elapsed 0m 0s (remain 2m 50s) Loss: 0.0128 \n",
            "Epoch: [3][100/306] Elapsed 0m 33s (remain 1m 7s) Loss: 0.0381 \n",
            "Epoch: [3][200/306] Elapsed 1m 5s (remain 0m 34s) Loss: 0.0425 \n",
            "Epoch: [3][300/306] Elapsed 1m 38s (remain 0m 1s) Loss: 0.0445 \n",
            "Epoch: [3][305/306] Elapsed 1m 39s (remain 0m 0s) Loss: 0.0447 \n",
            "EVAL: [0/77] Elapsed 0m 0s (remain 0m 24s) Loss: 0.0136 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3 - avg_train_loss: 0.0447  avg_val_loss: 0.1154  time: 108s\n",
            "Epoch 3 - avg_train_loss: 0.0447  avg_val_loss: 0.1154  time: 108s\n",
            "Epoch 3 - Score: 0.11543244831448082\n",
            "Epoch 3 - Score: 0.11543244831448082\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EVAL: [76/77] Elapsed 0m 8s (remain 0m 0s) Loss: 0.1154 \n",
            "Log loss for 2 is 0.11543244831448082\n",
            "Epoch: [4][0/306] Elapsed 0m 0s (remain 3m 1s) Loss: 0.0113 \n",
            "Epoch: [4][100/306] Elapsed 0m 33s (remain 1m 7s) Loss: 0.0285 \n",
            "Epoch: [4][200/306] Elapsed 1m 5s (remain 0m 34s) Loss: 0.0278 \n",
            "Epoch: [4][300/306] Elapsed 1m 38s (remain 0m 1s) Loss: 0.0342 \n",
            "Epoch: [4][305/306] Elapsed 1m 39s (remain 0m 0s) Loss: 0.0338 \n",
            "EVAL: [0/77] Elapsed 0m 0s (remain 0m 24s) Loss: 0.0339 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4 - avg_train_loss: 0.0338  avg_val_loss: 0.1640  time: 108s\n",
            "Epoch 4 - avg_train_loss: 0.0338  avg_val_loss: 0.1640  time: 108s\n",
            "Epoch 4 - Score: 0.16400067000172633\n",
            "Epoch 4 - Score: 0.16400067000172633\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EVAL: [76/77] Elapsed 0m 8s (remain 0m 0s) Loss: 0.1640 \n",
            "Log loss for 3 is 0.16400067000172633\n",
            "Epoch: [5][0/306] Elapsed 0m 0s (remain 2m 49s) Loss: 0.0404 \n",
            "Epoch: [5][100/306] Elapsed 0m 33s (remain 1m 7s) Loss: 0.0239 \n",
            "Epoch: [5][200/306] Elapsed 1m 5s (remain 0m 34s) Loss: 0.0182 \n",
            "Epoch: [5][300/306] Elapsed 1m 38s (remain 0m 1s) Loss: 0.0187 \n",
            "Epoch: [5][305/306] Elapsed 1m 39s (remain 0m 0s) Loss: 0.0186 \n",
            "EVAL: [0/77] Elapsed 0m 0s (remain 0m 24s) Loss: 0.0410 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5 - avg_train_loss: 0.0186  avg_val_loss: 0.1811  time: 108s\n",
            "Epoch 5 - avg_train_loss: 0.0186  avg_val_loss: 0.1811  time: 108s\n",
            "Epoch 5 - Score: 0.18111891430479488\n",
            "Epoch 5 - Score: 0.18111891430479488\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EVAL: [76/77] Elapsed 0m 8s (remain 0m 0s) Loss: 0.1811 \n",
            "Log loss for 4 is 0.18111891430479488\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "========== fold: 3 result ==========\n",
            "========== fold: 3 result ==========\n",
            "Score: 0.10838\n",
            "Score: 0.10838\n",
            "========== fold: 4 training ==========\n",
            "========== fold: 4 training ==========\n",
            "Didn't find file /content/activebus_bert_final/added_tokens.json. We won't load it.\n",
            "loading file /content/activebus_bert_final/vocab.txt\n",
            "loading file None\n",
            "loading file /content/activebus_bert_final/special_tokens_map.json\n",
            "loading file /content/activebus_bert_final/tokenizer_config.json\n",
            "loading file /content/activebus_bert_final/tokenizer.json\n",
            "Didn't find file /content/activebus_bert_final/added_tokens.json. We won't load it.\n",
            "loading file /content/activebus_bert_final/vocab.txt\n",
            "loading file None\n",
            "loading file /content/activebus_bert_final/special_tokens_map.json\n",
            "loading file /content/activebus_bert_final/tokenizer_config.json\n",
            "loading file /content/activebus_bert_final/tokenizer.json\n",
            "loading configuration file /content/activebus_bert_final/config.json\n",
            "Model config BertConfig {\n",
            "  \"_name_or_path\": \"activebus/BERT-XD_Review\",\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_ids\": 0,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.15.0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "loading weights file /content/activebus_bert_final/pytorch_model.bin\n",
            "Some weights of the model checkpoint at /content/activebus_bert_final were not used when initializing BertForSequenceClassification: ['cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /content/activebus_bert_final and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: [1][0/306] Elapsed 0m 0s (remain 2m 51s) Loss: 0.6358 \n",
            "Epoch: [1][100/306] Elapsed 0m 33s (remain 1m 7s) Loss: 0.2852 \n",
            "Epoch: [1][200/306] Elapsed 1m 5s (remain 0m 34s) Loss: 0.2086 \n",
            "Epoch: [1][300/306] Elapsed 1m 38s (remain 0m 1s) Loss: 0.1861 \n",
            "Epoch: [1][305/306] Elapsed 1m 39s (remain 0m 0s) Loss: 0.1853 \n",
            "EVAL: [0/77] Elapsed 0m 0s (remain 0m 25s) Loss: 0.4891 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1 - avg_train_loss: 0.1853  avg_val_loss: 0.1009  time: 108s\n",
            "Epoch 1 - avg_train_loss: 0.1853  avg_val_loss: 0.1009  time: 108s\n",
            "Epoch 1 - Score: 0.10091481223294835\n",
            "Epoch 1 - Score: 0.10091481223294835\n",
            "Epoch 1 - Save Best Score: 0.1009 Model\n",
            "Epoch 1 - Save Best Score: 0.1009 Model\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EVAL: [76/77] Elapsed 0m 8s (remain 0m 0s) Loss: 0.1009 \n",
            "Log loss for 0 is 0.10091481223294835\n",
            "Epoch: [2][0/306] Elapsed 0m 0s (remain 2m 56s) Loss: 0.0224 \n",
            "Epoch: [2][100/306] Elapsed 0m 32s (remain 1m 6s) Loss: 0.0889 \n",
            "Epoch: [2][200/306] Elapsed 1m 5s (remain 0m 34s) Loss: 0.0870 \n",
            "Epoch: [2][300/306] Elapsed 1m 37s (remain 0m 1s) Loss: 0.0802 \n",
            "Epoch: [2][305/306] Elapsed 1m 39s (remain 0m 0s) Loss: 0.0816 \n",
            "EVAL: [0/77] Elapsed 0m 0s (remain 0m 25s) Loss: 0.5555 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2 - avg_train_loss: 0.0816  avg_val_loss: 0.1186  time: 108s\n",
            "Epoch 2 - avg_train_loss: 0.0816  avg_val_loss: 0.1186  time: 108s\n",
            "Epoch 2 - Score: 0.11861486699221126\n",
            "Epoch 2 - Score: 0.11861486699221126\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EVAL: [76/77] Elapsed 0m 8s (remain 0m 0s) Loss: 0.1186 \n",
            "Log loss for 1 is 0.11861486699221126\n",
            "Epoch: [3][0/306] Elapsed 0m 0s (remain 2m 52s) Loss: 0.0112 \n",
            "Epoch: [3][100/306] Elapsed 0m 33s (remain 1m 7s) Loss: 0.0505 \n",
            "Epoch: [3][200/306] Elapsed 1m 5s (remain 0m 34s) Loss: 0.0472 \n",
            "Epoch: [3][300/306] Elapsed 1m 37s (remain 0m 1s) Loss: 0.0426 \n",
            "Epoch: [3][305/306] Elapsed 1m 39s (remain 0m 0s) Loss: 0.0420 \n",
            "EVAL: [0/77] Elapsed 0m 0s (remain 0m 24s) Loss: 0.9297 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3 - avg_train_loss: 0.0420  avg_val_loss: 0.2282  time: 108s\n",
            "Epoch 3 - avg_train_loss: 0.0420  avg_val_loss: 0.2282  time: 108s\n",
            "Epoch 3 - Score: 0.22818490030419897\n",
            "Epoch 3 - Score: 0.22818490030419897\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EVAL: [76/77] Elapsed 0m 8s (remain 0m 0s) Loss: 0.2282 \n",
            "Log loss for 2 is 0.22818490030419897\n",
            "Epoch: [4][0/306] Elapsed 0m 0s (remain 2m 52s) Loss: 0.0033 \n",
            "Epoch: [4][100/306] Elapsed 0m 33s (remain 1m 7s) Loss: 0.0235 \n",
            "Epoch: [4][200/306] Elapsed 1m 5s (remain 0m 34s) Loss: 0.0257 \n",
            "Epoch: [4][300/306] Elapsed 1m 38s (remain 0m 1s) Loss: 0.0286 \n",
            "Epoch: [4][305/306] Elapsed 1m 39s (remain 0m 0s) Loss: 0.0288 \n",
            "EVAL: [0/77] Elapsed 0m 0s (remain 0m 24s) Loss: 0.7660 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4 - avg_train_loss: 0.0288  avg_val_loss: 0.1796  time: 108s\n",
            "Epoch 4 - avg_train_loss: 0.0288  avg_val_loss: 0.1796  time: 108s\n",
            "Epoch 4 - Score: 0.17959661755769948\n",
            "Epoch 4 - Score: 0.17959661755769948\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EVAL: [76/77] Elapsed 0m 8s (remain 0m 0s) Loss: 0.1796 \n",
            "Log loss for 3 is 0.17959661755769948\n",
            "Epoch: [5][0/306] Elapsed 0m 0s (remain 2m 51s) Loss: 0.0014 \n",
            "Epoch: [5][100/306] Elapsed 0m 33s (remain 1m 7s) Loss: 0.0126 \n",
            "Epoch: [5][200/306] Elapsed 1m 5s (remain 0m 34s) Loss: 0.0119 \n",
            "Epoch: [5][300/306] Elapsed 1m 37s (remain 0m 1s) Loss: 0.0186 \n",
            "Epoch: [5][305/306] Elapsed 1m 39s (remain 0m 0s) Loss: 0.0198 \n",
            "EVAL: [0/77] Elapsed 0m 0s (remain 0m 24s) Loss: 1.0927 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5 - avg_train_loss: 0.0198  avg_val_loss: 0.1885  time: 108s\n",
            "Epoch 5 - avg_train_loss: 0.0198  avg_val_loss: 0.1885  time: 108s\n",
            "Epoch 5 - Score: 0.18849467921246563\n",
            "Epoch 5 - Score: 0.18849467921246563\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EVAL: [76/77] Elapsed 0m 8s (remain 0m 0s) Loss: 0.1885 \n",
            "Log loss for 4 is 0.18849467921246563\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "========== fold: 4 result ==========\n",
            "========== fold: 4 result ==========\n",
            "Score: 0.10091\n",
            "Score: 0.10091\n",
            "========== CV ==========\n",
            "========== CV ==========\n",
            "Score: 0.10626\n",
            "Score: 0.10626\n",
            "Didn't find file /content/activebus_bert_final/added_tokens.json. We won't load it.\n",
            "loading file /content/activebus_bert_final/vocab.txt\n",
            "loading file None\n",
            "loading file /content/activebus_bert_final/special_tokens_map.json\n",
            "loading file /content/activebus_bert_final/tokenizer_config.json\n",
            "loading file /content/activebus_bert_final/tokenizer.json\n",
            "========== model: bert-base-uncased fold: 0 inference ==========\n",
            "========== model: bert-base-uncased fold: 0 inference ==========\n",
            "loading configuration file /content/activebus_bert_final/config.json\n",
            "Model config BertConfig {\n",
            "  \"_name_or_path\": \"activebus/BERT-XD_Review\",\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_ids\": 0,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.15.0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "loading weights file /content/activebus_bert_final/pytorch_model.bin\n",
            "Some weights of the model checkpoint at /content/activebus_bert_final were not used when initializing BertForSequenceClassification: ['cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /content/activebus_bert_final and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "\n",
            "  0%|          | 0/165 [00:00<?, ?it/s]\u001b[A\n",
            "  1%|          | 1/165 [00:00<00:21,  7.69it/s]\u001b[A\n",
            "  1%|          | 2/165 [00:00<00:20,  8.00it/s]\u001b[A\n",
            "  2%|▏         | 3/165 [00:00<00:19,  8.39it/s]\u001b[A\n",
            "  2%|▏         | 4/165 [00:00<00:18,  8.61it/s]\u001b[A\n",
            "  3%|▎         | 5/165 [00:00<00:18,  8.80it/s]\u001b[A\n",
            "  4%|▎         | 6/165 [00:00<00:18,  8.73it/s]\u001b[A\n",
            "  4%|▍         | 7/165 [00:00<00:17,  8.88it/s]\u001b[A\n",
            "  5%|▍         | 8/165 [00:00<00:17,  8.96it/s]\u001b[A\n",
            "  5%|▌         | 9/165 [00:01<00:17,  8.98it/s]\u001b[A\n",
            "  6%|▌         | 10/165 [00:01<00:17,  9.02it/s]\u001b[A\n",
            "  7%|▋         | 11/165 [00:01<00:16,  9.07it/s]\u001b[A\n",
            "  7%|▋         | 12/165 [00:01<00:16,  9.08it/s]\u001b[A\n",
            "  8%|▊         | 13/165 [00:01<00:16,  9.11it/s]\u001b[A\n",
            "  8%|▊         | 14/165 [00:01<00:16,  9.13it/s]\u001b[A\n",
            "  9%|▉         | 15/165 [00:01<00:16,  9.10it/s]\u001b[A\n",
            " 10%|▉         | 16/165 [00:01<00:16,  9.09it/s]\u001b[A\n",
            " 10%|█         | 17/165 [00:01<00:16,  9.09it/s]\u001b[A\n",
            " 11%|█         | 18/165 [00:02<00:16,  9.10it/s]\u001b[A\n",
            " 12%|█▏        | 19/165 [00:02<00:16,  9.11it/s]\u001b[A\n",
            " 12%|█▏        | 20/165 [00:02<00:15,  9.10it/s]\u001b[A\n",
            " 13%|█▎        | 21/165 [00:02<00:15,  9.10it/s]\u001b[A\n",
            " 13%|█▎        | 22/165 [00:02<00:15,  9.09it/s]\u001b[A\n",
            " 14%|█▍        | 23/165 [00:02<00:15,  9.08it/s]\u001b[A\n",
            " 15%|█▍        | 24/165 [00:02<00:15,  9.10it/s]\u001b[A\n",
            " 15%|█▌        | 25/165 [00:02<00:15,  9.09it/s]\u001b[A\n",
            " 16%|█▌        | 26/165 [00:02<00:15,  9.10it/s]\u001b[A\n",
            " 16%|█▋        | 27/165 [00:03<00:15,  9.08it/s]\u001b[A\n",
            " 17%|█▋        | 28/165 [00:03<00:15,  9.09it/s]\u001b[A\n",
            " 18%|█▊        | 29/165 [00:03<00:14,  9.10it/s]\u001b[A\n",
            " 18%|█▊        | 30/165 [00:03<00:14,  9.06it/s]\u001b[A\n",
            " 19%|█▉        | 31/165 [00:03<00:14,  9.08it/s]\u001b[A\n",
            " 19%|█▉        | 32/165 [00:03<00:14,  9.08it/s]\u001b[A\n",
            " 20%|██        | 33/165 [00:03<00:14,  9.09it/s]\u001b[A\n",
            " 21%|██        | 34/165 [00:03<00:14,  9.10it/s]\u001b[A\n",
            " 21%|██        | 35/165 [00:03<00:14,  9.09it/s]\u001b[A\n",
            " 22%|██▏       | 36/165 [00:03<00:14,  9.09it/s]\u001b[A\n",
            " 22%|██▏       | 37/165 [00:04<00:14,  9.09it/s]\u001b[A\n",
            " 23%|██▎       | 38/165 [00:04<00:13,  9.11it/s]\u001b[A\n",
            " 24%|██▎       | 39/165 [00:04<00:13,  9.11it/s]\u001b[A\n",
            " 24%|██▍       | 40/165 [00:04<00:13,  9.11it/s]\u001b[A\n",
            " 25%|██▍       | 41/165 [00:04<00:13,  9.12it/s]\u001b[A\n",
            " 25%|██▌       | 42/165 [00:04<00:13,  9.13it/s]\u001b[A\n",
            " 26%|██▌       | 43/165 [00:04<00:13,  9.12it/s]\u001b[A\n",
            " 27%|██▋       | 44/165 [00:04<00:13,  9.13it/s]\u001b[A\n",
            " 27%|██▋       | 45/165 [00:04<00:13,  9.12it/s]\u001b[A\n",
            " 28%|██▊       | 46/165 [00:05<00:13,  9.13it/s]\u001b[A\n",
            " 28%|██▊       | 47/165 [00:05<00:12,  9.12it/s]\u001b[A\n",
            " 29%|██▉       | 48/165 [00:05<00:12,  9.12it/s]\u001b[A\n",
            " 30%|██▉       | 49/165 [00:05<00:12,  9.12it/s]\u001b[A\n",
            " 30%|███       | 50/165 [00:05<00:12,  9.12it/s]\u001b[A\n",
            " 31%|███       | 51/165 [00:05<00:12,  9.13it/s]\u001b[A\n",
            " 32%|███▏      | 52/165 [00:05<00:12,  9.13it/s]\u001b[A\n",
            " 32%|███▏      | 53/165 [00:05<00:12,  9.13it/s]\u001b[A\n",
            " 33%|███▎      | 54/165 [00:05<00:12,  9.14it/s]\u001b[A\n",
            " 33%|███▎      | 55/165 [00:06<00:12,  9.13it/s]\u001b[A\n",
            " 34%|███▍      | 56/165 [00:06<00:11,  9.13it/s]\u001b[A\n",
            " 35%|███▍      | 57/165 [00:06<00:11,  9.14it/s]\u001b[A\n",
            " 35%|███▌      | 58/165 [00:06<00:11,  9.13it/s]\u001b[A\n",
            " 36%|███▌      | 59/165 [00:06<00:11,  9.14it/s]\u001b[A\n",
            " 36%|███▋      | 60/165 [00:06<00:11,  9.14it/s]\u001b[A\n",
            " 37%|███▋      | 61/165 [00:06<00:11,  9.12it/s]\u001b[A\n",
            " 38%|███▊      | 62/165 [00:06<00:11,  9.12it/s]\u001b[A\n",
            " 38%|███▊      | 63/165 [00:06<00:11,  9.12it/s]\u001b[A\n",
            " 39%|███▉      | 64/165 [00:07<00:11,  9.10it/s]\u001b[A\n",
            " 39%|███▉      | 65/165 [00:07<00:10,  9.10it/s]\u001b[A\n",
            " 40%|████      | 66/165 [00:07<00:10,  9.08it/s]\u001b[A\n",
            " 41%|████      | 67/165 [00:07<00:10,  9.07it/s]\u001b[A\n",
            " 41%|████      | 68/165 [00:07<00:10,  9.08it/s]\u001b[A\n",
            " 42%|████▏     | 69/165 [00:07<00:10,  8.98it/s]\u001b[A\n",
            " 42%|████▏     | 70/165 [00:07<00:10,  9.04it/s]\u001b[A\n",
            " 43%|████▎     | 71/165 [00:07<00:10,  9.05it/s]\u001b[A\n",
            " 44%|████▎     | 72/165 [00:07<00:10,  9.09it/s]\u001b[A\n",
            " 44%|████▍     | 73/165 [00:08<00:10,  9.09it/s]\u001b[A\n",
            " 45%|████▍     | 74/165 [00:08<00:09,  9.10it/s]\u001b[A\n",
            " 45%|████▌     | 75/165 [00:08<00:09,  9.12it/s]\u001b[A\n",
            " 46%|████▌     | 76/165 [00:08<00:09,  9.11it/s]\u001b[A\n",
            " 47%|████▋     | 77/165 [00:08<00:09,  9.10it/s]\u001b[A\n",
            " 47%|████▋     | 78/165 [00:08<00:09,  9.12it/s]\u001b[A\n",
            " 48%|████▊     | 79/165 [00:08<00:09,  9.12it/s]\u001b[A\n",
            " 48%|████▊     | 80/165 [00:08<00:09,  9.04it/s]\u001b[A\n",
            " 49%|████▉     | 81/165 [00:08<00:09,  9.05it/s]\u001b[A\n",
            " 50%|████▉     | 82/165 [00:09<00:09,  9.08it/s]\u001b[A\n",
            " 50%|█████     | 83/165 [00:09<00:09,  9.10it/s]\u001b[A\n",
            " 51%|█████     | 84/165 [00:09<00:08,  9.11it/s]\u001b[A\n",
            " 52%|█████▏    | 85/165 [00:09<00:08,  9.12it/s]\u001b[A\n",
            " 52%|█████▏    | 86/165 [00:09<00:08,  9.12it/s]\u001b[A\n",
            " 53%|█████▎    | 87/165 [00:09<00:08,  9.13it/s]\u001b[A\n",
            " 53%|█████▎    | 88/165 [00:09<00:08,  9.14it/s]\u001b[A\n",
            " 54%|█████▍    | 89/165 [00:09<00:08,  9.13it/s]\u001b[A\n",
            " 55%|█████▍    | 90/165 [00:09<00:08,  9.12it/s]\u001b[A\n",
            " 55%|█████▌    | 91/165 [00:10<00:08,  9.11it/s]\u001b[A\n",
            " 56%|█████▌    | 92/165 [00:10<00:08,  9.11it/s]\u001b[A\n",
            " 56%|█████▋    | 93/165 [00:10<00:07,  9.12it/s]\u001b[A\n",
            " 57%|█████▋    | 94/165 [00:10<00:07,  9.12it/s]\u001b[A\n",
            " 58%|█████▊    | 95/165 [00:10<00:07,  9.11it/s]\u001b[A\n",
            " 58%|█████▊    | 96/165 [00:10<00:07,  9.10it/s]\u001b[A\n",
            " 59%|█████▉    | 97/165 [00:10<00:07,  9.11it/s]\u001b[A\n",
            " 59%|█████▉    | 98/165 [00:10<00:07,  9.10it/s]\u001b[A\n",
            " 60%|██████    | 99/165 [00:10<00:07,  9.06it/s]\u001b[A\n",
            " 61%|██████    | 100/165 [00:11<00:07,  9.03it/s]\u001b[A\n",
            " 61%|██████    | 101/165 [00:11<00:07,  9.06it/s]\u001b[A\n",
            " 62%|██████▏   | 102/165 [00:11<00:06,  9.06it/s]\u001b[A\n",
            " 62%|██████▏   | 103/165 [00:11<00:06,  9.06it/s]\u001b[A\n",
            " 63%|██████▎   | 104/165 [00:11<00:06,  9.07it/s]\u001b[A\n",
            " 64%|██████▎   | 105/165 [00:11<00:06,  9.09it/s]\u001b[A\n",
            " 64%|██████▍   | 106/165 [00:11<00:06,  9.11it/s]\u001b[A\n",
            " 65%|██████▍   | 107/165 [00:11<00:06,  9.10it/s]\u001b[A\n",
            " 65%|██████▌   | 108/165 [00:11<00:06,  9.10it/s]\u001b[A\n",
            " 66%|██████▌   | 109/165 [00:12<00:06,  9.07it/s]\u001b[A\n",
            " 67%|██████▋   | 110/165 [00:12<00:06,  9.08it/s]\u001b[A\n",
            " 67%|██████▋   | 111/165 [00:12<00:05,  9.08it/s]\u001b[A\n",
            " 68%|██████▊   | 112/165 [00:12<00:05,  9.07it/s]\u001b[A\n",
            " 68%|██████▊   | 113/165 [00:12<00:05,  9.09it/s]\u001b[A\n",
            " 69%|██████▉   | 114/165 [00:12<00:05,  9.10it/s]\u001b[A\n",
            " 70%|██████▉   | 115/165 [00:12<00:05,  9.10it/s]\u001b[A\n",
            " 70%|███████   | 116/165 [00:12<00:05,  9.12it/s]\u001b[A\n",
            " 71%|███████   | 117/165 [00:12<00:05,  9.10it/s]\u001b[A\n",
            " 72%|███████▏  | 118/165 [00:13<00:05,  9.13it/s]\u001b[A\n",
            " 72%|███████▏  | 119/165 [00:13<00:05,  9.12it/s]\u001b[A\n",
            " 73%|███████▎  | 120/165 [00:13<00:04,  9.12it/s]\u001b[A\n",
            " 73%|███████▎  | 121/165 [00:13<00:04,  9.12it/s]\u001b[A\n",
            " 74%|███████▍  | 122/165 [00:13<00:04,  9.15it/s]\u001b[A\n",
            " 75%|███████▍  | 123/165 [00:13<00:04,  9.15it/s]\u001b[A\n",
            " 75%|███████▌  | 124/165 [00:13<00:04,  9.13it/s]\u001b[A\n",
            " 76%|███████▌  | 125/165 [00:13<00:04,  9.13it/s]\u001b[A\n",
            " 76%|███████▋  | 126/165 [00:13<00:04,  9.13it/s]\u001b[A\n",
            " 77%|███████▋  | 127/165 [00:13<00:04,  9.14it/s]\u001b[A\n",
            " 78%|███████▊  | 128/165 [00:14<00:04,  9.12it/s]\u001b[A\n",
            " 78%|███████▊  | 129/165 [00:14<00:03,  9.12it/s]\u001b[A\n",
            " 79%|███████▉  | 130/165 [00:14<00:03,  9.12it/s]\u001b[A\n",
            " 79%|███████▉  | 131/165 [00:14<00:03,  9.14it/s]\u001b[A\n",
            " 80%|████████  | 132/165 [00:14<00:03,  9.09it/s]\u001b[A\n",
            " 81%|████████  | 133/165 [00:14<00:03,  9.11it/s]\u001b[A\n",
            " 81%|████████  | 134/165 [00:14<00:03,  9.12it/s]\u001b[A\n",
            " 82%|████████▏ | 135/165 [00:14<00:03,  9.12it/s]\u001b[A\n",
            " 82%|████████▏ | 136/165 [00:14<00:03,  9.14it/s]\u001b[A\n",
            " 83%|████████▎ | 137/165 [00:15<00:03,  9.13it/s]\u001b[A\n",
            " 84%|████████▎ | 138/165 [00:15<00:02,  9.11it/s]\u001b[A\n",
            " 84%|████████▍ | 139/165 [00:15<00:02,  9.12it/s]\u001b[A\n",
            " 85%|████████▍ | 140/165 [00:15<00:02,  9.13it/s]\u001b[A\n",
            " 85%|████████▌ | 141/165 [00:15<00:02,  9.13it/s]\u001b[A\n",
            " 86%|████████▌ | 142/165 [00:15<00:02,  9.12it/s]\u001b[A\n",
            " 87%|████████▋ | 143/165 [00:15<00:02,  9.06it/s]\u001b[A\n",
            " 87%|████████▋ | 144/165 [00:15<00:02,  9.08it/s]\u001b[A\n",
            " 88%|████████▊ | 145/165 [00:15<00:02,  9.09it/s]\u001b[A\n",
            " 88%|████████▊ | 146/165 [00:16<00:02,  9.12it/s]\u001b[A\n",
            " 89%|████████▉ | 147/165 [00:16<00:01,  9.11it/s]\u001b[A\n",
            " 90%|████████▉ | 148/165 [00:16<00:01,  9.13it/s]\u001b[A\n",
            " 90%|█████████ | 149/165 [00:16<00:01,  9.13it/s]\u001b[A\n",
            " 91%|█████████ | 150/165 [00:16<00:01,  9.00it/s]\u001b[A\n",
            " 92%|█████████▏| 151/165 [00:16<00:01,  9.02it/s]\u001b[A\n",
            " 92%|█████████▏| 152/165 [00:16<00:01,  9.05it/s]\u001b[A\n",
            " 93%|█████████▎| 153/165 [00:16<00:01,  9.06it/s]\u001b[A\n",
            " 93%|█████████▎| 154/165 [00:16<00:01,  9.09it/s]\u001b[A\n",
            " 94%|█████████▍| 155/165 [00:17<00:01,  9.10it/s]\u001b[A\n",
            " 95%|█████████▍| 156/165 [00:17<00:00,  9.10it/s]\u001b[A\n",
            " 95%|█████████▌| 157/165 [00:17<00:00,  9.12it/s]\u001b[A\n",
            " 96%|█████████▌| 158/165 [00:17<00:00,  9.13it/s]\u001b[A\n",
            " 96%|█████████▋| 159/165 [00:17<00:00,  9.13it/s]\u001b[A\n",
            " 97%|█████████▋| 160/165 [00:17<00:00,  9.12it/s]\u001b[A\n",
            " 98%|█████████▊| 161/165 [00:17<00:00,  9.12it/s]\u001b[A\n",
            " 98%|█████████▊| 162/165 [00:17<00:00,  9.13it/s]\u001b[A\n",
            " 99%|█████████▉| 163/165 [00:17<00:00,  9.14it/s]\u001b[A\n",
            "100%|██████████| 165/165 [00:18<00:00,  9.07it/s]\n",
            "========== model: bert-base-uncased fold: 1 inference ==========\n",
            "========== model: bert-base-uncased fold: 1 inference ==========\n",
            "loading configuration file /content/activebus_bert_final/config.json\n",
            "Model config BertConfig {\n",
            "  \"_name_or_path\": \"activebus/BERT-XD_Review\",\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_ids\": 0,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.15.0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "loading weights file /content/activebus_bert_final/pytorch_model.bin\n",
            "Some weights of the model checkpoint at /content/activebus_bert_final were not used when initializing BertForSequenceClassification: ['cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /content/activebus_bert_final and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "\n",
            "  0%|          | 0/165 [00:00<?, ?it/s]\u001b[A\n",
            "  1%|          | 1/165 [00:00<00:19,  8.27it/s]\u001b[A\n",
            "  1%|          | 2/165 [00:00<00:18,  8.77it/s]\u001b[A\n",
            "  2%|▏         | 3/165 [00:00<00:18,  8.94it/s]\u001b[A\n",
            "  2%|▏         | 4/165 [00:00<00:17,  8.98it/s]\u001b[A\n",
            "  3%|▎         | 5/165 [00:00<00:17,  9.01it/s]\u001b[A\n",
            "  4%|▎         | 6/165 [00:00<00:17,  9.02it/s]\u001b[A\n",
            "  4%|▍         | 7/165 [00:00<00:17,  9.04it/s]\u001b[A\n",
            "  5%|▍         | 8/165 [00:00<00:17,  9.03it/s]\u001b[A\n",
            "  5%|▌         | 9/165 [00:01<00:17,  9.05it/s]\u001b[A\n",
            "  6%|▌         | 10/165 [00:01<00:17,  9.06it/s]\u001b[A\n",
            "  7%|▋         | 11/165 [00:01<00:16,  9.08it/s]\u001b[A\n",
            "  7%|▋         | 12/165 [00:01<00:16,  9.11it/s]\u001b[A\n",
            "  8%|▊         | 13/165 [00:01<00:16,  9.12it/s]\u001b[A\n",
            "  8%|▊         | 14/165 [00:01<00:16,  9.13it/s]\u001b[A\n",
            "  9%|▉         | 15/165 [00:01<00:16,  9.11it/s]\u001b[A\n",
            " 10%|▉         | 16/165 [00:01<00:16,  9.11it/s]\u001b[A\n",
            " 10%|█         | 17/165 [00:01<00:16,  9.12it/s]\u001b[A\n",
            " 11%|█         | 18/165 [00:01<00:16,  9.14it/s]\u001b[A\n",
            " 12%|█▏        | 19/165 [00:02<00:16,  9.12it/s]\u001b[A\n",
            " 12%|█▏        | 20/165 [00:02<00:15,  9.14it/s]\u001b[A\n",
            " 13%|█▎        | 21/165 [00:02<00:15,  9.15it/s]\u001b[A\n",
            " 13%|█▎        | 22/165 [00:02<00:15,  9.15it/s]\u001b[A\n",
            " 14%|█▍        | 23/165 [00:02<00:15,  9.15it/s]\u001b[A\n",
            " 15%|█▍        | 24/165 [00:02<00:15,  9.15it/s]\u001b[A\n",
            " 15%|█▌        | 25/165 [00:02<00:15,  9.14it/s]\u001b[A\n",
            " 16%|█▌        | 26/165 [00:02<00:15,  9.14it/s]\u001b[A\n",
            " 16%|█▋        | 27/165 [00:02<00:15,  9.12it/s]\u001b[A\n",
            " 17%|█▋        | 28/165 [00:03<00:15,  9.11it/s]\u001b[A\n",
            " 18%|█▊        | 29/165 [00:03<00:14,  9.12it/s]\u001b[A\n",
            " 18%|█▊        | 30/165 [00:03<00:14,  9.12it/s]\u001b[A\n",
            " 19%|█▉        | 31/165 [00:03<00:14,  9.11it/s]\u001b[A\n",
            " 19%|█▉        | 32/165 [00:03<00:14,  9.12it/s]\u001b[A\n",
            " 20%|██        | 33/165 [00:03<00:14,  9.13it/s]\u001b[A\n",
            " 21%|██        | 34/165 [00:03<00:14,  9.13it/s]\u001b[A\n",
            " 21%|██        | 35/165 [00:03<00:14,  9.13it/s]\u001b[A\n",
            " 22%|██▏       | 36/165 [00:03<00:14,  9.14it/s]\u001b[A\n",
            " 22%|██▏       | 37/165 [00:04<00:13,  9.15it/s]\u001b[A\n",
            " 23%|██▎       | 38/165 [00:04<00:13,  9.15it/s]\u001b[A\n",
            " 24%|██▎       | 39/165 [00:04<00:13,  9.15it/s]\u001b[A\n",
            " 24%|██▍       | 40/165 [00:04<00:13,  9.13it/s]\u001b[A\n",
            " 25%|██▍       | 41/165 [00:04<00:13,  9.08it/s]\u001b[A\n",
            " 25%|██▌       | 42/165 [00:04<00:13,  9.08it/s]\u001b[A\n",
            " 26%|██▌       | 43/165 [00:04<00:13,  9.07it/s]\u001b[A\n",
            " 27%|██▋       | 44/165 [00:04<00:13,  9.08it/s]\u001b[A\n",
            " 27%|██▋       | 45/165 [00:04<00:13,  9.09it/s]\u001b[A\n",
            " 28%|██▊       | 46/165 [00:05<00:13,  9.08it/s]\u001b[A\n",
            " 28%|██▊       | 47/165 [00:05<00:12,  9.11it/s]\u001b[A\n",
            " 29%|██▉       | 48/165 [00:05<00:12,  9.10it/s]\u001b[A\n",
            " 30%|██▉       | 49/165 [00:05<00:12,  9.10it/s]\u001b[A\n",
            " 30%|███       | 50/165 [00:05<00:12,  9.10it/s]\u001b[A\n",
            " 31%|███       | 51/165 [00:05<00:12,  9.10it/s]\u001b[A\n",
            " 32%|███▏      | 52/165 [00:05<00:12,  9.09it/s]\u001b[A\n",
            " 32%|███▏      | 53/165 [00:05<00:12,  9.10it/s]\u001b[A\n",
            " 33%|███▎      | 54/165 [00:05<00:12,  9.11it/s]\u001b[A\n",
            " 33%|███▎      | 55/165 [00:06<00:12,  9.12it/s]\u001b[A\n",
            " 34%|███▍      | 56/165 [00:06<00:11,  9.12it/s]\u001b[A\n",
            " 35%|███▍      | 57/165 [00:06<00:11,  9.13it/s]\u001b[A\n",
            " 35%|███▌      | 58/165 [00:06<00:11,  9.14it/s]\u001b[A\n",
            " 36%|███▌      | 59/165 [00:06<00:11,  9.14it/s]\u001b[A\n",
            " 36%|███▋      | 60/165 [00:06<00:11,  9.12it/s]\u001b[A\n",
            " 37%|███▋      | 61/165 [00:06<00:11,  9.10it/s]\u001b[A\n",
            " 38%|███▊      | 62/165 [00:06<00:11,  9.11it/s]\u001b[A\n",
            " 38%|███▊      | 63/165 [00:06<00:11,  9.10it/s]\u001b[A\n",
            " 39%|███▉      | 64/165 [00:07<00:11,  9.07it/s]\u001b[A\n",
            " 39%|███▉      | 65/165 [00:07<00:11,  9.08it/s]\u001b[A\n",
            " 40%|████      | 66/165 [00:07<00:10,  9.09it/s]\u001b[A\n",
            " 41%|████      | 67/165 [00:07<00:10,  9.08it/s]\u001b[A\n",
            " 41%|████      | 68/165 [00:07<00:10,  9.09it/s]\u001b[A\n",
            " 42%|████▏     | 69/165 [00:07<00:10,  9.07it/s]\u001b[A\n",
            " 42%|████▏     | 70/165 [00:07<00:10,  9.05it/s]\u001b[A\n",
            " 43%|████▎     | 71/165 [00:07<00:10,  9.06it/s]\u001b[A\n",
            " 44%|████▎     | 72/165 [00:07<00:10,  9.06it/s]\u001b[A\n",
            " 44%|████▍     | 73/165 [00:08<00:10,  9.07it/s]\u001b[A\n",
            " 45%|████▍     | 74/165 [00:08<00:09,  9.10it/s]\u001b[A\n",
            " 45%|████▌     | 75/165 [00:08<00:09,  9.11it/s]\u001b[A\n",
            " 46%|████▌     | 76/165 [00:08<00:09,  9.11it/s]\u001b[A\n",
            " 47%|████▋     | 77/165 [00:08<00:09,  9.09it/s]\u001b[A\n",
            " 47%|████▋     | 78/165 [00:08<00:09,  9.10it/s]\u001b[A\n",
            " 48%|████▊     | 79/165 [00:08<00:09,  9.11it/s]\u001b[A\n",
            " 48%|████▊     | 80/165 [00:08<00:09,  9.09it/s]\u001b[A\n",
            " 49%|████▉     | 81/165 [00:08<00:09,  9.07it/s]\u001b[A\n",
            " 50%|████▉     | 82/165 [00:09<00:09,  9.05it/s]\u001b[A\n",
            " 50%|█████     | 83/165 [00:09<00:09,  9.05it/s]\u001b[A\n",
            " 51%|█████     | 84/165 [00:09<00:08,  9.06it/s]\u001b[A\n",
            " 52%|█████▏    | 85/165 [00:09<00:08,  9.07it/s]\u001b[A\n",
            " 52%|█████▏    | 86/165 [00:09<00:08,  9.08it/s]\u001b[A\n",
            " 53%|█████▎    | 87/165 [00:09<00:08,  9.10it/s]\u001b[A\n",
            " 53%|█████▎    | 88/165 [00:09<00:08,  9.11it/s]\u001b[A\n",
            " 54%|█████▍    | 89/165 [00:09<00:08,  9.11it/s]\u001b[A\n",
            " 55%|█████▍    | 90/165 [00:09<00:08,  9.07it/s]\u001b[A\n",
            " 55%|█████▌    | 91/165 [00:10<00:08,  9.07it/s]\u001b[A\n",
            " 56%|█████▌    | 92/165 [00:10<00:08,  9.09it/s]\u001b[A\n",
            " 56%|█████▋    | 93/165 [00:10<00:07,  9.10it/s]\u001b[A\n",
            " 57%|█████▋    | 94/165 [00:10<00:07,  9.11it/s]\u001b[A\n",
            " 58%|█████▊    | 95/165 [00:10<00:07,  9.13it/s]\u001b[A\n",
            " 58%|█████▊    | 96/165 [00:10<00:07,  9.13it/s]\u001b[A\n",
            " 59%|█████▉    | 97/165 [00:10<00:07,  9.12it/s]\u001b[A\n",
            " 59%|█████▉    | 98/165 [00:10<00:07,  9.14it/s]\u001b[A\n",
            " 60%|██████    | 99/165 [00:10<00:07,  9.07it/s]\u001b[A\n",
            " 61%|██████    | 100/165 [00:10<00:07,  9.07it/s]\u001b[A\n",
            " 61%|██████    | 101/165 [00:11<00:07,  9.09it/s]\u001b[A\n",
            " 62%|██████▏   | 102/165 [00:11<00:06,  9.09it/s]\u001b[A\n",
            " 62%|██████▏   | 103/165 [00:11<00:06,  9.10it/s]\u001b[A\n",
            " 63%|██████▎   | 104/165 [00:11<00:06,  9.09it/s]\u001b[A\n",
            " 64%|██████▎   | 105/165 [00:11<00:06,  9.04it/s]\u001b[A\n",
            " 64%|██████▍   | 106/165 [00:11<00:06,  9.08it/s]\u001b[A\n",
            " 65%|██████▍   | 107/165 [00:11<00:06,  9.10it/s]\u001b[A\n",
            " 65%|██████▌   | 108/165 [00:11<00:06,  9.11it/s]\u001b[A\n",
            " 66%|██████▌   | 109/165 [00:11<00:06,  9.12it/s]\u001b[A\n",
            " 67%|██████▋   | 110/165 [00:12<00:06,  9.13it/s]\u001b[A\n",
            " 67%|██████▋   | 111/165 [00:12<00:05,  9.12it/s]\u001b[A\n",
            " 68%|██████▊   | 112/165 [00:12<00:05,  9.10it/s]\u001b[A\n",
            " 68%|██████▊   | 113/165 [00:12<00:05,  9.10it/s]\u001b[A\n",
            " 69%|██████▉   | 114/165 [00:12<00:05,  9.12it/s]\u001b[A\n",
            " 70%|██████▉   | 115/165 [00:12<00:05,  9.13it/s]\u001b[A\n",
            " 70%|███████   | 116/165 [00:12<00:05,  9.11it/s]\u001b[A\n",
            " 71%|███████   | 117/165 [00:12<00:05,  9.11it/s]\u001b[A\n",
            " 72%|███████▏  | 118/165 [00:12<00:05,  9.11it/s]\u001b[A\n",
            " 72%|███████▏  | 119/165 [00:13<00:05,  9.11it/s]\u001b[A\n",
            " 73%|███████▎  | 120/165 [00:13<00:04,  9.11it/s]\u001b[A\n",
            " 73%|███████▎  | 121/165 [00:13<00:04,  9.08it/s]\u001b[A\n",
            " 74%|███████▍  | 122/165 [00:13<00:04,  9.09it/s]\u001b[A\n",
            " 75%|███████▍  | 123/165 [00:13<00:04,  9.09it/s]\u001b[A\n",
            " 75%|███████▌  | 124/165 [00:13<00:04,  9.11it/s]\u001b[A\n",
            " 76%|███████▌  | 125/165 [00:13<00:04,  9.13it/s]\u001b[A\n",
            " 76%|███████▋  | 126/165 [00:13<00:04,  9.12it/s]\u001b[A\n",
            " 77%|███████▋  | 127/165 [00:13<00:04,  9.12it/s]\u001b[A\n",
            " 78%|███████▊  | 128/165 [00:14<00:04,  9.12it/s]\u001b[A\n",
            " 78%|███████▊  | 129/165 [00:14<00:03,  9.13it/s]\u001b[A\n",
            " 79%|███████▉  | 130/165 [00:14<00:03,  9.15it/s]\u001b[A\n",
            " 79%|███████▉  | 131/165 [00:14<00:03,  9.15it/s]\u001b[A\n",
            " 80%|████████  | 132/165 [00:14<00:03,  9.12it/s]\u001b[A\n",
            " 81%|████████  | 133/165 [00:14<00:03,  9.13it/s]\u001b[A\n",
            " 81%|████████  | 134/165 [00:14<00:03,  9.13it/s]\u001b[A\n",
            " 82%|████████▏ | 135/165 [00:14<00:03,  9.11it/s]\u001b[A\n",
            " 82%|████████▏ | 136/165 [00:14<00:03,  9.10it/s]\u001b[A\n",
            " 83%|████████▎ | 137/165 [00:15<00:03,  9.12it/s]\u001b[A\n",
            " 84%|████████▎ | 138/165 [00:15<00:02,  9.10it/s]\u001b[A\n",
            " 84%|████████▍ | 139/165 [00:15<00:02,  9.10it/s]\u001b[A\n",
            " 85%|████████▍ | 140/165 [00:15<00:02,  9.08it/s]\u001b[A\n",
            " 85%|████████▌ | 141/165 [00:15<00:02,  9.08it/s]\u001b[A\n",
            " 86%|████████▌ | 142/165 [00:15<00:02,  9.09it/s]\u001b[A\n",
            " 87%|████████▋ | 143/165 [00:15<00:02,  9.10it/s]\u001b[A\n",
            " 87%|████████▋ | 144/165 [00:15<00:02,  9.07it/s]\u001b[A\n",
            " 88%|████████▊ | 145/165 [00:15<00:02,  9.08it/s]\u001b[A\n",
            " 88%|████████▊ | 146/165 [00:16<00:02,  9.09it/s]\u001b[A\n",
            " 89%|████████▉ | 147/165 [00:16<00:01,  9.12it/s]\u001b[A\n",
            " 90%|████████▉ | 148/165 [00:16<00:01,  9.12it/s]\u001b[A\n",
            " 90%|█████████ | 149/165 [00:16<00:01,  9.10it/s]\u001b[A\n",
            " 91%|█████████ | 150/165 [00:16<00:01,  9.11it/s]\u001b[A\n",
            " 92%|█████████▏| 151/165 [00:16<00:01,  9.11it/s]\u001b[A\n",
            " 92%|█████████▏| 152/165 [00:16<00:01,  9.11it/s]\u001b[A\n",
            " 93%|█████████▎| 153/165 [00:16<00:01,  9.10it/s]\u001b[A\n",
            " 93%|█████████▎| 154/165 [00:16<00:01,  9.09it/s]\u001b[A\n",
            " 94%|█████████▍| 155/165 [00:17<00:01,  9.07it/s]\u001b[A\n",
            " 95%|█████████▍| 156/165 [00:17<00:00,  9.09it/s]\u001b[A\n",
            " 95%|█████████▌| 157/165 [00:17<00:00,  9.08it/s]\u001b[A\n",
            " 96%|█████████▌| 158/165 [00:17<00:00,  9.08it/s]\u001b[A\n",
            " 96%|█████████▋| 159/165 [00:17<00:00,  9.11it/s]\u001b[A\n",
            " 97%|█████████▋| 160/165 [00:17<00:00,  9.12it/s]\u001b[A\n",
            " 98%|█████████▊| 161/165 [00:17<00:00,  9.13it/s]\u001b[A\n",
            " 98%|█████████▊| 162/165 [00:17<00:00,  9.12it/s]\u001b[A\n",
            " 99%|█████████▉| 163/165 [00:17<00:00,  9.14it/s]\u001b[A\n",
            "100%|██████████| 165/165 [00:18<00:00,  9.09it/s]\n",
            "========== model: bert-base-uncased fold: 2 inference ==========\n",
            "========== model: bert-base-uncased fold: 2 inference ==========\n",
            "loading configuration file /content/activebus_bert_final/config.json\n",
            "Model config BertConfig {\n",
            "  \"_name_or_path\": \"activebus/BERT-XD_Review\",\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_ids\": 0,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.15.0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "loading weights file /content/activebus_bert_final/pytorch_model.bin\n",
            "Some weights of the model checkpoint at /content/activebus_bert_final were not used when initializing BertForSequenceClassification: ['cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /content/activebus_bert_final and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "\n",
            "  0%|          | 0/165 [00:00<?, ?it/s]\u001b[A\n",
            "  1%|          | 1/165 [00:00<00:19,  8.52it/s]\u001b[A\n",
            "  1%|          | 2/165 [00:00<00:18,  8.84it/s]\u001b[A\n",
            "  2%|▏         | 3/165 [00:00<00:18,  8.96it/s]\u001b[A\n",
            "  2%|▏         | 4/165 [00:00<00:17,  9.02it/s]\u001b[A\n",
            "  3%|▎         | 5/165 [00:00<00:17,  9.03it/s]\u001b[A\n",
            "  4%|▎         | 6/165 [00:00<00:17,  9.06it/s]\u001b[A\n",
            "  4%|▍         | 7/165 [00:00<00:17,  9.07it/s]\u001b[A\n",
            "  5%|▍         | 8/165 [00:00<00:17,  9.09it/s]\u001b[A\n",
            "  5%|▌         | 9/165 [00:00<00:17,  9.08it/s]\u001b[A\n",
            "  6%|▌         | 10/165 [00:01<00:17,  9.09it/s]\u001b[A\n",
            "  7%|▋         | 11/165 [00:01<00:16,  9.09it/s]\u001b[A\n",
            "  7%|▋         | 12/165 [00:01<00:16,  9.10it/s]\u001b[A\n",
            "  8%|▊         | 13/165 [00:01<00:16,  9.08it/s]\u001b[A\n",
            "  8%|▊         | 14/165 [00:01<00:16,  9.07it/s]\u001b[A\n",
            "  9%|▉         | 15/165 [00:01<00:16,  9.07it/s]\u001b[A\n",
            " 10%|▉         | 16/165 [00:01<00:16,  9.09it/s]\u001b[A\n",
            " 10%|█         | 17/165 [00:01<00:16,  9.09it/s]\u001b[A\n",
            " 11%|█         | 18/165 [00:01<00:16,  9.11it/s]\u001b[A\n",
            " 12%|█▏        | 19/165 [00:02<00:16,  9.12it/s]\u001b[A\n",
            " 12%|█▏        | 20/165 [00:02<00:15,  9.11it/s]\u001b[A\n",
            " 13%|█▎        | 21/165 [00:02<00:15,  9.10it/s]\u001b[A\n",
            " 13%|█▎        | 22/165 [00:02<00:15,  9.08it/s]\u001b[A\n",
            " 14%|█▍        | 23/165 [00:02<00:15,  9.09it/s]\u001b[A\n",
            " 15%|█▍        | 24/165 [00:02<00:15,  9.06it/s]\u001b[A\n",
            " 15%|█▌        | 25/165 [00:02<00:15,  9.09it/s]\u001b[A\n",
            " 16%|█▌        | 26/165 [00:02<00:15,  9.07it/s]\u001b[A\n",
            " 16%|█▋        | 27/165 [00:02<00:15,  9.08it/s]\u001b[A\n",
            " 17%|█▋        | 28/165 [00:03<00:15,  9.09it/s]\u001b[A\n",
            " 18%|█▊        | 29/165 [00:03<00:14,  9.10it/s]\u001b[A\n",
            " 18%|█▊        | 30/165 [00:03<00:14,  9.10it/s]\u001b[A\n",
            " 19%|█▉        | 31/165 [00:03<00:14,  9.12it/s]\u001b[A\n",
            " 19%|█▉        | 32/165 [00:03<00:14,  9.13it/s]\u001b[A\n",
            " 20%|██        | 33/165 [00:03<00:14,  9.12it/s]\u001b[A\n",
            " 21%|██        | 34/165 [00:03<00:14,  9.12it/s]\u001b[A\n",
            " 21%|██        | 35/165 [00:03<00:14,  9.12it/s]\u001b[A\n",
            " 22%|██▏       | 36/165 [00:03<00:14,  9.12it/s]\u001b[A\n",
            " 22%|██▏       | 37/165 [00:04<00:14,  9.12it/s]\u001b[A\n",
            " 23%|██▎       | 38/165 [00:04<00:13,  9.12it/s]\u001b[A\n",
            " 24%|██▎       | 39/165 [00:04<00:13,  9.09it/s]\u001b[A\n",
            " 24%|██▍       | 40/165 [00:04<00:13,  9.06it/s]\u001b[A\n",
            " 25%|██▍       | 41/165 [00:04<00:13,  9.07it/s]\u001b[A\n",
            " 25%|██▌       | 42/165 [00:04<00:13,  9.08it/s]\u001b[A\n",
            " 26%|██▌       | 43/165 [00:04<00:13,  9.08it/s]\u001b[A\n",
            " 27%|██▋       | 44/165 [00:04<00:13,  9.08it/s]\u001b[A\n",
            " 27%|██▋       | 45/165 [00:04<00:13,  9.09it/s]\u001b[A\n",
            " 28%|██▊       | 46/165 [00:05<00:13,  9.09it/s]\u001b[A\n",
            " 28%|██▊       | 47/165 [00:05<00:13,  9.06it/s]\u001b[A\n",
            " 29%|██▉       | 48/165 [00:05<00:12,  9.07it/s]\u001b[A\n",
            " 30%|██▉       | 49/165 [00:05<00:12,  9.11it/s]\u001b[A\n",
            " 30%|███       | 50/165 [00:05<00:12,  9.09it/s]\u001b[A\n",
            " 31%|███       | 51/165 [00:05<00:12,  9.09it/s]\u001b[A\n",
            " 32%|███▏      | 52/165 [00:05<00:12,  9.09it/s]\u001b[A\n",
            " 32%|███▏      | 53/165 [00:05<00:12,  9.11it/s]\u001b[A\n",
            " 33%|███▎      | 54/165 [00:05<00:12,  9.12it/s]\u001b[A\n",
            " 33%|███▎      | 55/165 [00:06<00:12,  9.11it/s]\u001b[A\n",
            " 34%|███▍      | 56/165 [00:06<00:11,  9.12it/s]\u001b[A\n",
            " 35%|███▍      | 57/165 [00:06<00:11,  9.14it/s]\u001b[A\n",
            " 35%|███▌      | 58/165 [00:06<00:11,  9.12it/s]\u001b[A\n",
            " 36%|███▌      | 59/165 [00:06<00:11,  9.13it/s]\u001b[A\n",
            " 36%|███▋      | 60/165 [00:06<00:11,  9.12it/s]\u001b[A\n",
            " 37%|███▋      | 61/165 [00:06<00:11,  9.10it/s]\u001b[A\n",
            " 38%|███▊      | 62/165 [00:06<00:11,  9.10it/s]\u001b[A\n",
            " 38%|███▊      | 63/165 [00:06<00:11,  9.13it/s]\u001b[A\n",
            " 39%|███▉      | 64/165 [00:07<00:11,  9.13it/s]\u001b[A\n",
            " 39%|███▉      | 65/165 [00:07<00:10,  9.12it/s]\u001b[A\n",
            " 40%|████      | 66/165 [00:07<00:10,  9.13it/s]\u001b[A\n",
            " 41%|████      | 67/165 [00:07<00:10,  9.14it/s]\u001b[A\n",
            " 41%|████      | 68/165 [00:07<00:10,  9.16it/s]\u001b[A\n",
            " 42%|████▏     | 69/165 [00:07<00:10,  9.14it/s]\u001b[A\n",
            " 42%|████▏     | 70/165 [00:07<00:10,  9.13it/s]\u001b[A\n",
            " 43%|████▎     | 71/165 [00:07<00:10,  9.13it/s]\u001b[A\n",
            " 44%|████▎     | 72/165 [00:07<00:10,  9.12it/s]\u001b[A\n",
            " 44%|████▍     | 73/165 [00:08<00:10,  9.11it/s]\u001b[A\n",
            " 45%|████▍     | 74/165 [00:08<00:09,  9.12it/s]\u001b[A\n",
            " 45%|████▌     | 75/165 [00:08<00:09,  9.09it/s]\u001b[A\n",
            " 46%|████▌     | 76/165 [00:08<00:09,  9.10it/s]\u001b[A\n",
            " 47%|████▋     | 77/165 [00:08<00:09,  9.09it/s]\u001b[A\n",
            " 47%|████▋     | 78/165 [00:08<00:09,  9.10it/s]\u001b[A\n",
            " 48%|████▊     | 79/165 [00:08<00:09,  9.11it/s]\u001b[A\n",
            " 48%|████▊     | 80/165 [00:08<00:09,  9.10it/s]\u001b[A\n",
            " 49%|████▉     | 81/165 [00:08<00:09,  9.10it/s]\u001b[A\n",
            " 50%|████▉     | 82/165 [00:09<00:09,  9.13it/s]\u001b[A\n",
            " 50%|█████     | 83/165 [00:09<00:08,  9.14it/s]\u001b[A\n",
            " 51%|█████     | 84/165 [00:09<00:08,  9.14it/s]\u001b[A\n",
            " 52%|█████▏    | 85/165 [00:09<00:08,  9.15it/s]\u001b[A\n",
            " 52%|█████▏    | 86/165 [00:09<00:08,  9.15it/s]\u001b[A\n",
            " 53%|█████▎    | 87/165 [00:09<00:08,  9.14it/s]\u001b[A\n",
            " 53%|█████▎    | 88/165 [00:09<00:08,  9.11it/s]\u001b[A\n",
            " 54%|█████▍    | 89/165 [00:09<00:08,  9.12it/s]\u001b[A\n",
            " 55%|█████▍    | 90/165 [00:09<00:08,  9.11it/s]\u001b[A\n",
            " 55%|█████▌    | 91/165 [00:10<00:08,  9.12it/s]\u001b[A\n",
            " 56%|█████▌    | 92/165 [00:10<00:07,  9.13it/s]\u001b[A\n",
            " 56%|█████▋    | 93/165 [00:10<00:07,  9.13it/s]\u001b[A\n",
            " 57%|█████▋    | 94/165 [00:10<00:07,  9.11it/s]\u001b[A\n",
            " 58%|█████▊    | 95/165 [00:10<00:07,  9.10it/s]\u001b[A\n",
            " 58%|█████▊    | 96/165 [00:10<00:07,  9.06it/s]\u001b[A\n",
            " 59%|█████▉    | 97/165 [00:10<00:07,  9.09it/s]\u001b[A\n",
            " 59%|█████▉    | 98/165 [00:10<00:07,  9.11it/s]\u001b[A\n",
            " 60%|██████    | 99/165 [00:10<00:07,  9.09it/s]\u001b[A\n",
            " 61%|██████    | 100/165 [00:10<00:07,  9.10it/s]\u001b[A\n",
            " 61%|██████    | 101/165 [00:11<00:07,  9.10it/s]\u001b[A\n",
            " 62%|██████▏   | 102/165 [00:11<00:06,  9.10it/s]\u001b[A\n",
            " 62%|██████▏   | 103/165 [00:11<00:06,  9.11it/s]\u001b[A\n",
            " 63%|██████▎   | 104/165 [00:11<00:06,  9.11it/s]\u001b[A\n",
            " 64%|██████▎   | 105/165 [00:11<00:06,  9.11it/s]\u001b[A\n",
            " 64%|██████▍   | 106/165 [00:11<00:06,  9.09it/s]\u001b[A\n",
            " 65%|██████▍   | 107/165 [00:11<00:06,  9.08it/s]\u001b[A\n",
            " 65%|██████▌   | 108/165 [00:11<00:06,  9.08it/s]\u001b[A\n",
            " 66%|██████▌   | 109/165 [00:11<00:06,  9.08it/s]\u001b[A\n",
            " 67%|██████▋   | 110/165 [00:12<00:06,  9.10it/s]\u001b[A\n",
            " 67%|██████▋   | 111/165 [00:12<00:05,  9.12it/s]\u001b[A\n",
            " 68%|██████▊   | 112/165 [00:12<00:05,  9.12it/s]\u001b[A\n",
            " 68%|██████▊   | 113/165 [00:12<00:05,  9.12it/s]\u001b[A\n",
            " 69%|██████▉   | 114/165 [00:12<00:05,  9.13it/s]\u001b[A\n",
            " 70%|██████▉   | 115/165 [00:12<00:05,  9.11it/s]\u001b[A\n",
            " 70%|███████   | 116/165 [00:12<00:05,  9.13it/s]\u001b[A\n",
            " 71%|███████   | 117/165 [00:12<00:05,  9.13it/s]\u001b[A\n",
            " 72%|███████▏  | 118/165 [00:12<00:05,  9.14it/s]\u001b[A\n",
            " 72%|███████▏  | 119/165 [00:13<00:05,  9.15it/s]\u001b[A\n",
            " 73%|███████▎  | 120/165 [00:13<00:04,  9.14it/s]\u001b[A\n",
            " 73%|███████▎  | 121/165 [00:13<00:04,  9.13it/s]\u001b[A\n",
            " 74%|███████▍  | 122/165 [00:13<00:04,  9.14it/s]\u001b[A\n",
            " 75%|███████▍  | 123/165 [00:13<00:04,  9.14it/s]\u001b[A\n",
            " 75%|███████▌  | 124/165 [00:13<00:04,  9.11it/s]\u001b[A\n",
            " 76%|███████▌  | 125/165 [00:13<00:04,  9.11it/s]\u001b[A\n",
            " 76%|███████▋  | 126/165 [00:13<00:04,  9.12it/s]\u001b[A\n",
            " 77%|███████▋  | 127/165 [00:13<00:04,  9.14it/s]\u001b[A\n",
            " 78%|███████▊  | 128/165 [00:14<00:04,  9.14it/s]\u001b[A\n",
            " 78%|███████▊  | 129/165 [00:14<00:03,  9.13it/s]\u001b[A\n",
            " 79%|███████▉  | 130/165 [00:14<00:03,  9.14it/s]\u001b[A\n",
            " 79%|███████▉  | 131/165 [00:14<00:03,  9.12it/s]\u001b[A\n",
            " 80%|████████  | 132/165 [00:14<00:03,  9.13it/s]\u001b[A\n",
            " 81%|████████  | 133/165 [00:14<00:03,  9.12it/s]\u001b[A\n",
            " 81%|████████  | 134/165 [00:14<00:03,  9.10it/s]\u001b[A\n",
            " 82%|████████▏ | 135/165 [00:14<00:03,  9.09it/s]\u001b[A\n",
            " 82%|████████▏ | 136/165 [00:14<00:03,  9.11it/s]\u001b[A\n",
            " 83%|████████▎ | 137/165 [00:15<00:03,  9.10it/s]\u001b[A\n",
            " 84%|████████▎ | 138/165 [00:15<00:02,  9.11it/s]\u001b[A\n",
            " 84%|████████▍ | 139/165 [00:15<00:02,  9.11it/s]\u001b[A\n",
            " 85%|████████▍ | 140/165 [00:15<00:02,  9.12it/s]\u001b[A\n",
            " 85%|████████▌ | 141/165 [00:15<00:02,  9.13it/s]\u001b[A\n",
            " 86%|████████▌ | 142/165 [00:15<00:02,  9.11it/s]\u001b[A\n",
            " 87%|████████▋ | 143/165 [00:15<00:02,  9.11it/s]\u001b[A\n",
            " 87%|████████▋ | 144/165 [00:15<00:02,  9.11it/s]\u001b[A\n",
            " 88%|████████▊ | 145/165 [00:15<00:02,  9.11it/s]\u001b[A\n",
            " 88%|████████▊ | 146/165 [00:16<00:02,  8.97it/s]\u001b[A\n",
            " 89%|████████▉ | 147/165 [00:16<00:01,  9.00it/s]\u001b[A\n",
            " 90%|████████▉ | 148/165 [00:16<00:01,  9.06it/s]\u001b[A\n",
            " 90%|█████████ | 149/165 [00:16<00:01,  9.07it/s]\u001b[A\n",
            " 91%|█████████ | 150/165 [00:16<00:01,  9.09it/s]\u001b[A\n",
            " 92%|█████████▏| 151/165 [00:16<00:01,  9.10it/s]\u001b[A\n",
            " 92%|█████████▏| 152/165 [00:16<00:01,  9.12it/s]\u001b[A\n",
            " 93%|█████████▎| 153/165 [00:16<00:01,  9.13it/s]\u001b[A\n",
            " 93%|█████████▎| 154/165 [00:16<00:01,  9.12it/s]\u001b[A\n",
            " 94%|█████████▍| 155/165 [00:17<00:01,  9.13it/s]\u001b[A\n",
            " 95%|█████████▍| 156/165 [00:17<00:00,  9.14it/s]\u001b[A\n",
            " 95%|█████████▌| 157/165 [00:17<00:00,  9.13it/s]\u001b[A\n",
            " 96%|█████████▌| 158/165 [00:17<00:00,  9.15it/s]\u001b[A\n",
            " 96%|█████████▋| 159/165 [00:17<00:00,  9.12it/s]\u001b[A\n",
            " 97%|█████████▋| 160/165 [00:17<00:00,  9.13it/s]\u001b[A\n",
            " 98%|█████████▊| 161/165 [00:17<00:00,  9.13it/s]\u001b[A\n",
            " 98%|█████████▊| 162/165 [00:17<00:00,  9.15it/s]\u001b[A\n",
            " 99%|█████████▉| 163/165 [00:17<00:00,  9.15it/s]\u001b[A\n",
            "100%|██████████| 165/165 [00:18<00:00,  9.10it/s]\n",
            "========== model: bert-base-uncased fold: 3 inference ==========\n",
            "========== model: bert-base-uncased fold: 3 inference ==========\n",
            "loading configuration file /content/activebus_bert_final/config.json\n",
            "Model config BertConfig {\n",
            "  \"_name_or_path\": \"activebus/BERT-XD_Review\",\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_ids\": 0,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.15.0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "loading weights file /content/activebus_bert_final/pytorch_model.bin\n",
            "Some weights of the model checkpoint at /content/activebus_bert_final were not used when initializing BertForSequenceClassification: ['cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /content/activebus_bert_final and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "\n",
            "  0%|          | 0/165 [00:00<?, ?it/s]\u001b[A\n",
            "  1%|          | 1/165 [00:00<00:18,  8.64it/s]\u001b[A\n",
            "  1%|          | 2/165 [00:00<00:18,  8.90it/s]\u001b[A\n",
            "  2%|▏         | 3/165 [00:00<00:18,  8.96it/s]\u001b[A\n",
            "  2%|▏         | 4/165 [00:00<00:17,  9.04it/s]\u001b[A\n",
            "  3%|▎         | 5/165 [00:00<00:17,  9.09it/s]\u001b[A\n",
            "  4%|▎         | 6/165 [00:00<00:17,  9.11it/s]\u001b[A\n",
            "  4%|▍         | 7/165 [00:00<00:17,  9.12it/s]\u001b[A\n",
            "  5%|▍         | 8/165 [00:00<00:17,  9.11it/s]\u001b[A\n",
            "  5%|▌         | 9/165 [00:00<00:17,  9.11it/s]\u001b[A\n",
            "  6%|▌         | 10/165 [00:01<00:16,  9.13it/s]\u001b[A\n",
            "  7%|▋         | 11/165 [00:01<00:16,  9.10it/s]\u001b[A\n",
            "  7%|▋         | 12/165 [00:01<00:16,  9.12it/s]\u001b[A\n",
            "  8%|▊         | 13/165 [00:01<00:16,  9.14it/s]\u001b[A\n",
            "  8%|▊         | 14/165 [00:01<00:16,  9.15it/s]\u001b[A\n",
            "  9%|▉         | 15/165 [00:01<00:16,  9.08it/s]\u001b[A\n",
            " 10%|▉         | 16/165 [00:01<00:16,  9.09it/s]\u001b[A\n",
            " 10%|█         | 17/165 [00:01<00:16,  9.08it/s]\u001b[A\n",
            " 11%|█         | 18/165 [00:01<00:16,  9.10it/s]\u001b[A\n",
            " 12%|█▏        | 19/165 [00:02<00:16,  9.08it/s]\u001b[A\n",
            " 12%|█▏        | 20/165 [00:02<00:15,  9.10it/s]\u001b[A\n",
            " 13%|█▎        | 21/165 [00:02<00:15,  9.12it/s]\u001b[A\n",
            " 13%|█▎        | 22/165 [00:02<00:15,  9.12it/s]\u001b[A\n",
            " 14%|█▍        | 23/165 [00:02<00:15,  9.12it/s]\u001b[A\n",
            " 15%|█▍        | 24/165 [00:02<00:15,  9.12it/s]\u001b[A\n",
            " 15%|█▌        | 25/165 [00:02<00:15,  9.14it/s]\u001b[A\n",
            " 16%|█▌        | 26/165 [00:02<00:15,  9.14it/s]\u001b[A\n",
            " 16%|█▋        | 27/165 [00:02<00:15,  9.10it/s]\u001b[A\n",
            " 17%|█▋        | 28/165 [00:03<00:15,  9.09it/s]\u001b[A\n",
            " 18%|█▊        | 29/165 [00:03<00:14,  9.11it/s]\u001b[A\n",
            " 18%|█▊        | 30/165 [00:03<00:14,  9.12it/s]\u001b[A\n",
            " 19%|█▉        | 31/165 [00:03<00:14,  9.12it/s]\u001b[A\n",
            " 19%|█▉        | 32/165 [00:03<00:14,  9.10it/s]\u001b[A\n",
            " 20%|██        | 33/165 [00:03<00:14,  9.09it/s]\u001b[A\n",
            " 21%|██        | 34/165 [00:03<00:14,  9.11it/s]\u001b[A\n",
            " 21%|██        | 35/165 [00:03<00:14,  9.13it/s]\u001b[A\n",
            " 22%|██▏       | 36/165 [00:03<00:14,  9.11it/s]\u001b[A\n",
            " 22%|██▏       | 37/165 [00:04<00:14,  9.12it/s]\u001b[A\n",
            " 23%|██▎       | 38/165 [00:04<00:13,  9.10it/s]\u001b[A\n",
            " 24%|██▎       | 39/165 [00:04<00:13,  9.11it/s]\u001b[A\n",
            " 24%|██▍       | 40/165 [00:04<00:13,  9.10it/s]\u001b[A\n",
            " 25%|██▍       | 41/165 [00:04<00:13,  9.11it/s]\u001b[A\n",
            " 25%|██▌       | 42/165 [00:04<00:13,  9.09it/s]\u001b[A\n",
            " 26%|██▌       | 43/165 [00:04<00:13,  9.10it/s]\u001b[A\n",
            " 27%|██▋       | 44/165 [00:04<00:13,  9.11it/s]\u001b[A\n",
            " 27%|██▋       | 45/165 [00:04<00:13,  9.11it/s]\u001b[A\n",
            " 28%|██▊       | 46/165 [00:05<00:13,  9.11it/s]\u001b[A\n",
            " 28%|██▊       | 47/165 [00:05<00:12,  9.11it/s]\u001b[A\n",
            " 29%|██▉       | 48/165 [00:05<00:12,  9.11it/s]\u001b[A\n",
            " 30%|██▉       | 49/165 [00:05<00:12,  9.11it/s]\u001b[A\n",
            " 30%|███       | 50/165 [00:05<00:12,  9.11it/s]\u001b[A\n",
            " 31%|███       | 51/165 [00:05<00:12,  9.12it/s]\u001b[A\n",
            " 32%|███▏      | 52/165 [00:05<00:12,  9.11it/s]\u001b[A\n",
            " 32%|███▏      | 53/165 [00:05<00:12,  9.09it/s]\u001b[A\n",
            " 33%|███▎      | 54/165 [00:05<00:12,  9.10it/s]\u001b[A\n",
            " 33%|███▎      | 55/165 [00:06<00:12,  9.09it/s]\u001b[A\n",
            " 34%|███▍      | 56/165 [00:06<00:12,  9.08it/s]\u001b[A\n",
            " 35%|███▍      | 57/165 [00:06<00:11,  9.07it/s]\u001b[A\n",
            " 35%|███▌      | 58/165 [00:06<00:11,  9.08it/s]\u001b[A\n",
            " 36%|███▌      | 59/165 [00:06<00:11,  9.08it/s]\u001b[A\n",
            " 36%|███▋      | 60/165 [00:06<00:11,  9.08it/s]\u001b[A\n",
            " 37%|███▋      | 61/165 [00:06<00:11,  9.10it/s]\u001b[A\n",
            " 38%|███▊      | 62/165 [00:06<00:11,  9.10it/s]\u001b[A\n",
            " 38%|███▊      | 63/165 [00:06<00:11,  9.12it/s]\u001b[A\n",
            " 39%|███▉      | 64/165 [00:07<00:11,  9.13it/s]\u001b[A\n",
            " 39%|███▉      | 65/165 [00:07<00:10,  9.13it/s]\u001b[A\n",
            " 40%|████      | 66/165 [00:07<00:10,  9.12it/s]\u001b[A\n",
            " 41%|████      | 67/165 [00:07<00:10,  9.12it/s]\u001b[A\n",
            " 41%|████      | 68/165 [00:07<00:10,  9.12it/s]\u001b[A\n",
            " 42%|████▏     | 69/165 [00:07<00:10,  9.12it/s]\u001b[A\n",
            " 42%|████▏     | 70/165 [00:07<00:10,  9.13it/s]\u001b[A\n",
            " 43%|████▎     | 71/165 [00:07<00:10,  9.14it/s]\u001b[A\n",
            " 44%|████▎     | 72/165 [00:07<00:10,  9.13it/s]\u001b[A\n",
            " 44%|████▍     | 73/165 [00:08<00:10,  9.11it/s]\u001b[A\n",
            " 45%|████▍     | 74/165 [00:08<00:10,  9.10it/s]\u001b[A\n",
            " 45%|████▌     | 75/165 [00:08<00:09,  9.11it/s]\u001b[A\n",
            " 46%|████▌     | 76/165 [00:08<00:09,  9.12it/s]\u001b[A\n",
            " 47%|████▋     | 77/165 [00:08<00:09,  9.11it/s]\u001b[A\n",
            " 47%|████▋     | 78/165 [00:08<00:09,  9.10it/s]\u001b[A\n",
            " 48%|████▊     | 79/165 [00:08<00:09,  9.11it/s]\u001b[A\n",
            " 48%|████▊     | 80/165 [00:08<00:09,  9.10it/s]\u001b[A\n",
            " 49%|████▉     | 81/165 [00:08<00:09,  9.13it/s]\u001b[A\n",
            " 50%|████▉     | 82/165 [00:09<00:09,  9.14it/s]\u001b[A\n",
            " 50%|█████     | 83/165 [00:09<00:08,  9.14it/s]\u001b[A\n",
            " 51%|█████     | 84/165 [00:09<00:08,  9.13it/s]\u001b[A\n",
            " 52%|█████▏    | 85/165 [00:09<00:08,  9.13it/s]\u001b[A\n",
            " 52%|█████▏    | 86/165 [00:09<00:08,  9.12it/s]\u001b[A\n",
            " 53%|█████▎    | 87/165 [00:09<00:08,  9.12it/s]\u001b[A\n",
            " 53%|█████▎    | 88/165 [00:09<00:08,  9.13it/s]\u001b[A\n",
            " 54%|█████▍    | 89/165 [00:09<00:08,  9.14it/s]\u001b[A\n",
            " 55%|█████▍    | 90/165 [00:09<00:08,  9.14it/s]\u001b[A\n",
            " 55%|█████▌    | 91/165 [00:09<00:08,  9.13it/s]\u001b[A\n",
            " 56%|█████▌    | 92/165 [00:10<00:07,  9.15it/s]\u001b[A\n",
            " 56%|█████▋    | 93/165 [00:10<00:07,  9.15it/s]\u001b[A\n",
            " 57%|█████▋    | 94/165 [00:10<00:07,  9.15it/s]\u001b[A\n",
            " 58%|█████▊    | 95/165 [00:10<00:07,  9.14it/s]\u001b[A\n",
            " 58%|█████▊    | 96/165 [00:10<00:07,  9.12it/s]\u001b[A\n",
            " 59%|█████▉    | 97/165 [00:10<00:07,  9.10it/s]\u001b[A\n",
            " 59%|█████▉    | 98/165 [00:10<00:07,  9.11it/s]\u001b[A\n",
            " 60%|██████    | 99/165 [00:10<00:07,  9.11it/s]\u001b[A\n",
            " 61%|██████    | 100/165 [00:10<00:07,  9.10it/s]\u001b[A\n",
            " 61%|██████    | 101/165 [00:11<00:07,  9.10it/s]\u001b[A\n",
            " 62%|██████▏   | 102/165 [00:11<00:06,  9.11it/s]\u001b[A\n",
            " 62%|██████▏   | 103/165 [00:11<00:06,  9.12it/s]\u001b[A\n",
            " 63%|██████▎   | 104/165 [00:11<00:06,  9.12it/s]\u001b[A\n",
            " 64%|██████▎   | 105/165 [00:11<00:06,  9.11it/s]\u001b[A\n",
            " 64%|██████▍   | 106/165 [00:11<00:06,  9.11it/s]\u001b[A\n",
            " 65%|██████▍   | 107/165 [00:11<00:06,  9.11it/s]\u001b[A\n",
            " 65%|██████▌   | 108/165 [00:11<00:06,  9.11it/s]\u001b[A\n",
            " 66%|██████▌   | 109/165 [00:11<00:06,  9.12it/s]\u001b[A\n",
            " 67%|██████▋   | 110/165 [00:12<00:06,  9.14it/s]\u001b[A\n",
            " 67%|██████▋   | 111/165 [00:12<00:05,  9.12it/s]\u001b[A\n",
            " 68%|██████▊   | 112/165 [00:12<00:05,  9.12it/s]\u001b[A\n",
            " 68%|██████▊   | 113/165 [00:12<00:05,  9.12it/s]\u001b[A\n",
            " 69%|██████▉   | 114/165 [00:12<00:05,  9.13it/s]\u001b[A\n",
            " 70%|██████▉   | 115/165 [00:12<00:05,  9.13it/s]\u001b[A\n",
            " 70%|███████   | 116/165 [00:12<00:05,  9.14it/s]\u001b[A\n",
            " 71%|███████   | 117/165 [00:12<00:05,  9.15it/s]\u001b[A\n",
            " 72%|███████▏  | 118/165 [00:12<00:05,  9.08it/s]\u001b[A\n",
            " 72%|███████▏  | 119/165 [00:13<00:05,  9.09it/s]\u001b[A\n",
            " 73%|███████▎  | 120/165 [00:13<00:04,  9.10it/s]\u001b[A\n",
            " 73%|███████▎  | 121/165 [00:13<00:04,  9.09it/s]\u001b[A\n",
            " 74%|███████▍  | 122/165 [00:13<00:04,  9.12it/s]\u001b[A\n",
            " 75%|███████▍  | 123/165 [00:13<00:04,  9.11it/s]\u001b[A\n",
            " 75%|███████▌  | 124/165 [00:13<00:04,  9.12it/s]\u001b[A\n",
            " 76%|███████▌  | 125/165 [00:13<00:04,  9.13it/s]\u001b[A\n",
            " 76%|███████▋  | 126/165 [00:13<00:04,  9.14it/s]\u001b[A\n",
            " 77%|███████▋  | 127/165 [00:13<00:04,  9.12it/s]\u001b[A\n",
            " 78%|███████▊  | 128/165 [00:14<00:04,  9.11it/s]\u001b[A\n",
            " 78%|███████▊  | 129/165 [00:14<00:03,  9.11it/s]\u001b[A\n",
            " 79%|███████▉  | 130/165 [00:14<00:03,  9.11it/s]\u001b[A\n",
            " 79%|███████▉  | 131/165 [00:14<00:03,  9.12it/s]\u001b[A\n",
            " 80%|████████  | 132/165 [00:14<00:03,  9.13it/s]\u001b[A\n",
            " 81%|████████  | 133/165 [00:14<00:03,  9.13it/s]\u001b[A\n",
            " 81%|████████  | 134/165 [00:14<00:03,  9.11it/s]\u001b[A\n",
            " 82%|████████▏ | 135/165 [00:14<00:03,  9.09it/s]\u001b[A\n",
            " 82%|████████▏ | 136/165 [00:14<00:03,  9.10it/s]\u001b[A\n",
            " 83%|████████▎ | 137/165 [00:15<00:03,  9.10it/s]\u001b[A\n",
            " 84%|████████▎ | 138/165 [00:15<00:02,  9.11it/s]\u001b[A\n",
            " 84%|████████▍ | 139/165 [00:15<00:02,  9.13it/s]\u001b[A\n",
            " 85%|████████▍ | 140/165 [00:15<00:02,  9.12it/s]\u001b[A\n",
            " 85%|████████▌ | 141/165 [00:15<00:02,  9.11it/s]\u001b[A\n",
            " 86%|████████▌ | 142/165 [00:15<00:02,  9.11it/s]\u001b[A\n",
            " 87%|████████▋ | 143/165 [00:15<00:02,  9.08it/s]\u001b[A\n",
            " 87%|████████▋ | 144/165 [00:15<00:02,  9.09it/s]\u001b[A\n",
            " 88%|████████▊ | 145/165 [00:15<00:02,  9.08it/s]\u001b[A\n",
            " 88%|████████▊ | 146/165 [00:16<00:02,  9.08it/s]\u001b[A\n",
            " 89%|████████▉ | 147/165 [00:16<00:01,  9.08it/s]\u001b[A\n",
            " 90%|████████▉ | 148/165 [00:16<00:01,  9.09it/s]\u001b[A\n",
            " 90%|█████████ | 149/165 [00:16<00:01,  9.11it/s]\u001b[A\n",
            " 91%|█████████ | 150/165 [00:16<00:01,  9.12it/s]\u001b[A\n",
            " 92%|█████████▏| 151/165 [00:16<00:01,  9.12it/s]\u001b[A\n",
            " 92%|█████████▏| 152/165 [00:16<00:01,  9.10it/s]\u001b[A\n",
            " 93%|█████████▎| 153/165 [00:16<00:01,  9.07it/s]\u001b[A\n",
            " 93%|█████████▎| 154/165 [00:16<00:01,  9.08it/s]\u001b[A\n",
            " 94%|█████████▍| 155/165 [00:17<00:01,  9.08it/s]\u001b[A\n",
            " 95%|█████████▍| 156/165 [00:17<00:00,  9.09it/s]\u001b[A\n",
            " 95%|█████████▌| 157/165 [00:17<00:00,  9.11it/s]\u001b[A\n",
            " 96%|█████████▌| 158/165 [00:17<00:00,  9.13it/s]\u001b[A\n",
            " 96%|█████████▋| 159/165 [00:17<00:00,  9.14it/s]\u001b[A\n",
            " 97%|█████████▋| 160/165 [00:17<00:00,  9.15it/s]\u001b[A\n",
            " 98%|█████████▊| 161/165 [00:17<00:00,  9.15it/s]\u001b[A\n",
            " 98%|█████████▊| 162/165 [00:17<00:00,  9.13it/s]\u001b[A\n",
            " 99%|█████████▉| 163/165 [00:17<00:00,  9.11it/s]\u001b[A\n",
            "100%|██████████| 165/165 [00:18<00:00,  9.10it/s]\n",
            "========== model: bert-base-uncased fold: 4 inference ==========\n",
            "========== model: bert-base-uncased fold: 4 inference ==========\n",
            "loading configuration file /content/activebus_bert_final/config.json\n",
            "Model config BertConfig {\n",
            "  \"_name_or_path\": \"activebus/BERT-XD_Review\",\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_ids\": 0,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.15.0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "loading weights file /content/activebus_bert_final/pytorch_model.bin\n",
            "Some weights of the model checkpoint at /content/activebus_bert_final were not used when initializing BertForSequenceClassification: ['cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /content/activebus_bert_final and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "\n",
            "  0%|          | 0/165 [00:00<?, ?it/s]\u001b[A\n",
            "  1%|          | 1/165 [00:00<00:18,  8.63it/s]\u001b[A\n",
            "  1%|          | 2/165 [00:00<00:18,  8.87it/s]\u001b[A\n",
            "  2%|▏         | 3/165 [00:00<00:18,  8.98it/s]\u001b[A\n",
            "  2%|▏         | 4/165 [00:00<00:17,  9.02it/s]\u001b[A\n",
            "  3%|▎         | 5/165 [00:00<00:17,  9.04it/s]\u001b[A\n",
            "  4%|▎         | 6/165 [00:00<00:17,  9.06it/s]\u001b[A\n",
            "  4%|▍         | 7/165 [00:00<00:17,  9.09it/s]\u001b[A\n",
            "  5%|▍         | 8/165 [00:00<00:17,  9.11it/s]\u001b[A\n",
            "  5%|▌         | 9/165 [00:00<00:17,  9.12it/s]\u001b[A\n",
            "  6%|▌         | 10/165 [00:01<00:16,  9.13it/s]\u001b[A\n",
            "  7%|▋         | 11/165 [00:01<00:16,  9.11it/s]\u001b[A\n",
            "  7%|▋         | 12/165 [00:01<00:16,  9.11it/s]\u001b[A\n",
            "  8%|▊         | 13/165 [00:01<00:16,  9.11it/s]\u001b[A\n",
            "  8%|▊         | 14/165 [00:01<00:16,  9.10it/s]\u001b[A\n",
            "  9%|▉         | 15/165 [00:01<00:16,  9.11it/s]\u001b[A\n",
            " 10%|▉         | 16/165 [00:01<00:16,  9.13it/s]\u001b[A\n",
            " 10%|█         | 17/165 [00:01<00:16,  9.11it/s]\u001b[A\n",
            " 11%|█         | 18/165 [00:01<00:16,  9.10it/s]\u001b[A\n",
            " 12%|█▏        | 19/165 [00:02<00:16,  9.11it/s]\u001b[A\n",
            " 12%|█▏        | 20/165 [00:02<00:15,  9.10it/s]\u001b[A\n",
            " 13%|█▎        | 21/165 [00:02<00:15,  9.12it/s]\u001b[A\n",
            " 13%|█▎        | 22/165 [00:02<00:15,  9.13it/s]\u001b[A\n",
            " 14%|█▍        | 23/165 [00:02<00:15,  9.14it/s]\u001b[A\n",
            " 15%|█▍        | 24/165 [00:02<00:15,  9.13it/s]\u001b[A\n",
            " 15%|█▌        | 25/165 [00:02<00:15,  9.14it/s]\u001b[A\n",
            " 16%|█▌        | 26/165 [00:02<00:15,  9.12it/s]\u001b[A\n",
            " 16%|█▋        | 27/165 [00:02<00:15,  9.11it/s]\u001b[A\n",
            " 17%|█▋        | 28/165 [00:03<00:15,  9.11it/s]\u001b[A\n",
            " 18%|█▊        | 29/165 [00:03<00:14,  9.11it/s]\u001b[A\n",
            " 18%|█▊        | 30/165 [00:03<00:14,  9.10it/s]\u001b[A\n",
            " 19%|█▉        | 31/165 [00:03<00:14,  9.09it/s]\u001b[A\n",
            " 19%|█▉        | 32/165 [00:03<00:14,  9.08it/s]\u001b[A\n",
            " 20%|██        | 33/165 [00:03<00:14,  9.10it/s]\u001b[A\n",
            " 21%|██        | 34/165 [00:03<00:14,  9.12it/s]\u001b[A\n",
            " 21%|██        | 35/165 [00:03<00:14,  9.11it/s]\u001b[A\n",
            " 22%|██▏       | 36/165 [00:03<00:14,  9.10it/s]\u001b[A\n",
            " 22%|██▏       | 37/165 [00:04<00:14,  9.12it/s]\u001b[A\n",
            " 23%|██▎       | 38/165 [00:04<00:13,  9.12it/s]\u001b[A\n",
            " 24%|██▎       | 39/165 [00:04<00:13,  9.12it/s]\u001b[A\n",
            " 24%|██▍       | 40/165 [00:04<00:13,  9.14it/s]\u001b[A\n",
            " 25%|██▍       | 41/165 [00:04<00:13,  9.13it/s]\u001b[A\n",
            " 25%|██▌       | 42/165 [00:04<00:13,  9.11it/s]\u001b[A\n",
            " 26%|██▌       | 43/165 [00:04<00:13,  9.09it/s]\u001b[A\n",
            " 27%|██▋       | 44/165 [00:04<00:13,  9.08it/s]\u001b[A\n",
            " 27%|██▋       | 45/165 [00:04<00:13,  9.08it/s]\u001b[A\n",
            " 28%|██▊       | 46/165 [00:05<00:13,  9.08it/s]\u001b[A\n",
            " 28%|██▊       | 47/165 [00:05<00:12,  9.10it/s]\u001b[A\n",
            " 29%|██▉       | 48/165 [00:05<00:12,  9.11it/s]\u001b[A\n",
            " 30%|██▉       | 49/165 [00:05<00:12,  9.12it/s]\u001b[A\n",
            " 30%|███       | 50/165 [00:05<00:12,  9.13it/s]\u001b[A\n",
            " 31%|███       | 51/165 [00:05<00:12,  9.10it/s]\u001b[A\n",
            " 32%|███▏      | 52/165 [00:05<00:12,  9.08it/s]\u001b[A\n",
            " 32%|███▏      | 53/165 [00:05<00:12,  9.09it/s]\u001b[A\n",
            " 33%|███▎      | 54/165 [00:05<00:12,  9.12it/s]\u001b[A\n",
            " 33%|███▎      | 55/165 [00:06<00:12,  9.13it/s]\u001b[A\n",
            " 34%|███▍      | 56/165 [00:06<00:11,  9.12it/s]\u001b[A\n",
            " 35%|███▍      | 57/165 [00:06<00:11,  9.13it/s]\u001b[A\n",
            " 35%|███▌      | 58/165 [00:06<00:11,  9.12it/s]\u001b[A\n",
            " 36%|███▌      | 59/165 [00:06<00:11,  9.11it/s]\u001b[A\n",
            " 36%|███▋      | 60/165 [00:06<00:11,  9.11it/s]\u001b[A\n",
            " 37%|███▋      | 61/165 [00:06<00:11,  9.12it/s]\u001b[A\n",
            " 38%|███▊      | 62/165 [00:06<00:11,  9.10it/s]\u001b[A\n",
            " 38%|███▊      | 63/165 [00:06<00:11,  9.11it/s]\u001b[A\n",
            " 39%|███▉      | 64/165 [00:07<00:11,  9.12it/s]\u001b[A\n",
            " 39%|███▉      | 65/165 [00:07<00:10,  9.10it/s]\u001b[A\n",
            " 40%|████      | 66/165 [00:07<00:10,  9.10it/s]\u001b[A\n",
            " 41%|████      | 67/165 [00:07<00:10,  9.12it/s]\u001b[A\n",
            " 41%|████      | 68/165 [00:07<00:10,  9.13it/s]\u001b[A\n",
            " 42%|████▏     | 69/165 [00:07<00:10,  9.14it/s]\u001b[A\n",
            " 42%|████▏     | 70/165 [00:07<00:10,  9.11it/s]\u001b[A\n",
            " 43%|████▎     | 71/165 [00:07<00:10,  9.12it/s]\u001b[A\n",
            " 44%|████▎     | 72/165 [00:07<00:10,  9.12it/s]\u001b[A\n",
            " 44%|████▍     | 73/165 [00:08<00:10,  9.11it/s]\u001b[A\n",
            " 45%|████▍     | 74/165 [00:08<00:09,  9.10it/s]\u001b[A\n",
            " 45%|████▌     | 75/165 [00:08<00:09,  9.09it/s]\u001b[A\n",
            " 46%|████▌     | 76/165 [00:08<00:09,  9.10it/s]\u001b[A\n",
            " 47%|████▋     | 77/165 [00:08<00:09,  9.10it/s]\u001b[A\n",
            " 47%|████▋     | 78/165 [00:08<00:09,  9.10it/s]\u001b[A\n",
            " 48%|████▊     | 79/165 [00:08<00:09,  9.10it/s]\u001b[A\n",
            " 48%|████▊     | 80/165 [00:08<00:09,  9.11it/s]\u001b[A\n",
            " 49%|████▉     | 81/165 [00:08<00:09,  9.12it/s]\u001b[A\n",
            " 50%|████▉     | 82/165 [00:09<00:09,  9.11it/s]\u001b[A\n",
            " 50%|█████     | 83/165 [00:09<00:08,  9.12it/s]\u001b[A\n",
            " 51%|█████     | 84/165 [00:09<00:08,  9.11it/s]\u001b[A\n",
            " 52%|█████▏    | 85/165 [00:09<00:08,  9.10it/s]\u001b[A\n",
            " 52%|█████▏    | 86/165 [00:09<00:08,  9.09it/s]\u001b[A\n",
            " 53%|█████▎    | 87/165 [00:09<00:08,  9.11it/s]\u001b[A\n",
            " 53%|█████▎    | 88/165 [00:09<00:08,  9.11it/s]\u001b[A\n",
            " 54%|█████▍    | 89/165 [00:09<00:08,  9.12it/s]\u001b[A\n",
            " 55%|█████▍    | 90/165 [00:09<00:08,  9.11it/s]\u001b[A\n",
            " 55%|█████▌    | 91/165 [00:09<00:08,  9.11it/s]\u001b[A\n",
            " 56%|█████▌    | 92/165 [00:10<00:07,  9.13it/s]\u001b[A\n",
            " 56%|█████▋    | 93/165 [00:10<00:07,  9.13it/s]\u001b[A\n",
            " 57%|█████▋    | 94/165 [00:10<00:07,  9.14it/s]\u001b[A\n",
            " 58%|█████▊    | 95/165 [00:10<00:07,  9.13it/s]\u001b[A\n",
            " 58%|█████▊    | 96/165 [00:10<00:07,  9.13it/s]\u001b[A\n",
            " 59%|█████▉    | 97/165 [00:10<00:07,  9.15it/s]\u001b[A\n",
            " 59%|█████▉    | 98/165 [00:10<00:07,  9.13it/s]\u001b[A\n",
            " 60%|██████    | 99/165 [00:10<00:07,  9.16it/s]\u001b[A\n",
            " 61%|██████    | 100/165 [00:10<00:07,  9.15it/s]\u001b[A\n",
            " 61%|██████    | 101/165 [00:11<00:06,  9.16it/s]\u001b[A\n",
            " 62%|██████▏   | 102/165 [00:11<00:06,  9.17it/s]\u001b[A\n",
            " 62%|██████▏   | 103/165 [00:11<00:06,  9.15it/s]\u001b[A\n",
            " 63%|██████▎   | 104/165 [00:11<00:06,  9.14it/s]\u001b[A\n",
            " 64%|██████▎   | 105/165 [00:11<00:06,  9.15it/s]\u001b[A\n",
            " 64%|██████▍   | 106/165 [00:11<00:06,  9.14it/s]\u001b[A\n",
            " 65%|██████▍   | 107/165 [00:11<00:06,  9.10it/s]\u001b[A\n",
            " 65%|██████▌   | 108/165 [00:11<00:06,  9.11it/s]\u001b[A\n",
            " 66%|██████▌   | 109/165 [00:11<00:06,  9.09it/s]\u001b[A\n",
            " 67%|██████▋   | 110/165 [00:12<00:06,  9.05it/s]\u001b[A\n",
            " 67%|██████▋   | 111/165 [00:12<00:05,  9.07it/s]\u001b[A\n",
            " 68%|██████▊   | 112/165 [00:12<00:05,  9.09it/s]\u001b[A\n",
            " 68%|██████▊   | 113/165 [00:12<00:05,  9.09it/s]\u001b[A\n",
            " 69%|██████▉   | 114/165 [00:12<00:05,  9.07it/s]\u001b[A\n",
            " 70%|██████▉   | 115/165 [00:12<00:05,  9.10it/s]\u001b[A\n",
            " 70%|███████   | 116/165 [00:12<00:05,  9.11it/s]\u001b[A\n",
            " 71%|███████   | 117/165 [00:12<00:05,  9.08it/s]\u001b[A\n",
            " 72%|███████▏  | 118/165 [00:12<00:05,  9.11it/s]\u001b[A\n",
            " 72%|███████▏  | 119/165 [00:13<00:05,  9.09it/s]\u001b[A\n",
            " 73%|███████▎  | 120/165 [00:13<00:04,  9.11it/s]\u001b[A\n",
            " 73%|███████▎  | 121/165 [00:13<00:04,  9.10it/s]\u001b[A\n",
            " 74%|███████▍  | 122/165 [00:13<00:04,  9.10it/s]\u001b[A\n",
            " 75%|███████▍  | 123/165 [00:13<00:04,  9.09it/s]\u001b[A\n",
            " 75%|███████▌  | 124/165 [00:13<00:04,  9.09it/s]\u001b[A\n",
            " 76%|███████▌  | 125/165 [00:13<00:04,  9.11it/s]\u001b[A\n",
            " 76%|███████▋  | 126/165 [00:13<00:04,  9.12it/s]\u001b[A\n",
            " 77%|███████▋  | 127/165 [00:13<00:04,  9.12it/s]\u001b[A\n",
            " 78%|███████▊  | 128/165 [00:14<00:04,  9.11it/s]\u001b[A\n",
            " 78%|███████▊  | 129/165 [00:14<00:03,  9.12it/s]\u001b[A\n",
            " 79%|███████▉  | 130/165 [00:14<00:03,  9.11it/s]\u001b[A\n",
            " 79%|███████▉  | 131/165 [00:14<00:03,  9.10it/s]\u001b[A\n",
            " 80%|████████  | 132/165 [00:14<00:03,  9.10it/s]\u001b[A\n",
            " 81%|████████  | 133/165 [00:14<00:03,  9.08it/s]\u001b[A\n",
            " 81%|████████  | 134/165 [00:14<00:03,  9.08it/s]\u001b[A\n",
            " 82%|████████▏ | 135/165 [00:14<00:03,  9.07it/s]\u001b[A\n",
            " 82%|████████▏ | 136/165 [00:14<00:03,  9.10it/s]\u001b[A\n",
            " 83%|████████▎ | 137/165 [00:15<00:03,  9.08it/s]\u001b[A\n",
            " 84%|████████▎ | 138/165 [00:15<00:02,  9.09it/s]\u001b[A\n",
            " 84%|████████▍ | 139/165 [00:15<00:02,  9.09it/s]\u001b[A\n",
            " 85%|████████▍ | 140/165 [00:15<00:02,  9.10it/s]\u001b[A\n",
            " 85%|████████▌ | 141/165 [00:15<00:02,  9.09it/s]\u001b[A\n",
            " 86%|████████▌ | 142/165 [00:15<00:02,  9.08it/s]\u001b[A\n",
            " 87%|████████▋ | 143/165 [00:15<00:02,  9.09it/s]\u001b[A\n",
            " 87%|████████▋ | 144/165 [00:15<00:02,  9.10it/s]\u001b[A\n",
            " 88%|████████▊ | 145/165 [00:15<00:02,  9.09it/s]\u001b[A\n",
            " 88%|████████▊ | 146/165 [00:16<00:02,  9.10it/s]\u001b[A\n",
            " 89%|████████▉ | 147/165 [00:16<00:01,  9.08it/s]\u001b[A\n",
            " 90%|████████▉ | 148/165 [00:16<00:01,  9.06it/s]\u001b[A\n",
            " 90%|█████████ | 149/165 [00:16<00:01,  9.05it/s]\u001b[A\n",
            " 91%|█████████ | 150/165 [00:16<00:01,  9.08it/s]\u001b[A\n",
            " 92%|█████████▏| 151/165 [00:16<00:01,  9.07it/s]\u001b[A\n",
            " 92%|█████████▏| 152/165 [00:16<00:01,  9.07it/s]\u001b[A\n",
            " 93%|█████████▎| 153/165 [00:16<00:01,  9.06it/s]\u001b[A\n",
            " 93%|█████████▎| 154/165 [00:16<00:01,  9.07it/s]\u001b[A\n",
            " 94%|█████████▍| 155/165 [00:17<00:01,  9.10it/s]\u001b[A\n",
            " 95%|█████████▍| 156/165 [00:17<00:00,  9.11it/s]\u001b[A\n",
            " 95%|█████████▌| 157/165 [00:17<00:00,  9.09it/s]\u001b[A\n",
            " 96%|█████████▌| 158/165 [00:17<00:00,  9.09it/s]\u001b[A\n",
            " 96%|█████████▋| 159/165 [00:17<00:00,  9.10it/s]\u001b[A\n",
            " 97%|█████████▋| 160/165 [00:17<00:00,  9.11it/s]\u001b[A\n",
            " 98%|█████████▊| 161/165 [00:17<00:00,  9.12it/s]\u001b[A\n",
            " 98%|█████████▊| 162/165 [00:17<00:00,  9.12it/s]\u001b[A\n",
            " 99%|█████████▉| 163/165 [00:17<00:00,  9.12it/s]\u001b[A\n",
            "100%|██████████| 165/165 [00:18<00:00,  9.09it/s]\n",
            " 33%|███▎      | 2/6 [1:35:11<3:10:20, 2855.08s/it]Didn't find file /content/activebus_bert_final/added_tokens.json. We won't load it.\n",
            "loading file /content/activebus_bert_final/vocab.txt\n",
            "loading file /content/activebus_bert_final/tokenizer.json\n",
            "loading file None\n",
            "loading file /content/activebus_bert_final/special_tokens_map.json\n",
            "loading file /content/activebus_bert_final/tokenizer_config.json\n",
            "========== fold: 0 training ==========\n",
            "========== fold: 0 training ==========\n",
            "========== fold: 0 training ==========\n",
            "Didn't find file /content/activebus_bert_final/added_tokens.json. We won't load it.\n",
            "loading file /content/activebus_bert_final/vocab.txt\n",
            "loading file None\n",
            "loading file /content/activebus_bert_final/special_tokens_map.json\n",
            "loading file /content/activebus_bert_final/tokenizer_config.json\n",
            "loading file /content/activebus_bert_final/tokenizer.json\n",
            "Didn't find file /content/activebus_bert_final/added_tokens.json. We won't load it.\n",
            "loading file /content/activebus_bert_final/vocab.txt\n",
            "loading file None\n",
            "loading file /content/activebus_bert_final/special_tokens_map.json\n",
            "loading file /content/activebus_bert_final/tokenizer_config.json\n",
            "loading file /content/activebus_bert_final/tokenizer.json\n",
            "loading configuration file /content/activebus_bert_final/config.json\n",
            "Model config BertConfig {\n",
            "  \"_name_or_path\": \"activebus/BERT-XD_Review\",\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_ids\": 0,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.15.0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "loading weights file /content/activebus_bert_final/pytorch_model.bin\n",
            "Some weights of the model checkpoint at /content/activebus_bert_final were not used when initializing BertForSequenceClassification: ['cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /content/activebus_bert_final and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: [1][0/306] Elapsed 0m 0s (remain 3m 4s) Loss: 0.7043 \n",
            "Epoch: [1][100/306] Elapsed 0m 33s (remain 1m 7s) Loss: 0.4595 \n",
            "Epoch: [1][200/306] Elapsed 1m 5s (remain 0m 34s) Loss: 0.3759 \n",
            "Epoch: [1][300/306] Elapsed 1m 38s (remain 0m 1s) Loss: 0.3426 \n",
            "Epoch: [1][305/306] Elapsed 1m 39s (remain 0m 0s) Loss: 0.3404 \n",
            "EVAL: [0/77] Elapsed 0m 0s (remain 0m 27s) Loss: 0.5160 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1 - avg_train_loss: 0.3404  avg_val_loss: 0.2397  time: 108s\n",
            "Epoch 1 - avg_train_loss: 0.3404  avg_val_loss: 0.2397  time: 108s\n",
            "Epoch 1 - avg_train_loss: 0.3404  avg_val_loss: 0.2397  time: 108s\n",
            "Epoch 1 - Score: 0.23973632776421108\n",
            "Epoch 1 - Score: 0.23973632776421108\n",
            "Epoch 1 - Score: 0.23973632776421108\n",
            "Epoch 1 - Save Best Score: 0.2397 Model\n",
            "Epoch 1 - Save Best Score: 0.2397 Model\n",
            "Epoch 1 - Save Best Score: 0.2397 Model\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EVAL: [76/77] Elapsed 0m 8s (remain 0m 0s) Loss: 0.2397 \n",
            "Log loss for 0 is 0.23973632776421108\n",
            "Epoch: [2][0/306] Elapsed 0m 0s (remain 3m 4s) Loss: 0.1961 \n",
            "Epoch: [2][100/306] Elapsed 0m 33s (remain 1m 7s) Loss: 0.1915 \n",
            "Epoch: [2][200/306] Elapsed 1m 5s (remain 0m 34s) Loss: 0.1794 \n",
            "Epoch: [2][300/306] Elapsed 1m 38s (remain 0m 1s) Loss: 0.1810 \n",
            "Epoch: [2][305/306] Elapsed 1m 39s (remain 0m 0s) Loss: 0.1806 \n",
            "EVAL: [0/77] Elapsed 0m 0s (remain 0m 26s) Loss: 0.4182 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2 - avg_train_loss: 0.1806  avg_val_loss: 0.2269  time: 108s\n",
            "Epoch 2 - avg_train_loss: 0.1806  avg_val_loss: 0.2269  time: 108s\n",
            "Epoch 2 - avg_train_loss: 0.1806  avg_val_loss: 0.2269  time: 108s\n",
            "Epoch 2 - Score: 0.226916235490514\n",
            "Epoch 2 - Score: 0.226916235490514\n",
            "Epoch 2 - Score: 0.226916235490514\n",
            "Epoch 2 - Save Best Score: 0.2269 Model\n",
            "Epoch 2 - Save Best Score: 0.2269 Model\n",
            "Epoch 2 - Save Best Score: 0.2269 Model\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EVAL: [76/77] Elapsed 0m 8s (remain 0m 0s) Loss: 0.2269 \n",
            "Log loss for 1 is 0.226916235490514\n",
            "Epoch: [3][0/306] Elapsed 0m 0s (remain 2m 58s) Loss: 0.0419 \n",
            "Epoch: [3][100/306] Elapsed 0m 33s (remain 1m 7s) Loss: 0.1003 \n",
            "Epoch: [3][200/306] Elapsed 1m 5s (remain 0m 34s) Loss: 0.0909 \n",
            "Epoch: [3][300/306] Elapsed 1m 38s (remain 0m 1s) Loss: 0.1017 \n",
            "Epoch: [3][305/306] Elapsed 1m 39s (remain 0m 0s) Loss: 0.1013 \n",
            "EVAL: [0/77] Elapsed 0m 0s (remain 0m 28s) Loss: 0.4073 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3 - avg_train_loss: 0.1013  avg_val_loss: 0.2621  time: 108s\n",
            "Epoch 3 - avg_train_loss: 0.1013  avg_val_loss: 0.2621  time: 108s\n",
            "Epoch 3 - avg_train_loss: 0.1013  avg_val_loss: 0.2621  time: 108s\n",
            "Epoch 3 - Score: 0.2620917561410734\n",
            "Epoch 3 - Score: 0.2620917561410734\n",
            "Epoch 3 - Score: 0.2620917561410734\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EVAL: [76/77] Elapsed 0m 8s (remain 0m 0s) Loss: 0.2621 \n",
            "Log loss for 2 is 0.2620917561410734\n",
            "Epoch: [4][0/306] Elapsed 0m 0s (remain 2m 59s) Loss: 0.0318 \n",
            "Epoch: [4][100/306] Elapsed 0m 33s (remain 1m 7s) Loss: 0.0554 \n",
            "Epoch: [4][200/306] Elapsed 1m 5s (remain 0m 34s) Loss: 0.0592 \n",
            "Epoch: [4][300/306] Elapsed 1m 38s (remain 0m 1s) Loss: 0.0605 \n",
            "Epoch: [4][305/306] Elapsed 1m 39s (remain 0m 0s) Loss: 0.0617 \n",
            "EVAL: [0/77] Elapsed 0m 0s (remain 0m 27s) Loss: 0.4946 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4 - avg_train_loss: 0.0617  avg_val_loss: 0.3474  time: 108s\n",
            "Epoch 4 - avg_train_loss: 0.0617  avg_val_loss: 0.3474  time: 108s\n",
            "Epoch 4 - avg_train_loss: 0.0617  avg_val_loss: 0.3474  time: 108s\n",
            "Epoch 4 - Score: 0.34739333699680697\n",
            "Epoch 4 - Score: 0.34739333699680697\n",
            "Epoch 4 - Score: 0.34739333699680697\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EVAL: [76/77] Elapsed 0m 8s (remain 0m 0s) Loss: 0.3474 \n",
            "Log loss for 3 is 0.34739333699680697\n",
            "Epoch: [5][0/306] Elapsed 0m 0s (remain 3m 4s) Loss: 0.0058 \n",
            "Epoch: [5][100/306] Elapsed 0m 33s (remain 1m 7s) Loss: 0.0246 \n",
            "Epoch: [5][200/306] Elapsed 1m 5s (remain 0m 34s) Loss: 0.0357 \n",
            "Epoch: [5][300/306] Elapsed 1m 38s (remain 0m 1s) Loss: 0.0366 \n",
            "Epoch: [5][305/306] Elapsed 1m 39s (remain 0m 0s) Loss: 0.0362 \n",
            "EVAL: [0/77] Elapsed 0m 0s (remain 0m 27s) Loss: 0.3487 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5 - avg_train_loss: 0.0362  avg_val_loss: 0.3584  time: 108s\n",
            "Epoch 5 - avg_train_loss: 0.0362  avg_val_loss: 0.3584  time: 108s\n",
            "Epoch 5 - avg_train_loss: 0.0362  avg_val_loss: 0.3584  time: 108s\n",
            "Epoch 5 - Score: 0.3583646989141683\n",
            "Epoch 5 - Score: 0.3583646989141683\n",
            "Epoch 5 - Score: 0.3583646989141683\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EVAL: [76/77] Elapsed 0m 8s (remain 0m 0s) Loss: 0.3584 \n",
            "Log loss for 4 is 0.3583646989141683\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "========== fold: 0 result ==========\n",
            "========== fold: 0 result ==========\n",
            "========== fold: 0 result ==========\n",
            "Score: 0.22692\n",
            "Score: 0.22692\n",
            "Score: 0.22692\n",
            "========== fold: 1 training ==========\n",
            "========== fold: 1 training ==========\n",
            "========== fold: 1 training ==========\n",
            "Didn't find file /content/activebus_bert_final/added_tokens.json. We won't load it.\n",
            "loading file /content/activebus_bert_final/vocab.txt\n",
            "loading file None\n",
            "loading file /content/activebus_bert_final/special_tokens_map.json\n",
            "loading file /content/activebus_bert_final/tokenizer_config.json\n",
            "loading file /content/activebus_bert_final/tokenizer.json\n",
            "Didn't find file /content/activebus_bert_final/added_tokens.json. We won't load it.\n",
            "loading file /content/activebus_bert_final/vocab.txt\n",
            "loading file None\n",
            "loading file /content/activebus_bert_final/special_tokens_map.json\n",
            "loading file /content/activebus_bert_final/tokenizer_config.json\n",
            "loading file /content/activebus_bert_final/tokenizer.json\n",
            "loading configuration file /content/activebus_bert_final/config.json\n",
            "Model config BertConfig {\n",
            "  \"_name_or_path\": \"activebus/BERT-XD_Review\",\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_ids\": 0,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.15.0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "loading weights file /content/activebus_bert_final/pytorch_model.bin\n",
            "Some weights of the model checkpoint at /content/activebus_bert_final were not used when initializing BertForSequenceClassification: ['cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /content/activebus_bert_final and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: [1][0/306] Elapsed 0m 0s (remain 3m 10s) Loss: 0.7053 \n",
            "Epoch: [1][100/306] Elapsed 0m 33s (remain 1m 7s) Loss: 0.4723 \n",
            "Epoch: [1][200/306] Elapsed 1m 5s (remain 0m 34s) Loss: 0.3787 \n",
            "Epoch: [1][300/306] Elapsed 1m 38s (remain 0m 1s) Loss: 0.3379 \n",
            "Epoch: [1][305/306] Elapsed 1m 39s (remain 0m 0s) Loss: 0.3365 \n",
            "EVAL: [0/77] Elapsed 0m 0s (remain 0m 27s) Loss: 0.1351 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1 - avg_train_loss: 0.3365  avg_val_loss: 0.1991  time: 108s\n",
            "Epoch 1 - avg_train_loss: 0.3365  avg_val_loss: 0.1991  time: 108s\n",
            "Epoch 1 - avg_train_loss: 0.3365  avg_val_loss: 0.1991  time: 108s\n",
            "Epoch 1 - Score: 0.19909736707263126\n",
            "Epoch 1 - Score: 0.19909736707263126\n",
            "Epoch 1 - Score: 0.19909736707263126\n",
            "Epoch 1 - Save Best Score: 0.1991 Model\n",
            "Epoch 1 - Save Best Score: 0.1991 Model\n",
            "Epoch 1 - Save Best Score: 0.1991 Model\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EVAL: [76/77] Elapsed 0m 8s (remain 0m 0s) Loss: 0.1991 \n",
            "Log loss for 0 is 0.19909736707263126\n",
            "Epoch: [2][0/306] Elapsed 0m 0s (remain 2m 59s) Loss: 0.1357 \n",
            "Epoch: [2][100/306] Elapsed 0m 33s (remain 1m 7s) Loss: 0.1914 \n",
            "Epoch: [2][200/306] Elapsed 1m 5s (remain 0m 34s) Loss: 0.1800 \n",
            "Epoch: [2][300/306] Elapsed 1m 38s (remain 0m 1s) Loss: 0.1767 \n",
            "Epoch: [2][305/306] Elapsed 1m 39s (remain 0m 0s) Loss: 0.1756 \n",
            "EVAL: [0/77] Elapsed 0m 0s (remain 0m 26s) Loss: 0.1330 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2 - avg_train_loss: 0.1756  avg_val_loss: 0.1848  time: 108s\n",
            "Epoch 2 - avg_train_loss: 0.1756  avg_val_loss: 0.1848  time: 108s\n",
            "Epoch 2 - avg_train_loss: 0.1756  avg_val_loss: 0.1848  time: 108s\n",
            "Epoch 2 - Score: 0.1848186635451745\n",
            "Epoch 2 - Score: 0.1848186635451745\n",
            "Epoch 2 - Score: 0.1848186635451745\n",
            "Epoch 2 - Save Best Score: 0.1848 Model\n",
            "Epoch 2 - Save Best Score: 0.1848 Model\n",
            "Epoch 2 - Save Best Score: 0.1848 Model\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EVAL: [76/77] Elapsed 0m 8s (remain 0m 0s) Loss: 0.1848 \n",
            "Log loss for 1 is 0.1848186635451745\n",
            "Epoch: [3][0/306] Elapsed 0m 0s (remain 2m 57s) Loss: 0.0247 \n",
            "Epoch: [3][100/306] Elapsed 0m 33s (remain 1m 7s) Loss: 0.0873 \n",
            "Epoch: [3][200/306] Elapsed 1m 5s (remain 0m 34s) Loss: 0.0950 \n",
            "Epoch: [3][300/306] Elapsed 1m 37s (remain 0m 1s) Loss: 0.0939 \n",
            "Epoch: [3][305/306] Elapsed 1m 39s (remain 0m 0s) Loss: 0.0937 \n",
            "EVAL: [0/77] Elapsed 0m 0s (remain 0m 27s) Loss: 0.0372 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3 - avg_train_loss: 0.0937  avg_val_loss: 0.2202  time: 108s\n",
            "Epoch 3 - avg_train_loss: 0.0937  avg_val_loss: 0.2202  time: 108s\n",
            "Epoch 3 - avg_train_loss: 0.0937  avg_val_loss: 0.2202  time: 108s\n",
            "Epoch 3 - Score: 0.22021399370078515\n",
            "Epoch 3 - Score: 0.22021399370078515\n",
            "Epoch 3 - Score: 0.22021399370078515\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EVAL: [76/77] Elapsed 0m 8s (remain 0m 0s) Loss: 0.2202 \n",
            "Log loss for 2 is 0.22021399370078515\n",
            "Epoch: [4][0/306] Elapsed 0m 0s (remain 2m 56s) Loss: 0.0953 \n",
            "Epoch: [4][100/306] Elapsed 0m 33s (remain 1m 7s) Loss: 0.0463 \n",
            "Epoch: [4][200/306] Elapsed 1m 5s (remain 0m 34s) Loss: 0.0513 \n",
            "Epoch: [4][300/306] Elapsed 1m 38s (remain 0m 1s) Loss: 0.0558 \n",
            "Epoch: [4][305/306] Elapsed 1m 39s (remain 0m 0s) Loss: 0.0554 \n",
            "EVAL: [0/77] Elapsed 0m 0s (remain 0m 27s) Loss: 0.3349 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4 - avg_train_loss: 0.0554  avg_val_loss: 0.2881  time: 108s\n",
            "Epoch 4 - avg_train_loss: 0.0554  avg_val_loss: 0.2881  time: 108s\n",
            "Epoch 4 - avg_train_loss: 0.0554  avg_val_loss: 0.2881  time: 108s\n",
            "Epoch 4 - Score: 0.2881291811714995\n",
            "Epoch 4 - Score: 0.2881291811714995\n",
            "Epoch 4 - Score: 0.2881291811714995\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EVAL: [76/77] Elapsed 0m 8s (remain 0m 0s) Loss: 0.2881 \n",
            "Log loss for 3 is 0.2881291811714995\n",
            "Epoch: [5][0/306] Elapsed 0m 0s (remain 3m 2s) Loss: 0.0392 \n",
            "Epoch: [5][100/306] Elapsed 0m 33s (remain 1m 7s) Loss: 0.0221 \n",
            "Epoch: [5][200/306] Elapsed 1m 5s (remain 0m 34s) Loss: 0.0296 \n",
            "Epoch: [5][300/306] Elapsed 1m 38s (remain 0m 1s) Loss: 0.0360 \n",
            "Epoch: [5][305/306] Elapsed 1m 39s (remain 0m 0s) Loss: 0.0358 \n",
            "EVAL: [0/77] Elapsed 0m 0s (remain 0m 25s) Loss: 0.3634 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5 - avg_train_loss: 0.0358  avg_val_loss: 0.3360  time: 108s\n",
            "Epoch 5 - avg_train_loss: 0.0358  avg_val_loss: 0.3360  time: 108s\n",
            "Epoch 5 - avg_train_loss: 0.0358  avg_val_loss: 0.3360  time: 108s\n",
            "Epoch 5 - Score: 0.33604012471237565\n",
            "Epoch 5 - Score: 0.33604012471237565\n",
            "Epoch 5 - Score: 0.33604012471237565\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EVAL: [76/77] Elapsed 0m 8s (remain 0m 0s) Loss: 0.3360 \n",
            "Log loss for 4 is 0.33604012471237565\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "========== fold: 1 result ==========\n",
            "========== fold: 1 result ==========\n",
            "========== fold: 1 result ==========\n",
            "Score: 0.18482\n",
            "Score: 0.18482\n",
            "Score: 0.18482\n",
            "========== fold: 2 training ==========\n",
            "========== fold: 2 training ==========\n",
            "========== fold: 2 training ==========\n",
            "Didn't find file /content/activebus_bert_final/added_tokens.json. We won't load it.\n",
            "loading file /content/activebus_bert_final/vocab.txt\n",
            "loading file None\n",
            "loading file /content/activebus_bert_final/special_tokens_map.json\n",
            "loading file /content/activebus_bert_final/tokenizer_config.json\n",
            "loading file /content/activebus_bert_final/tokenizer.json\n",
            "Didn't find file /content/activebus_bert_final/added_tokens.json. We won't load it.\n",
            "loading file /content/activebus_bert_final/vocab.txt\n",
            "loading file None\n",
            "loading file /content/activebus_bert_final/special_tokens_map.json\n",
            "loading file /content/activebus_bert_final/tokenizer_config.json\n",
            "loading file /content/activebus_bert_final/tokenizer.json\n",
            "loading configuration file /content/activebus_bert_final/config.json\n",
            "Model config BertConfig {\n",
            "  \"_name_or_path\": \"activebus/BERT-XD_Review\",\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_ids\": 0,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.15.0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "loading weights file /content/activebus_bert_final/pytorch_model.bin\n",
            "Some weights of the model checkpoint at /content/activebus_bert_final were not used when initializing BertForSequenceClassification: ['cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /content/activebus_bert_final and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: [1][0/306] Elapsed 0m 0s (remain 3m 2s) Loss: 0.6969 \n",
            "Epoch: [1][100/306] Elapsed 0m 33s (remain 1m 7s) Loss: 0.4295 \n",
            "Epoch: [1][200/306] Elapsed 1m 5s (remain 0m 34s) Loss: 0.3666 \n",
            "Epoch: [1][300/306] Elapsed 1m 38s (remain 0m 1s) Loss: 0.3266 \n",
            "Epoch: [1][305/306] Elapsed 1m 39s (remain 0m 0s) Loss: 0.3247 \n",
            "EVAL: [0/77] Elapsed 0m 0s (remain 0m 27s) Loss: 0.3328 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1 - avg_train_loss: 0.3247  avg_val_loss: 0.2382  time: 108s\n",
            "Epoch 1 - avg_train_loss: 0.3247  avg_val_loss: 0.2382  time: 108s\n",
            "Epoch 1 - avg_train_loss: 0.3247  avg_val_loss: 0.2382  time: 108s\n",
            "Epoch 1 - Score: 0.23824229214064163\n",
            "Epoch 1 - Score: 0.23824229214064163\n",
            "Epoch 1 - Score: 0.23824229214064163\n",
            "Epoch 1 - Save Best Score: 0.2382 Model\n",
            "Epoch 1 - Save Best Score: 0.2382 Model\n",
            "Epoch 1 - Save Best Score: 0.2382 Model\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EVAL: [76/77] Elapsed 0m 8s (remain 0m 0s) Loss: 0.2382 \n",
            "Log loss for 0 is 0.23824229214064163\n",
            "Epoch: [2][0/306] Elapsed 0m 0s (remain 3m 0s) Loss: 0.3081 \n",
            "Epoch: [2][100/306] Elapsed 0m 33s (remain 1m 7s) Loss: 0.1588 \n",
            "Epoch: [2][200/306] Elapsed 1m 5s (remain 0m 34s) Loss: 0.1684 \n",
            "Epoch: [2][300/306] Elapsed 1m 38s (remain 0m 1s) Loss: 0.1767 \n",
            "Epoch: [2][305/306] Elapsed 1m 39s (remain 0m 0s) Loss: 0.1771 \n",
            "EVAL: [0/77] Elapsed 0m 0s (remain 0m 26s) Loss: 0.3858 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2 - avg_train_loss: 0.1771  avg_val_loss: 0.2252  time: 108s\n",
            "Epoch 2 - avg_train_loss: 0.1771  avg_val_loss: 0.2252  time: 108s\n",
            "Epoch 2 - avg_train_loss: 0.1771  avg_val_loss: 0.2252  time: 108s\n",
            "Epoch 2 - Score: 0.22520596189739392\n",
            "Epoch 2 - Score: 0.22520596189739392\n",
            "Epoch 2 - Score: 0.22520596189739392\n",
            "Epoch 2 - Save Best Score: 0.2252 Model\n",
            "Epoch 2 - Save Best Score: 0.2252 Model\n",
            "Epoch 2 - Save Best Score: 0.2252 Model\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EVAL: [76/77] Elapsed 0m 8s (remain 0m 0s) Loss: 0.2252 \n",
            "Log loss for 1 is 0.22520596189739392\n",
            "Epoch: [3][0/306] Elapsed 0m 0s (remain 2m 55s) Loss: 0.0960 \n",
            "Epoch: [3][100/306] Elapsed 0m 33s (remain 1m 7s) Loss: 0.0934 \n",
            "Epoch: [3][200/306] Elapsed 1m 5s (remain 0m 34s) Loss: 0.0919 \n",
            "Epoch: [3][300/306] Elapsed 1m 38s (remain 0m 1s) Loss: 0.0956 \n",
            "Epoch: [3][305/306] Elapsed 1m 39s (remain 0m 0s) Loss: 0.0946 \n",
            "EVAL: [0/77] Elapsed 0m 0s (remain 0m 26s) Loss: 0.4612 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3 - avg_train_loss: 0.0946  avg_val_loss: 0.2645  time: 108s\n",
            "Epoch 3 - avg_train_loss: 0.0946  avg_val_loss: 0.2645  time: 108s\n",
            "Epoch 3 - avg_train_loss: 0.0946  avg_val_loss: 0.2645  time: 108s\n",
            "Epoch 3 - Score: 0.2644638260105396\n",
            "Epoch 3 - Score: 0.2644638260105396\n",
            "Epoch 3 - Score: 0.2644638260105396\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EVAL: [76/77] Elapsed 0m 8s (remain 0m 0s) Loss: 0.2645 \n",
            "Log loss for 2 is 0.2644638260105396\n",
            "Epoch: [4][0/306] Elapsed 0m 0s (remain 3m 10s) Loss: 0.0384 \n",
            "Epoch: [4][100/306] Elapsed 0m 33s (remain 1m 7s) Loss: 0.0439 \n",
            "Epoch: [4][200/306] Elapsed 1m 5s (remain 0m 34s) Loss: 0.0492 \n",
            "Epoch: [4][300/306] Elapsed 1m 37s (remain 0m 1s) Loss: 0.0452 \n",
            "Epoch: [4][305/306] Elapsed 1m 39s (remain 0m 0s) Loss: 0.0454 \n",
            "EVAL: [0/77] Elapsed 0m 0s (remain 0m 27s) Loss: 0.5146 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4 - avg_train_loss: 0.0454  avg_val_loss: 0.3534  time: 108s\n",
            "Epoch 4 - avg_train_loss: 0.0454  avg_val_loss: 0.3534  time: 108s\n",
            "Epoch 4 - avg_train_loss: 0.0454  avg_val_loss: 0.3534  time: 108s\n",
            "Epoch 4 - Score: 0.35340690497862487\n",
            "Epoch 4 - Score: 0.35340690497862487\n",
            "Epoch 4 - Score: 0.35340690497862487\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EVAL: [76/77] Elapsed 0m 8s (remain 0m 0s) Loss: 0.3534 \n",
            "Log loss for 3 is 0.35340690497862487\n",
            "Epoch: [5][0/306] Elapsed 0m 0s (remain 2m 58s) Loss: 0.0023 \n",
            "Epoch: [5][100/306] Elapsed 0m 33s (remain 1m 7s) Loss: 0.0213 \n",
            "Epoch: [5][200/306] Elapsed 1m 5s (remain 0m 34s) Loss: 0.0249 \n",
            "Epoch: [5][300/306] Elapsed 1m 38s (remain 0m 1s) Loss: 0.0342 \n",
            "Epoch: [5][305/306] Elapsed 1m 39s (remain 0m 0s) Loss: 0.0339 \n",
            "EVAL: [0/77] Elapsed 0m 0s (remain 0m 27s) Loss: 0.3268 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5 - avg_train_loss: 0.0339  avg_val_loss: 0.3284  time: 108s\n",
            "Epoch 5 - avg_train_loss: 0.0339  avg_val_loss: 0.3284  time: 108s\n",
            "Epoch 5 - avg_train_loss: 0.0339  avg_val_loss: 0.3284  time: 108s\n",
            "Epoch 5 - Score: 0.3284181346081709\n",
            "Epoch 5 - Score: 0.3284181346081709\n",
            "Epoch 5 - Score: 0.3284181346081709\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EVAL: [76/77] Elapsed 0m 8s (remain 0m 0s) Loss: 0.3284 \n",
            "Log loss for 4 is 0.3284181346081709\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "========== fold: 2 result ==========\n",
            "========== fold: 2 result ==========\n",
            "========== fold: 2 result ==========\n",
            "Score: 0.22521\n",
            "Score: 0.22521\n",
            "Score: 0.22521\n",
            "========== fold: 3 training ==========\n",
            "========== fold: 3 training ==========\n",
            "========== fold: 3 training ==========\n",
            "Didn't find file /content/activebus_bert_final/added_tokens.json. We won't load it.\n",
            "loading file /content/activebus_bert_final/vocab.txt\n",
            "loading file None\n",
            "loading file /content/activebus_bert_final/special_tokens_map.json\n",
            "loading file /content/activebus_bert_final/tokenizer_config.json\n",
            "loading file /content/activebus_bert_final/tokenizer.json\n",
            "Didn't find file /content/activebus_bert_final/added_tokens.json. We won't load it.\n",
            "loading file /content/activebus_bert_final/vocab.txt\n",
            "loading file None\n",
            "loading file /content/activebus_bert_final/special_tokens_map.json\n",
            "loading file /content/activebus_bert_final/tokenizer_config.json\n",
            "loading file /content/activebus_bert_final/tokenizer.json\n",
            "loading configuration file /content/activebus_bert_final/config.json\n",
            "Model config BertConfig {\n",
            "  \"_name_or_path\": \"activebus/BERT-XD_Review\",\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_ids\": 0,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.15.0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "loading weights file /content/activebus_bert_final/pytorch_model.bin\n",
            "Some weights of the model checkpoint at /content/activebus_bert_final were not used when initializing BertForSequenceClassification: ['cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /content/activebus_bert_final and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: [1][0/306] Elapsed 0m 0s (remain 3m 11s) Loss: 0.7336 \n",
            "Epoch: [1][100/306] Elapsed 0m 33s (remain 1m 7s) Loss: 0.4594 \n",
            "Epoch: [1][200/306] Elapsed 1m 5s (remain 0m 34s) Loss: 0.3670 \n",
            "Epoch: [1][300/306] Elapsed 1m 38s (remain 0m 1s) Loss: 0.3344 \n",
            "Epoch: [1][305/306] Elapsed 1m 39s (remain 0m 0s) Loss: 0.3322 \n",
            "EVAL: [0/77] Elapsed 0m 0s (remain 0m 26s) Loss: 0.1777 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1 - avg_train_loss: 0.3322  avg_val_loss: 0.2602  time: 108s\n",
            "Epoch 1 - avg_train_loss: 0.3322  avg_val_loss: 0.2602  time: 108s\n",
            "Epoch 1 - avg_train_loss: 0.3322  avg_val_loss: 0.2602  time: 108s\n",
            "Epoch 1 - Score: 0.2602095755823594\n",
            "Epoch 1 - Score: 0.2602095755823594\n",
            "Epoch 1 - Score: 0.2602095755823594\n",
            "Epoch 1 - Save Best Score: 0.2602 Model\n",
            "Epoch 1 - Save Best Score: 0.2602 Model\n",
            "Epoch 1 - Save Best Score: 0.2602 Model\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EVAL: [76/77] Elapsed 0m 8s (remain 0m 0s) Loss: 0.2602 \n",
            "Log loss for 0 is 0.2602095755823594\n",
            "Epoch: [2][0/306] Elapsed 0m 0s (remain 2m 57s) Loss: 0.1704 \n",
            "Epoch: [2][100/306] Elapsed 0m 33s (remain 1m 7s) Loss: 0.1820 \n",
            "Epoch: [2][200/306] Elapsed 1m 5s (remain 0m 34s) Loss: 0.1825 \n",
            "Epoch: [2][300/306] Elapsed 1m 38s (remain 0m 1s) Loss: 0.1711 \n",
            "Epoch: [2][305/306] Elapsed 1m 39s (remain 0m 0s) Loss: 0.1708 \n",
            "EVAL: [0/77] Elapsed 0m 0s (remain 0m 26s) Loss: 0.0300 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2 - avg_train_loss: 0.1708  avg_val_loss: 0.2972  time: 108s\n",
            "Epoch 2 - avg_train_loss: 0.1708  avg_val_loss: 0.2972  time: 108s\n",
            "Epoch 2 - avg_train_loss: 0.1708  avg_val_loss: 0.2972  time: 108s\n",
            "Epoch 2 - Score: 0.2972262719169429\n",
            "Epoch 2 - Score: 0.2972262719169429\n",
            "Epoch 2 - Score: 0.2972262719169429\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EVAL: [76/77] Elapsed 0m 8s (remain 0m 0s) Loss: 0.2972 \n",
            "Log loss for 1 is 0.2972262719169429\n",
            "Epoch: [3][0/306] Elapsed 0m 0s (remain 3m 2s) Loss: 0.2544 \n",
            "Epoch: [3][100/306] Elapsed 0m 33s (remain 1m 7s) Loss: 0.0828 \n",
            "Epoch: [3][200/306] Elapsed 1m 5s (remain 0m 34s) Loss: 0.0899 \n",
            "Epoch: [3][300/306] Elapsed 1m 38s (remain 0m 1s) Loss: 0.0909 \n",
            "Epoch: [3][305/306] Elapsed 1m 39s (remain 0m 0s) Loss: 0.0909 \n",
            "EVAL: [0/77] Elapsed 0m 0s (remain 0m 27s) Loss: 0.0949 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3 - avg_train_loss: 0.0909  avg_val_loss: 0.4039  time: 108s\n",
            "Epoch 3 - avg_train_loss: 0.0909  avg_val_loss: 0.4039  time: 108s\n",
            "Epoch 3 - avg_train_loss: 0.0909  avg_val_loss: 0.4039  time: 108s\n",
            "Epoch 3 - Score: 0.40390313729171706\n",
            "Epoch 3 - Score: 0.40390313729171706\n",
            "Epoch 3 - Score: 0.40390313729171706\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EVAL: [76/77] Elapsed 0m 8s (remain 0m 0s) Loss: 0.4039 \n",
            "Log loss for 2 is 0.40390313729171706\n",
            "Epoch: [4][0/306] Elapsed 0m 0s (remain 3m 1s) Loss: 0.0558 \n",
            "Epoch: [4][100/306] Elapsed 0m 33s (remain 1m 7s) Loss: 0.0606 \n",
            "Epoch: [4][200/306] Elapsed 1m 5s (remain 0m 34s) Loss: 0.0561 \n",
            "Epoch: [4][300/306] Elapsed 1m 38s (remain 0m 1s) Loss: 0.0527 \n",
            "Epoch: [4][305/306] Elapsed 1m 39s (remain 0m 0s) Loss: 0.0523 \n",
            "EVAL: [0/77] Elapsed 0m 0s (remain 0m 28s) Loss: 0.0034 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4 - avg_train_loss: 0.0523  avg_val_loss: 0.4051  time: 108s\n",
            "Epoch 4 - avg_train_loss: 0.0523  avg_val_loss: 0.4051  time: 108s\n",
            "Epoch 4 - avg_train_loss: 0.0523  avg_val_loss: 0.4051  time: 108s\n",
            "Epoch 4 - Score: 0.40506195264129147\n",
            "Epoch 4 - Score: 0.40506195264129147\n",
            "Epoch 4 - Score: 0.40506195264129147\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EVAL: [76/77] Elapsed 0m 8s (remain 0m 0s) Loss: 0.4051 \n",
            "Log loss for 3 is 0.40506195264129147\n",
            "Epoch: [5][0/306] Elapsed 0m 0s (remain 2m 57s) Loss: 0.0112 \n",
            "Epoch: [5][100/306] Elapsed 0m 33s (remain 1m 7s) Loss: 0.0316 \n",
            "Epoch: [5][200/306] Elapsed 1m 5s (remain 0m 34s) Loss: 0.0313 \n",
            "Epoch: [5][300/306] Elapsed 1m 38s (remain 0m 1s) Loss: 0.0313 \n",
            "Epoch: [5][305/306] Elapsed 1m 39s (remain 0m 0s) Loss: 0.0309 \n",
            "EVAL: [0/77] Elapsed 0m 0s (remain 0m 28s) Loss: 0.2989 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5 - avg_train_loss: 0.0309  avg_val_loss: 0.4579  time: 108s\n",
            "Epoch 5 - avg_train_loss: 0.0309  avg_val_loss: 0.4579  time: 108s\n",
            "Epoch 5 - avg_train_loss: 0.0309  avg_val_loss: 0.4579  time: 108s\n",
            "Epoch 5 - Score: 0.4578563102335326\n",
            "Epoch 5 - Score: 0.4578563102335326\n",
            "Epoch 5 - Score: 0.4578563102335326\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EVAL: [76/77] Elapsed 0m 8s (remain 0m 0s) Loss: 0.4579 \n",
            "Log loss for 4 is 0.4578563102335326\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "========== fold: 3 result ==========\n",
            "========== fold: 3 result ==========\n",
            "========== fold: 3 result ==========\n",
            "Score: 0.26021\n",
            "Score: 0.26021\n",
            "Score: 0.26021\n",
            "========== fold: 4 training ==========\n",
            "========== fold: 4 training ==========\n",
            "========== fold: 4 training ==========\n",
            "Didn't find file /content/activebus_bert_final/added_tokens.json. We won't load it.\n",
            "loading file /content/activebus_bert_final/vocab.txt\n",
            "loading file None\n",
            "loading file /content/activebus_bert_final/special_tokens_map.json\n",
            "loading file /content/activebus_bert_final/tokenizer_config.json\n",
            "loading file /content/activebus_bert_final/tokenizer.json\n",
            "Didn't find file /content/activebus_bert_final/added_tokens.json. We won't load it.\n",
            "loading file /content/activebus_bert_final/vocab.txt\n",
            "loading file None\n",
            "loading file /content/activebus_bert_final/special_tokens_map.json\n",
            "loading file /content/activebus_bert_final/tokenizer_config.json\n",
            "loading file /content/activebus_bert_final/tokenizer.json\n",
            "loading configuration file /content/activebus_bert_final/config.json\n",
            "Model config BertConfig {\n",
            "  \"_name_or_path\": \"activebus/BERT-XD_Review\",\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_ids\": 0,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.15.0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "loading weights file /content/activebus_bert_final/pytorch_model.bin\n",
            "Some weights of the model checkpoint at /content/activebus_bert_final were not used when initializing BertForSequenceClassification: ['cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /content/activebus_bert_final and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: [1][0/306] Elapsed 0m 0s (remain 3m 8s) Loss: 0.6772 \n",
            "Epoch: [1][100/306] Elapsed 0m 33s (remain 1m 7s) Loss: 0.4412 \n",
            "Epoch: [1][200/306] Elapsed 1m 5s (remain 0m 34s) Loss: 0.3602 \n",
            "Epoch: [1][300/306] Elapsed 1m 38s (remain 0m 1s) Loss: 0.3249 \n",
            "Epoch: [1][305/306] Elapsed 1m 39s (remain 0m 0s) Loss: 0.3240 \n",
            "EVAL: [0/77] Elapsed 0m 0s (remain 0m 26s) Loss: 0.0272 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1 - avg_train_loss: 0.3240  avg_val_loss: 0.2503  time: 108s\n",
            "Epoch 1 - avg_train_loss: 0.3240  avg_val_loss: 0.2503  time: 108s\n",
            "Epoch 1 - avg_train_loss: 0.3240  avg_val_loss: 0.2503  time: 108s\n",
            "Epoch 1 - Score: 0.25034040882090486\n",
            "Epoch 1 - Score: 0.25034040882090486\n",
            "Epoch 1 - Score: 0.25034040882090486\n",
            "Epoch 1 - Save Best Score: 0.2503 Model\n",
            "Epoch 1 - Save Best Score: 0.2503 Model\n",
            "Epoch 1 - Save Best Score: 0.2503 Model\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EVAL: [76/77] Elapsed 0m 8s (remain 0m 0s) Loss: 0.2503 \n",
            "Log loss for 0 is 0.25034040882090486\n",
            "Epoch: [2][0/306] Elapsed 0m 0s (remain 3m 0s) Loss: 0.1905 \n",
            "Epoch: [2][100/306] Elapsed 0m 33s (remain 1m 7s) Loss: 0.1584 \n",
            "Epoch: [2][200/306] Elapsed 1m 5s (remain 0m 34s) Loss: 0.1632 \n",
            "Epoch: [2][300/306] Elapsed 1m 37s (remain 0m 1s) Loss: 0.1675 \n",
            "Epoch: [2][305/306] Elapsed 1m 39s (remain 0m 0s) Loss: 0.1677 \n",
            "EVAL: [0/77] Elapsed 0m 0s (remain 0m 26s) Loss: 0.0206 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2 - avg_train_loss: 0.1677  avg_val_loss: 0.2388  time: 108s\n",
            "Epoch 2 - avg_train_loss: 0.1677  avg_val_loss: 0.2388  time: 108s\n",
            "Epoch 2 - avg_train_loss: 0.1677  avg_val_loss: 0.2388  time: 108s\n",
            "Epoch 2 - Score: 0.23878150382371408\n",
            "Epoch 2 - Score: 0.23878150382371408\n",
            "Epoch 2 - Score: 0.23878150382371408\n",
            "Epoch 2 - Save Best Score: 0.2388 Model\n",
            "Epoch 2 - Save Best Score: 0.2388 Model\n",
            "Epoch 2 - Save Best Score: 0.2388 Model\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EVAL: [76/77] Elapsed 0m 8s (remain 0m 0s) Loss: 0.2388 \n",
            "Log loss for 1 is 0.23878150382371408\n",
            "Epoch: [3][0/306] Elapsed 0m 0s (remain 3m 0s) Loss: 0.0634 \n",
            "Epoch: [3][100/306] Elapsed 0m 33s (remain 1m 7s) Loss: 0.0819 \n",
            "Epoch: [3][200/306] Elapsed 1m 5s (remain 0m 34s) Loss: 0.0802 \n",
            "Epoch: [3][300/306] Elapsed 1m 38s (remain 0m 1s) Loss: 0.0874 \n",
            "Epoch: [3][305/306] Elapsed 1m 39s (remain 0m 0s) Loss: 0.0874 \n",
            "EVAL: [0/77] Elapsed 0m 0s (remain 0m 27s) Loss: 0.0083 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3 - avg_train_loss: 0.0874  avg_val_loss: 0.2858  time: 108s\n",
            "Epoch 3 - avg_train_loss: 0.0874  avg_val_loss: 0.2858  time: 108s\n",
            "Epoch 3 - avg_train_loss: 0.0874  avg_val_loss: 0.2858  time: 108s\n",
            "Epoch 3 - Score: 0.2857786888877551\n",
            "Epoch 3 - Score: 0.2857786888877551\n",
            "Epoch 3 - Score: 0.2857786888877551\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EVAL: [76/77] Elapsed 0m 8s (remain 0m 0s) Loss: 0.2858 \n",
            "Log loss for 2 is 0.2857786888877551\n",
            "Epoch: [4][0/306] Elapsed 0m 0s (remain 3m 6s) Loss: 0.0128 \n",
            "Epoch: [4][100/306] Elapsed 0m 33s (remain 1m 7s) Loss: 0.0443 \n",
            "Epoch: [4][200/306] Elapsed 1m 5s (remain 0m 34s) Loss: 0.0551 \n",
            "Epoch: [4][300/306] Elapsed 1m 38s (remain 0m 1s) Loss: 0.0556 \n",
            "Epoch: [4][305/306] Elapsed 1m 39s (remain 0m 0s) Loss: 0.0549 \n",
            "EVAL: [0/77] Elapsed 0m 0s (remain 0m 25s) Loss: 0.0036 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4 - avg_train_loss: 0.0549  avg_val_loss: 0.3545  time: 108s\n",
            "Epoch 4 - avg_train_loss: 0.0549  avg_val_loss: 0.3545  time: 108s\n",
            "Epoch 4 - avg_train_loss: 0.0549  avg_val_loss: 0.3545  time: 108s\n",
            "Epoch 4 - Score: 0.354479095413742\n",
            "Epoch 4 - Score: 0.354479095413742\n",
            "Epoch 4 - Score: 0.354479095413742\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EVAL: [76/77] Elapsed 0m 8s (remain 0m 0s) Loss: 0.3545 \n",
            "Log loss for 3 is 0.354479095413742\n",
            "Epoch: [5][0/306] Elapsed 0m 0s (remain 2m 59s) Loss: 0.0168 \n",
            "Epoch: [5][100/306] Elapsed 0m 33s (remain 1m 7s) Loss: 0.0302 \n",
            "Epoch: [5][200/306] Elapsed 1m 5s (remain 0m 34s) Loss: 0.0316 \n",
            "Epoch: [5][300/306] Elapsed 1m 38s (remain 0m 1s) Loss: 0.0328 \n",
            "Epoch: [5][305/306] Elapsed 1m 39s (remain 0m 0s) Loss: 0.0326 \n",
            "EVAL: [0/77] Elapsed 0m 0s (remain 0m 29s) Loss: 0.0059 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5 - avg_train_loss: 0.0326  avg_val_loss: 0.4237  time: 108s\n",
            "Epoch 5 - avg_train_loss: 0.0326  avg_val_loss: 0.4237  time: 108s\n",
            "Epoch 5 - avg_train_loss: 0.0326  avg_val_loss: 0.4237  time: 108s\n",
            "Epoch 5 - Score: 0.42374403573625385\n",
            "Epoch 5 - Score: 0.42374403573625385\n",
            "Epoch 5 - Score: 0.42374403573625385\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EVAL: [76/77] Elapsed 0m 8s (remain 0m 0s) Loss: 0.4237 \n",
            "Log loss for 4 is 0.42374403573625385\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "========== fold: 4 result ==========\n",
            "========== fold: 4 result ==========\n",
            "========== fold: 4 result ==========\n",
            "Score: 0.23878\n",
            "Score: 0.23878\n",
            "Score: 0.23878\n",
            "========== CV ==========\n",
            "========== CV ==========\n",
            "========== CV ==========\n",
            "Score: 0.22719\n",
            "Score: 0.22719\n",
            "Score: 0.22719\n",
            "Didn't find file /content/activebus_bert_final/added_tokens.json. We won't load it.\n",
            "loading file /content/activebus_bert_final/vocab.txt\n",
            "loading file None\n",
            "loading file /content/activebus_bert_final/special_tokens_map.json\n",
            "loading file /content/activebus_bert_final/tokenizer_config.json\n",
            "loading file /content/activebus_bert_final/tokenizer.json\n",
            "========== model: bert-base-uncased fold: 0 inference ==========\n",
            "========== model: bert-base-uncased fold: 0 inference ==========\n",
            "========== model: bert-base-uncased fold: 0 inference ==========\n",
            "loading configuration file /content/activebus_bert_final/config.json\n",
            "Model config BertConfig {\n",
            "  \"_name_or_path\": \"activebus/BERT-XD_Review\",\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_ids\": 0,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.15.0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "loading weights file /content/activebus_bert_final/pytorch_model.bin\n",
            "Some weights of the model checkpoint at /content/activebus_bert_final were not used when initializing BertForSequenceClassification: ['cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /content/activebus_bert_final and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "\n",
            "  0%|          | 0/165 [00:00<?, ?it/s]\u001b[A\n",
            "  1%|          | 1/165 [00:00<00:20,  8.02it/s]\u001b[A\n",
            "  1%|          | 2/165 [00:00<00:19,  8.21it/s]\u001b[A\n",
            "  2%|▏         | 3/165 [00:00<00:18,  8.56it/s]\u001b[A\n",
            "  2%|▏         | 4/165 [00:00<00:18,  8.74it/s]\u001b[A\n",
            "  3%|▎         | 5/165 [00:00<00:18,  8.86it/s]\u001b[A\n",
            "  4%|▎         | 6/165 [00:00<00:17,  8.94it/s]\u001b[A\n",
            "  4%|▍         | 7/165 [00:00<00:17,  8.98it/s]\u001b[A\n",
            "  5%|▍         | 8/165 [00:00<00:17,  9.01it/s]\u001b[A\n",
            "  5%|▌         | 9/165 [00:01<00:17,  9.02it/s]\u001b[A\n",
            "  6%|▌         | 10/165 [00:01<00:17,  9.05it/s]\u001b[A\n",
            "  7%|▋         | 11/165 [00:01<00:16,  9.07it/s]\u001b[A\n",
            "  7%|▋         | 12/165 [00:01<00:16,  9.06it/s]\u001b[A\n",
            "  8%|▊         | 13/165 [00:01<00:16,  9.05it/s]\u001b[A\n",
            "  8%|▊         | 14/165 [00:01<00:16,  9.05it/s]\u001b[A\n",
            "  9%|▉         | 15/165 [00:01<00:16,  9.07it/s]\u001b[A\n",
            " 10%|▉         | 16/165 [00:01<00:16,  9.07it/s]\u001b[A\n",
            " 10%|█         | 17/165 [00:01<00:16,  9.07it/s]\u001b[A\n",
            " 11%|█         | 18/165 [00:02<00:16,  9.09it/s]\u001b[A\n",
            " 12%|█▏        | 19/165 [00:02<00:16,  9.09it/s]\u001b[A\n",
            " 12%|█▏        | 20/165 [00:02<00:15,  9.11it/s]\u001b[A\n",
            " 13%|█▎        | 21/165 [00:02<00:15,  9.10it/s]\u001b[A\n",
            " 13%|█▎        | 22/165 [00:02<00:15,  9.11it/s]\u001b[A\n",
            " 14%|█▍        | 23/165 [00:02<00:15,  9.11it/s]\u001b[A\n",
            " 15%|█▍        | 24/165 [00:02<00:15,  9.12it/s]\u001b[A\n",
            " 15%|█▌        | 25/165 [00:02<00:15,  9.12it/s]\u001b[A\n",
            " 16%|█▌        | 26/165 [00:02<00:15,  9.12it/s]\u001b[A\n",
            " 16%|█▋        | 27/165 [00:02<00:15,  9.12it/s]\u001b[A\n",
            " 17%|█▋        | 28/165 [00:03<00:15,  9.12it/s]\u001b[A\n",
            " 18%|█▊        | 29/165 [00:03<00:14,  9.13it/s]\u001b[A\n",
            " 18%|█▊        | 30/165 [00:03<00:14,  9.14it/s]\u001b[A\n",
            " 19%|█▉        | 31/165 [00:03<00:14,  9.12it/s]\u001b[A\n",
            " 19%|█▉        | 32/165 [00:03<00:14,  9.12it/s]\u001b[A\n",
            " 20%|██        | 33/165 [00:03<00:14,  9.11it/s]\u001b[A\n",
            " 21%|██        | 34/165 [00:03<00:14,  9.09it/s]\u001b[A\n",
            " 21%|██        | 35/165 [00:03<00:14,  9.08it/s]\u001b[A\n",
            " 22%|██▏       | 36/165 [00:03<00:14,  9.07it/s]\u001b[A\n",
            " 22%|██▏       | 37/165 [00:04<00:14,  9.07it/s]\u001b[A\n",
            " 23%|██▎       | 38/165 [00:04<00:14,  9.06it/s]\u001b[A\n",
            " 24%|██▎       | 39/165 [00:04<00:13,  9.05it/s]\u001b[A\n",
            " 24%|██▍       | 40/165 [00:04<00:13,  9.06it/s]\u001b[A\n",
            " 25%|██▍       | 41/165 [00:04<00:13,  9.07it/s]\u001b[A\n",
            " 25%|██▌       | 42/165 [00:04<00:13,  9.08it/s]\u001b[A\n",
            " 26%|██▌       | 43/165 [00:04<00:13,  9.09it/s]\u001b[A\n",
            " 27%|██▋       | 44/165 [00:04<00:13,  9.09it/s]\u001b[A\n",
            " 27%|██▋       | 45/165 [00:04<00:13,  9.09it/s]\u001b[A\n",
            " 28%|██▊       | 46/165 [00:05<00:13,  9.05it/s]\u001b[A\n",
            " 28%|██▊       | 47/165 [00:05<00:13,  9.07it/s]\u001b[A\n",
            " 29%|██▉       | 48/165 [00:05<00:12,  9.07it/s]\u001b[A\n",
            " 30%|██▉       | 49/165 [00:05<00:12,  9.08it/s]\u001b[A\n",
            " 30%|███       | 50/165 [00:05<00:12,  9.08it/s]\u001b[A\n",
            " 31%|███       | 51/165 [00:05<00:12,  9.08it/s]\u001b[A\n",
            " 32%|███▏      | 52/165 [00:05<00:12,  9.09it/s]\u001b[A\n",
            " 32%|███▏      | 53/165 [00:05<00:12,  9.09it/s]\u001b[A\n",
            " 33%|███▎      | 54/165 [00:05<00:12,  9.08it/s]\u001b[A\n",
            " 33%|███▎      | 55/165 [00:06<00:12,  9.10it/s]\u001b[A\n",
            " 34%|███▍      | 56/165 [00:06<00:12,  9.08it/s]\u001b[A\n",
            " 35%|███▍      | 57/165 [00:06<00:11,  9.09it/s]\u001b[A\n",
            " 35%|███▌      | 58/165 [00:06<00:11,  9.07it/s]\u001b[A\n",
            " 36%|███▌      | 59/165 [00:06<00:11,  9.08it/s]\u001b[A\n",
            " 36%|███▋      | 60/165 [00:06<00:11,  9.06it/s]\u001b[A\n",
            " 37%|███▋      | 61/165 [00:06<00:11,  9.06it/s]\u001b[A\n",
            " 38%|███▊      | 62/165 [00:06<00:11,  9.07it/s]\u001b[A\n",
            " 38%|███▊      | 63/165 [00:06<00:11,  9.07it/s]\u001b[A\n",
            " 39%|███▉      | 64/165 [00:07<00:11,  9.08it/s]\u001b[A\n",
            " 39%|███▉      | 65/165 [00:07<00:11,  9.08it/s]\u001b[A\n",
            " 40%|████      | 66/165 [00:07<00:10,  9.09it/s]\u001b[A\n",
            " 41%|████      | 67/165 [00:07<00:10,  9.08it/s]\u001b[A\n",
            " 41%|████      | 68/165 [00:07<00:10,  9.08it/s]\u001b[A\n",
            " 42%|████▏     | 69/165 [00:07<00:10,  9.09it/s]\u001b[A\n",
            " 42%|████▏     | 70/165 [00:07<00:10,  9.10it/s]\u001b[A\n",
            " 43%|████▎     | 71/165 [00:07<00:10,  9.11it/s]\u001b[A\n",
            " 44%|████▎     | 72/165 [00:07<00:10,  9.12it/s]\u001b[A\n",
            " 44%|████▍     | 73/165 [00:08<00:10,  9.11it/s]\u001b[A\n",
            " 45%|████▍     | 74/165 [00:08<00:09,  9.10it/s]\u001b[A\n",
            " 45%|████▌     | 75/165 [00:08<00:09,  9.12it/s]\u001b[A\n",
            " 46%|████▌     | 76/165 [00:08<00:09,  9.11it/s]\u001b[A\n",
            " 47%|████▋     | 77/165 [00:08<00:09,  9.11it/s]\u001b[A\n",
            " 47%|████▋     | 78/165 [00:08<00:09,  9.09it/s]\u001b[A\n",
            " 48%|████▊     | 79/165 [00:08<00:09,  9.09it/s]\u001b[A\n",
            " 48%|████▊     | 80/165 [00:08<00:09,  9.11it/s]\u001b[A\n",
            " 49%|████▉     | 81/165 [00:08<00:09,  9.12it/s]\u001b[A\n",
            " 50%|████▉     | 82/165 [00:09<00:09,  9.11it/s]\u001b[A\n",
            " 50%|█████     | 83/165 [00:09<00:09,  9.10it/s]\u001b[A\n",
            " 51%|█████     | 84/165 [00:09<00:08,  9.08it/s]\u001b[A\n",
            " 52%|█████▏    | 85/165 [00:09<00:08,  9.07it/s]\u001b[A\n",
            " 52%|█████▏    | 86/165 [00:09<00:08,  9.05it/s]\u001b[A\n",
            " 53%|█████▎    | 87/165 [00:09<00:08,  9.05it/s]\u001b[A\n",
            " 53%|█████▎    | 88/165 [00:09<00:08,  9.07it/s]\u001b[A\n",
            " 54%|█████▍    | 89/165 [00:09<00:08,  9.07it/s]\u001b[A\n",
            " 55%|█████▍    | 90/165 [00:09<00:08,  9.10it/s]\u001b[A\n",
            " 55%|█████▌    | 91/165 [00:10<00:08,  9.10it/s]\u001b[A\n",
            " 56%|█████▌    | 92/165 [00:10<00:08,  9.10it/s]\u001b[A\n",
            " 56%|█████▋    | 93/165 [00:10<00:07,  9.10it/s]\u001b[A\n",
            " 57%|█████▋    | 94/165 [00:10<00:07,  9.12it/s]\u001b[A\n",
            " 58%|█████▊    | 95/165 [00:10<00:07,  9.10it/s]\u001b[A\n",
            " 58%|█████▊    | 96/165 [00:10<00:07,  9.13it/s]\u001b[A\n",
            " 59%|█████▉    | 97/165 [00:10<00:07,  9.13it/s]\u001b[A\n",
            " 59%|█████▉    | 98/165 [00:10<00:07,  9.12it/s]\u001b[A\n",
            " 60%|██████    | 99/165 [00:10<00:07,  9.12it/s]\u001b[A\n",
            " 61%|██████    | 100/165 [00:11<00:07,  9.12it/s]\u001b[A\n",
            " 61%|██████    | 101/165 [00:11<00:07,  9.13it/s]\u001b[A\n",
            " 62%|██████▏   | 102/165 [00:11<00:06,  9.13it/s]\u001b[A\n",
            " 62%|██████▏   | 103/165 [00:11<00:06,  9.13it/s]\u001b[A\n",
            " 63%|██████▎   | 104/165 [00:11<00:06,  9.14it/s]\u001b[A\n",
            " 64%|██████▎   | 105/165 [00:11<00:06,  9.12it/s]\u001b[A\n",
            " 64%|██████▍   | 106/165 [00:11<00:06,  9.13it/s]\u001b[A\n",
            " 65%|██████▍   | 107/165 [00:11<00:06,  9.12it/s]\u001b[A\n",
            " 65%|██████▌   | 108/165 [00:11<00:06,  9.13it/s]\u001b[A\n",
            " 66%|██████▌   | 109/165 [00:12<00:06,  9.11it/s]\u001b[A\n",
            " 67%|██████▋   | 110/165 [00:12<00:06,  9.10it/s]\u001b[A\n",
            " 67%|██████▋   | 111/165 [00:12<00:05,  9.09it/s]\u001b[A\n",
            " 68%|██████▊   | 112/165 [00:12<00:05,  9.10it/s]\u001b[A\n",
            " 68%|██████▊   | 113/165 [00:12<00:05,  9.10it/s]\u001b[A\n",
            " 69%|██████▉   | 114/165 [00:12<00:05,  9.09it/s]\u001b[A\n",
            " 70%|██████▉   | 115/165 [00:12<00:05,  9.02it/s]\u001b[A\n",
            " 70%|███████   | 116/165 [00:12<00:05,  9.05it/s]\u001b[A\n",
            " 71%|███████   | 117/165 [00:12<00:05,  9.07it/s]\u001b[A\n",
            " 72%|███████▏  | 118/165 [00:13<00:05,  9.07it/s]\u001b[A\n",
            " 72%|███████▏  | 119/165 [00:13<00:05,  9.08it/s]\u001b[A\n",
            " 73%|███████▎  | 120/165 [00:13<00:04,  9.10it/s]\u001b[A\n",
            " 73%|███████▎  | 121/165 [00:13<00:04,  9.10it/s]\u001b[A\n",
            " 74%|███████▍  | 122/165 [00:13<00:04,  9.12it/s]\u001b[A\n",
            " 75%|███████▍  | 123/165 [00:13<00:04,  9.12it/s]\u001b[A\n",
            " 75%|███████▌  | 124/165 [00:13<00:04,  9.13it/s]\u001b[A\n",
            " 76%|███████▌  | 125/165 [00:13<00:04,  9.13it/s]\u001b[A\n",
            " 76%|███████▋  | 126/165 [00:13<00:04,  9.11it/s]\u001b[A\n",
            " 77%|███████▋  | 127/165 [00:13<00:04,  9.08it/s]\u001b[A\n",
            " 78%|███████▊  | 128/165 [00:14<00:04,  9.09it/s]\u001b[A\n",
            " 78%|███████▊  | 129/165 [00:14<00:03,  9.09it/s]\u001b[A\n",
            " 79%|███████▉  | 130/165 [00:14<00:03,  9.07it/s]\u001b[A\n",
            " 79%|███████▉  | 131/165 [00:14<00:03,  9.07it/s]\u001b[A\n",
            " 80%|████████  | 132/165 [00:14<00:03,  9.09it/s]\u001b[A\n",
            " 81%|████████  | 133/165 [00:14<00:03,  9.09it/s]\u001b[A\n",
            " 81%|████████  | 134/165 [00:14<00:03,  9.08it/s]\u001b[A\n",
            " 82%|████████▏ | 135/165 [00:14<00:03,  9.07it/s]\u001b[A\n",
            " 82%|████████▏ | 136/165 [00:14<00:03,  9.07it/s]\u001b[A\n",
            " 83%|████████▎ | 137/165 [00:15<00:03,  9.09it/s]\u001b[A\n",
            " 84%|████████▎ | 138/165 [00:15<00:02,  9.09it/s]\u001b[A\n",
            " 84%|████████▍ | 139/165 [00:15<00:02,  9.09it/s]\u001b[A\n",
            " 85%|████████▍ | 140/165 [00:15<00:02,  9.09it/s]\u001b[A\n",
            " 85%|████████▌ | 141/165 [00:15<00:02,  9.10it/s]\u001b[A\n",
            " 86%|████████▌ | 142/165 [00:15<00:02,  9.07it/s]\u001b[A\n",
            " 87%|████████▋ | 143/165 [00:15<00:02,  9.08it/s]\u001b[A\n",
            " 87%|████████▋ | 144/165 [00:15<00:02,  9.07it/s]\u001b[A\n",
            " 88%|████████▊ | 145/165 [00:15<00:02,  9.05it/s]\u001b[A\n",
            " 88%|████████▊ | 146/165 [00:16<00:02,  9.05it/s]\u001b[A\n",
            " 89%|████████▉ | 147/165 [00:16<00:01,  9.07it/s]\u001b[A\n",
            " 90%|████████▉ | 148/165 [00:16<00:01,  9.08it/s]\u001b[A\n",
            " 90%|█████████ | 149/165 [00:16<00:01,  9.10it/s]\u001b[A\n",
            " 91%|█████████ | 150/165 [00:16<00:01,  9.10it/s]\u001b[A\n",
            " 92%|█████████▏| 151/165 [00:16<00:01,  9.11it/s]\u001b[A\n",
            " 92%|█████████▏| 152/165 [00:16<00:01,  9.12it/s]\u001b[A\n",
            " 93%|█████████▎| 153/165 [00:16<00:01,  9.12it/s]\u001b[A\n",
            " 93%|█████████▎| 154/165 [00:16<00:01,  9.12it/s]\u001b[A\n",
            " 94%|█████████▍| 155/165 [00:17<00:01,  9.11it/s]\u001b[A\n",
            " 95%|█████████▍| 156/165 [00:17<00:00,  9.03it/s]\u001b[A\n",
            " 95%|█████████▌| 157/165 [00:17<00:00,  9.02it/s]\u001b[A\n",
            " 96%|█████████▌| 158/165 [00:17<00:00,  9.07it/s]\u001b[A\n",
            " 96%|█████████▋| 159/165 [00:17<00:00,  9.09it/s]\u001b[A\n",
            " 97%|█████████▋| 160/165 [00:17<00:00,  9.10it/s]\u001b[A\n",
            " 98%|█████████▊| 161/165 [00:17<00:00,  9.10it/s]\u001b[A\n",
            " 98%|█████████▊| 162/165 [00:17<00:00,  9.12it/s]\u001b[A\n",
            " 99%|█████████▉| 163/165 [00:17<00:00,  9.14it/s]\u001b[A\n",
            "100%|██████████| 165/165 [00:18<00:00,  9.07it/s]\n",
            "========== model: bert-base-uncased fold: 1 inference ==========\n",
            "========== model: bert-base-uncased fold: 1 inference ==========\n",
            "========== model: bert-base-uncased fold: 1 inference ==========\n",
            "loading configuration file /content/activebus_bert_final/config.json\n",
            "Model config BertConfig {\n",
            "  \"_name_or_path\": \"activebus/BERT-XD_Review\",\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_ids\": 0,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.15.0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "loading weights file /content/activebus_bert_final/pytorch_model.bin\n",
            "Some weights of the model checkpoint at /content/activebus_bert_final were not used when initializing BertForSequenceClassification: ['cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /content/activebus_bert_final and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "\n",
            "  0%|          | 0/165 [00:00<?, ?it/s]\u001b[A\n",
            "  1%|          | 1/165 [00:00<00:18,  8.77it/s]\u001b[A\n",
            "  1%|          | 2/165 [00:00<00:18,  8.94it/s]\u001b[A\n",
            "  2%|▏         | 3/165 [00:00<00:18,  9.00it/s]\u001b[A\n",
            "  2%|▏         | 4/165 [00:00<00:17,  9.05it/s]\u001b[A\n",
            "  3%|▎         | 5/165 [00:00<00:17,  9.06it/s]\u001b[A\n",
            "  4%|▎         | 6/165 [00:00<00:17,  9.07it/s]\u001b[A\n",
            "  4%|▍         | 7/165 [00:00<00:17,  9.08it/s]\u001b[A\n",
            "  5%|▍         | 8/165 [00:00<00:17,  9.10it/s]\u001b[A\n",
            "  5%|▌         | 9/165 [00:00<00:17,  9.11it/s]\u001b[A\n",
            "  6%|▌         | 10/165 [00:01<00:17,  9.11it/s]\u001b[A\n",
            "  7%|▋         | 11/165 [00:01<00:16,  9.10it/s]\u001b[A\n",
            "  7%|▋         | 12/165 [00:01<00:16,  9.10it/s]\u001b[A\n",
            "  8%|▊         | 13/165 [00:01<00:16,  9.08it/s]\u001b[A\n",
            "  8%|▊         | 14/165 [00:01<00:16,  9.07it/s]\u001b[A\n",
            "  9%|▉         | 15/165 [00:01<00:16,  9.08it/s]\u001b[A\n",
            " 10%|▉         | 16/165 [00:01<00:16,  9.09it/s]\u001b[A\n",
            " 10%|█         | 17/165 [00:01<00:16,  9.08it/s]\u001b[A\n",
            " 11%|█         | 18/165 [00:01<00:16,  9.10it/s]\u001b[A\n",
            " 12%|█▏        | 19/165 [00:02<00:16,  9.09it/s]\u001b[A\n",
            " 12%|█▏        | 20/165 [00:02<00:16,  9.01it/s]\u001b[A\n",
            " 13%|█▎        | 21/165 [00:02<00:15,  9.04it/s]\u001b[A\n",
            " 13%|█▎        | 22/165 [00:02<00:15,  9.04it/s]\u001b[A\n",
            " 14%|█▍        | 23/165 [00:02<00:15,  9.07it/s]\u001b[A\n",
            " 15%|█▍        | 24/165 [00:02<00:15,  9.08it/s]\u001b[A\n",
            " 15%|█▌        | 25/165 [00:02<00:15,  9.09it/s]\u001b[A\n",
            " 16%|█▌        | 26/165 [00:02<00:15,  9.07it/s]\u001b[A\n",
            " 16%|█▋        | 27/165 [00:02<00:15,  9.08it/s]\u001b[A\n",
            " 17%|█▋        | 28/165 [00:03<00:15,  9.08it/s]\u001b[A\n",
            " 18%|█▊        | 29/165 [00:03<00:14,  9.10it/s]\u001b[A\n",
            " 18%|█▊        | 30/165 [00:03<00:14,  9.11it/s]\u001b[A\n",
            " 19%|█▉        | 31/165 [00:03<00:14,  9.14it/s]\u001b[A\n",
            " 19%|█▉        | 32/165 [00:03<00:14,  9.13it/s]\u001b[A\n",
            " 20%|██        | 33/165 [00:03<00:14,  9.14it/s]\u001b[A\n",
            " 21%|██        | 34/165 [00:03<00:14,  9.11it/s]\u001b[A\n",
            " 21%|██        | 35/165 [00:03<00:14,  9.12it/s]\u001b[A\n",
            " 22%|██▏       | 36/165 [00:03<00:14,  9.11it/s]\u001b[A\n",
            " 22%|██▏       | 37/165 [00:04<00:14,  9.10it/s]\u001b[A\n",
            " 23%|██▎       | 38/165 [00:04<00:14,  9.06it/s]\u001b[A\n",
            " 24%|██▎       | 39/165 [00:04<00:13,  9.08it/s]\u001b[A\n",
            " 24%|██▍       | 40/165 [00:04<00:13,  9.10it/s]\u001b[A\n",
            " 25%|██▍       | 41/165 [00:04<00:13,  9.11it/s]\u001b[A\n",
            " 25%|██▌       | 42/165 [00:04<00:13,  9.11it/s]\u001b[A\n",
            " 26%|██▌       | 43/165 [00:04<00:13,  9.10it/s]\u001b[A\n",
            " 27%|██▋       | 44/165 [00:04<00:13,  9.12it/s]\u001b[A\n",
            " 27%|██▋       | 45/165 [00:04<00:13,  9.13it/s]\u001b[A\n",
            " 28%|██▊       | 46/165 [00:05<00:13,  9.13it/s]\u001b[A\n",
            " 28%|██▊       | 47/165 [00:05<00:12,  9.13it/s]\u001b[A\n",
            " 29%|██▉       | 48/165 [00:05<00:12,  9.14it/s]\u001b[A\n",
            " 30%|██▉       | 49/165 [00:05<00:12,  9.15it/s]\u001b[A\n",
            " 30%|███       | 50/165 [00:05<00:12,  9.12it/s]\u001b[A\n",
            " 31%|███       | 51/165 [00:05<00:12,  9.12it/s]\u001b[A\n",
            " 32%|███▏      | 52/165 [00:05<00:12,  9.12it/s]\u001b[A\n",
            " 32%|███▏      | 53/165 [00:05<00:12,  9.12it/s]\u001b[A\n",
            " 33%|███▎      | 54/165 [00:05<00:12,  9.13it/s]\u001b[A\n",
            " 33%|███▎      | 55/165 [00:06<00:12,  9.12it/s]\u001b[A\n",
            " 34%|███▍      | 56/165 [00:06<00:11,  9.14it/s]\u001b[A\n",
            " 35%|███▍      | 57/165 [00:06<00:11,  9.13it/s]\u001b[A\n",
            " 35%|███▌      | 58/165 [00:06<00:11,  9.10it/s]\u001b[A\n",
            " 36%|███▌      | 59/165 [00:06<00:11,  9.11it/s]\u001b[A\n",
            " 36%|███▋      | 60/165 [00:06<00:11,  9.10it/s]\u001b[A\n",
            " 37%|███▋      | 61/165 [00:06<00:11,  9.11it/s]\u001b[A\n",
            " 38%|███▊      | 62/165 [00:06<00:11,  9.09it/s]\u001b[A\n",
            " 38%|███▊      | 63/165 [00:06<00:11,  9.08it/s]\u001b[A\n",
            " 39%|███▉      | 64/165 [00:07<00:11,  9.04it/s]\u001b[A\n",
            " 39%|███▉      | 65/165 [00:07<00:11,  8.99it/s]\u001b[A\n",
            " 40%|████      | 66/165 [00:07<00:10,  9.03it/s]\u001b[A\n",
            " 41%|████      | 67/165 [00:07<00:10,  9.07it/s]\u001b[A\n",
            " 41%|████      | 68/165 [00:07<00:10,  9.06it/s]\u001b[A\n",
            " 42%|████▏     | 69/165 [00:07<00:10,  9.01it/s]\u001b[A\n",
            " 42%|████▏     | 70/165 [00:07<00:10,  9.00it/s]\u001b[A\n",
            " 43%|████▎     | 71/165 [00:07<00:10,  9.00it/s]\u001b[A\n",
            " 44%|████▎     | 72/165 [00:07<00:10,  9.02it/s]\u001b[A\n",
            " 44%|████▍     | 73/165 [00:08<00:10,  9.05it/s]\u001b[A\n",
            " 45%|████▍     | 74/165 [00:08<00:10,  9.08it/s]\u001b[A\n",
            " 45%|████▌     | 75/165 [00:08<00:09,  9.08it/s]\u001b[A\n",
            " 46%|████▌     | 76/165 [00:08<00:09,  9.08it/s]\u001b[A\n",
            " 47%|████▋     | 77/165 [00:08<00:09,  9.08it/s]\u001b[A\n",
            " 47%|████▋     | 78/165 [00:08<00:09,  9.10it/s]\u001b[A\n",
            " 48%|████▊     | 79/165 [00:08<00:09,  9.10it/s]\u001b[A\n",
            " 48%|████▊     | 80/165 [00:08<00:09,  9.10it/s]\u001b[A\n",
            " 49%|████▉     | 81/165 [00:08<00:09,  9.11it/s]\u001b[A\n",
            " 50%|████▉     | 82/165 [00:09<00:09,  9.10it/s]\u001b[A\n",
            " 50%|█████     | 83/165 [00:09<00:09,  9.09it/s]\u001b[A\n",
            " 51%|█████     | 84/165 [00:09<00:08,  9.10it/s]\u001b[A\n",
            " 52%|█████▏    | 85/165 [00:09<00:08,  9.09it/s]\u001b[A\n",
            " 52%|█████▏    | 86/165 [00:09<00:08,  9.11it/s]\u001b[A\n",
            " 53%|█████▎    | 87/165 [00:09<00:08,  9.08it/s]\u001b[A\n",
            " 53%|█████▎    | 88/165 [00:09<00:08,  9.08it/s]\u001b[A\n",
            " 54%|█████▍    | 89/165 [00:09<00:08,  9.07it/s]\u001b[A\n",
            " 55%|█████▍    | 90/165 [00:09<00:08,  9.05it/s]\u001b[A\n",
            " 55%|█████▌    | 91/165 [00:10<00:08,  9.08it/s]\u001b[A\n",
            " 56%|█████▌    | 92/165 [00:10<00:08,  9.09it/s]\u001b[A\n",
            " 56%|█████▋    | 93/165 [00:10<00:07,  9.10it/s]\u001b[A\n",
            " 57%|█████▋    | 94/165 [00:10<00:07,  9.10it/s]\u001b[A\n",
            " 58%|█████▊    | 95/165 [00:10<00:07,  9.10it/s]\u001b[A\n",
            " 58%|█████▊    | 96/165 [00:10<00:07,  9.11it/s]\u001b[A\n",
            " 59%|█████▉    | 97/165 [00:10<00:07,  9.09it/s]\u001b[A\n",
            " 59%|█████▉    | 98/165 [00:10<00:07,  9.09it/s]\u001b[A\n",
            " 60%|██████    | 99/165 [00:10<00:07,  9.10it/s]\u001b[A\n",
            " 61%|██████    | 100/165 [00:11<00:07,  9.10it/s]\u001b[A\n",
            " 61%|██████    | 101/165 [00:11<00:07,  9.12it/s]\u001b[A\n",
            " 62%|██████▏   | 102/165 [00:11<00:06,  9.12it/s]\u001b[A\n",
            " 62%|██████▏   | 103/165 [00:11<00:06,  9.10it/s]\u001b[A\n",
            " 63%|██████▎   | 104/165 [00:11<00:06,  9.12it/s]\u001b[A\n",
            " 64%|██████▎   | 105/165 [00:11<00:06,  9.10it/s]\u001b[A\n",
            " 64%|██████▍   | 106/165 [00:11<00:06,  9.10it/s]\u001b[A\n",
            " 65%|██████▍   | 107/165 [00:11<00:06,  9.08it/s]\u001b[A\n",
            " 65%|██████▌   | 108/165 [00:11<00:06,  9.06it/s]\u001b[A\n",
            " 66%|██████▌   | 109/165 [00:11<00:06,  9.05it/s]\u001b[A\n",
            " 67%|██████▋   | 110/165 [00:12<00:06,  9.08it/s]\u001b[A\n",
            " 67%|██████▋   | 111/165 [00:12<00:05,  9.08it/s]\u001b[A\n",
            " 68%|██████▊   | 112/165 [00:12<00:05,  9.09it/s]\u001b[A\n",
            " 68%|██████▊   | 113/165 [00:12<00:05,  9.07it/s]\u001b[A\n",
            " 69%|██████▉   | 114/165 [00:12<00:05,  9.04it/s]\u001b[A\n",
            " 70%|██████▉   | 115/165 [00:12<00:05,  9.06it/s]\u001b[A\n",
            " 70%|███████   | 116/165 [00:12<00:05,  9.07it/s]\u001b[A\n",
            " 71%|███████   | 117/165 [00:12<00:05,  9.07it/s]\u001b[A\n",
            " 72%|███████▏  | 118/165 [00:12<00:05,  9.08it/s]\u001b[A\n",
            " 72%|███████▏  | 119/165 [00:13<00:05,  9.09it/s]\u001b[A\n",
            " 73%|███████▎  | 120/165 [00:13<00:04,  9.06it/s]\u001b[A\n",
            " 73%|███████▎  | 121/165 [00:13<00:04,  9.08it/s]\u001b[A\n",
            " 74%|███████▍  | 122/165 [00:13<00:04,  9.07it/s]\u001b[A\n",
            " 75%|███████▍  | 123/165 [00:13<00:04,  9.08it/s]\u001b[A\n",
            " 75%|███████▌  | 124/165 [00:13<00:04,  9.08it/s]\u001b[A\n",
            " 76%|███████▌  | 125/165 [00:13<00:04,  9.09it/s]\u001b[A\n",
            " 76%|███████▋  | 126/165 [00:13<00:04,  9.09it/s]\u001b[A\n",
            " 77%|███████▋  | 127/165 [00:13<00:04,  9.10it/s]\u001b[A\n",
            " 78%|███████▊  | 128/165 [00:14<00:04,  9.10it/s]\u001b[A\n",
            " 78%|███████▊  | 129/165 [00:14<00:03,  9.10it/s]\u001b[A\n",
            " 79%|███████▉  | 130/165 [00:14<00:03,  9.07it/s]\u001b[A\n",
            " 79%|███████▉  | 131/165 [00:14<00:03,  9.08it/s]\u001b[A\n",
            " 80%|████████  | 132/165 [00:14<00:03,  9.09it/s]\u001b[A\n",
            " 81%|████████  | 133/165 [00:14<00:03,  9.11it/s]\u001b[A\n",
            " 81%|████████  | 134/165 [00:14<00:03,  9.11it/s]\u001b[A\n",
            " 82%|████████▏ | 135/165 [00:14<00:03,  9.11it/s]\u001b[A\n",
            " 82%|████████▏ | 136/165 [00:14<00:03,  9.10it/s]\u001b[A\n",
            " 83%|████████▎ | 137/165 [00:15<00:03,  9.10it/s]\u001b[A\n",
            " 84%|████████▎ | 138/165 [00:15<00:02,  9.11it/s]\u001b[A\n",
            " 84%|████████▍ | 139/165 [00:15<00:02,  9.10it/s]\u001b[A\n",
            " 85%|████████▍ | 140/165 [00:15<00:02,  9.08it/s]\u001b[A\n",
            " 85%|████████▌ | 141/165 [00:15<00:02,  9.03it/s]\u001b[A\n",
            " 86%|████████▌ | 142/165 [00:15<00:02,  9.03it/s]\u001b[A\n",
            " 87%|████████▋ | 143/165 [00:15<00:02,  9.04it/s]\u001b[A\n",
            " 87%|████████▋ | 144/165 [00:15<00:02,  9.06it/s]\u001b[A\n",
            " 88%|████████▊ | 145/165 [00:15<00:02,  9.07it/s]\u001b[A\n",
            " 88%|████████▊ | 146/165 [00:16<00:02,  9.06it/s]\u001b[A\n",
            " 89%|████████▉ | 147/165 [00:16<00:01,  9.09it/s]\u001b[A\n",
            " 90%|████████▉ | 148/165 [00:16<00:01,  9.10it/s]\u001b[A\n",
            " 90%|█████████ | 149/165 [00:16<00:01,  9.10it/s]\u001b[A\n",
            " 91%|█████████ | 150/165 [00:16<00:01,  9.09it/s]\u001b[A\n",
            " 92%|█████████▏| 151/165 [00:16<00:01,  9.11it/s]\u001b[A\n",
            " 92%|█████████▏| 152/165 [00:16<00:01,  9.06it/s]\u001b[A\n",
            " 93%|█████████▎| 153/165 [00:16<00:01,  9.07it/s]\u001b[A\n",
            " 93%|█████████▎| 154/165 [00:16<00:01,  9.09it/s]\u001b[A\n",
            " 94%|█████████▍| 155/165 [00:17<00:01,  9.11it/s]\u001b[A\n",
            " 95%|█████████▍| 156/165 [00:17<00:00,  9.10it/s]\u001b[A\n",
            " 95%|█████████▌| 157/165 [00:17<00:00,  9.08it/s]\u001b[A\n",
            " 96%|█████████▌| 158/165 [00:17<00:00,  9.11it/s]\u001b[A\n",
            " 96%|█████████▋| 159/165 [00:17<00:00,  9.12it/s]\u001b[A\n",
            " 97%|█████████▋| 160/165 [00:17<00:00,  9.12it/s]\u001b[A\n",
            " 98%|█████████▊| 161/165 [00:17<00:00,  9.12it/s]\u001b[A\n",
            " 98%|█████████▊| 162/165 [00:17<00:00,  9.13it/s]\u001b[A\n",
            " 99%|█████████▉| 163/165 [00:17<00:00,  9.12it/s]\u001b[A\n",
            "100%|██████████| 165/165 [00:18<00:00,  9.07it/s]\n",
            "========== model: bert-base-uncased fold: 2 inference ==========\n",
            "========== model: bert-base-uncased fold: 2 inference ==========\n",
            "========== model: bert-base-uncased fold: 2 inference ==========\n",
            "loading configuration file /content/activebus_bert_final/config.json\n",
            "Model config BertConfig {\n",
            "  \"_name_or_path\": \"activebus/BERT-XD_Review\",\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_ids\": 0,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.15.0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "loading weights file /content/activebus_bert_final/pytorch_model.bin\n",
            "Some weights of the model checkpoint at /content/activebus_bert_final were not used when initializing BertForSequenceClassification: ['cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /content/activebus_bert_final and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "\n",
            "  0%|          | 0/165 [00:00<?, ?it/s]\u001b[A\n",
            "  1%|          | 1/165 [00:00<00:19,  8.57it/s]\u001b[A\n",
            "  1%|          | 2/165 [00:00<00:18,  8.85it/s]\u001b[A\n",
            "  2%|▏         | 3/165 [00:00<00:18,  8.95it/s]\u001b[A\n",
            "  2%|▏         | 4/165 [00:00<00:17,  9.00it/s]\u001b[A\n",
            "  3%|▎         | 5/165 [00:00<00:17,  9.03it/s]\u001b[A\n",
            "  4%|▎         | 6/165 [00:00<00:17,  9.06it/s]\u001b[A\n",
            "  4%|▍         | 7/165 [00:00<00:17,  9.06it/s]\u001b[A\n",
            "  5%|▍         | 8/165 [00:00<00:17,  9.08it/s]\u001b[A\n",
            "  5%|▌         | 9/165 [00:00<00:17,  9.09it/s]\u001b[A\n",
            "  6%|▌         | 10/165 [00:01<00:17,  9.11it/s]\u001b[A\n",
            "  7%|▋         | 11/165 [00:01<00:16,  9.10it/s]\u001b[A\n",
            "  7%|▋         | 12/165 [00:01<00:16,  9.10it/s]\u001b[A\n",
            "  8%|▊         | 13/165 [00:01<00:16,  9.11it/s]\u001b[A\n",
            "  8%|▊         | 14/165 [00:01<00:16,  9.10it/s]\u001b[A\n",
            "  9%|▉         | 15/165 [00:01<00:16,  9.11it/s]\u001b[A\n",
            " 10%|▉         | 16/165 [00:01<00:16,  9.12it/s]\u001b[A\n",
            " 10%|█         | 17/165 [00:01<00:16,  9.11it/s]\u001b[A\n",
            " 11%|█         | 18/165 [00:01<00:16,  9.11it/s]\u001b[A\n",
            " 12%|█▏        | 19/165 [00:02<00:16,  9.12it/s]\u001b[A\n",
            " 12%|█▏        | 20/165 [00:02<00:15,  9.10it/s]\u001b[A\n",
            " 13%|█▎        | 21/165 [00:02<00:15,  9.10it/s]\u001b[A\n",
            " 13%|█▎        | 22/165 [00:02<00:15,  9.08it/s]\u001b[A\n",
            " 14%|█▍        | 23/165 [00:02<00:15,  9.07it/s]\u001b[A\n",
            " 15%|█▍        | 24/165 [00:02<00:15,  9.06it/s]\u001b[A\n",
            " 15%|█▌        | 25/165 [00:02<00:15,  9.09it/s]\u001b[A\n",
            " 16%|█▌        | 26/165 [00:02<00:15,  9.04it/s]\u001b[A\n",
            " 16%|█▋        | 27/165 [00:02<00:15,  9.04it/s]\u001b[A\n",
            " 17%|█▋        | 28/165 [00:03<00:15,  9.05it/s]\u001b[A\n",
            " 18%|█▊        | 29/165 [00:03<00:15,  9.06it/s]\u001b[A\n",
            " 18%|█▊        | 30/165 [00:03<00:14,  9.07it/s]\u001b[A\n",
            " 19%|█▉        | 31/165 [00:03<00:14,  9.08it/s]\u001b[A\n",
            " 19%|█▉        | 32/165 [00:03<00:14,  9.09it/s]\u001b[A\n",
            " 20%|██        | 33/165 [00:03<00:14,  9.09it/s]\u001b[A\n",
            " 21%|██        | 34/165 [00:03<00:14,  9.10it/s]\u001b[A\n",
            " 21%|██        | 35/165 [00:03<00:14,  9.08it/s]\u001b[A\n",
            " 22%|██▏       | 36/165 [00:03<00:14,  9.06it/s]\u001b[A\n",
            " 22%|██▏       | 37/165 [00:04<00:14,  9.07it/s]\u001b[A\n",
            " 23%|██▎       | 38/165 [00:04<00:13,  9.08it/s]\u001b[A\n",
            " 24%|██▎       | 39/165 [00:04<00:13,  9.07it/s]\u001b[A\n",
            " 24%|██▍       | 40/165 [00:04<00:13,  9.07it/s]\u001b[A\n",
            " 25%|██▍       | 41/165 [00:04<00:13,  9.09it/s]\u001b[A\n",
            " 25%|██▌       | 42/165 [00:04<00:13,  9.09it/s]\u001b[A\n",
            " 26%|██▌       | 43/165 [00:04<00:13,  9.08it/s]\u001b[A\n",
            " 27%|██▋       | 44/165 [00:04<00:13,  9.10it/s]\u001b[A\n",
            " 27%|██▋       | 45/165 [00:04<00:13,  9.07it/s]\u001b[A\n",
            " 28%|██▊       | 46/165 [00:05<00:13,  9.06it/s]\u001b[A\n",
            " 28%|██▊       | 47/165 [00:05<00:13,  9.05it/s]\u001b[A\n",
            " 29%|██▉       | 48/165 [00:05<00:12,  9.06it/s]\u001b[A\n",
            " 30%|██▉       | 49/165 [00:05<00:12,  9.07it/s]\u001b[A\n",
            " 30%|███       | 50/165 [00:05<00:12,  9.08it/s]\u001b[A\n",
            " 31%|███       | 51/165 [00:05<00:12,  9.07it/s]\u001b[A\n",
            " 32%|███▏      | 52/165 [00:05<00:12,  9.08it/s]\u001b[A\n",
            " 32%|███▏      | 53/165 [00:05<00:12,  9.10it/s]\u001b[A\n",
            " 33%|███▎      | 54/165 [00:05<00:12,  9.11it/s]\u001b[A\n",
            " 33%|███▎      | 55/165 [00:06<00:12,  9.13it/s]\u001b[A\n",
            " 34%|███▍      | 56/165 [00:06<00:11,  9.14it/s]\u001b[A\n",
            " 35%|███▍      | 57/165 [00:06<00:11,  9.15it/s]\u001b[A\n",
            " 35%|███▌      | 58/165 [00:06<00:11,  9.14it/s]\u001b[A\n",
            " 36%|███▌      | 59/165 [00:06<00:11,  9.14it/s]\u001b[A\n",
            " 36%|███▋      | 60/165 [00:06<00:11,  9.14it/s]\u001b[A\n",
            " 37%|███▋      | 61/165 [00:06<00:11,  9.14it/s]\u001b[A\n",
            " 38%|███▊      | 62/165 [00:06<00:11,  9.15it/s]\u001b[A\n",
            " 38%|███▊      | 63/165 [00:06<00:11,  9.14it/s]\u001b[A\n",
            " 39%|███▉      | 64/165 [00:07<00:11,  9.14it/s]\u001b[A\n",
            " 39%|███▉      | 65/165 [00:07<00:10,  9.13it/s]\u001b[A\n",
            " 40%|████      | 66/165 [00:07<00:10,  9.11it/s]\u001b[A\n",
            " 41%|████      | 67/165 [00:07<00:10,  9.12it/s]\u001b[A\n",
            " 41%|████      | 68/165 [00:07<00:10,  9.11it/s]\u001b[A\n",
            " 42%|████▏     | 69/165 [00:07<00:10,  9.09it/s]\u001b[A\n",
            " 42%|████▏     | 70/165 [00:07<00:10,  9.10it/s]\u001b[A\n",
            " 43%|████▎     | 71/165 [00:07<00:10,  9.10it/s]\u001b[A\n",
            " 44%|████▎     | 72/165 [00:07<00:10,  9.10it/s]\u001b[A\n",
            " 44%|████▍     | 73/165 [00:08<00:10,  9.10it/s]\u001b[A\n",
            " 45%|████▍     | 74/165 [00:08<00:10,  9.08it/s]\u001b[A\n",
            " 45%|████▌     | 75/165 [00:08<00:09,  9.07it/s]\u001b[A\n",
            " 46%|████▌     | 76/165 [00:08<00:09,  9.06it/s]\u001b[A\n",
            " 47%|████▋     | 77/165 [00:08<00:09,  9.07it/s]\u001b[A\n",
            " 47%|████▋     | 78/165 [00:08<00:09,  9.08it/s]\u001b[A\n",
            " 48%|████▊     | 79/165 [00:08<00:09,  9.06it/s]\u001b[A\n",
            " 48%|████▊     | 80/165 [00:08<00:09,  9.05it/s]\u001b[A\n",
            " 49%|████▉     | 81/165 [00:08<00:09,  9.04it/s]\u001b[A\n",
            " 50%|████▉     | 82/165 [00:09<00:09,  9.04it/s]\u001b[A\n",
            " 50%|█████     | 83/165 [00:09<00:09,  9.07it/s]\u001b[A\n",
            " 51%|█████     | 84/165 [00:09<00:08,  9.06it/s]\u001b[A\n",
            " 52%|█████▏    | 85/165 [00:09<00:08,  9.09it/s]\u001b[A\n",
            " 52%|█████▏    | 86/165 [00:09<00:08,  9.10it/s]\u001b[A\n",
            " 53%|█████▎    | 87/165 [00:09<00:08,  9.10it/s]\u001b[A\n",
            " 53%|█████▎    | 88/165 [00:09<00:08,  9.11it/s]\u001b[A\n",
            " 54%|█████▍    | 89/165 [00:09<00:08,  9.12it/s]\u001b[A\n",
            " 55%|█████▍    | 90/165 [00:09<00:08,  9.12it/s]\u001b[A\n",
            " 55%|█████▌    | 91/165 [00:10<00:08,  9.13it/s]\u001b[A\n",
            " 56%|█████▌    | 92/165 [00:10<00:07,  9.14it/s]\u001b[A\n",
            " 56%|█████▋    | 93/165 [00:10<00:07,  9.13it/s]\u001b[A\n",
            " 57%|█████▋    | 94/165 [00:10<00:07,  9.13it/s]\u001b[A\n",
            " 58%|█████▊    | 95/165 [00:10<00:07,  9.12it/s]\u001b[A\n",
            " 58%|█████▊    | 96/165 [00:10<00:07,  9.11it/s]\u001b[A\n",
            " 59%|█████▉    | 97/165 [00:10<00:07,  9.07it/s]\u001b[A\n",
            " 59%|█████▉    | 98/165 [00:10<00:07,  9.08it/s]\u001b[A\n",
            " 60%|██████    | 99/165 [00:10<00:07,  9.08it/s]\u001b[A\n",
            " 61%|██████    | 100/165 [00:11<00:07,  9.09it/s]\u001b[A\n",
            " 61%|██████    | 101/165 [00:11<00:07,  9.10it/s]\u001b[A\n",
            " 62%|██████▏   | 102/165 [00:11<00:06,  9.10it/s]\u001b[A\n",
            " 62%|██████▏   | 103/165 [00:11<00:06,  9.10it/s]\u001b[A\n",
            " 63%|██████▎   | 104/165 [00:11<00:06,  9.10it/s]\u001b[A\n",
            " 64%|██████▎   | 105/165 [00:11<00:06,  9.10it/s]\u001b[A\n",
            " 64%|██████▍   | 106/165 [00:11<00:06,  9.09it/s]\u001b[A\n",
            " 65%|██████▍   | 107/165 [00:11<00:06,  9.09it/s]\u001b[A\n",
            " 65%|██████▌   | 108/165 [00:11<00:06,  9.10it/s]\u001b[A\n",
            " 66%|██████▌   | 109/165 [00:11<00:06,  9.10it/s]\u001b[A\n",
            " 67%|██████▋   | 110/165 [00:12<00:06,  9.11it/s]\u001b[A\n",
            " 67%|██████▋   | 111/165 [00:12<00:05,  9.11it/s]\u001b[A\n",
            " 68%|██████▊   | 112/165 [00:12<00:05,  9.10it/s]\u001b[A\n",
            " 68%|██████▊   | 113/165 [00:12<00:05,  9.08it/s]\u001b[A\n",
            " 69%|██████▉   | 114/165 [00:12<00:05,  9.08it/s]\u001b[A\n",
            " 70%|██████▉   | 115/165 [00:12<00:05,  9.06it/s]\u001b[A\n",
            " 70%|███████   | 116/165 [00:12<00:05,  9.08it/s]\u001b[A\n",
            " 71%|███████   | 117/165 [00:12<00:05,  9.08it/s]\u001b[A\n",
            " 72%|███████▏  | 118/165 [00:12<00:05,  9.10it/s]\u001b[A\n",
            " 72%|███████▏  | 119/165 [00:13<00:05,  9.10it/s]\u001b[A\n",
            " 73%|███████▎  | 120/165 [00:13<00:04,  9.09it/s]\u001b[A\n",
            " 73%|███████▎  | 121/165 [00:13<00:04,  9.09it/s]\u001b[A\n",
            " 74%|███████▍  | 122/165 [00:13<00:04,  9.09it/s]\u001b[A\n",
            " 75%|███████▍  | 123/165 [00:13<00:04,  9.09it/s]\u001b[A\n",
            " 75%|███████▌  | 124/165 [00:13<00:04,  9.08it/s]\u001b[A\n",
            " 76%|███████▌  | 125/165 [00:13<00:04,  9.10it/s]\u001b[A\n",
            " 76%|███████▋  | 126/165 [00:13<00:04,  9.01it/s]\u001b[A\n",
            " 77%|███████▋  | 127/165 [00:13<00:04,  9.03it/s]\u001b[A\n",
            " 78%|███████▊  | 128/165 [00:14<00:04,  9.04it/s]\u001b[A\n",
            " 78%|███████▊  | 129/165 [00:14<00:03,  9.06it/s]\u001b[A\n",
            " 79%|███████▉  | 130/165 [00:14<00:03,  9.06it/s]\u001b[A\n",
            " 79%|███████▉  | 131/165 [00:14<00:03,  9.07it/s]\u001b[A\n",
            " 80%|████████  | 132/165 [00:14<00:03,  9.07it/s]\u001b[A\n",
            " 81%|████████  | 133/165 [00:14<00:03,  9.09it/s]\u001b[A\n",
            " 81%|████████  | 134/165 [00:14<00:03,  9.07it/s]\u001b[A\n",
            " 82%|████████▏ | 135/165 [00:14<00:03,  9.06it/s]\u001b[A\n",
            " 82%|████████▏ | 136/165 [00:14<00:03,  9.07it/s]\u001b[A\n",
            " 83%|████████▎ | 137/165 [00:15<00:03,  9.09it/s]\u001b[A\n",
            " 84%|████████▎ | 138/165 [00:15<00:02,  9.10it/s]\u001b[A\n",
            " 84%|████████▍ | 139/165 [00:15<00:02,  9.07it/s]\u001b[A\n",
            " 85%|████████▍ | 140/165 [00:15<00:02,  9.08it/s]\u001b[A\n",
            " 85%|████████▌ | 141/165 [00:15<00:02,  9.07it/s]\u001b[A\n",
            " 86%|████████▌ | 142/165 [00:15<00:02,  9.07it/s]\u001b[A\n",
            " 87%|████████▋ | 143/165 [00:15<00:02,  9.07it/s]\u001b[A\n",
            " 87%|████████▋ | 144/165 [00:15<00:02,  9.09it/s]\u001b[A\n",
            " 88%|████████▊ | 145/165 [00:15<00:02,  9.09it/s]\u001b[A\n",
            " 88%|████████▊ | 146/165 [00:16<00:02,  9.09it/s]\u001b[A\n",
            " 89%|████████▉ | 147/165 [00:16<00:01,  9.10it/s]\u001b[A\n",
            " 90%|████████▉ | 148/165 [00:16<00:01,  9.10it/s]\u001b[A\n",
            " 90%|█████████ | 149/165 [00:16<00:01,  9.09it/s]\u001b[A\n",
            " 91%|█████████ | 150/165 [00:16<00:01,  9.10it/s]\u001b[A\n",
            " 92%|█████████▏| 151/165 [00:16<00:01,  9.11it/s]\u001b[A\n",
            " 92%|█████████▏| 152/165 [00:16<00:01,  9.10it/s]\u001b[A\n",
            " 93%|█████████▎| 153/165 [00:16<00:01,  9.11it/s]\u001b[A\n",
            " 93%|█████████▎| 154/165 [00:16<00:01,  9.13it/s]\u001b[A\n",
            " 94%|█████████▍| 155/165 [00:17<00:01,  9.12it/s]\u001b[A\n",
            " 95%|█████████▍| 156/165 [00:17<00:00,  9.10it/s]\u001b[A\n",
            " 95%|█████████▌| 157/165 [00:17<00:00,  9.11it/s]\u001b[A\n",
            " 96%|█████████▌| 158/165 [00:17<00:00,  9.12it/s]\u001b[A\n",
            " 96%|█████████▋| 159/165 [00:17<00:00,  9.11it/s]\u001b[A\n",
            " 97%|█████████▋| 160/165 [00:17<00:00,  9.13it/s]\u001b[A\n",
            " 98%|█████████▊| 161/165 [00:17<00:00,  9.13it/s]\u001b[A\n",
            " 98%|█████████▊| 162/165 [00:17<00:00,  9.13it/s]\u001b[A\n",
            " 99%|█████████▉| 163/165 [00:17<00:00,  9.12it/s]\u001b[A\n",
            "100%|██████████| 165/165 [00:18<00:00,  9.08it/s]\n",
            "========== model: bert-base-uncased fold: 3 inference ==========\n",
            "========== model: bert-base-uncased fold: 3 inference ==========\n",
            "========== model: bert-base-uncased fold: 3 inference ==========\n",
            "loading configuration file /content/activebus_bert_final/config.json\n",
            "Model config BertConfig {\n",
            "  \"_name_or_path\": \"activebus/BERT-XD_Review\",\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_ids\": 0,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.15.0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "loading weights file /content/activebus_bert_final/pytorch_model.bin\n",
            "Some weights of the model checkpoint at /content/activebus_bert_final were not used when initializing BertForSequenceClassification: ['cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /content/activebus_bert_final and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "\n",
            "  0%|          | 0/165 [00:00<?, ?it/s]\u001b[A\n",
            "  1%|          | 1/165 [00:00<00:20,  7.82it/s]\u001b[A\n",
            "  1%|          | 2/165 [00:00<00:19,  8.53it/s]\u001b[A\n",
            "  2%|▏         | 3/165 [00:00<00:18,  8.79it/s]\u001b[A\n",
            "  2%|▏         | 4/165 [00:00<00:18,  8.90it/s]\u001b[A\n",
            "  3%|▎         | 5/165 [00:00<00:17,  8.97it/s]\u001b[A\n",
            "  4%|▎         | 6/165 [00:00<00:17,  9.02it/s]\u001b[A\n",
            "  4%|▍         | 7/165 [00:00<00:17,  9.06it/s]\u001b[A\n",
            "  5%|▍         | 8/165 [00:00<00:17,  9.07it/s]\u001b[A\n",
            "  5%|▌         | 9/165 [00:01<00:17,  9.09it/s]\u001b[A\n",
            "  6%|▌         | 10/165 [00:01<00:17,  9.11it/s]\u001b[A\n",
            "  7%|▋         | 11/165 [00:01<00:16,  9.07it/s]\u001b[A\n",
            "  7%|▋         | 12/165 [00:01<00:16,  9.08it/s]\u001b[A\n",
            "  8%|▊         | 13/165 [00:01<00:16,  9.08it/s]\u001b[A\n",
            "  8%|▊         | 14/165 [00:01<00:16,  9.10it/s]\u001b[A\n",
            "  9%|▉         | 15/165 [00:01<00:16,  9.10it/s]\u001b[A\n",
            " 10%|▉         | 16/165 [00:01<00:16,  9.07it/s]\u001b[A\n",
            " 10%|█         | 17/165 [00:01<00:16,  9.05it/s]\u001b[A\n",
            " 11%|█         | 18/165 [00:02<00:16,  9.02it/s]\u001b[A\n",
            " 12%|█▏        | 19/165 [00:02<00:16,  9.03it/s]\u001b[A\n",
            " 12%|█▏        | 20/165 [00:02<00:16,  9.03it/s]\u001b[A\n",
            " 13%|█▎        | 21/165 [00:02<00:15,  9.05it/s]\u001b[A\n",
            " 13%|█▎        | 22/165 [00:02<00:15,  9.06it/s]\u001b[A\n",
            " 14%|█▍        | 23/165 [00:02<00:15,  9.07it/s]\u001b[A\n",
            " 15%|█▍        | 24/165 [00:02<00:15,  9.09it/s]\u001b[A\n",
            " 15%|█▌        | 25/165 [00:02<00:15,  9.08it/s]\u001b[A\n",
            " 16%|█▌        | 26/165 [00:02<00:15,  9.09it/s]\u001b[A\n",
            " 16%|█▋        | 27/165 [00:02<00:15,  9.10it/s]\u001b[A\n",
            " 17%|█▋        | 28/165 [00:03<00:15,  9.10it/s]\u001b[A\n",
            " 18%|█▊        | 29/165 [00:03<00:14,  9.11it/s]\u001b[A\n",
            " 18%|█▊        | 30/165 [00:03<00:14,  9.10it/s]\u001b[A\n",
            " 19%|█▉        | 31/165 [00:03<00:14,  9.12it/s]\u001b[A\n",
            " 19%|█▉        | 32/165 [00:03<00:14,  9.10it/s]\u001b[A\n",
            " 20%|██        | 33/165 [00:03<00:14,  9.11it/s]\u001b[A\n",
            " 21%|██        | 34/165 [00:03<00:14,  9.11it/s]\u001b[A\n",
            " 21%|██        | 35/165 [00:03<00:14,  9.10it/s]\u001b[A\n",
            " 22%|██▏       | 36/165 [00:03<00:14,  9.09it/s]\u001b[A\n",
            " 22%|██▏       | 37/165 [00:04<00:14,  9.09it/s]\u001b[A\n",
            " 23%|██▎       | 38/165 [00:04<00:13,  9.08it/s]\u001b[A\n",
            " 24%|██▎       | 39/165 [00:04<00:13,  9.07it/s]\u001b[A\n",
            " 24%|██▍       | 40/165 [00:04<00:13,  9.10it/s]\u001b[A\n",
            " 25%|██▍       | 41/165 [00:04<00:13,  9.11it/s]\u001b[A\n",
            " 25%|██▌       | 42/165 [00:04<00:13,  9.12it/s]\u001b[A\n",
            " 26%|██▌       | 43/165 [00:04<00:13,  9.10it/s]\u001b[A\n",
            " 27%|██▋       | 44/165 [00:04<00:13,  9.10it/s]\u001b[A\n",
            " 27%|██▋       | 45/165 [00:04<00:13,  9.09it/s]\u001b[A\n",
            " 28%|██▊       | 46/165 [00:05<00:13,  9.08it/s]\u001b[A\n",
            " 28%|██▊       | 47/165 [00:05<00:12,  9.09it/s]\u001b[A\n",
            " 29%|██▉       | 48/165 [00:05<00:12,  9.10it/s]\u001b[A\n",
            " 30%|██▉       | 49/165 [00:05<00:12,  9.10it/s]\u001b[A\n",
            " 30%|███       | 50/165 [00:05<00:12,  9.09it/s]\u001b[A\n",
            " 31%|███       | 51/165 [00:05<00:12,  9.08it/s]\u001b[A\n",
            " 32%|███▏      | 52/165 [00:05<00:12,  9.07it/s]\u001b[A\n",
            " 32%|███▏      | 53/165 [00:05<00:12,  9.05it/s]\u001b[A\n",
            " 33%|███▎      | 54/165 [00:05<00:12,  9.07it/s]\u001b[A\n",
            " 33%|███▎      | 55/165 [00:06<00:12,  9.07it/s]\u001b[A\n",
            " 34%|███▍      | 56/165 [00:06<00:12,  9.08it/s]\u001b[A\n",
            " 35%|███▍      | 57/165 [00:06<00:11,  9.07it/s]\u001b[A\n",
            " 35%|███▌      | 58/165 [00:06<00:11,  9.08it/s]\u001b[A\n",
            " 36%|███▌      | 59/165 [00:06<00:11,  9.08it/s]\u001b[A\n",
            " 36%|███▋      | 60/165 [00:06<00:11,  9.10it/s]\u001b[A\n",
            " 37%|███▋      | 61/165 [00:06<00:11,  9.12it/s]\u001b[A\n",
            " 38%|███▊      | 62/165 [00:06<00:11,  9.11it/s]\u001b[A\n",
            " 38%|███▊      | 63/165 [00:06<00:11,  9.11it/s]\u001b[A\n",
            " 39%|███▉      | 64/165 [00:07<00:11,  9.13it/s]\u001b[A\n",
            " 39%|███▉      | 65/165 [00:07<00:10,  9.11it/s]\u001b[A\n",
            " 40%|████      | 66/165 [00:07<00:10,  9.10it/s]\u001b[A\n",
            " 41%|████      | 67/165 [00:07<00:10,  9.11it/s]\u001b[A\n",
            " 41%|████      | 68/165 [00:07<00:10,  9.11it/s]\u001b[A\n",
            " 42%|████▏     | 69/165 [00:07<00:10,  9.11it/s]\u001b[A\n",
            " 42%|████▏     | 70/165 [00:07<00:10,  9.12it/s]\u001b[A\n",
            " 43%|████▎     | 71/165 [00:07<00:10,  9.12it/s]\u001b[A\n",
            " 44%|████▎     | 72/165 [00:07<00:10,  9.12it/s]\u001b[A\n",
            " 44%|████▍     | 73/165 [00:08<00:10,  9.08it/s]\u001b[A\n",
            " 45%|████▍     | 74/165 [00:08<00:10,  9.07it/s]\u001b[A\n",
            " 45%|████▌     | 75/165 [00:08<00:09,  9.08it/s]\u001b[A\n",
            " 46%|████▌     | 76/165 [00:08<00:09,  9.10it/s]\u001b[A\n",
            " 47%|████▋     | 77/165 [00:08<00:09,  9.11it/s]\u001b[A\n",
            " 47%|████▋     | 78/165 [00:08<00:09,  9.10it/s]\u001b[A\n",
            " 48%|████▊     | 79/165 [00:08<00:09,  9.08it/s]\u001b[A\n",
            " 48%|████▊     | 80/165 [00:08<00:09,  9.01it/s]\u001b[A\n",
            " 49%|████▉     | 81/165 [00:08<00:09,  9.04it/s]\u001b[A\n",
            " 50%|████▉     | 82/165 [00:09<00:09,  9.06it/s]\u001b[A\n",
            " 50%|█████     | 83/165 [00:09<00:09,  9.07it/s]\u001b[A\n",
            " 51%|█████     | 84/165 [00:09<00:08,  9.08it/s]\u001b[A\n",
            " 52%|█████▏    | 85/165 [00:09<00:08,  9.09it/s]\u001b[A\n",
            " 52%|█████▏    | 86/165 [00:09<00:08,  9.12it/s]\u001b[A\n",
            " 53%|█████▎    | 87/165 [00:09<00:08,  9.12it/s]\u001b[A\n",
            " 53%|█████▎    | 88/165 [00:09<00:08,  9.13it/s]\u001b[A\n",
            " 54%|█████▍    | 89/165 [00:09<00:08,  9.13it/s]\u001b[A\n",
            " 55%|█████▍    | 90/165 [00:09<00:08,  9.11it/s]\u001b[A\n",
            " 55%|█████▌    | 91/165 [00:10<00:08,  9.11it/s]\u001b[A\n",
            " 56%|█████▌    | 92/165 [00:10<00:08,  9.09it/s]\u001b[A\n",
            " 56%|█████▋    | 93/165 [00:10<00:07,  9.09it/s]\u001b[A\n",
            " 57%|█████▋    | 94/165 [00:10<00:07,  9.10it/s]\u001b[A\n",
            " 58%|█████▊    | 95/165 [00:10<00:07,  9.10it/s]\u001b[A\n",
            " 58%|█████▊    | 96/165 [00:10<00:07,  9.10it/s]\u001b[A\n",
            " 59%|█████▉    | 97/165 [00:10<00:07,  9.11it/s]\u001b[A\n",
            " 59%|█████▉    | 98/165 [00:10<00:07,  9.11it/s]\u001b[A\n",
            " 60%|██████    | 99/165 [00:10<00:07,  9.11it/s]\u001b[A\n",
            " 61%|██████    | 100/165 [00:11<00:07,  9.08it/s]\u001b[A\n",
            " 61%|██████    | 101/165 [00:11<00:07,  9.10it/s]\u001b[A\n",
            " 62%|██████▏   | 102/165 [00:11<00:06,  9.08it/s]\u001b[A\n",
            " 62%|██████▏   | 103/165 [00:11<00:06,  9.09it/s]\u001b[A\n",
            " 63%|██████▎   | 104/165 [00:11<00:06,  9.10it/s]\u001b[A\n",
            " 64%|██████▎   | 105/165 [00:11<00:06,  9.11it/s]\u001b[A\n",
            " 64%|██████▍   | 106/165 [00:11<00:06,  9.10it/s]\u001b[A\n",
            " 65%|██████▍   | 107/165 [00:11<00:06,  9.11it/s]\u001b[A\n",
            " 65%|██████▌   | 108/165 [00:11<00:06,  9.11it/s]\u001b[A\n",
            " 66%|██████▌   | 109/165 [00:12<00:06,  9.10it/s]\u001b[A\n",
            " 67%|██████▋   | 110/165 [00:12<00:06,  9.11it/s]\u001b[A\n",
            " 67%|██████▋   | 111/165 [00:12<00:05,  9.12it/s]\u001b[A\n",
            " 68%|██████▊   | 112/165 [00:12<00:05,  9.12it/s]\u001b[A\n",
            " 68%|██████▊   | 113/165 [00:12<00:05,  9.13it/s]\u001b[A\n",
            " 69%|██████▉   | 114/165 [00:12<00:05,  9.12it/s]\u001b[A\n",
            " 70%|██████▉   | 115/165 [00:12<00:05,  9.12it/s]\u001b[A\n",
            " 70%|███████   | 116/165 [00:12<00:05,  9.11it/s]\u001b[A\n",
            " 71%|███████   | 117/165 [00:12<00:05,  9.12it/s]\u001b[A\n",
            " 72%|███████▏  | 118/165 [00:12<00:05,  9.11it/s]\u001b[A\n",
            " 72%|███████▏  | 119/165 [00:13<00:05,  9.13it/s]\u001b[A\n",
            " 73%|███████▎  | 120/165 [00:13<00:04,  9.13it/s]\u001b[A\n",
            " 73%|███████▎  | 121/165 [00:13<00:04,  9.12it/s]\u001b[A\n",
            " 74%|███████▍  | 122/165 [00:13<00:04,  9.11it/s]\u001b[A\n",
            " 75%|███████▍  | 123/165 [00:13<00:04,  9.09it/s]\u001b[A\n",
            " 75%|███████▌  | 124/165 [00:13<00:04,  9.09it/s]\u001b[A\n",
            " 76%|███████▌  | 125/165 [00:13<00:04,  9.11it/s]\u001b[A\n",
            " 76%|███████▋  | 126/165 [00:13<00:04,  9.12it/s]\u001b[A\n",
            " 77%|███████▋  | 127/165 [00:13<00:04,  9.13it/s]\u001b[A\n",
            " 78%|███████▊  | 128/165 [00:14<00:04,  9.13it/s]\u001b[A\n",
            " 78%|███████▊  | 129/165 [00:14<00:03,  9.12it/s]\u001b[A\n",
            " 79%|███████▉  | 130/165 [00:14<00:03,  9.12it/s]\u001b[A\n",
            " 79%|███████▉  | 131/165 [00:14<00:03,  9.14it/s]\u001b[A\n",
            " 80%|████████  | 132/165 [00:14<00:03,  9.13it/s]\u001b[A\n",
            " 81%|████████  | 133/165 [00:14<00:03,  9.13it/s]\u001b[A\n",
            " 81%|████████  | 134/165 [00:14<00:03,  9.12it/s]\u001b[A\n",
            " 82%|████████▏ | 135/165 [00:14<00:03,  9.10it/s]\u001b[A\n",
            " 82%|████████▏ | 136/165 [00:14<00:03,  9.10it/s]\u001b[A\n",
            " 83%|████████▎ | 137/165 [00:15<00:03,  9.07it/s]\u001b[A\n",
            " 84%|████████▎ | 138/165 [00:15<00:02,  9.09it/s]\u001b[A\n",
            " 84%|████████▍ | 139/165 [00:15<00:02,  9.09it/s]\u001b[A\n",
            " 85%|████████▍ | 140/165 [00:15<00:02,  9.11it/s]\u001b[A\n",
            " 85%|████████▌ | 141/165 [00:15<00:02,  9.11it/s]\u001b[A\n",
            " 86%|████████▌ | 142/165 [00:15<00:02,  9.10it/s]\u001b[A\n",
            " 87%|████████▋ | 143/165 [00:15<00:02,  9.03it/s]\u001b[A\n",
            " 87%|████████▋ | 144/165 [00:15<00:02,  9.06it/s]\u001b[A\n",
            " 88%|████████▊ | 145/165 [00:15<00:02,  9.06it/s]\u001b[A\n",
            " 88%|████████▊ | 146/165 [00:16<00:02,  9.09it/s]\u001b[A\n",
            " 89%|████████▉ | 147/165 [00:16<00:01,  9.09it/s]\u001b[A\n",
            " 90%|████████▉ | 148/165 [00:16<00:01,  9.10it/s]\u001b[A\n",
            " 90%|█████████ | 149/165 [00:16<00:01,  9.08it/s]\u001b[A\n",
            " 91%|█████████ | 150/165 [00:16<00:01,  9.11it/s]\u001b[A\n",
            " 92%|█████████▏| 151/165 [00:16<00:01,  9.09it/s]\u001b[A\n",
            " 92%|█████████▏| 152/165 [00:16<00:01,  9.09it/s]\u001b[A\n",
            " 93%|█████████▎| 153/165 [00:16<00:01,  9.11it/s]\u001b[A\n",
            " 93%|█████████▎| 154/165 [00:16<00:01,  9.12it/s]\u001b[A\n",
            " 94%|█████████▍| 155/165 [00:17<00:01,  9.08it/s]\u001b[A\n",
            " 95%|█████████▍| 156/165 [00:17<00:00,  9.06it/s]\u001b[A\n",
            " 95%|█████████▌| 157/165 [00:17<00:00,  9.04it/s]\u001b[A\n",
            " 96%|█████████▌| 158/165 [00:17<00:00,  9.07it/s]\u001b[A\n",
            " 96%|█████████▋| 159/165 [00:17<00:00,  9.07it/s]\u001b[A\n",
            " 97%|█████████▋| 160/165 [00:17<00:00,  9.10it/s]\u001b[A\n",
            " 98%|█████████▊| 161/165 [00:17<00:00,  9.12it/s]\u001b[A\n",
            " 98%|█████████▊| 162/165 [00:17<00:00,  9.11it/s]\u001b[A\n",
            " 99%|█████████▉| 163/165 [00:17<00:00,  9.10it/s]\u001b[A\n",
            "100%|██████████| 165/165 [00:18<00:00,  9.07it/s]\n",
            "========== model: bert-base-uncased fold: 4 inference ==========\n",
            "========== model: bert-base-uncased fold: 4 inference ==========\n",
            "========== model: bert-base-uncased fold: 4 inference ==========\n",
            "loading configuration file /content/activebus_bert_final/config.json\n",
            "Model config BertConfig {\n",
            "  \"_name_or_path\": \"activebus/BERT-XD_Review\",\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_ids\": 0,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.15.0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "loading weights file /content/activebus_bert_final/pytorch_model.bin\n",
            "Some weights of the model checkpoint at /content/activebus_bert_final were not used when initializing BertForSequenceClassification: ['cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /content/activebus_bert_final and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "\n",
            "  0%|          | 0/165 [00:00<?, ?it/s]\u001b[A\n",
            "  1%|          | 1/165 [00:00<00:19,  8.49it/s]\u001b[A\n",
            "  1%|          | 2/165 [00:00<00:18,  8.84it/s]\u001b[A\n",
            "  2%|▏         | 3/165 [00:00<00:18,  8.93it/s]\u001b[A\n",
            "  2%|▏         | 4/165 [00:00<00:17,  9.02it/s]\u001b[A\n",
            "  3%|▎         | 5/165 [00:00<00:17,  9.05it/s]\u001b[A\n",
            "  4%|▎         | 6/165 [00:00<00:17,  9.08it/s]\u001b[A\n",
            "  4%|▍         | 7/165 [00:00<00:17,  9.09it/s]\u001b[A\n",
            "  5%|▍         | 8/165 [00:00<00:17,  9.12it/s]\u001b[A\n",
            "  5%|▌         | 9/165 [00:00<00:17,  9.11it/s]\u001b[A\n",
            "  6%|▌         | 10/165 [00:01<00:17,  9.11it/s]\u001b[A\n",
            "  7%|▋         | 11/165 [00:01<00:16,  9.09it/s]\u001b[A\n",
            "  7%|▋         | 12/165 [00:01<00:16,  9.08it/s]\u001b[A\n",
            "  8%|▊         | 13/165 [00:01<00:16,  9.11it/s]\u001b[A\n",
            "  8%|▊         | 14/165 [00:01<00:16,  9.10it/s]\u001b[A\n",
            "  9%|▉         | 15/165 [00:01<00:16,  9.11it/s]\u001b[A\n",
            " 10%|▉         | 16/165 [00:01<00:16,  9.10it/s]\u001b[A\n",
            " 10%|█         | 17/165 [00:01<00:16,  9.09it/s]\u001b[A\n",
            " 11%|█         | 18/165 [00:01<00:16,  9.08it/s]\u001b[A\n",
            " 12%|█▏        | 19/165 [00:02<00:16,  9.08it/s]\u001b[A\n",
            " 12%|█▏        | 20/165 [00:02<00:15,  9.07it/s]\u001b[A\n",
            " 13%|█▎        | 21/165 [00:02<00:15,  9.07it/s]\u001b[A\n",
            " 13%|█▎        | 22/165 [00:02<00:15,  9.05it/s]\u001b[A\n",
            " 14%|█▍        | 23/165 [00:02<00:15,  9.07it/s]\u001b[A\n",
            " 15%|█▍        | 24/165 [00:02<00:15,  9.09it/s]\u001b[A\n",
            " 15%|█▌        | 25/165 [00:02<00:15,  9.11it/s]\u001b[A\n",
            " 16%|█▌        | 26/165 [00:02<00:15,  9.10it/s]\u001b[A\n",
            " 16%|█▋        | 27/165 [00:02<00:15,  9.09it/s]\u001b[A\n",
            " 17%|█▋        | 28/165 [00:03<00:15,  9.07it/s]\u001b[A\n",
            " 18%|█▊        | 29/165 [00:03<00:14,  9.07it/s]\u001b[A\n",
            " 18%|█▊        | 30/165 [00:03<00:14,  9.07it/s]\u001b[A\n",
            " 19%|█▉        | 31/165 [00:03<00:14,  9.08it/s]\u001b[A\n",
            " 19%|█▉        | 32/165 [00:03<00:14,  9.10it/s]\u001b[A\n",
            " 20%|██        | 33/165 [00:03<00:14,  9.10it/s]\u001b[A\n",
            " 21%|██        | 34/165 [00:03<00:14,  9.09it/s]\u001b[A\n",
            " 21%|██        | 35/165 [00:03<00:14,  9.04it/s]\u001b[A\n",
            " 22%|██▏       | 36/165 [00:03<00:14,  9.06it/s]\u001b[A\n",
            " 22%|██▏       | 37/165 [00:04<00:14,  9.08it/s]\u001b[A\n",
            " 23%|██▎       | 38/165 [00:04<00:13,  9.09it/s]\u001b[A\n",
            " 24%|██▎       | 39/165 [00:04<00:13,  9.11it/s]\u001b[A\n",
            " 24%|██▍       | 40/165 [00:04<00:13,  9.09it/s]\u001b[A\n",
            " 25%|██▍       | 41/165 [00:04<00:13,  9.10it/s]\u001b[A\n",
            " 25%|██▌       | 42/165 [00:04<00:13,  9.12it/s]\u001b[A\n",
            " 26%|██▌       | 43/165 [00:04<00:13,  9.11it/s]\u001b[A\n",
            " 27%|██▋       | 44/165 [00:04<00:13,  9.09it/s]\u001b[A\n",
            " 27%|██▋       | 45/165 [00:04<00:13,  9.10it/s]\u001b[A\n",
            " 28%|██▊       | 46/165 [00:05<00:13,  9.09it/s]\u001b[A\n",
            " 28%|██▊       | 47/165 [00:05<00:12,  9.11it/s]\u001b[A\n",
            " 29%|██▉       | 48/165 [00:05<00:12,  9.11it/s]\u001b[A\n",
            " 30%|██▉       | 49/165 [00:05<00:12,  9.11it/s]\u001b[A\n",
            " 30%|███       | 50/165 [00:05<00:12,  9.10it/s]\u001b[A\n",
            " 31%|███       | 51/165 [00:05<00:12,  9.10it/s]\u001b[A\n",
            " 32%|███▏      | 52/165 [00:05<00:12,  9.08it/s]\u001b[A\n",
            " 32%|███▏      | 53/165 [00:05<00:12,  9.07it/s]\u001b[A\n",
            " 33%|███▎      | 54/165 [00:05<00:12,  9.07it/s]\u001b[A\n",
            " 33%|███▎      | 55/165 [00:06<00:12,  9.09it/s]\u001b[A\n",
            " 34%|███▍      | 56/165 [00:06<00:11,  9.10it/s]\u001b[A\n",
            " 35%|███▍      | 57/165 [00:06<00:11,  9.11it/s]\u001b[A\n",
            " 35%|███▌      | 58/165 [00:06<00:11,  9.10it/s]\u001b[A\n",
            " 36%|███▌      | 59/165 [00:06<00:11,  9.05it/s]\u001b[A\n",
            " 36%|███▋      | 60/165 [00:06<00:11,  9.06it/s]\u001b[A\n",
            " 37%|███▋      | 61/165 [00:06<00:11,  9.09it/s]\u001b[A\n",
            " 38%|███▊      | 62/165 [00:06<00:11,  9.09it/s]\u001b[A\n",
            " 38%|███▊      | 63/165 [00:06<00:11,  9.07it/s]\u001b[A\n",
            " 39%|███▉      | 64/165 [00:07<00:11,  9.07it/s]\u001b[A\n",
            " 39%|███▉      | 65/165 [00:07<00:10,  9.10it/s]\u001b[A\n",
            " 40%|████      | 66/165 [00:07<00:10,  9.09it/s]\u001b[A\n",
            " 41%|████      | 67/165 [00:07<00:10,  9.10it/s]\u001b[A\n",
            " 41%|████      | 68/165 [00:07<00:10,  9.12it/s]\u001b[A\n",
            " 42%|████▏     | 69/165 [00:07<00:10,  9.12it/s]\u001b[A\n",
            " 42%|████▏     | 70/165 [00:07<00:10,  9.13it/s]\u001b[A\n",
            " 43%|████▎     | 71/165 [00:07<00:10,  9.10it/s]\u001b[A\n",
            " 44%|████▎     | 72/165 [00:07<00:10,  9.09it/s]\u001b[A\n",
            " 44%|████▍     | 73/165 [00:08<00:10,  9.07it/s]\u001b[A\n",
            " 45%|████▍     | 74/165 [00:08<00:10,  9.09it/s]\u001b[A\n",
            " 45%|████▌     | 75/165 [00:08<00:09,  9.10it/s]\u001b[A\n",
            " 46%|████▌     | 76/165 [00:08<00:09,  9.11it/s]\u001b[A\n",
            " 47%|████▋     | 77/165 [00:08<00:09,  9.11it/s]\u001b[A\n",
            " 47%|████▋     | 78/165 [00:08<00:09,  9.11it/s]\u001b[A\n",
            " 48%|████▊     | 79/165 [00:08<00:09,  9.12it/s]\u001b[A\n",
            " 48%|████▊     | 80/165 [00:08<00:09,  9.12it/s]\u001b[A\n",
            " 49%|████▉     | 81/165 [00:08<00:09,  9.12it/s]\u001b[A\n",
            " 50%|████▉     | 82/165 [00:09<00:09,  9.10it/s]\u001b[A\n",
            " 50%|█████     | 83/165 [00:09<00:09,  9.09it/s]\u001b[A\n",
            " 51%|█████     | 84/165 [00:09<00:08,  9.07it/s]\u001b[A\n",
            " 52%|█████▏    | 85/165 [00:09<00:08,  9.08it/s]\u001b[A\n",
            " 52%|█████▏    | 86/165 [00:09<00:08,  9.10it/s]\u001b[A\n",
            " 53%|█████▎    | 87/165 [00:09<00:08,  9.12it/s]\u001b[A\n",
            " 53%|█████▎    | 88/165 [00:09<00:08,  9.10it/s]\u001b[A\n",
            " 54%|█████▍    | 89/165 [00:09<00:08,  9.08it/s]\u001b[A\n",
            " 55%|█████▍    | 90/165 [00:09<00:08,  9.07it/s]\u001b[A\n",
            " 55%|█████▌    | 91/165 [00:10<00:08,  9.08it/s]\u001b[A\n",
            " 56%|█████▌    | 92/165 [00:10<00:08,  9.09it/s]\u001b[A\n",
            " 56%|█████▋    | 93/165 [00:10<00:07,  9.08it/s]\u001b[A\n",
            " 57%|█████▋    | 94/165 [00:10<00:07,  9.07it/s]\u001b[A\n",
            " 58%|█████▊    | 95/165 [00:10<00:07,  9.06it/s]\u001b[A\n",
            " 58%|█████▊    | 96/165 [00:10<00:07,  9.07it/s]\u001b[A\n",
            " 59%|█████▉    | 97/165 [00:10<00:07,  9.07it/s]\u001b[A\n",
            " 59%|█████▉    | 98/165 [00:10<00:07,  9.08it/s]\u001b[A\n",
            " 60%|██████    | 99/165 [00:10<00:07,  9.10it/s]\u001b[A\n",
            " 61%|██████    | 100/165 [00:11<00:07,  9.09it/s]\u001b[A\n",
            " 61%|██████    | 101/165 [00:11<00:07,  9.08it/s]\u001b[A\n",
            " 62%|██████▏   | 102/165 [00:11<00:06,  9.07it/s]\u001b[A\n",
            " 62%|██████▏   | 103/165 [00:11<00:06,  9.09it/s]\u001b[A\n",
            " 63%|██████▎   | 104/165 [00:11<00:06,  9.08it/s]\u001b[A\n",
            " 64%|██████▎   | 105/165 [00:11<00:06,  9.09it/s]\u001b[A\n",
            " 64%|██████▍   | 106/165 [00:11<00:06,  9.10it/s]\u001b[A\n",
            " 65%|██████▍   | 107/165 [00:11<00:06,  9.09it/s]\u001b[A\n",
            " 65%|██████▌   | 108/165 [00:11<00:06,  9.10it/s]\u001b[A\n",
            " 66%|██████▌   | 109/165 [00:11<00:06,  9.08it/s]\u001b[A\n",
            " 67%|██████▋   | 110/165 [00:12<00:06,  9.07it/s]\u001b[A\n",
            " 67%|██████▋   | 111/165 [00:12<00:05,  9.07it/s]\u001b[A\n",
            " 68%|██████▊   | 112/165 [00:12<00:05,  9.04it/s]\u001b[A\n",
            " 68%|██████▊   | 113/165 [00:12<00:05,  9.06it/s]\u001b[A\n",
            " 69%|██████▉   | 114/165 [00:12<00:05,  9.06it/s]\u001b[A\n",
            " 70%|██████▉   | 115/165 [00:12<00:05,  9.08it/s]\u001b[A\n",
            " 70%|███████   | 116/165 [00:12<00:05,  9.10it/s]\u001b[A\n",
            " 71%|███████   | 117/165 [00:12<00:05,  9.08it/s]\u001b[A\n",
            " 72%|███████▏  | 118/165 [00:12<00:05,  9.05it/s]\u001b[A\n",
            " 72%|███████▏  | 119/165 [00:13<00:05,  9.05it/s]\u001b[A\n",
            " 73%|███████▎  | 120/165 [00:13<00:04,  9.06it/s]\u001b[A\n",
            " 73%|███████▎  | 121/165 [00:13<00:04,  9.06it/s]\u001b[A\n",
            " 74%|███████▍  | 122/165 [00:13<00:04,  9.07it/s]\u001b[A\n",
            " 75%|███████▍  | 123/165 [00:13<00:04,  9.08it/s]\u001b[A\n",
            " 75%|███████▌  | 124/165 [00:13<00:04,  9.10it/s]\u001b[A\n",
            " 76%|███████▌  | 125/165 [00:13<00:04,  9.10it/s]\u001b[A\n",
            " 76%|███████▋  | 126/165 [00:13<00:04,  9.09it/s]\u001b[A\n",
            " 77%|███████▋  | 127/165 [00:13<00:04,  9.07it/s]\u001b[A\n",
            " 78%|███████▊  | 128/165 [00:14<00:04,  9.07it/s]\u001b[A\n",
            " 78%|███████▊  | 129/165 [00:14<00:03,  9.09it/s]\u001b[A\n",
            " 79%|███████▉  | 130/165 [00:14<00:03,  9.07it/s]\u001b[A\n",
            " 79%|███████▉  | 131/165 [00:14<00:03,  9.09it/s]\u001b[A\n",
            " 80%|████████  | 132/165 [00:14<00:03,  9.08it/s]\u001b[A\n",
            " 81%|████████  | 133/165 [00:14<00:03,  9.08it/s]\u001b[A\n",
            " 81%|████████  | 134/165 [00:14<00:03,  9.10it/s]\u001b[A\n",
            " 82%|████████▏ | 135/165 [00:14<00:03,  9.11it/s]\u001b[A\n",
            " 82%|████████▏ | 136/165 [00:14<00:03,  9.11it/s]\u001b[A\n",
            " 83%|████████▎ | 137/165 [00:15<00:03,  9.09it/s]\u001b[A\n",
            " 84%|████████▎ | 138/165 [00:15<00:02,  9.09it/s]\u001b[A\n",
            " 84%|████████▍ | 139/165 [00:15<00:02,  9.11it/s]\u001b[A\n",
            " 85%|████████▍ | 140/165 [00:15<00:02,  9.12it/s]\u001b[A\n",
            " 85%|████████▌ | 141/165 [00:15<00:02,  9.09it/s]\u001b[A\n",
            " 86%|████████▌ | 142/165 [00:15<00:02,  9.10it/s]\u001b[A\n",
            " 87%|████████▋ | 143/165 [00:15<00:02,  9.10it/s]\u001b[A\n",
            " 87%|████████▋ | 144/165 [00:15<00:02,  9.10it/s]\u001b[A\n",
            " 88%|████████▊ | 145/165 [00:15<00:02,  9.09it/s]\u001b[A\n",
            " 88%|████████▊ | 146/165 [00:16<00:02,  9.08it/s]\u001b[A\n",
            " 89%|████████▉ | 147/165 [00:16<00:01,  9.10it/s]\u001b[A\n",
            " 90%|████████▉ | 148/165 [00:16<00:01,  9.11it/s]\u001b[A\n",
            " 90%|█████████ | 149/165 [00:16<00:01,  9.11it/s]\u001b[A\n",
            " 91%|█████████ | 150/165 [00:16<00:01,  9.08it/s]\u001b[A\n",
            " 92%|█████████▏| 151/165 [00:16<00:01,  9.07it/s]\u001b[A\n",
            " 92%|█████████▏| 152/165 [00:16<00:01,  9.08it/s]\u001b[A\n",
            " 93%|█████████▎| 153/165 [00:16<00:01,  9.10it/s]\u001b[A\n",
            " 93%|█████████▎| 154/165 [00:16<00:01,  9.09it/s]\u001b[A\n",
            " 94%|█████████▍| 155/165 [00:17<00:01,  9.08it/s]\u001b[A\n",
            " 95%|█████████▍| 156/165 [00:17<00:00,  9.09it/s]\u001b[A\n",
            " 95%|█████████▌| 157/165 [00:17<00:00,  9.10it/s]\u001b[A\n",
            " 96%|█████████▌| 158/165 [00:17<00:00,  9.12it/s]\u001b[A\n",
            " 96%|█████████▋| 159/165 [00:17<00:00,  9.12it/s]\u001b[A\n",
            " 97%|█████████▋| 160/165 [00:17<00:00,  9.13it/s]\u001b[A\n",
            " 98%|█████████▊| 161/165 [00:17<00:00,  9.15it/s]\u001b[A\n",
            " 98%|█████████▊| 162/165 [00:17<00:00,  9.15it/s]\u001b[A\n",
            " 99%|█████████▉| 163/165 [00:17<00:00,  9.13it/s]\u001b[A\n",
            "100%|██████████| 165/165 [00:18<00:00,  9.08it/s]\n",
            " 50%|█████     | 3/6 [2:22:52<2:22:54, 2858.15s/it]Didn't find file /content/activebus_bert_final/added_tokens.json. We won't load it.\n",
            "loading file /content/activebus_bert_final/vocab.txt\n",
            "loading file /content/activebus_bert_final/tokenizer.json\n",
            "loading file None\n",
            "loading file /content/activebus_bert_final/special_tokens_map.json\n",
            "loading file /content/activebus_bert_final/tokenizer_config.json\n",
            "========== fold: 0 training ==========\n",
            "========== fold: 0 training ==========\n",
            "========== fold: 0 training ==========\n",
            "========== fold: 0 training ==========\n",
            "Didn't find file /content/activebus_bert_final/added_tokens.json. We won't load it.\n",
            "loading file /content/activebus_bert_final/vocab.txt\n",
            "loading file None\n",
            "loading file /content/activebus_bert_final/special_tokens_map.json\n",
            "loading file /content/activebus_bert_final/tokenizer_config.json\n",
            "loading file /content/activebus_bert_final/tokenizer.json\n",
            "Didn't find file /content/activebus_bert_final/added_tokens.json. We won't load it.\n",
            "loading file /content/activebus_bert_final/vocab.txt\n",
            "loading file None\n",
            "loading file /content/activebus_bert_final/special_tokens_map.json\n",
            "loading file /content/activebus_bert_final/tokenizer_config.json\n",
            "loading file /content/activebus_bert_final/tokenizer.json\n",
            "loading configuration file /content/activebus_bert_final/config.json\n",
            "Model config BertConfig {\n",
            "  \"_name_or_path\": \"activebus/BERT-XD_Review\",\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_ids\": 0,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.15.0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "loading weights file /content/activebus_bert_final/pytorch_model.bin\n",
            "Some weights of the model checkpoint at /content/activebus_bert_final were not used when initializing BertForSequenceClassification: ['cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /content/activebus_bert_final and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: [1][0/306] Elapsed 0m 0s (remain 3m 15s) Loss: 0.6878 \n",
            "Epoch: [1][100/306] Elapsed 0m 33s (remain 1m 7s) Loss: 0.2883 \n",
            "Epoch: [1][200/306] Elapsed 1m 5s (remain 0m 34s) Loss: 0.2019 \n",
            "Epoch: [1][300/306] Elapsed 1m 38s (remain 0m 1s) Loss: 0.1698 \n",
            "Epoch: [1][305/306] Elapsed 1m 39s (remain 0m 0s) Loss: 0.1685 \n",
            "EVAL: [0/77] Elapsed 0m 0s (remain 0m 26s) Loss: 0.0199 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1 - avg_train_loss: 0.1685  avg_val_loss: 0.0750  time: 108s\n",
            "Epoch 1 - avg_train_loss: 0.1685  avg_val_loss: 0.0750  time: 108s\n",
            "Epoch 1 - avg_train_loss: 0.1685  avg_val_loss: 0.0750  time: 108s\n",
            "Epoch 1 - avg_train_loss: 0.1685  avg_val_loss: 0.0750  time: 108s\n",
            "Epoch 1 - Score: 0.07502962567567437\n",
            "Epoch 1 - Score: 0.07502962567567437\n",
            "Epoch 1 - Score: 0.07502962567567437\n",
            "Epoch 1 - Score: 0.07502962567567437\n",
            "Epoch 1 - Save Best Score: 0.0750 Model\n",
            "Epoch 1 - Save Best Score: 0.0750 Model\n",
            "Epoch 1 - Save Best Score: 0.0750 Model\n",
            "Epoch 1 - Save Best Score: 0.0750 Model\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EVAL: [76/77] Elapsed 0m 8s (remain 0m 0s) Loss: 0.0750 \n",
            "Log loss for 0 is 0.07502962567567437\n",
            "Epoch: [2][0/306] Elapsed 0m 0s (remain 2m 57s) Loss: 0.1301 \n",
            "Epoch: [2][100/306] Elapsed 0m 33s (remain 1m 7s) Loss: 0.0629 \n",
            "Epoch: [2][200/306] Elapsed 1m 5s (remain 0m 34s) Loss: 0.0619 \n",
            "Epoch: [2][300/306] Elapsed 1m 38s (remain 0m 1s) Loss: 0.0682 \n",
            "Epoch: [2][305/306] Elapsed 1m 39s (remain 0m 0s) Loss: 0.0689 \n",
            "EVAL: [0/77] Elapsed 0m 0s (remain 0m 27s) Loss: 0.0203 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2 - avg_train_loss: 0.0689  avg_val_loss: 0.0748  time: 108s\n",
            "Epoch 2 - avg_train_loss: 0.0689  avg_val_loss: 0.0748  time: 108s\n",
            "Epoch 2 - avg_train_loss: 0.0689  avg_val_loss: 0.0748  time: 108s\n",
            "Epoch 2 - avg_train_loss: 0.0689  avg_val_loss: 0.0748  time: 108s\n",
            "Epoch 2 - Score: 0.07477665863190217\n",
            "Epoch 2 - Score: 0.07477665863190217\n",
            "Epoch 2 - Score: 0.07477665863190217\n",
            "Epoch 2 - Score: 0.07477665863190217\n",
            "Epoch 2 - Save Best Score: 0.0748 Model\n",
            "Epoch 2 - Save Best Score: 0.0748 Model\n",
            "Epoch 2 - Save Best Score: 0.0748 Model\n",
            "Epoch 2 - Save Best Score: 0.0748 Model\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EVAL: [76/77] Elapsed 0m 8s (remain 0m 0s) Loss: 0.0748 \n",
            "Log loss for 1 is 0.07477665863190217\n",
            "Epoch: [3][0/306] Elapsed 0m 0s (remain 3m 5s) Loss: 0.0086 \n",
            "Epoch: [3][100/306] Elapsed 0m 33s (remain 1m 7s) Loss: 0.0477 \n",
            "Epoch: [3][200/306] Elapsed 1m 5s (remain 0m 34s) Loss: 0.0482 \n",
            "Epoch: [3][300/306] Elapsed 1m 38s (remain 0m 1s) Loss: 0.0445 \n",
            "Epoch: [3][305/306] Elapsed 1m 39s (remain 0m 0s) Loss: 0.0449 \n",
            "EVAL: [0/77] Elapsed 0m 0s (remain 0m 26s) Loss: 0.0052 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3 - avg_train_loss: 0.0449  avg_val_loss: 0.0784  time: 108s\n",
            "Epoch 3 - avg_train_loss: 0.0449  avg_val_loss: 0.0784  time: 108s\n",
            "Epoch 3 - avg_train_loss: 0.0449  avg_val_loss: 0.0784  time: 108s\n",
            "Epoch 3 - avg_train_loss: 0.0449  avg_val_loss: 0.0784  time: 108s\n",
            "Epoch 3 - Score: 0.07835814685397516\n",
            "Epoch 3 - Score: 0.07835814685397516\n",
            "Epoch 3 - Score: 0.07835814685397516\n",
            "Epoch 3 - Score: 0.07835814685397516\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EVAL: [76/77] Elapsed 0m 8s (remain 0m 0s) Loss: 0.0784 \n",
            "Log loss for 2 is 0.07835814685397516\n",
            "Epoch: [4][0/306] Elapsed 0m 0s (remain 2m 56s) Loss: 0.2254 \n",
            "Epoch: [4][100/306] Elapsed 0m 33s (remain 1m 7s) Loss: 0.0160 \n",
            "Epoch: [4][200/306] Elapsed 1m 5s (remain 0m 34s) Loss: 0.0323 \n",
            "Epoch: [4][300/306] Elapsed 1m 38s (remain 0m 1s) Loss: 0.0317 \n",
            "Epoch: [4][305/306] Elapsed 1m 39s (remain 0m 0s) Loss: 0.0312 \n",
            "EVAL: [0/77] Elapsed 0m 0s (remain 0m 27s) Loss: 0.0066 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4 - avg_train_loss: 0.0312  avg_val_loss: 0.1163  time: 108s\n",
            "Epoch 4 - avg_train_loss: 0.0312  avg_val_loss: 0.1163  time: 108s\n",
            "Epoch 4 - avg_train_loss: 0.0312  avg_val_loss: 0.1163  time: 108s\n",
            "Epoch 4 - avg_train_loss: 0.0312  avg_val_loss: 0.1163  time: 108s\n",
            "Epoch 4 - Score: 0.1163066835758403\n",
            "Epoch 4 - Score: 0.1163066835758403\n",
            "Epoch 4 - Score: 0.1163066835758403\n",
            "Epoch 4 - Score: 0.1163066835758403\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EVAL: [76/77] Elapsed 0m 8s (remain 0m 0s) Loss: 0.1163 \n",
            "Log loss for 3 is 0.1163066835758403\n",
            "Epoch: [5][0/306] Elapsed 0m 0s (remain 3m 2s) Loss: 0.0013 \n",
            "Epoch: [5][100/306] Elapsed 0m 33s (remain 1m 7s) Loss: 0.0200 \n",
            "Epoch: [5][200/306] Elapsed 1m 5s (remain 0m 34s) Loss: 0.0213 \n",
            "Epoch: [5][300/306] Elapsed 1m 38s (remain 0m 1s) Loss: 0.0205 \n",
            "Epoch: [5][305/306] Elapsed 1m 39s (remain 0m 0s) Loss: 0.0203 \n",
            "EVAL: [0/77] Elapsed 0m 0s (remain 0m 26s) Loss: 0.0021 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5 - avg_train_loss: 0.0203  avg_val_loss: 0.0921  time: 108s\n",
            "Epoch 5 - avg_train_loss: 0.0203  avg_val_loss: 0.0921  time: 108s\n",
            "Epoch 5 - avg_train_loss: 0.0203  avg_val_loss: 0.0921  time: 108s\n",
            "Epoch 5 - avg_train_loss: 0.0203  avg_val_loss: 0.0921  time: 108s\n",
            "Epoch 5 - Score: 0.09210813439526809\n",
            "Epoch 5 - Score: 0.09210813439526809\n",
            "Epoch 5 - Score: 0.09210813439526809\n",
            "Epoch 5 - Score: 0.09210813439526809\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EVAL: [76/77] Elapsed 0m 8s (remain 0m 0s) Loss: 0.0921 \n",
            "Log loss for 4 is 0.09210813439526809\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "========== fold: 0 result ==========\n",
            "========== fold: 0 result ==========\n",
            "========== fold: 0 result ==========\n",
            "========== fold: 0 result ==========\n",
            "Score: 0.07478\n",
            "Score: 0.07478\n",
            "Score: 0.07478\n",
            "Score: 0.07478\n",
            "========== fold: 1 training ==========\n",
            "========== fold: 1 training ==========\n",
            "========== fold: 1 training ==========\n",
            "========== fold: 1 training ==========\n",
            "Didn't find file /content/activebus_bert_final/added_tokens.json. We won't load it.\n",
            "loading file /content/activebus_bert_final/vocab.txt\n",
            "loading file None\n",
            "loading file /content/activebus_bert_final/special_tokens_map.json\n",
            "loading file /content/activebus_bert_final/tokenizer_config.json\n",
            "loading file /content/activebus_bert_final/tokenizer.json\n",
            "Didn't find file /content/activebus_bert_final/added_tokens.json. We won't load it.\n",
            "loading file /content/activebus_bert_final/vocab.txt\n",
            "loading file None\n",
            "loading file /content/activebus_bert_final/special_tokens_map.json\n",
            "loading file /content/activebus_bert_final/tokenizer_config.json\n",
            "loading file /content/activebus_bert_final/tokenizer.json\n",
            "loading configuration file /content/activebus_bert_final/config.json\n",
            "Model config BertConfig {\n",
            "  \"_name_or_path\": \"activebus/BERT-XD_Review\",\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_ids\": 0,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.15.0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "loading weights file /content/activebus_bert_final/pytorch_model.bin\n",
            "Some weights of the model checkpoint at /content/activebus_bert_final were not used when initializing BertForSequenceClassification: ['cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /content/activebus_bert_final and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: [1][0/306] Elapsed 0m 0s (remain 3m 12s) Loss: 0.6008 \n",
            "Epoch: [1][100/306] Elapsed 0m 33s (remain 1m 7s) Loss: 0.2402 \n",
            "Epoch: [1][200/306] Elapsed 1m 5s (remain 0m 34s) Loss: 0.1789 \n",
            "Epoch: [1][300/306] Elapsed 1m 38s (remain 0m 1s) Loss: 0.1462 \n",
            "Epoch: [1][305/306] Elapsed 1m 39s (remain 0m 0s) Loss: 0.1452 \n",
            "EVAL: [0/77] Elapsed 0m 0s (remain 0m 26s) Loss: 0.0494 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1 - avg_train_loss: 0.1452  avg_val_loss: 0.0911  time: 109s\n",
            "Epoch 1 - avg_train_loss: 0.1452  avg_val_loss: 0.0911  time: 109s\n",
            "Epoch 1 - avg_train_loss: 0.1452  avg_val_loss: 0.0911  time: 109s\n",
            "Epoch 1 - avg_train_loss: 0.1452  avg_val_loss: 0.0911  time: 109s\n",
            "Epoch 1 - Score: 0.09109314862933213\n",
            "Epoch 1 - Score: 0.09109314862933213\n",
            "Epoch 1 - Score: 0.09109314862933213\n",
            "Epoch 1 - Score: 0.09109314862933213\n",
            "Epoch 1 - Save Best Score: 0.0911 Model\n",
            "Epoch 1 - Save Best Score: 0.0911 Model\n",
            "Epoch 1 - Save Best Score: 0.0911 Model\n",
            "Epoch 1 - Save Best Score: 0.0911 Model\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EVAL: [76/77] Elapsed 0m 8s (remain 0m 0s) Loss: 0.0911 \n",
            "Log loss for 0 is 0.09109314862933213\n",
            "Epoch: [2][0/306] Elapsed 0m 0s (remain 3m 2s) Loss: 0.0147 \n",
            "Epoch: [2][100/306] Elapsed 0m 33s (remain 1m 7s) Loss: 0.0629 \n",
            "Epoch: [2][200/306] Elapsed 1m 5s (remain 0m 34s) Loss: 0.0606 \n",
            "Epoch: [2][300/306] Elapsed 1m 38s (remain 0m 1s) Loss: 0.0593 \n",
            "Epoch: [2][305/306] Elapsed 1m 39s (remain 0m 0s) Loss: 0.0596 \n",
            "EVAL: [0/77] Elapsed 0m 0s (remain 0m 26s) Loss: 0.0751 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2 - avg_train_loss: 0.0596  avg_val_loss: 0.1131  time: 108s\n",
            "Epoch 2 - avg_train_loss: 0.0596  avg_val_loss: 0.1131  time: 108s\n",
            "Epoch 2 - avg_train_loss: 0.0596  avg_val_loss: 0.1131  time: 108s\n",
            "Epoch 2 - avg_train_loss: 0.0596  avg_val_loss: 0.1131  time: 108s\n",
            "Epoch 2 - Score: 0.11307315184469482\n",
            "Epoch 2 - Score: 0.11307315184469482\n",
            "Epoch 2 - Score: 0.11307315184469482\n",
            "Epoch 2 - Score: 0.11307315184469482\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EVAL: [76/77] Elapsed 0m 8s (remain 0m 0s) Loss: 0.1131 \n",
            "Log loss for 1 is 0.11307315184469482\n",
            "Epoch: [3][0/306] Elapsed 0m 0s (remain 3m 9s) Loss: 0.0019 \n",
            "Epoch: [3][100/306] Elapsed 0m 33s (remain 1m 7s) Loss: 0.0315 \n",
            "Epoch: [3][200/306] Elapsed 1m 5s (remain 0m 34s) Loss: 0.0317 \n",
            "Epoch: [3][300/306] Elapsed 1m 38s (remain 0m 1s) Loss: 0.0348 \n",
            "Epoch: [3][305/306] Elapsed 1m 39s (remain 0m 0s) Loss: 0.0351 \n",
            "EVAL: [0/77] Elapsed 0m 0s (remain 0m 26s) Loss: 0.0242 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3 - avg_train_loss: 0.0351  avg_val_loss: 0.1057  time: 108s\n",
            "Epoch 3 - avg_train_loss: 0.0351  avg_val_loss: 0.1057  time: 108s\n",
            "Epoch 3 - avg_train_loss: 0.0351  avg_val_loss: 0.1057  time: 108s\n",
            "Epoch 3 - avg_train_loss: 0.0351  avg_val_loss: 0.1057  time: 108s\n",
            "Epoch 3 - Score: 0.10567164339856476\n",
            "Epoch 3 - Score: 0.10567164339856476\n",
            "Epoch 3 - Score: 0.10567164339856476\n",
            "Epoch 3 - Score: 0.10567164339856476\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EVAL: [76/77] Elapsed 0m 8s (remain 0m 0s) Loss: 0.1057 \n",
            "Log loss for 2 is 0.10567164339856476\n",
            "Epoch: [4][0/306] Elapsed 0m 0s (remain 3m 1s) Loss: 0.0252 \n",
            "Epoch: [4][100/306] Elapsed 0m 33s (remain 1m 7s) Loss: 0.0345 \n",
            "Epoch: [4][200/306] Elapsed 1m 5s (remain 0m 34s) Loss: 0.0262 \n",
            "Epoch: [4][300/306] Elapsed 1m 38s (remain 0m 1s) Loss: 0.0274 \n",
            "Epoch: [4][305/306] Elapsed 1m 39s (remain 0m 0s) Loss: 0.0270 \n",
            "EVAL: [0/77] Elapsed 0m 0s (remain 0m 26s) Loss: 0.0077 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4 - avg_train_loss: 0.0270  avg_val_loss: 0.1457  time: 108s\n",
            "Epoch 4 - avg_train_loss: 0.0270  avg_val_loss: 0.1457  time: 108s\n",
            "Epoch 4 - avg_train_loss: 0.0270  avg_val_loss: 0.1457  time: 108s\n",
            "Epoch 4 - avg_train_loss: 0.0270  avg_val_loss: 0.1457  time: 108s\n",
            "Epoch 4 - Score: 0.14567271062154827\n",
            "Epoch 4 - Score: 0.14567271062154827\n",
            "Epoch 4 - Score: 0.14567271062154827\n",
            "Epoch 4 - Score: 0.14567271062154827\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EVAL: [76/77] Elapsed 0m 8s (remain 0m 0s) Loss: 0.1457 \n",
            "Log loss for 3 is 0.14567271062154827\n",
            "Epoch: [5][0/306] Elapsed 0m 0s (remain 3m 6s) Loss: 0.0025 \n",
            "Epoch: [5][100/306] Elapsed 0m 33s (remain 1m 7s) Loss: 0.0143 \n",
            "Epoch: [5][200/306] Elapsed 1m 5s (remain 0m 34s) Loss: 0.0159 \n",
            "Epoch: [5][300/306] Elapsed 1m 38s (remain 0m 1s) Loss: 0.0153 \n",
            "Epoch: [5][305/306] Elapsed 1m 39s (remain 0m 0s) Loss: 0.0151 \n",
            "EVAL: [0/77] Elapsed 0m 0s (remain 0m 26s) Loss: 0.0035 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5 - avg_train_loss: 0.0151  avg_val_loss: 0.1748  time: 108s\n",
            "Epoch 5 - avg_train_loss: 0.0151  avg_val_loss: 0.1748  time: 108s\n",
            "Epoch 5 - avg_train_loss: 0.0151  avg_val_loss: 0.1748  time: 108s\n",
            "Epoch 5 - avg_train_loss: 0.0151  avg_val_loss: 0.1748  time: 108s\n",
            "Epoch 5 - Score: 0.1748031996248993\n",
            "Epoch 5 - Score: 0.1748031996248993\n",
            "Epoch 5 - Score: 0.1748031996248993\n",
            "Epoch 5 - Score: 0.1748031996248993\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EVAL: [76/77] Elapsed 0m 8s (remain 0m 0s) Loss: 0.1748 \n",
            "Log loss for 4 is 0.1748031996248993\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "========== fold: 1 result ==========\n",
            "========== fold: 1 result ==========\n",
            "========== fold: 1 result ==========\n",
            "========== fold: 1 result ==========\n",
            "Score: 0.09109\n",
            "Score: 0.09109\n",
            "Score: 0.09109\n",
            "Score: 0.09109\n",
            "========== fold: 2 training ==========\n",
            "========== fold: 2 training ==========\n",
            "========== fold: 2 training ==========\n",
            "========== fold: 2 training ==========\n",
            "Didn't find file /content/activebus_bert_final/added_tokens.json. We won't load it.\n",
            "loading file /content/activebus_bert_final/vocab.txt\n",
            "loading file None\n",
            "loading file /content/activebus_bert_final/special_tokens_map.json\n",
            "loading file /content/activebus_bert_final/tokenizer_config.json\n",
            "loading file /content/activebus_bert_final/tokenizer.json\n",
            "Didn't find file /content/activebus_bert_final/added_tokens.json. We won't load it.\n",
            "loading file /content/activebus_bert_final/vocab.txt\n",
            "loading file None\n",
            "loading file /content/activebus_bert_final/special_tokens_map.json\n",
            "loading file /content/activebus_bert_final/tokenizer_config.json\n",
            "loading file /content/activebus_bert_final/tokenizer.json\n",
            "loading configuration file /content/activebus_bert_final/config.json\n",
            "Model config BertConfig {\n",
            "  \"_name_or_path\": \"activebus/BERT-XD_Review\",\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_ids\": 0,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.15.0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "loading weights file /content/activebus_bert_final/pytorch_model.bin\n",
            "Some weights of the model checkpoint at /content/activebus_bert_final were not used when initializing BertForSequenceClassification: ['cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /content/activebus_bert_final and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: [1][0/306] Elapsed 0m 0s (remain 3m 4s) Loss: 0.6866 \n",
            "Epoch: [1][100/306] Elapsed 0m 33s (remain 1m 7s) Loss: 0.2518 \n",
            "Epoch: [1][200/306] Elapsed 1m 5s (remain 0m 34s) Loss: 0.1764 \n",
            "Epoch: [1][300/306] Elapsed 1m 38s (remain 0m 1s) Loss: 0.1474 \n",
            "Epoch: [1][305/306] Elapsed 1m 39s (remain 0m 0s) Loss: 0.1459 \n",
            "EVAL: [0/77] Elapsed 0m 0s (remain 0m 28s) Loss: 0.2052 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1 - avg_train_loss: 0.1459  avg_val_loss: 0.1021  time: 108s\n",
            "Epoch 1 - avg_train_loss: 0.1459  avg_val_loss: 0.1021  time: 108s\n",
            "Epoch 1 - avg_train_loss: 0.1459  avg_val_loss: 0.1021  time: 108s\n",
            "Epoch 1 - avg_train_loss: 0.1459  avg_val_loss: 0.1021  time: 108s\n",
            "Epoch 1 - Score: 0.10206884487715004\n",
            "Epoch 1 - Score: 0.10206884487715004\n",
            "Epoch 1 - Score: 0.10206884487715004\n",
            "Epoch 1 - Score: 0.10206884487715004\n",
            "Epoch 1 - Save Best Score: 0.1021 Model\n",
            "Epoch 1 - Save Best Score: 0.1021 Model\n",
            "Epoch 1 - Save Best Score: 0.1021 Model\n",
            "Epoch 1 - Save Best Score: 0.1021 Model\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EVAL: [76/77] Elapsed 0m 8s (remain 0m 0s) Loss: 0.1021 \n",
            "Log loss for 0 is 0.10206884487715004\n",
            "Epoch: [2][0/306] Elapsed 0m 0s (remain 3m 0s) Loss: 0.0827 \n",
            "Epoch: [2][100/306] Elapsed 0m 33s (remain 1m 7s) Loss: 0.0628 \n",
            "Epoch: [2][200/306] Elapsed 1m 5s (remain 0m 34s) Loss: 0.0646 \n",
            "Epoch: [2][300/306] Elapsed 1m 38s (remain 0m 1s) Loss: 0.0641 \n",
            "Epoch: [2][305/306] Elapsed 1m 39s (remain 0m 0s) Loss: 0.0647 \n",
            "EVAL: [0/77] Elapsed 0m 0s (remain 0m 27s) Loss: 0.0595 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2 - avg_train_loss: 0.0647  avg_val_loss: 0.0785  time: 108s\n",
            "Epoch 2 - avg_train_loss: 0.0647  avg_val_loss: 0.0785  time: 108s\n",
            "Epoch 2 - avg_train_loss: 0.0647  avg_val_loss: 0.0785  time: 108s\n",
            "Epoch 2 - avg_train_loss: 0.0647  avg_val_loss: 0.0785  time: 108s\n",
            "Epoch 2 - Score: 0.07853403637391008\n",
            "Epoch 2 - Score: 0.07853403637391008\n",
            "Epoch 2 - Score: 0.07853403637391008\n",
            "Epoch 2 - Score: 0.07853403637391008\n",
            "Epoch 2 - Save Best Score: 0.0785 Model\n",
            "Epoch 2 - Save Best Score: 0.0785 Model\n",
            "Epoch 2 - Save Best Score: 0.0785 Model\n",
            "Epoch 2 - Save Best Score: 0.0785 Model\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EVAL: [76/77] Elapsed 0m 8s (remain 0m 0s) Loss: 0.0785 \n",
            "Log loss for 1 is 0.07853403637391008\n",
            "Epoch: [3][0/306] Elapsed 0m 0s (remain 2m 59s) Loss: 0.0194 \n",
            "Epoch: [3][100/306] Elapsed 0m 33s (remain 1m 7s) Loss: 0.0393 \n",
            "Epoch: [3][200/306] Elapsed 1m 5s (remain 0m 34s) Loss: 0.0375 \n",
            "Epoch: [3][300/306] Elapsed 1m 38s (remain 0m 1s) Loss: 0.0378 \n",
            "Epoch: [3][305/306] Elapsed 1m 39s (remain 0m 0s) Loss: 0.0383 \n",
            "EVAL: [0/77] Elapsed 0m 0s (remain 0m 27s) Loss: 0.0295 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3 - avg_train_loss: 0.0383  avg_val_loss: 0.1146  time: 108s\n",
            "Epoch 3 - avg_train_loss: 0.0383  avg_val_loss: 0.1146  time: 108s\n",
            "Epoch 3 - avg_train_loss: 0.0383  avg_val_loss: 0.1146  time: 108s\n",
            "Epoch 3 - avg_train_loss: 0.0383  avg_val_loss: 0.1146  time: 108s\n",
            "Epoch 3 - Score: 0.11463940002605966\n",
            "Epoch 3 - Score: 0.11463940002605966\n",
            "Epoch 3 - Score: 0.11463940002605966\n",
            "Epoch 3 - Score: 0.11463940002605966\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EVAL: [76/77] Elapsed 0m 8s (remain 0m 0s) Loss: 0.1146 \n",
            "Log loss for 2 is 0.11463940002605966\n",
            "Epoch: [4][0/306] Elapsed 0m 0s (remain 3m 4s) Loss: 0.0176 \n",
            "Epoch: [4][100/306] Elapsed 0m 33s (remain 1m 7s) Loss: 0.0282 \n",
            "Epoch: [4][200/306] Elapsed 1m 5s (remain 0m 34s) Loss: 0.0253 \n",
            "Epoch: [4][300/306] Elapsed 1m 38s (remain 0m 1s) Loss: 0.0242 \n",
            "Epoch: [4][305/306] Elapsed 1m 39s (remain 0m 0s) Loss: 0.0255 \n",
            "EVAL: [0/77] Elapsed 0m 0s (remain 0m 26s) Loss: 0.0169 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4 - avg_train_loss: 0.0255  avg_val_loss: 0.1390  time: 108s\n",
            "Epoch 4 - avg_train_loss: 0.0255  avg_val_loss: 0.1390  time: 108s\n",
            "Epoch 4 - avg_train_loss: 0.0255  avg_val_loss: 0.1390  time: 108s\n",
            "Epoch 4 - avg_train_loss: 0.0255  avg_val_loss: 0.1390  time: 108s\n",
            "Epoch 4 - Score: 0.13896953380421317\n",
            "Epoch 4 - Score: 0.13896953380421317\n",
            "Epoch 4 - Score: 0.13896953380421317\n",
            "Epoch 4 - Score: 0.13896953380421317\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EVAL: [76/77] Elapsed 0m 8s (remain 0m 0s) Loss: 0.1390 \n",
            "Log loss for 3 is 0.13896953380421317\n",
            "Epoch: [5][0/306] Elapsed 0m 0s (remain 3m 8s) Loss: 0.0041 \n",
            "Epoch: [5][100/306] Elapsed 0m 33s (remain 1m 7s) Loss: 0.0256 \n",
            "Epoch: [5][200/306] Elapsed 1m 5s (remain 0m 34s) Loss: 0.0168 \n",
            "Epoch: [5][300/306] Elapsed 1m 38s (remain 0m 1s) Loss: 0.0175 \n",
            "Epoch: [5][305/306] Elapsed 1m 39s (remain 0m 0s) Loss: 0.0178 \n",
            "EVAL: [0/77] Elapsed 0m 0s (remain 0m 26s) Loss: 0.2099 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5 - avg_train_loss: 0.0178  avg_val_loss: 0.1315  time: 108s\n",
            "Epoch 5 - avg_train_loss: 0.0178  avg_val_loss: 0.1315  time: 108s\n",
            "Epoch 5 - avg_train_loss: 0.0178  avg_val_loss: 0.1315  time: 108s\n",
            "Epoch 5 - avg_train_loss: 0.0178  avg_val_loss: 0.1315  time: 108s\n",
            "Epoch 5 - Score: 0.13151080534751594\n",
            "Epoch 5 - Score: 0.13151080534751594\n",
            "Epoch 5 - Score: 0.13151080534751594\n",
            "Epoch 5 - Score: 0.13151080534751594\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EVAL: [76/77] Elapsed 0m 8s (remain 0m 0s) Loss: 0.1315 \n",
            "Log loss for 4 is 0.13151080534751594\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "========== fold: 2 result ==========\n",
            "========== fold: 2 result ==========\n",
            "========== fold: 2 result ==========\n",
            "========== fold: 2 result ==========\n",
            "Score: 0.07853\n",
            "Score: 0.07853\n",
            "Score: 0.07853\n",
            "Score: 0.07853\n",
            "========== fold: 3 training ==========\n",
            "========== fold: 3 training ==========\n",
            "========== fold: 3 training ==========\n",
            "========== fold: 3 training ==========\n",
            "Didn't find file /content/activebus_bert_final/added_tokens.json. We won't load it.\n",
            "loading file /content/activebus_bert_final/vocab.txt\n",
            "loading file None\n",
            "loading file /content/activebus_bert_final/special_tokens_map.json\n",
            "loading file /content/activebus_bert_final/tokenizer_config.json\n",
            "loading file /content/activebus_bert_final/tokenizer.json\n",
            "Didn't find file /content/activebus_bert_final/added_tokens.json. We won't load it.\n",
            "loading file /content/activebus_bert_final/vocab.txt\n",
            "loading file None\n",
            "loading file /content/activebus_bert_final/special_tokens_map.json\n",
            "loading file /content/activebus_bert_final/tokenizer_config.json\n",
            "loading file /content/activebus_bert_final/tokenizer.json\n",
            "loading configuration file /content/activebus_bert_final/config.json\n",
            "Model config BertConfig {\n",
            "  \"_name_or_path\": \"activebus/BERT-XD_Review\",\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_ids\": 0,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.15.0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "loading weights file /content/activebus_bert_final/pytorch_model.bin\n",
            "Some weights of the model checkpoint at /content/activebus_bert_final were not used when initializing BertForSequenceClassification: ['cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /content/activebus_bert_final and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: [1][0/306] Elapsed 0m 0s (remain 3m 25s) Loss: 0.7038 \n",
            "Epoch: [1][100/306] Elapsed 0m 33s (remain 1m 7s) Loss: 0.2598 \n",
            "Epoch: [1][200/306] Elapsed 1m 5s (remain 0m 34s) Loss: 0.1801 \n",
            "Epoch: [1][300/306] Elapsed 1m 38s (remain 0m 1s) Loss: 0.1517 \n",
            "Epoch: [1][305/306] Elapsed 1m 39s (remain 0m 0s) Loss: 0.1500 \n",
            "EVAL: [0/77] Elapsed 0m 0s (remain 0m 27s) Loss: 0.0105 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1 - avg_train_loss: 0.1500  avg_val_loss: 0.0742  time: 108s\n",
            "Epoch 1 - avg_train_loss: 0.1500  avg_val_loss: 0.0742  time: 108s\n",
            "Epoch 1 - avg_train_loss: 0.1500  avg_val_loss: 0.0742  time: 108s\n",
            "Epoch 1 - avg_train_loss: 0.1500  avg_val_loss: 0.0742  time: 108s\n",
            "Epoch 1 - Score: 0.07417681355472615\n",
            "Epoch 1 - Score: 0.07417681355472615\n",
            "Epoch 1 - Score: 0.07417681355472615\n",
            "Epoch 1 - Score: 0.07417681355472615\n",
            "Epoch 1 - Save Best Score: 0.0742 Model\n",
            "Epoch 1 - Save Best Score: 0.0742 Model\n",
            "Epoch 1 - Save Best Score: 0.0742 Model\n",
            "Epoch 1 - Save Best Score: 0.0742 Model\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EVAL: [76/77] Elapsed 0m 8s (remain 0m 0s) Loss: 0.0742 \n",
            "Log loss for 0 is 0.07417681355472615\n",
            "Epoch: [2][0/306] Elapsed 0m 0s (remain 2m 59s) Loss: 0.0215 \n",
            "Epoch: [2][100/306] Elapsed 0m 33s (remain 1m 7s) Loss: 0.0676 \n",
            "Epoch: [2][200/306] Elapsed 1m 5s (remain 0m 34s) Loss: 0.0685 \n",
            "Epoch: [2][300/306] Elapsed 1m 38s (remain 0m 1s) Loss: 0.0680 \n",
            "Epoch: [2][305/306] Elapsed 1m 39s (remain 0m 0s) Loss: 0.0672 \n",
            "EVAL: [0/77] Elapsed 0m 0s (remain 0m 27s) Loss: 0.0102 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2 - avg_train_loss: 0.0672  avg_val_loss: 0.0707  time: 108s\n",
            "Epoch 2 - avg_train_loss: 0.0672  avg_val_loss: 0.0707  time: 108s\n",
            "Epoch 2 - avg_train_loss: 0.0672  avg_val_loss: 0.0707  time: 108s\n",
            "Epoch 2 - avg_train_loss: 0.0672  avg_val_loss: 0.0707  time: 108s\n",
            "Epoch 2 - Score: 0.07068706880887733\n",
            "Epoch 2 - Score: 0.07068706880887733\n",
            "Epoch 2 - Score: 0.07068706880887733\n",
            "Epoch 2 - Score: 0.07068706880887733\n",
            "Epoch 2 - Save Best Score: 0.0707 Model\n",
            "Epoch 2 - Save Best Score: 0.0707 Model\n",
            "Epoch 2 - Save Best Score: 0.0707 Model\n",
            "Epoch 2 - Save Best Score: 0.0707 Model\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EVAL: [76/77] Elapsed 0m 8s (remain 0m 0s) Loss: 0.0707 \n",
            "Log loss for 1 is 0.07068706880887733\n",
            "Epoch: [3][0/306] Elapsed 0m 0s (remain 2m 56s) Loss: 0.1048 \n",
            "Epoch: [3][100/306] Elapsed 0m 33s (remain 1m 7s) Loss: 0.0477 \n",
            "Epoch: [3][200/306] Elapsed 1m 5s (remain 0m 34s) Loss: 0.0430 \n",
            "Epoch: [3][300/306] Elapsed 1m 38s (remain 0m 1s) Loss: 0.0419 \n",
            "Epoch: [3][305/306] Elapsed 1m 39s (remain 0m 0s) Loss: 0.0418 \n",
            "EVAL: [0/77] Elapsed 0m 0s (remain 0m 26s) Loss: 0.0160 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3 - avg_train_loss: 0.0418  avg_val_loss: 0.0747  time: 108s\n",
            "Epoch 3 - avg_train_loss: 0.0418  avg_val_loss: 0.0747  time: 108s\n",
            "Epoch 3 - avg_train_loss: 0.0418  avg_val_loss: 0.0747  time: 108s\n",
            "Epoch 3 - avg_train_loss: 0.0418  avg_val_loss: 0.0747  time: 108s\n",
            "Epoch 3 - Score: 0.07465707584828467\n",
            "Epoch 3 - Score: 0.07465707584828467\n",
            "Epoch 3 - Score: 0.07465707584828467\n",
            "Epoch 3 - Score: 0.07465707584828467\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EVAL: [76/77] Elapsed 0m 8s (remain 0m 0s) Loss: 0.0747 \n",
            "Log loss for 2 is 0.07465707584828467\n",
            "Epoch: [4][0/306] Elapsed 0m 0s (remain 3m 2s) Loss: 0.0992 \n",
            "Epoch: [4][100/306] Elapsed 0m 33s (remain 1m 7s) Loss: 0.0206 \n",
            "Epoch: [4][200/306] Elapsed 1m 5s (remain 0m 34s) Loss: 0.0269 \n",
            "Epoch: [4][300/306] Elapsed 1m 38s (remain 0m 1s) Loss: 0.0292 \n",
            "Epoch: [4][305/306] Elapsed 1m 39s (remain 0m 0s) Loss: 0.0292 \n",
            "EVAL: [0/77] Elapsed 0m 0s (remain 0m 27s) Loss: 0.0028 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4 - avg_train_loss: 0.0292  avg_val_loss: 0.0986  time: 108s\n",
            "Epoch 4 - avg_train_loss: 0.0292  avg_val_loss: 0.0986  time: 108s\n",
            "Epoch 4 - avg_train_loss: 0.0292  avg_val_loss: 0.0986  time: 108s\n",
            "Epoch 4 - avg_train_loss: 0.0292  avg_val_loss: 0.0986  time: 108s\n",
            "Epoch 4 - Score: 0.09856363069018449\n",
            "Epoch 4 - Score: 0.09856363069018449\n",
            "Epoch 4 - Score: 0.09856363069018449\n",
            "Epoch 4 - Score: 0.09856363069018449\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EVAL: [76/77] Elapsed 0m 8s (remain 0m 0s) Loss: 0.0986 \n",
            "Log loss for 3 is 0.09856363069018449\n",
            "Epoch: [5][0/306] Elapsed 0m 0s (remain 3m 4s) Loss: 0.0040 \n",
            "Epoch: [5][100/306] Elapsed 0m 33s (remain 1m 7s) Loss: 0.0262 \n",
            "Epoch: [5][200/306] Elapsed 1m 5s (remain 0m 34s) Loss: 0.0217 \n",
            "Epoch: [5][300/306] Elapsed 1m 38s (remain 0m 1s) Loss: 0.0231 \n",
            "Epoch: [5][305/306] Elapsed 1m 39s (remain 0m 0s) Loss: 0.0236 \n",
            "EVAL: [0/77] Elapsed 0m 0s (remain 0m 26s) Loss: 0.0339 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5 - avg_train_loss: 0.0236  avg_val_loss: 0.1067  time: 108s\n",
            "Epoch 5 - avg_train_loss: 0.0236  avg_val_loss: 0.1067  time: 108s\n",
            "Epoch 5 - avg_train_loss: 0.0236  avg_val_loss: 0.1067  time: 108s\n",
            "Epoch 5 - avg_train_loss: 0.0236  avg_val_loss: 0.1067  time: 108s\n",
            "Epoch 5 - Score: 0.10672907421731893\n",
            "Epoch 5 - Score: 0.10672907421731893\n",
            "Epoch 5 - Score: 0.10672907421731893\n",
            "Epoch 5 - Score: 0.10672907421731893\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EVAL: [76/77] Elapsed 0m 8s (remain 0m 0s) Loss: 0.1067 \n",
            "Log loss for 4 is 0.10672907421731893\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "========== fold: 3 result ==========\n",
            "========== fold: 3 result ==========\n",
            "========== fold: 3 result ==========\n",
            "========== fold: 3 result ==========\n",
            "Score: 0.07069\n",
            "Score: 0.07069\n",
            "Score: 0.07069\n",
            "Score: 0.07069\n",
            "========== fold: 4 training ==========\n",
            "========== fold: 4 training ==========\n",
            "========== fold: 4 training ==========\n",
            "========== fold: 4 training ==========\n",
            "Didn't find file /content/activebus_bert_final/added_tokens.json. We won't load it.\n",
            "loading file /content/activebus_bert_final/vocab.txt\n",
            "loading file None\n",
            "loading file /content/activebus_bert_final/special_tokens_map.json\n",
            "loading file /content/activebus_bert_final/tokenizer_config.json\n",
            "loading file /content/activebus_bert_final/tokenizer.json\n",
            "Didn't find file /content/activebus_bert_final/added_tokens.json. We won't load it.\n",
            "loading file /content/activebus_bert_final/vocab.txt\n",
            "loading file None\n",
            "loading file /content/activebus_bert_final/special_tokens_map.json\n",
            "loading file /content/activebus_bert_final/tokenizer_config.json\n",
            "loading file /content/activebus_bert_final/tokenizer.json\n",
            "loading configuration file /content/activebus_bert_final/config.json\n",
            "Model config BertConfig {\n",
            "  \"_name_or_path\": \"activebus/BERT-XD_Review\",\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_ids\": 0,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.15.0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "loading weights file /content/activebus_bert_final/pytorch_model.bin\n",
            "Some weights of the model checkpoint at /content/activebus_bert_final were not used when initializing BertForSequenceClassification: ['cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /content/activebus_bert_final and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: [1][0/306] Elapsed 0m 0s (remain 2m 58s) Loss: 0.6309 \n",
            "Epoch: [1][100/306] Elapsed 0m 33s (remain 1m 7s) Loss: 0.2608 \n",
            "Epoch: [1][200/306] Elapsed 1m 5s (remain 0m 34s) Loss: 0.1870 \n",
            "Epoch: [1][300/306] Elapsed 1m 38s (remain 0m 1s) Loss: 0.1582 \n",
            "Epoch: [1][305/306] Elapsed 1m 39s (remain 0m 0s) Loss: 0.1561 \n",
            "EVAL: [0/77] Elapsed 0m 0s (remain 0m 29s) Loss: 0.0056 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1 - avg_train_loss: 0.1561  avg_val_loss: 0.1198  time: 108s\n",
            "Epoch 1 - avg_train_loss: 0.1561  avg_val_loss: 0.1198  time: 108s\n",
            "Epoch 1 - avg_train_loss: 0.1561  avg_val_loss: 0.1198  time: 108s\n",
            "Epoch 1 - avg_train_loss: 0.1561  avg_val_loss: 0.1198  time: 108s\n",
            "Epoch 1 - Score: 0.11979490747548542\n",
            "Epoch 1 - Score: 0.11979490747548542\n",
            "Epoch 1 - Score: 0.11979490747548542\n",
            "Epoch 1 - Score: 0.11979490747548542\n",
            "Epoch 1 - Save Best Score: 0.1198 Model\n",
            "Epoch 1 - Save Best Score: 0.1198 Model\n",
            "Epoch 1 - Save Best Score: 0.1198 Model\n",
            "Epoch 1 - Save Best Score: 0.1198 Model\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EVAL: [76/77] Elapsed 0m 8s (remain 0m 0s) Loss: 0.1198 \n",
            "Log loss for 0 is 0.11979490747548542\n",
            "Epoch: [2][0/306] Elapsed 0m 0s (remain 2m 58s) Loss: 0.0080 \n",
            "Epoch: [2][100/306] Elapsed 0m 33s (remain 1m 7s) Loss: 0.0763 \n",
            "Epoch: [2][200/306] Elapsed 1m 5s (remain 0m 34s) Loss: 0.0720 \n",
            "Epoch: [2][300/306] Elapsed 1m 37s (remain 0m 1s) Loss: 0.0700 \n",
            "Epoch: [2][305/306] Elapsed 1m 39s (remain 0m 0s) Loss: 0.0708 \n",
            "EVAL: [0/77] Elapsed 0m 0s (remain 0m 26s) Loss: 0.0079 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2 - avg_train_loss: 0.0708  avg_val_loss: 0.0812  time: 108s\n",
            "Epoch 2 - avg_train_loss: 0.0708  avg_val_loss: 0.0812  time: 108s\n",
            "Epoch 2 - avg_train_loss: 0.0708  avg_val_loss: 0.0812  time: 108s\n",
            "Epoch 2 - avg_train_loss: 0.0708  avg_val_loss: 0.0812  time: 108s\n",
            "Epoch 2 - Score: 0.08124755986832605\n",
            "Epoch 2 - Score: 0.08124755986832605\n",
            "Epoch 2 - Score: 0.08124755986832605\n",
            "Epoch 2 - Score: 0.08124755986832605\n",
            "Epoch 2 - Save Best Score: 0.0812 Model\n",
            "Epoch 2 - Save Best Score: 0.0812 Model\n",
            "Epoch 2 - Save Best Score: 0.0812 Model\n",
            "Epoch 2 - Save Best Score: 0.0812 Model\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EVAL: [76/77] Elapsed 0m 8s (remain 0m 0s) Loss: 0.0812 \n",
            "Log loss for 1 is 0.08124755986832605\n",
            "Epoch: [3][0/306] Elapsed 0m 0s (remain 2m 54s) Loss: 0.0364 \n",
            "Epoch: [3][100/306] Elapsed 0m 33s (remain 1m 7s) Loss: 0.0429 \n",
            "Epoch: [3][200/306] Elapsed 1m 5s (remain 0m 34s) Loss: 0.0407 \n",
            "Epoch: [3][300/306] Elapsed 1m 38s (remain 0m 1s) Loss: 0.0368 \n",
            "Epoch: [3][305/306] Elapsed 1m 39s (remain 0m 0s) Loss: 0.0365 \n",
            "EVAL: [0/77] Elapsed 0m 0s (remain 0m 27s) Loss: 0.0017 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3 - avg_train_loss: 0.0365  avg_val_loss: 0.1033  time: 108s\n",
            "Epoch 3 - avg_train_loss: 0.0365  avg_val_loss: 0.1033  time: 108s\n",
            "Epoch 3 - avg_train_loss: 0.0365  avg_val_loss: 0.1033  time: 108s\n",
            "Epoch 3 - avg_train_loss: 0.0365  avg_val_loss: 0.1033  time: 108s\n",
            "Epoch 3 - Score: 0.10333259935677128\n",
            "Epoch 3 - Score: 0.10333259935677128\n",
            "Epoch 3 - Score: 0.10333259935677128\n",
            "Epoch 3 - Score: 0.10333259935677128\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EVAL: [76/77] Elapsed 0m 8s (remain 0m 0s) Loss: 0.1033 \n",
            "Log loss for 2 is 0.10333259935677128\n",
            "Epoch: [4][0/306] Elapsed 0m 0s (remain 2m 56s) Loss: 0.0077 \n",
            "Epoch: [4][100/306] Elapsed 0m 33s (remain 1m 7s) Loss: 0.0210 \n",
            "Epoch: [4][200/306] Elapsed 1m 5s (remain 0m 34s) Loss: 0.0257 \n",
            "Epoch: [4][300/306] Elapsed 1m 38s (remain 0m 1s) Loss: 0.0255 \n",
            "Epoch: [4][305/306] Elapsed 1m 39s (remain 0m 0s) Loss: 0.0252 \n",
            "EVAL: [0/77] Elapsed 0m 0s (remain 0m 29s) Loss: 0.0009 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4 - avg_train_loss: 0.0252  avg_val_loss: 0.1362  time: 108s\n",
            "Epoch 4 - avg_train_loss: 0.0252  avg_val_loss: 0.1362  time: 108s\n",
            "Epoch 4 - avg_train_loss: 0.0252  avg_val_loss: 0.1362  time: 108s\n",
            "Epoch 4 - avg_train_loss: 0.0252  avg_val_loss: 0.1362  time: 108s\n",
            "Epoch 4 - Score: 0.13621439002348967\n",
            "Epoch 4 - Score: 0.13621439002348967\n",
            "Epoch 4 - Score: 0.13621439002348967\n",
            "Epoch 4 - Score: 0.13621439002348967\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EVAL: [76/77] Elapsed 0m 8s (remain 0m 0s) Loss: 0.1362 \n",
            "Log loss for 3 is 0.13621439002348967\n",
            "Epoch: [5][0/306] Elapsed 0m 0s (remain 3m 8s) Loss: 0.0019 \n",
            "Epoch: [5][100/306] Elapsed 0m 33s (remain 1m 7s) Loss: 0.0323 \n",
            "Epoch: [5][200/306] Elapsed 1m 5s (remain 0m 34s) Loss: 0.0239 \n",
            "Epoch: [5][300/306] Elapsed 1m 38s (remain 0m 1s) Loss: 0.0259 \n",
            "Epoch: [5][305/306] Elapsed 1m 39s (remain 0m 0s) Loss: 0.0257 \n",
            "EVAL: [0/77] Elapsed 0m 0s (remain 0m 27s) Loss: 0.0012 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5 - avg_train_loss: 0.0257  avg_val_loss: 0.1014  time: 108s\n",
            "Epoch 5 - avg_train_loss: 0.0257  avg_val_loss: 0.1014  time: 108s\n",
            "Epoch 5 - avg_train_loss: 0.0257  avg_val_loss: 0.1014  time: 108s\n",
            "Epoch 5 - avg_train_loss: 0.0257  avg_val_loss: 0.1014  time: 108s\n",
            "Epoch 5 - Score: 0.10141972574068935\n",
            "Epoch 5 - Score: 0.10141972574068935\n",
            "Epoch 5 - Score: 0.10141972574068935\n",
            "Epoch 5 - Score: 0.10141972574068935\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EVAL: [76/77] Elapsed 0m 8s (remain 0m 0s) Loss: 0.1014 \n",
            "Log loss for 4 is 0.10141972574068935\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "========== fold: 4 result ==========\n",
            "========== fold: 4 result ==========\n",
            "========== fold: 4 result ==========\n",
            "========== fold: 4 result ==========\n",
            "Score: 0.08125\n",
            "Score: 0.08125\n",
            "Score: 0.08125\n",
            "Score: 0.08125\n",
            "========== CV ==========\n",
            "========== CV ==========\n",
            "========== CV ==========\n",
            "========== CV ==========\n",
            "Score: 0.07927\n",
            "Score: 0.07927\n",
            "Score: 0.07927\n",
            "Score: 0.07927\n",
            "Didn't find file /content/activebus_bert_final/added_tokens.json. We won't load it.\n",
            "loading file /content/activebus_bert_final/vocab.txt\n",
            "loading file None\n",
            "loading file /content/activebus_bert_final/special_tokens_map.json\n",
            "loading file /content/activebus_bert_final/tokenizer_config.json\n",
            "loading file /content/activebus_bert_final/tokenizer.json\n",
            "========== model: bert-base-uncased fold: 0 inference ==========\n",
            "========== model: bert-base-uncased fold: 0 inference ==========\n",
            "========== model: bert-base-uncased fold: 0 inference ==========\n",
            "========== model: bert-base-uncased fold: 0 inference ==========\n",
            "loading configuration file /content/activebus_bert_final/config.json\n",
            "Model config BertConfig {\n",
            "  \"_name_or_path\": \"activebus/BERT-XD_Review\",\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_ids\": 0,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.15.0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "loading weights file /content/activebus_bert_final/pytorch_model.bin\n",
            "Some weights of the model checkpoint at /content/activebus_bert_final were not used when initializing BertForSequenceClassification: ['cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /content/activebus_bert_final and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "\n",
            "  0%|          | 0/165 [00:00<?, ?it/s]\u001b[A\n",
            "  1%|          | 1/165 [00:00<00:20,  7.92it/s]\u001b[A\n",
            "  1%|          | 2/165 [00:00<00:19,  8.25it/s]\u001b[A\n",
            "  2%|▏         | 3/165 [00:00<00:18,  8.58it/s]\u001b[A\n",
            "  2%|▏         | 4/165 [00:00<00:18,  8.78it/s]\u001b[A\n",
            "  3%|▎         | 5/165 [00:00<00:18,  8.87it/s]\u001b[A\n",
            "  4%|▎         | 6/165 [00:00<00:17,  8.97it/s]\u001b[A\n",
            "  4%|▍         | 7/165 [00:00<00:17,  9.00it/s]\u001b[A\n",
            "  5%|▍         | 8/165 [00:00<00:17,  9.01it/s]\u001b[A\n",
            "  5%|▌         | 9/165 [00:01<00:17,  9.01it/s]\u001b[A\n",
            "  6%|▌         | 10/165 [00:01<00:17,  9.04it/s]\u001b[A\n",
            "  7%|▋         | 11/165 [00:01<00:17,  9.05it/s]\u001b[A\n",
            "  7%|▋         | 12/165 [00:01<00:16,  9.04it/s]\u001b[A\n",
            "  8%|▊         | 13/165 [00:01<00:16,  9.06it/s]\u001b[A\n",
            "  8%|▊         | 14/165 [00:01<00:16,  9.09it/s]\u001b[A\n",
            "  9%|▉         | 15/165 [00:01<00:16,  9.09it/s]\u001b[A\n",
            " 10%|▉         | 16/165 [00:01<00:16,  9.10it/s]\u001b[A\n",
            " 10%|█         | 17/165 [00:01<00:16,  9.12it/s]\u001b[A\n",
            " 11%|█         | 18/165 [00:02<00:16,  9.14it/s]\u001b[A\n",
            " 12%|█▏        | 19/165 [00:02<00:16,  9.11it/s]\u001b[A\n",
            " 12%|█▏        | 20/165 [00:02<00:15,  9.13it/s]\u001b[A\n",
            " 13%|█▎        | 21/165 [00:02<00:15,  9.14it/s]\u001b[A\n",
            " 13%|█▎        | 22/165 [00:02<00:15,  9.14it/s]\u001b[A\n",
            " 14%|█▍        | 23/165 [00:02<00:15,  9.14it/s]\u001b[A\n",
            " 15%|█▍        | 24/165 [00:02<00:15,  9.14it/s]\u001b[A\n",
            " 15%|█▌        | 25/165 [00:02<00:15,  9.13it/s]\u001b[A\n",
            " 16%|█▌        | 26/165 [00:02<00:15,  9.12it/s]\u001b[A\n",
            " 16%|█▋        | 27/165 [00:02<00:15,  9.09it/s]\u001b[A\n",
            " 17%|█▋        | 28/165 [00:03<00:15,  9.11it/s]\u001b[A\n",
            " 18%|█▊        | 29/165 [00:03<00:14,  9.08it/s]\u001b[A\n",
            " 18%|█▊        | 30/165 [00:03<00:14,  9.09it/s]\u001b[A\n",
            " 19%|█▉        | 31/165 [00:03<00:14,  9.10it/s]\u001b[A\n",
            " 19%|█▉        | 32/165 [00:03<00:14,  9.10it/s]\u001b[A\n",
            " 20%|██        | 33/165 [00:03<00:14,  9.10it/s]\u001b[A\n",
            " 21%|██        | 34/165 [00:03<00:14,  9.10it/s]\u001b[A\n",
            " 21%|██        | 35/165 [00:03<00:14,  9.12it/s]\u001b[A\n",
            " 22%|██▏       | 36/165 [00:03<00:14,  9.11it/s]\u001b[A\n",
            " 22%|██▏       | 37/165 [00:04<00:14,  9.11it/s]\u001b[A\n",
            " 23%|██▎       | 38/165 [00:04<00:13,  9.13it/s]\u001b[A\n",
            " 24%|██▎       | 39/165 [00:04<00:13,  9.12it/s]\u001b[A\n",
            " 24%|██▍       | 40/165 [00:04<00:13,  9.10it/s]\u001b[A\n",
            " 25%|██▍       | 41/165 [00:04<00:13,  9.11it/s]\u001b[A\n",
            " 25%|██▌       | 42/165 [00:04<00:13,  9.12it/s]\u001b[A\n",
            " 26%|██▌       | 43/165 [00:04<00:13,  9.13it/s]\u001b[A\n",
            " 27%|██▋       | 44/165 [00:04<00:13,  9.12it/s]\u001b[A\n",
            " 27%|██▋       | 45/165 [00:04<00:13,  9.13it/s]\u001b[A\n",
            " 28%|██▊       | 46/165 [00:05<00:13,  9.12it/s]\u001b[A\n",
            " 28%|██▊       | 47/165 [00:05<00:12,  9.12it/s]\u001b[A\n",
            " 29%|██▉       | 48/165 [00:05<00:12,  9.12it/s]\u001b[A\n",
            " 30%|██▉       | 49/165 [00:05<00:12,  9.12it/s]\u001b[A\n",
            " 30%|███       | 50/165 [00:05<00:12,  9.11it/s]\u001b[A\n",
            " 31%|███       | 51/165 [00:05<00:12,  9.12it/s]\u001b[A\n",
            " 32%|███▏      | 52/165 [00:05<00:12,  9.10it/s]\u001b[A\n",
            " 32%|███▏      | 53/165 [00:05<00:12,  9.11it/s]\u001b[A\n",
            " 33%|███▎      | 54/165 [00:05<00:12,  9.11it/s]\u001b[A\n",
            " 33%|███▎      | 55/165 [00:06<00:12,  9.12it/s]\u001b[A\n",
            " 34%|███▍      | 56/165 [00:06<00:11,  9.13it/s]\u001b[A\n",
            " 35%|███▍      | 57/165 [00:06<00:11,  9.12it/s]\u001b[A\n",
            " 35%|███▌      | 58/165 [00:06<00:11,  9.12it/s]\u001b[A\n",
            " 36%|███▌      | 59/165 [00:06<00:11,  9.12it/s]\u001b[A\n",
            " 36%|███▋      | 60/165 [00:06<00:11,  9.11it/s]\u001b[A\n",
            " 37%|███▋      | 61/165 [00:06<00:11,  9.12it/s]\u001b[A\n",
            " 38%|███▊      | 62/165 [00:06<00:11,  9.12it/s]\u001b[A\n",
            " 38%|███▊      | 63/165 [00:06<00:11,  9.12it/s]\u001b[A\n",
            " 39%|███▉      | 64/165 [00:07<00:11,  9.13it/s]\u001b[A\n",
            " 39%|███▉      | 65/165 [00:07<00:10,  9.14it/s]\u001b[A\n",
            " 40%|████      | 66/165 [00:07<00:10,  9.12it/s]\u001b[A\n",
            " 41%|████      | 67/165 [00:07<00:10,  9.11it/s]\u001b[A\n",
            " 41%|████      | 68/165 [00:07<00:10,  9.13it/s]\u001b[A\n",
            " 42%|████▏     | 69/165 [00:07<00:10,  9.12it/s]\u001b[A\n",
            " 42%|████▏     | 70/165 [00:07<00:10,  9.14it/s]\u001b[A\n",
            " 43%|████▎     | 71/165 [00:07<00:10,  9.14it/s]\u001b[A\n",
            " 44%|████▎     | 72/165 [00:07<00:10,  9.14it/s]\u001b[A\n",
            " 44%|████▍     | 73/165 [00:08<00:10,  9.14it/s]\u001b[A\n",
            " 45%|████▍     | 74/165 [00:08<00:09,  9.15it/s]\u001b[A\n",
            " 45%|████▌     | 75/165 [00:08<00:09,  9.14it/s]\u001b[A\n",
            " 46%|████▌     | 76/165 [00:08<00:09,  9.11it/s]\u001b[A\n",
            " 47%|████▋     | 77/165 [00:08<00:09,  9.11it/s]\u001b[A\n",
            " 47%|████▋     | 78/165 [00:08<00:09,  9.10it/s]\u001b[A\n",
            " 48%|████▊     | 79/165 [00:08<00:09,  9.13it/s]\u001b[A\n",
            " 48%|████▊     | 80/165 [00:08<00:09,  9.12it/s]\u001b[A\n",
            " 49%|████▉     | 81/165 [00:08<00:09,  9.12it/s]\u001b[A\n",
            " 50%|████▉     | 82/165 [00:09<00:09,  9.12it/s]\u001b[A\n",
            " 50%|█████     | 83/165 [00:09<00:08,  9.13it/s]\u001b[A\n",
            " 51%|█████     | 84/165 [00:09<00:08,  9.16it/s]\u001b[A\n",
            " 52%|█████▏    | 85/165 [00:09<00:08,  9.15it/s]\u001b[A\n",
            " 52%|█████▏    | 86/165 [00:09<00:08,  9.15it/s]\u001b[A\n",
            " 53%|█████▎    | 87/165 [00:09<00:08,  9.11it/s]\u001b[A\n",
            " 53%|█████▎    | 88/165 [00:09<00:08,  9.10it/s]\u001b[A\n",
            " 54%|█████▍    | 89/165 [00:09<00:08,  9.10it/s]\u001b[A\n",
            " 55%|█████▍    | 90/165 [00:09<00:08,  9.09it/s]\u001b[A\n",
            " 55%|█████▌    | 91/165 [00:10<00:08,  9.07it/s]\u001b[A\n",
            " 56%|█████▌    | 92/165 [00:10<00:08,  9.08it/s]\u001b[A\n",
            " 56%|█████▋    | 93/165 [00:10<00:07,  9.08it/s]\u001b[A\n",
            " 57%|█████▋    | 94/165 [00:10<00:07,  9.06it/s]\u001b[A\n",
            " 58%|█████▊    | 95/165 [00:10<00:07,  9.05it/s]\u001b[A\n",
            " 58%|█████▊    | 96/165 [00:10<00:07,  9.06it/s]\u001b[A\n",
            " 59%|█████▉    | 97/165 [00:10<00:07,  9.06it/s]\u001b[A\n",
            " 59%|█████▉    | 98/165 [00:10<00:07,  9.08it/s]\u001b[A\n",
            " 60%|██████    | 99/165 [00:10<00:07,  9.09it/s]\u001b[A\n",
            " 61%|██████    | 100/165 [00:11<00:07,  9.10it/s]\u001b[A\n",
            " 61%|██████    | 101/165 [00:11<00:07,  9.09it/s]\u001b[A\n",
            " 62%|██████▏   | 102/165 [00:11<00:06,  9.10it/s]\u001b[A\n",
            " 62%|██████▏   | 103/165 [00:11<00:06,  9.11it/s]\u001b[A\n",
            " 63%|██████▎   | 104/165 [00:11<00:06,  9.13it/s]\u001b[A\n",
            " 64%|██████▎   | 105/165 [00:11<00:06,  9.12it/s]\u001b[A\n",
            " 64%|██████▍   | 106/165 [00:11<00:06,  9.12it/s]\u001b[A\n",
            " 65%|██████▍   | 107/165 [00:11<00:06,  9.09it/s]\u001b[A\n",
            " 65%|██████▌   | 108/165 [00:11<00:06,  9.08it/s]\u001b[A\n",
            " 66%|██████▌   | 109/165 [00:11<00:06,  9.09it/s]\u001b[A\n",
            " 67%|██████▋   | 110/165 [00:12<00:06,  9.11it/s]\u001b[A\n",
            " 67%|██████▋   | 111/165 [00:12<00:05,  9.12it/s]\u001b[A\n",
            " 68%|██████▊   | 112/165 [00:12<00:05,  9.14it/s]\u001b[A\n",
            " 68%|██████▊   | 113/165 [00:12<00:05,  9.12it/s]\u001b[A\n",
            " 69%|██████▉   | 114/165 [00:12<00:05,  9.10it/s]\u001b[A\n",
            " 70%|██████▉   | 115/165 [00:12<00:05,  9.11it/s]\u001b[A\n",
            " 70%|███████   | 116/165 [00:12<00:05,  9.12it/s]\u001b[A\n",
            " 71%|███████   | 117/165 [00:12<00:05,  9.12it/s]\u001b[A\n",
            " 72%|███████▏  | 118/165 [00:12<00:05,  9.10it/s]\u001b[A\n",
            " 72%|███████▏  | 119/165 [00:13<00:05,  9.10it/s]\u001b[A\n",
            " 73%|███████▎  | 120/165 [00:13<00:04,  9.07it/s]\u001b[A\n",
            " 73%|███████▎  | 121/165 [00:13<00:04,  9.06it/s]\u001b[A\n",
            " 74%|███████▍  | 122/165 [00:13<00:04,  9.04it/s]\u001b[A\n",
            " 75%|███████▍  | 123/165 [00:13<00:04,  9.07it/s]\u001b[A\n",
            " 75%|███████▌  | 124/165 [00:13<00:04,  9.07it/s]\u001b[A\n",
            " 76%|███████▌  | 125/165 [00:13<00:04,  9.07it/s]\u001b[A\n",
            " 76%|███████▋  | 126/165 [00:13<00:04,  9.06it/s]\u001b[A\n",
            " 77%|███████▋  | 127/165 [00:13<00:04,  9.07it/s]\u001b[A\n",
            " 78%|███████▊  | 128/165 [00:14<00:04,  9.04it/s]\u001b[A\n",
            " 78%|███████▊  | 129/165 [00:14<00:03,  9.03it/s]\u001b[A\n",
            " 79%|███████▉  | 130/165 [00:14<00:03,  9.05it/s]\u001b[A\n",
            " 79%|███████▉  | 131/165 [00:14<00:03,  9.06it/s]\u001b[A\n",
            " 80%|████████  | 132/165 [00:14<00:03,  9.05it/s]\u001b[A\n",
            " 81%|████████  | 133/165 [00:14<00:03,  9.06it/s]\u001b[A\n",
            " 81%|████████  | 134/165 [00:14<00:03,  9.09it/s]\u001b[A\n",
            " 82%|████████▏ | 135/165 [00:14<00:03,  9.09it/s]\u001b[A\n",
            " 82%|████████▏ | 136/165 [00:14<00:03,  9.11it/s]\u001b[A\n",
            " 83%|████████▎ | 137/165 [00:15<00:03,  9.12it/s]\u001b[A\n",
            " 84%|████████▎ | 138/165 [00:15<00:02,  9.12it/s]\u001b[A\n",
            " 84%|████████▍ | 139/165 [00:15<00:02,  9.13it/s]\u001b[A\n",
            " 85%|████████▍ | 140/165 [00:15<00:02,  9.11it/s]\u001b[A\n",
            " 85%|████████▌ | 141/165 [00:15<00:02,  9.12it/s]\u001b[A\n",
            " 86%|████████▌ | 142/165 [00:15<00:02,  9.11it/s]\u001b[A\n",
            " 87%|████████▋ | 143/165 [00:15<00:02,  9.10it/s]\u001b[A\n",
            " 87%|████████▋ | 144/165 [00:15<00:02,  9.08it/s]\u001b[A\n",
            " 88%|████████▊ | 145/165 [00:15<00:02,  9.09it/s]\u001b[A\n",
            " 88%|████████▊ | 146/165 [00:16<00:02,  9.10it/s]\u001b[A\n",
            " 89%|████████▉ | 147/165 [00:16<00:01,  9.09it/s]\u001b[A\n",
            " 90%|████████▉ | 148/165 [00:16<00:01,  9.11it/s]\u001b[A\n",
            " 90%|█████████ | 149/165 [00:16<00:01,  9.10it/s]\u001b[A\n",
            " 91%|█████████ | 150/165 [00:16<00:01,  9.10it/s]\u001b[A\n",
            " 92%|█████████▏| 151/165 [00:16<00:01,  9.08it/s]\u001b[A\n",
            " 92%|█████████▏| 152/165 [00:16<00:01,  9.07it/s]\u001b[A\n",
            " 93%|█████████▎| 153/165 [00:16<00:01,  9.09it/s]\u001b[A\n",
            " 93%|█████████▎| 154/165 [00:16<00:01,  9.09it/s]\u001b[A\n",
            " 94%|█████████▍| 155/165 [00:17<00:01,  9.08it/s]\u001b[A\n",
            " 95%|█████████▍| 156/165 [00:17<00:00,  9.10it/s]\u001b[A\n",
            " 95%|█████████▌| 157/165 [00:17<00:00,  9.12it/s]\u001b[A\n",
            " 96%|█████████▌| 158/165 [00:17<00:00,  9.13it/s]\u001b[A\n",
            " 96%|█████████▋| 159/165 [00:17<00:00,  9.11it/s]\u001b[A\n",
            " 97%|█████████▋| 160/165 [00:17<00:00,  9.12it/s]\u001b[A\n",
            " 98%|█████████▊| 161/165 [00:17<00:00,  9.11it/s]\u001b[A\n",
            " 98%|█████████▊| 162/165 [00:17<00:00,  9.14it/s]\u001b[A\n",
            " 99%|█████████▉| 163/165 [00:17<00:00,  9.15it/s]\u001b[A\n",
            "100%|██████████| 165/165 [00:18<00:00,  9.08it/s]\n",
            "========== model: bert-base-uncased fold: 1 inference ==========\n",
            "========== model: bert-base-uncased fold: 1 inference ==========\n",
            "========== model: bert-base-uncased fold: 1 inference ==========\n",
            "========== model: bert-base-uncased fold: 1 inference ==========\n",
            "loading configuration file /content/activebus_bert_final/config.json\n",
            "Model config BertConfig {\n",
            "  \"_name_or_path\": \"activebus/BERT-XD_Review\",\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_ids\": 0,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.15.0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "loading weights file /content/activebus_bert_final/pytorch_model.bin\n",
            "Some weights of the model checkpoint at /content/activebus_bert_final were not used when initializing BertForSequenceClassification: ['cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /content/activebus_bert_final and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "\n",
            "  0%|          | 0/165 [00:00<?, ?it/s]\u001b[A\n",
            "  1%|          | 1/165 [00:00<00:19,  8.58it/s]\u001b[A\n",
            "  1%|          | 2/165 [00:00<00:18,  8.88it/s]\u001b[A\n",
            "  2%|▏         | 3/165 [00:00<00:17,  9.00it/s]\u001b[A\n",
            "  2%|▏         | 4/165 [00:00<00:17,  9.02it/s]\u001b[A\n",
            "  3%|▎         | 5/165 [00:00<00:17,  9.06it/s]\u001b[A\n",
            "  4%|▎         | 6/165 [00:00<00:17,  9.08it/s]\u001b[A\n",
            "  4%|▍         | 7/165 [00:00<00:17,  9.08it/s]\u001b[A\n",
            "  5%|▍         | 8/165 [00:00<00:17,  9.10it/s]\u001b[A\n",
            "  5%|▌         | 9/165 [00:00<00:17,  9.10it/s]\u001b[A\n",
            "  6%|▌         | 10/165 [00:01<00:17,  9.10it/s]\u001b[A\n",
            "  7%|▋         | 11/165 [00:01<00:16,  9.11it/s]\u001b[A\n",
            "  7%|▋         | 12/165 [00:01<00:16,  9.11it/s]\u001b[A\n",
            "  8%|▊         | 13/165 [00:01<00:16,  9.11it/s]\u001b[A\n",
            "  8%|▊         | 14/165 [00:01<00:16,  9.10it/s]\u001b[A\n",
            "  9%|▉         | 15/165 [00:01<00:16,  9.11it/s]\u001b[A\n",
            " 10%|▉         | 16/165 [00:01<00:16,  9.11it/s]\u001b[A\n",
            " 10%|█         | 17/165 [00:01<00:16,  9.10it/s]\u001b[A\n",
            " 11%|█         | 18/165 [00:01<00:16,  9.11it/s]\u001b[A\n",
            " 12%|█▏        | 19/165 [00:02<00:16,  9.04it/s]\u001b[A\n",
            " 12%|█▏        | 20/165 [00:02<00:16,  9.05it/s]\u001b[A\n",
            " 13%|█▎        | 21/165 [00:02<00:15,  9.06it/s]\u001b[A\n",
            " 13%|█▎        | 22/165 [00:02<00:15,  9.08it/s]\u001b[A\n",
            " 14%|█▍        | 23/165 [00:02<00:15,  9.09it/s]\u001b[A\n",
            " 15%|█▍        | 24/165 [00:02<00:15,  9.07it/s]\u001b[A\n",
            " 15%|█▌        | 25/165 [00:02<00:15,  9.06it/s]\u001b[A\n",
            " 16%|█▌        | 26/165 [00:02<00:15,  9.06it/s]\u001b[A\n",
            " 16%|█▋        | 27/165 [00:02<00:15,  9.05it/s]\u001b[A\n",
            " 17%|█▋        | 28/165 [00:03<00:15,  9.09it/s]\u001b[A\n",
            " 18%|█▊        | 29/165 [00:03<00:14,  9.08it/s]\u001b[A\n",
            " 18%|█▊        | 30/165 [00:03<00:14,  9.09it/s]\u001b[A\n",
            " 19%|█▉        | 31/165 [00:03<00:14,  9.11it/s]\u001b[A\n",
            " 19%|█▉        | 32/165 [00:03<00:14,  9.11it/s]\u001b[A\n",
            " 20%|██        | 33/165 [00:03<00:14,  9.12it/s]\u001b[A\n",
            " 21%|██        | 34/165 [00:03<00:14,  9.10it/s]\u001b[A\n",
            " 21%|██        | 35/165 [00:03<00:14,  9.10it/s]\u001b[A\n",
            " 22%|██▏       | 36/165 [00:03<00:14,  9.12it/s]\u001b[A\n",
            " 22%|██▏       | 37/165 [00:04<00:17,  7.46it/s]\u001b[A\n",
            " 23%|██▎       | 38/165 [00:04<00:16,  7.89it/s]\u001b[A\n",
            " 24%|██▎       | 39/165 [00:04<00:15,  8.23it/s]\u001b[A\n",
            " 24%|██▍       | 40/165 [00:04<00:14,  8.46it/s]\u001b[A\n",
            " 25%|██▍       | 41/165 [00:04<00:14,  8.65it/s]\u001b[A\n",
            " 25%|██▌       | 42/165 [00:04<00:13,  8.79it/s]\u001b[A\n",
            " 26%|██▌       | 43/165 [00:04<00:13,  8.90it/s]\u001b[A\n",
            " 27%|██▋       | 44/165 [00:04<00:13,  8.97it/s]\u001b[A\n",
            " 27%|██▋       | 45/165 [00:05<00:13,  9.02it/s]\u001b[A\n",
            " 28%|██▊       | 46/165 [00:05<00:13,  9.03it/s]\u001b[A\n",
            " 28%|██▊       | 47/165 [00:05<00:13,  9.07it/s]\u001b[A\n",
            " 29%|██▉       | 48/165 [00:05<00:12,  9.08it/s]\u001b[A\n",
            " 30%|██▉       | 49/165 [00:05<00:12,  9.07it/s]\u001b[A\n",
            " 30%|███       | 50/165 [00:05<00:12,  9.06it/s]\u001b[A\n",
            " 31%|███       | 51/165 [00:05<00:12,  9.08it/s]\u001b[A\n",
            " 32%|███▏      | 52/165 [00:05<00:12,  9.08it/s]\u001b[A\n",
            " 32%|███▏      | 53/165 [00:05<00:12,  9.08it/s]\u001b[A\n",
            " 33%|███▎      | 54/165 [00:06<00:12,  9.08it/s]\u001b[A\n",
            " 33%|███▎      | 55/165 [00:06<00:12,  9.09it/s]\u001b[A\n",
            " 34%|███▍      | 56/165 [00:06<00:11,  9.11it/s]\u001b[A\n",
            " 35%|███▍      | 57/165 [00:06<00:11,  9.11it/s]\u001b[A\n",
            " 35%|███▌      | 58/165 [00:06<00:11,  9.09it/s]\u001b[A\n",
            " 36%|███▌      | 59/165 [00:06<00:11,  9.09it/s]\u001b[A\n",
            " 36%|███▋      | 60/165 [00:06<00:11,  9.11it/s]\u001b[A\n",
            " 37%|███▋      | 61/165 [00:06<00:11,  9.12it/s]\u001b[A\n",
            " 38%|███▊      | 62/165 [00:06<00:11,  9.13it/s]\u001b[A\n",
            " 38%|███▊      | 63/165 [00:07<00:11,  9.13it/s]\u001b[A\n",
            " 39%|███▉      | 64/165 [00:07<00:11,  9.15it/s]\u001b[A\n",
            " 39%|███▉      | 65/165 [00:07<00:10,  9.14it/s]\u001b[A\n",
            " 40%|████      | 66/165 [00:07<00:10,  9.14it/s]\u001b[A\n",
            " 41%|████      | 67/165 [00:07<00:10,  9.12it/s]\u001b[A\n",
            " 41%|████      | 68/165 [00:07<00:10,  9.11it/s]\u001b[A\n",
            " 42%|████▏     | 69/165 [00:07<00:10,  9.12it/s]\u001b[A\n",
            " 42%|████▏     | 70/165 [00:07<00:10,  9.13it/s]\u001b[A\n",
            " 43%|████▎     | 71/165 [00:07<00:10,  9.10it/s]\u001b[A\n",
            " 44%|████▎     | 72/165 [00:07<00:10,  9.10it/s]\u001b[A\n",
            " 44%|████▍     | 73/165 [00:08<00:10,  9.10it/s]\u001b[A\n",
            " 45%|████▍     | 74/165 [00:08<00:10,  9.08it/s]\u001b[A\n",
            " 45%|████▌     | 75/165 [00:08<00:09,  9.08it/s]\u001b[A\n",
            " 46%|████▌     | 76/165 [00:08<00:09,  9.09it/s]\u001b[A\n",
            " 47%|████▋     | 77/165 [00:08<00:09,  9.10it/s]\u001b[A\n",
            " 47%|████▋     | 78/165 [00:08<00:09,  9.08it/s]\u001b[A\n",
            " 48%|████▊     | 79/165 [00:08<00:09,  9.09it/s]\u001b[A\n",
            " 48%|████▊     | 80/165 [00:08<00:09,  9.11it/s]\u001b[A\n",
            " 49%|████▉     | 81/165 [00:08<00:09,  9.07it/s]\u001b[A\n",
            " 50%|████▉     | 82/165 [00:09<00:09,  9.09it/s]\u001b[A\n",
            " 50%|█████     | 83/165 [00:09<00:09,  9.10it/s]\u001b[A\n",
            " 51%|█████     | 84/165 [00:09<00:08,  9.12it/s]\u001b[A\n",
            " 52%|█████▏    | 85/165 [00:09<00:08,  9.10it/s]\u001b[A\n",
            " 52%|█████▏    | 86/165 [00:09<00:08,  9.10it/s]\u001b[A\n",
            " 53%|█████▎    | 87/165 [00:09<00:08,  9.10it/s]\u001b[A\n",
            " 53%|█████▎    | 88/165 [00:09<00:08,  9.09it/s]\u001b[A\n",
            " 54%|█████▍    | 89/165 [00:09<00:08,  9.09it/s]\u001b[A\n",
            " 55%|█████▍    | 90/165 [00:09<00:08,  9.11it/s]\u001b[A\n",
            " 55%|█████▌    | 91/165 [00:10<00:08,  9.12it/s]\u001b[A\n",
            " 56%|█████▌    | 92/165 [00:10<00:08,  9.08it/s]\u001b[A\n",
            " 56%|█████▋    | 93/165 [00:10<00:07,  9.06it/s]\u001b[A\n",
            " 57%|█████▋    | 94/165 [00:10<00:07,  9.04it/s]\u001b[A\n",
            " 58%|█████▊    | 95/165 [00:10<00:07,  9.03it/s]\u001b[A\n",
            " 58%|█████▊    | 96/165 [00:10<00:07,  9.05it/s]\u001b[A\n",
            " 59%|█████▉    | 97/165 [00:10<00:07,  9.08it/s]\u001b[A\n",
            " 59%|█████▉    | 98/165 [00:10<00:07,  9.09it/s]\u001b[A\n",
            " 60%|██████    | 99/165 [00:10<00:07,  9.09it/s]\u001b[A\n",
            " 61%|██████    | 100/165 [00:11<00:07,  9.10it/s]\u001b[A\n",
            " 61%|██████    | 101/165 [00:11<00:07,  9.09it/s]\u001b[A\n",
            " 62%|██████▏   | 102/165 [00:11<00:06,  9.09it/s]\u001b[A\n",
            " 62%|██████▏   | 103/165 [00:11<00:06,  9.07it/s]\u001b[A\n",
            " 63%|██████▎   | 104/165 [00:11<00:06,  9.09it/s]\u001b[A\n",
            " 64%|██████▎   | 105/165 [00:11<00:06,  9.10it/s]\u001b[A\n",
            " 64%|██████▍   | 106/165 [00:11<00:06,  9.11it/s]\u001b[A\n",
            " 65%|██████▍   | 107/165 [00:11<00:06,  9.08it/s]\u001b[A\n",
            " 65%|██████▌   | 108/165 [00:11<00:06,  9.09it/s]\u001b[A\n",
            " 66%|██████▌   | 109/165 [00:12<00:06,  9.10it/s]\u001b[A\n",
            " 67%|██████▋   | 110/165 [00:12<00:06,  9.11it/s]\u001b[A\n",
            " 67%|██████▋   | 111/165 [00:12<00:05,  9.11it/s]\u001b[A\n",
            " 68%|██████▊   | 112/165 [00:12<00:05,  9.12it/s]\u001b[A\n",
            " 68%|██████▊   | 113/165 [00:12<00:05,  9.11it/s]\u001b[A\n",
            " 69%|██████▉   | 114/165 [00:12<00:05,  9.09it/s]\u001b[A\n",
            " 70%|██████▉   | 115/165 [00:12<00:05,  9.10it/s]\u001b[A\n",
            " 70%|███████   | 116/165 [00:12<00:05,  9.08it/s]\u001b[A\n",
            " 71%|███████   | 117/165 [00:12<00:05,  9.10it/s]\u001b[A\n",
            " 72%|███████▏  | 118/165 [00:13<00:05,  9.11it/s]\u001b[A\n",
            " 72%|███████▏  | 119/165 [00:13<00:05,  9.11it/s]\u001b[A\n",
            " 73%|███████▎  | 120/165 [00:13<00:04,  9.11it/s]\u001b[A\n",
            " 73%|███████▎  | 121/165 [00:13<00:04,  9.12it/s]\u001b[A\n",
            " 74%|███████▍  | 122/165 [00:13<00:04,  9.11it/s]\u001b[A\n",
            " 75%|███████▍  | 123/165 [00:13<00:04,  9.09it/s]\u001b[A\n",
            " 75%|███████▌  | 124/165 [00:13<00:04,  9.10it/s]\u001b[A\n",
            " 76%|███████▌  | 125/165 [00:13<00:04,  9.08it/s]\u001b[A\n",
            " 76%|███████▋  | 126/165 [00:13<00:04,  9.10it/s]\u001b[A\n",
            " 77%|███████▋  | 127/165 [00:14<00:04,  9.11it/s]\u001b[A\n",
            " 78%|███████▊  | 128/165 [00:14<00:04,  9.12it/s]\u001b[A\n",
            " 78%|███████▊  | 129/165 [00:14<00:03,  9.11it/s]\u001b[A\n",
            " 79%|███████▉  | 130/165 [00:14<00:03,  9.10it/s]\u001b[A\n",
            " 79%|███████▉  | 131/165 [00:14<00:03,  9.09it/s]\u001b[A\n",
            " 80%|████████  | 132/165 [00:14<00:03,  9.09it/s]\u001b[A\n",
            " 81%|████████  | 133/165 [00:14<00:03,  9.08it/s]\u001b[A\n",
            " 81%|████████  | 134/165 [00:14<00:03,  9.09it/s]\u001b[A\n",
            " 82%|████████▏ | 135/165 [00:14<00:03,  9.06it/s]\u001b[A\n",
            " 82%|████████▏ | 136/165 [00:15<00:03,  9.07it/s]\u001b[A\n",
            " 83%|████████▎ | 137/165 [00:15<00:03,  9.08it/s]\u001b[A\n",
            " 84%|████████▎ | 138/165 [00:15<00:02,  9.09it/s]\u001b[A\n",
            " 84%|████████▍ | 139/165 [00:15<00:02,  9.10it/s]\u001b[A\n",
            " 85%|████████▍ | 140/165 [00:15<00:02,  9.11it/s]\u001b[A\n",
            " 85%|████████▌ | 141/165 [00:15<00:02,  9.10it/s]\u001b[A\n",
            " 86%|████████▌ | 142/165 [00:15<00:02,  9.06it/s]\u001b[A\n",
            " 87%|████████▋ | 143/165 [00:15<00:02,  9.07it/s]\u001b[A\n",
            " 87%|████████▋ | 144/165 [00:15<00:02,  9.05it/s]\u001b[A\n",
            " 88%|████████▊ | 145/165 [00:16<00:02,  9.04it/s]\u001b[A\n",
            " 88%|████████▊ | 146/165 [00:16<00:02,  9.06it/s]\u001b[A\n",
            " 89%|████████▉ | 147/165 [00:16<00:01,  9.08it/s]\u001b[A\n",
            " 90%|████████▉ | 148/165 [00:16<00:01,  9.08it/s]\u001b[A\n",
            " 90%|█████████ | 149/165 [00:16<00:01,  9.05it/s]\u001b[A\n",
            " 91%|█████████ | 150/165 [00:16<00:01,  9.05it/s]\u001b[A\n",
            " 92%|█████████▏| 151/165 [00:16<00:01,  9.07it/s]\u001b[A\n",
            " 92%|█████████▏| 152/165 [00:16<00:01,  9.08it/s]\u001b[A\n",
            " 93%|█████████▎| 153/165 [00:16<00:01,  9.08it/s]\u001b[A\n",
            " 93%|█████████▎| 154/165 [00:17<00:01,  9.08it/s]\u001b[A\n",
            " 94%|█████████▍| 155/165 [00:17<00:01,  9.09it/s]\u001b[A\n",
            " 95%|█████████▍| 156/165 [00:17<00:00,  9.09it/s]\u001b[A\n",
            " 95%|█████████▌| 157/165 [00:17<00:00,  9.06it/s]\u001b[A\n",
            " 96%|█████████▌| 158/165 [00:17<00:00,  9.08it/s]\u001b[A\n",
            " 96%|█████████▋| 159/165 [00:17<00:00,  9.09it/s]\u001b[A\n",
            " 97%|█████████▋| 160/165 [00:17<00:00,  9.10it/s]\u001b[A\n",
            " 98%|█████████▊| 161/165 [00:17<00:00,  9.13it/s]\u001b[A\n",
            " 98%|█████████▊| 162/165 [00:17<00:00,  9.12it/s]\u001b[A\n",
            " 99%|█████████▉| 163/165 [00:18<00:00,  9.10it/s]\u001b[A\n",
            "100%|██████████| 165/165 [00:18<00:00,  9.04it/s]\n",
            "========== model: bert-base-uncased fold: 2 inference ==========\n",
            "========== model: bert-base-uncased fold: 2 inference ==========\n",
            "========== model: bert-base-uncased fold: 2 inference ==========\n",
            "========== model: bert-base-uncased fold: 2 inference ==========\n",
            "loading configuration file /content/activebus_bert_final/config.json\n",
            "Model config BertConfig {\n",
            "  \"_name_or_path\": \"activebus/BERT-XD_Review\",\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_ids\": 0,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.15.0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "loading weights file /content/activebus_bert_final/pytorch_model.bin\n",
            "Some weights of the model checkpoint at /content/activebus_bert_final were not used when initializing BertForSequenceClassification: ['cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /content/activebus_bert_final and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "\n",
            "  0%|          | 0/165 [00:00<?, ?it/s]\u001b[A\n",
            "  1%|          | 1/165 [00:00<00:17,  9.20it/s]\u001b[A\n",
            "  1%|          | 2/165 [00:00<00:17,  9.12it/s]\u001b[A\n",
            "  2%|▏         | 3/165 [00:00<00:17,  9.12it/s]\u001b[A\n",
            "  2%|▏         | 4/165 [00:00<00:17,  9.11it/s]\u001b[A\n",
            "  3%|▎         | 5/165 [00:00<00:17,  9.11it/s]\u001b[A\n",
            "  4%|▎         | 6/165 [00:00<00:17,  9.11it/s]\u001b[A\n",
            "  4%|▍         | 7/165 [00:00<00:17,  9.13it/s]\u001b[A\n",
            "  5%|▍         | 8/165 [00:00<00:17,  9.10it/s]\u001b[A\n",
            "  5%|▌         | 9/165 [00:00<00:17,  9.05it/s]\u001b[A\n",
            "  6%|▌         | 10/165 [00:01<00:17,  9.06it/s]\u001b[A\n",
            "  7%|▋         | 11/165 [00:01<00:16,  9.07it/s]\u001b[A\n",
            "  7%|▋         | 12/165 [00:01<00:16,  9.07it/s]\u001b[A\n",
            "  8%|▊         | 13/165 [00:01<00:16,  9.06it/s]\u001b[A\n",
            "  8%|▊         | 14/165 [00:01<00:16,  9.06it/s]\u001b[A\n",
            "  9%|▉         | 15/165 [00:01<00:16,  9.08it/s]\u001b[A\n",
            " 10%|▉         | 16/165 [00:01<00:16,  9.09it/s]\u001b[A\n",
            " 10%|█         | 17/165 [00:01<00:16,  9.09it/s]\u001b[A\n",
            " 11%|█         | 18/165 [00:01<00:16,  9.09it/s]\u001b[A\n",
            " 12%|█▏        | 19/165 [00:02<00:16,  9.10it/s]\u001b[A\n",
            " 12%|█▏        | 20/165 [00:02<00:15,  9.10it/s]\u001b[A\n",
            " 13%|█▎        | 21/165 [00:02<00:15,  9.10it/s]\u001b[A\n",
            " 13%|█▎        | 22/165 [00:02<00:15,  9.10it/s]\u001b[A\n",
            " 14%|█▍        | 23/165 [00:02<00:15,  9.08it/s]\u001b[A\n",
            " 15%|█▍        | 24/165 [00:02<00:15,  9.11it/s]\u001b[A\n",
            " 15%|█▌        | 25/165 [00:02<00:15,  9.11it/s]\u001b[A\n",
            " 16%|█▌        | 26/165 [00:02<00:15,  9.12it/s]\u001b[A\n",
            " 16%|█▋        | 27/165 [00:02<00:15,  9.12it/s]\u001b[A\n",
            " 17%|█▋        | 28/165 [00:03<00:14,  9.14it/s]\u001b[A\n",
            " 18%|█▊        | 29/165 [00:03<00:14,  9.13it/s]\u001b[A\n",
            " 18%|█▊        | 30/165 [00:03<00:14,  9.13it/s]\u001b[A\n",
            " 19%|█▉        | 31/165 [00:03<00:14,  9.13it/s]\u001b[A\n",
            " 19%|█▉        | 32/165 [00:03<00:14,  9.13it/s]\u001b[A\n",
            " 20%|██        | 33/165 [00:03<00:14,  9.12it/s]\u001b[A\n",
            " 21%|██        | 34/165 [00:03<00:14,  9.12it/s]\u001b[A\n",
            " 21%|██        | 35/165 [00:03<00:14,  9.13it/s]\u001b[A\n",
            " 22%|██▏       | 36/165 [00:03<00:14,  9.13it/s]\u001b[A\n",
            " 22%|██▏       | 37/165 [00:04<00:14,  9.13it/s]\u001b[A\n",
            " 23%|██▎       | 38/165 [00:04<00:13,  9.14it/s]\u001b[A\n",
            " 24%|██▎       | 39/165 [00:04<00:13,  9.14it/s]\u001b[A\n",
            " 24%|██▍       | 40/165 [00:04<00:13,  9.13it/s]\u001b[A\n",
            " 25%|██▍       | 41/165 [00:04<00:13,  9.10it/s]\u001b[A\n",
            " 25%|██▌       | 42/165 [00:04<00:13,  9.10it/s]\u001b[A\n",
            " 26%|██▌       | 43/165 [00:04<00:13,  9.11it/s]\u001b[A\n",
            " 27%|██▋       | 44/165 [00:04<00:13,  9.10it/s]\u001b[A\n",
            " 27%|██▋       | 45/165 [00:04<00:13,  9.11it/s]\u001b[A\n",
            " 28%|██▊       | 46/165 [00:05<00:13,  9.11it/s]\u001b[A\n",
            " 28%|██▊       | 47/165 [00:05<00:12,  9.11it/s]\u001b[A\n",
            " 29%|██▉       | 48/165 [00:05<00:12,  9.11it/s]\u001b[A\n",
            " 30%|██▉       | 49/165 [00:05<00:12,  9.12it/s]\u001b[A\n",
            " 30%|███       | 50/165 [00:05<00:12,  9.12it/s]\u001b[A\n",
            " 31%|███       | 51/165 [00:05<00:12,  9.10it/s]\u001b[A\n",
            " 32%|███▏      | 52/165 [00:05<00:12,  9.10it/s]\u001b[A\n",
            " 32%|███▏      | 53/165 [00:05<00:12,  9.11it/s]\u001b[A\n",
            " 33%|███▎      | 54/165 [00:05<00:12,  9.11it/s]\u001b[A\n",
            " 33%|███▎      | 55/165 [00:06<00:12,  9.11it/s]\u001b[A\n",
            " 34%|███▍      | 56/165 [00:06<00:11,  9.13it/s]\u001b[A\n",
            " 35%|███▍      | 57/165 [00:06<00:11,  9.14it/s]\u001b[A\n",
            " 35%|███▌      | 58/165 [00:06<00:11,  9.11it/s]\u001b[A\n",
            " 36%|███▌      | 59/165 [00:06<00:11,  9.10it/s]\u001b[A\n",
            " 36%|███▋      | 60/165 [00:06<00:11,  9.10it/s]\u001b[A\n",
            " 37%|███▋      | 61/165 [00:06<00:11,  9.08it/s]\u001b[A\n",
            " 38%|███▊      | 62/165 [00:06<00:11,  9.09it/s]\u001b[A\n",
            " 38%|███▊      | 63/165 [00:06<00:11,  9.10it/s]\u001b[A\n",
            " 39%|███▉      | 64/165 [00:07<00:11,  9.11it/s]\u001b[A\n",
            " 39%|███▉      | 65/165 [00:07<00:11,  9.09it/s]\u001b[A\n",
            " 40%|████      | 66/165 [00:07<00:10,  9.10it/s]\u001b[A\n",
            " 41%|████      | 67/165 [00:07<00:10,  9.08it/s]\u001b[A\n",
            " 41%|████      | 68/165 [00:07<00:10,  9.09it/s]\u001b[A\n",
            " 42%|████▏     | 69/165 [00:07<00:10,  9.10it/s]\u001b[A\n",
            " 42%|████▏     | 70/165 [00:07<00:10,  9.10it/s]\u001b[A\n",
            " 43%|████▎     | 71/165 [00:07<00:10,  9.10it/s]\u001b[A\n",
            " 44%|████▎     | 72/165 [00:07<00:10,  9.07it/s]\u001b[A\n",
            " 44%|████▍     | 73/165 [00:08<00:10,  9.10it/s]\u001b[A\n",
            " 45%|████▍     | 74/165 [00:08<00:09,  9.10it/s]\u001b[A\n",
            " 45%|████▌     | 75/165 [00:08<00:09,  9.02it/s]\u001b[A\n",
            " 46%|████▌     | 76/165 [00:08<00:09,  9.04it/s]\u001b[A\n",
            " 47%|████▋     | 77/165 [00:08<00:09,  9.06it/s]\u001b[A\n",
            " 47%|████▋     | 78/165 [00:08<00:09,  9.05it/s]\u001b[A\n",
            " 48%|████▊     | 79/165 [00:08<00:09,  9.06it/s]\u001b[A\n",
            " 48%|████▊     | 80/165 [00:08<00:09,  9.06it/s]\u001b[A\n",
            " 49%|████▉     | 81/165 [00:08<00:09,  9.08it/s]\u001b[A\n",
            " 50%|████▉     | 82/165 [00:09<00:09,  9.09it/s]\u001b[A\n",
            " 50%|█████     | 83/165 [00:09<00:09,  9.08it/s]\u001b[A\n",
            " 51%|█████     | 84/165 [00:09<00:08,  9.09it/s]\u001b[A\n",
            " 52%|█████▏    | 85/165 [00:09<00:08,  9.09it/s]\u001b[A\n",
            " 52%|█████▏    | 86/165 [00:09<00:08,  9.11it/s]\u001b[A\n",
            " 53%|█████▎    | 87/165 [00:09<00:08,  9.10it/s]\u001b[A\n",
            " 53%|█████▎    | 88/165 [00:09<00:08,  9.11it/s]\u001b[A\n",
            " 54%|█████▍    | 89/165 [00:09<00:08,  9.11it/s]\u001b[A\n",
            " 55%|█████▍    | 90/165 [00:09<00:08,  9.09it/s]\u001b[A\n",
            " 55%|█████▌    | 91/165 [00:10<00:08,  9.10it/s]\u001b[A\n",
            " 56%|█████▌    | 92/165 [00:10<00:08,  9.11it/s]\u001b[A\n",
            " 56%|█████▋    | 93/165 [00:10<00:07,  9.10it/s]\u001b[A\n",
            " 57%|█████▋    | 94/165 [00:10<00:07,  9.10it/s]\u001b[A\n",
            " 58%|█████▊    | 95/165 [00:10<00:07,  9.10it/s]\u001b[A\n",
            " 58%|█████▊    | 96/165 [00:10<00:07,  9.10it/s]\u001b[A\n",
            " 59%|█████▉    | 97/165 [00:10<00:07,  9.11it/s]\u001b[A\n",
            " 59%|█████▉    | 98/165 [00:10<00:07,  9.11it/s]\u001b[A\n",
            " 60%|██████    | 99/165 [00:10<00:07,  9.12it/s]\u001b[A\n",
            " 61%|██████    | 100/165 [00:10<00:07,  9.11it/s]\u001b[A\n",
            " 61%|██████    | 101/165 [00:11<00:07,  9.11it/s]\u001b[A\n",
            " 62%|██████▏   | 102/165 [00:11<00:06,  9.12it/s]\u001b[A\n",
            " 62%|██████▏   | 103/165 [00:11<00:06,  9.13it/s]\u001b[A\n",
            " 63%|██████▎   | 104/165 [00:11<00:06,  9.13it/s]\u001b[A\n",
            " 64%|██████▎   | 105/165 [00:11<00:06,  9.11it/s]\u001b[A\n",
            " 64%|██████▍   | 106/165 [00:11<00:06,  9.12it/s]\u001b[A\n",
            " 65%|██████▍   | 107/165 [00:11<00:06,  9.08it/s]\u001b[A\n",
            " 65%|██████▌   | 108/165 [00:11<00:06,  9.09it/s]\u001b[A\n",
            " 66%|██████▌   | 109/165 [00:11<00:06,  9.09it/s]\u001b[A\n",
            " 67%|██████▋   | 110/165 [00:12<00:06,  9.08it/s]\u001b[A\n",
            " 67%|██████▋   | 111/165 [00:12<00:05,  9.08it/s]\u001b[A\n",
            " 68%|██████▊   | 112/165 [00:12<00:05,  9.08it/s]\u001b[A\n",
            " 68%|██████▊   | 113/165 [00:12<00:05,  9.05it/s]\u001b[A\n",
            " 69%|██████▉   | 114/165 [00:12<00:05,  9.02it/s]\u001b[A\n",
            " 70%|██████▉   | 115/165 [00:12<00:05,  9.04it/s]\u001b[A\n",
            " 70%|███████   | 116/165 [00:12<00:05,  9.07it/s]\u001b[A\n",
            " 71%|███████   | 117/165 [00:12<00:05,  9.07it/s]\u001b[A\n",
            " 72%|███████▏  | 118/165 [00:12<00:05,  9.08it/s]\u001b[A\n",
            " 72%|███████▏  | 119/165 [00:13<00:05,  9.09it/s]\u001b[A\n",
            " 73%|███████▎  | 120/165 [00:13<00:04,  9.05it/s]\u001b[A\n",
            " 73%|███████▎  | 121/165 [00:13<00:04,  9.08it/s]\u001b[A\n",
            " 74%|███████▍  | 122/165 [00:13<00:04,  9.08it/s]\u001b[A\n",
            " 75%|███████▍  | 123/165 [00:13<00:04,  9.09it/s]\u001b[A\n",
            " 75%|███████▌  | 124/165 [00:13<00:04,  9.10it/s]\u001b[A\n",
            " 76%|███████▌  | 125/165 [00:13<00:04,  9.11it/s]\u001b[A\n",
            " 76%|███████▋  | 126/165 [00:13<00:04,  9.10it/s]\u001b[A\n",
            " 77%|███████▋  | 127/165 [00:13<00:04,  9.10it/s]\u001b[A\n",
            " 78%|███████▊  | 128/165 [00:14<00:04,  9.10it/s]\u001b[A\n",
            " 78%|███████▊  | 129/165 [00:14<00:03,  9.10it/s]\u001b[A\n",
            " 79%|███████▉  | 130/165 [00:14<00:03,  8.97it/s]\u001b[A\n",
            " 79%|███████▉  | 131/165 [00:14<00:03,  8.98it/s]\u001b[A\n",
            " 80%|████████  | 132/165 [00:14<00:03,  9.02it/s]\u001b[A\n",
            " 81%|████████  | 133/165 [00:14<00:03,  9.04it/s]\u001b[A\n",
            " 81%|████████  | 134/165 [00:14<00:03,  9.06it/s]\u001b[A\n",
            " 82%|████████▏ | 135/165 [00:14<00:03,  9.08it/s]\u001b[A\n",
            " 82%|████████▏ | 136/165 [00:14<00:03,  9.09it/s]\u001b[A\n",
            " 83%|████████▎ | 137/165 [00:15<00:03,  9.08it/s]\u001b[A\n",
            " 84%|████████▎ | 138/165 [00:15<00:02,  9.09it/s]\u001b[A\n",
            " 84%|████████▍ | 139/165 [00:15<00:02,  9.08it/s]\u001b[A\n",
            " 85%|████████▍ | 140/165 [00:15<00:02,  9.06it/s]\u001b[A\n",
            " 85%|████████▌ | 141/165 [00:15<00:02,  9.07it/s]\u001b[A\n",
            " 86%|████████▌ | 142/165 [00:15<00:02,  9.07it/s]\u001b[A\n",
            " 87%|████████▋ | 143/165 [00:15<00:02,  9.07it/s]\u001b[A\n",
            " 87%|████████▋ | 144/165 [00:15<00:02,  9.06it/s]\u001b[A\n",
            " 88%|████████▊ | 145/165 [00:15<00:02,  9.08it/s]\u001b[A\n",
            " 88%|████████▊ | 146/165 [00:16<00:02,  9.09it/s]\u001b[A\n",
            " 89%|████████▉ | 147/165 [00:16<00:01,  9.10it/s]\u001b[A\n",
            " 90%|████████▉ | 148/165 [00:16<00:01,  9.10it/s]\u001b[A\n",
            " 90%|█████████ | 149/165 [00:16<00:01,  9.09it/s]\u001b[A\n",
            " 91%|█████████ | 150/165 [00:16<00:01,  9.08it/s]\u001b[A\n",
            " 92%|█████████▏| 151/165 [00:16<00:01,  9.08it/s]\u001b[A\n",
            " 92%|█████████▏| 152/165 [00:16<00:01,  9.04it/s]\u001b[A\n",
            " 93%|█████████▎| 153/165 [00:16<00:01,  9.06it/s]\u001b[A\n",
            " 93%|█████████▎| 154/165 [00:16<00:01,  9.08it/s]\u001b[A\n",
            " 94%|█████████▍| 155/165 [00:17<00:01,  9.09it/s]\u001b[A\n",
            " 95%|█████████▍| 156/165 [00:17<00:00,  9.10it/s]\u001b[A\n",
            " 95%|█████████▌| 157/165 [00:17<00:00,  9.11it/s]\u001b[A\n",
            " 96%|█████████▌| 158/165 [00:17<00:00,  9.13it/s]\u001b[A\n",
            " 96%|█████████▋| 159/165 [00:17<00:00,  9.15it/s]\u001b[A\n",
            " 97%|█████████▋| 160/165 [00:17<00:00,  9.16it/s]\u001b[A\n",
            " 98%|█████████▊| 161/165 [00:17<00:00,  9.17it/s]\u001b[A\n",
            " 98%|█████████▊| 162/165 [00:17<00:00,  9.16it/s]\u001b[A\n",
            " 99%|█████████▉| 163/165 [00:17<00:00,  9.16it/s]\u001b[A\n",
            "100%|██████████| 165/165 [00:18<00:00,  9.08it/s]\n",
            "========== model: bert-base-uncased fold: 3 inference ==========\n",
            "========== model: bert-base-uncased fold: 3 inference ==========\n",
            "========== model: bert-base-uncased fold: 3 inference ==========\n",
            "========== model: bert-base-uncased fold: 3 inference ==========\n",
            "loading configuration file /content/activebus_bert_final/config.json\n",
            "Model config BertConfig {\n",
            "  \"_name_or_path\": \"activebus/BERT-XD_Review\",\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_ids\": 0,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.15.0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "loading weights file /content/activebus_bert_final/pytorch_model.bin\n",
            "Some weights of the model checkpoint at /content/activebus_bert_final were not used when initializing BertForSequenceClassification: ['cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /content/activebus_bert_final and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "\n",
            "  0%|          | 0/165 [00:00<?, ?it/s]\u001b[A\n",
            "  1%|          | 1/165 [00:00<00:18,  8.71it/s]\u001b[A\n",
            "  1%|          | 2/165 [00:00<00:18,  8.90it/s]\u001b[A\n",
            "  2%|▏         | 3/165 [00:00<00:18,  8.88it/s]\u001b[A\n",
            "  2%|▏         | 4/165 [00:00<00:17,  8.95it/s]\u001b[A\n",
            "  3%|▎         | 5/165 [00:00<00:17,  9.01it/s]\u001b[A\n",
            "  4%|▎         | 6/165 [00:00<00:17,  9.00it/s]\u001b[A\n",
            "  4%|▍         | 7/165 [00:00<00:17,  9.02it/s]\u001b[A\n",
            "  5%|▍         | 8/165 [00:00<00:17,  9.05it/s]\u001b[A\n",
            "  5%|▌         | 9/165 [00:01<00:17,  9.07it/s]\u001b[A\n",
            "  6%|▌         | 10/165 [00:01<00:17,  9.05it/s]\u001b[A\n",
            "  7%|▋         | 11/165 [00:01<00:17,  9.06it/s]\u001b[A\n",
            "  7%|▋         | 12/165 [00:01<00:16,  9.04it/s]\u001b[A\n",
            "  8%|▊         | 13/165 [00:01<00:16,  9.07it/s]\u001b[A\n",
            "  8%|▊         | 14/165 [00:01<00:16,  9.08it/s]\u001b[A\n",
            "  9%|▉         | 15/165 [00:01<00:16,  9.09it/s]\u001b[A\n",
            " 10%|▉         | 16/165 [00:01<00:16,  9.11it/s]\u001b[A\n",
            " 10%|█         | 17/165 [00:01<00:16,  9.10it/s]\u001b[A\n",
            " 11%|█         | 18/165 [00:01<00:16,  9.08it/s]\u001b[A\n",
            " 12%|█▏        | 19/165 [00:02<00:16,  9.07it/s]\u001b[A\n",
            " 12%|█▏        | 20/165 [00:02<00:15,  9.07it/s]\u001b[A\n",
            " 13%|█▎        | 21/165 [00:02<00:15,  9.09it/s]\u001b[A\n",
            " 13%|█▎        | 22/165 [00:02<00:15,  9.07it/s]\u001b[A\n",
            " 14%|█▍        | 23/165 [00:02<00:15,  9.08it/s]\u001b[A\n",
            " 15%|█▍        | 24/165 [00:02<00:15,  9.09it/s]\u001b[A\n",
            " 15%|█▌        | 25/165 [00:02<00:15,  9.10it/s]\u001b[A\n",
            " 16%|█▌        | 26/165 [00:02<00:15,  9.11it/s]\u001b[A\n",
            " 16%|█▋        | 27/165 [00:02<00:15,  9.11it/s]\u001b[A\n",
            " 17%|█▋        | 28/165 [00:03<00:15,  9.11it/s]\u001b[A\n",
            " 18%|█▊        | 29/165 [00:03<00:14,  9.10it/s]\u001b[A\n",
            " 18%|█▊        | 30/165 [00:03<00:14,  9.09it/s]\u001b[A\n",
            " 19%|█▉        | 31/165 [00:03<00:14,  9.10it/s]\u001b[A\n",
            " 19%|█▉        | 32/165 [00:03<00:14,  9.09it/s]\u001b[A\n",
            " 20%|██        | 33/165 [00:03<00:14,  9.08it/s]\u001b[A\n",
            " 21%|██        | 34/165 [00:03<00:14,  9.07it/s]\u001b[A\n",
            " 21%|██        | 35/165 [00:03<00:14,  9.07it/s]\u001b[A\n",
            " 22%|██▏       | 36/165 [00:03<00:14,  9.08it/s]\u001b[A\n",
            " 22%|██▏       | 37/165 [00:04<00:14,  9.06it/s]\u001b[A\n",
            " 23%|██▎       | 38/165 [00:04<00:14,  9.06it/s]\u001b[A\n",
            " 24%|██▎       | 39/165 [00:04<00:13,  9.06it/s]\u001b[A\n",
            " 24%|██▍       | 40/165 [00:04<00:13,  9.07it/s]\u001b[A\n",
            " 25%|██▍       | 41/165 [00:04<00:13,  9.06it/s]\u001b[A\n",
            " 25%|██▌       | 42/165 [00:04<00:13,  9.07it/s]\u001b[A\n",
            " 26%|██▌       | 43/165 [00:04<00:13,  9.06it/s]\u001b[A\n",
            " 27%|██▋       | 44/165 [00:04<00:13,  9.06it/s]\u001b[A\n",
            " 27%|██▋       | 45/165 [00:04<00:13,  9.08it/s]\u001b[A\n",
            " 28%|██▊       | 46/165 [00:05<00:13,  9.09it/s]\u001b[A\n",
            " 28%|██▊       | 47/165 [00:05<00:12,  9.09it/s]\u001b[A\n",
            " 29%|██▉       | 48/165 [00:05<00:12,  9.10it/s]\u001b[A\n",
            " 30%|██▉       | 49/165 [00:05<00:12,  9.09it/s]\u001b[A\n",
            " 30%|███       | 50/165 [00:05<00:12,  9.09it/s]\u001b[A\n",
            " 31%|███       | 51/165 [00:05<00:12,  9.09it/s]\u001b[A\n",
            " 32%|███▏      | 52/165 [00:05<00:12,  9.08it/s]\u001b[A\n",
            " 32%|███▏      | 53/165 [00:05<00:12,  9.07it/s]\u001b[A\n",
            " 33%|███▎      | 54/165 [00:05<00:12,  9.09it/s]\u001b[A\n",
            " 33%|███▎      | 55/165 [00:06<00:12,  9.10it/s]\u001b[A\n",
            " 34%|███▍      | 56/165 [00:06<00:11,  9.10it/s]\u001b[A\n",
            " 35%|███▍      | 57/165 [00:06<00:11,  9.12it/s]\u001b[A\n",
            " 35%|███▌      | 58/165 [00:06<00:11,  9.13it/s]\u001b[A\n",
            " 36%|███▌      | 59/165 [00:06<00:11,  9.14it/s]\u001b[A\n",
            " 36%|███▋      | 60/165 [00:06<00:11,  9.14it/s]\u001b[A\n",
            " 37%|███▋      | 61/165 [00:06<00:11,  9.13it/s]\u001b[A\n",
            " 38%|███▊      | 62/165 [00:06<00:11,  9.13it/s]\u001b[A\n",
            " 38%|███▊      | 63/165 [00:06<00:11,  9.10it/s]\u001b[A\n",
            " 39%|███▉      | 64/165 [00:07<00:11,  9.11it/s]\u001b[A\n",
            " 39%|███▉      | 65/165 [00:07<00:10,  9.12it/s]\u001b[A\n",
            " 40%|████      | 66/165 [00:07<00:10,  9.12it/s]\u001b[A\n",
            " 41%|████      | 67/165 [00:07<00:10,  9.10it/s]\u001b[A\n",
            " 41%|████      | 68/165 [00:07<00:10,  9.11it/s]\u001b[A\n",
            " 42%|████▏     | 69/165 [00:07<00:10,  9.12it/s]\u001b[A\n",
            " 42%|████▏     | 70/165 [00:07<00:10,  9.12it/s]\u001b[A\n",
            " 43%|████▎     | 71/165 [00:07<00:10,  9.12it/s]\u001b[A\n",
            " 44%|████▎     | 72/165 [00:07<00:10,  9.12it/s]\u001b[A\n",
            " 44%|████▍     | 73/165 [00:08<00:10,  9.11it/s]\u001b[A\n",
            " 45%|████▍     | 74/165 [00:08<00:10,  9.10it/s]\u001b[A\n",
            " 45%|████▌     | 75/165 [00:08<00:09,  9.11it/s]\u001b[A\n",
            " 46%|████▌     | 76/165 [00:08<00:09,  9.11it/s]\u001b[A\n",
            " 47%|████▋     | 77/165 [00:08<00:09,  9.13it/s]\u001b[A\n",
            " 47%|████▋     | 78/165 [00:08<00:09,  9.12it/s]\u001b[A\n",
            " 48%|████▊     | 79/165 [00:08<00:09,  9.12it/s]\u001b[A\n",
            " 48%|████▊     | 80/165 [00:08<00:09,  9.11it/s]\u001b[A\n",
            " 49%|████▉     | 81/165 [00:08<00:09,  9.10it/s]\u001b[A\n",
            " 50%|████▉     | 82/165 [00:09<00:09,  9.10it/s]\u001b[A\n",
            " 50%|█████     | 83/165 [00:09<00:09,  9.09it/s]\u001b[A\n",
            " 51%|█████     | 84/165 [00:09<00:08,  9.08it/s]\u001b[A\n",
            " 52%|█████▏    | 85/165 [00:09<00:08,  9.09it/s]\u001b[A\n",
            " 52%|█████▏    | 86/165 [00:09<00:08,  9.11it/s]\u001b[A\n",
            " 53%|█████▎    | 87/165 [00:09<00:08,  9.12it/s]\u001b[A\n",
            " 53%|█████▎    | 88/165 [00:09<00:08,  9.11it/s]\u001b[A\n",
            " 54%|█████▍    | 89/165 [00:09<00:08,  9.09it/s]\u001b[A\n",
            " 55%|█████▍    | 90/165 [00:09<00:08,  9.06it/s]\u001b[A\n",
            " 55%|█████▌    | 91/165 [00:10<00:08,  9.08it/s]\u001b[A\n",
            " 56%|█████▌    | 92/165 [00:10<00:08,  9.09it/s]\u001b[A\n",
            " 56%|█████▋    | 93/165 [00:10<00:07,  9.08it/s]\u001b[A\n",
            " 57%|█████▋    | 94/165 [00:10<00:07,  9.08it/s]\u001b[A\n",
            " 58%|█████▊    | 95/165 [00:10<00:07,  9.10it/s]\u001b[A\n",
            " 58%|█████▊    | 96/165 [00:10<00:07,  9.02it/s]\u001b[A\n",
            " 59%|█████▉    | 97/165 [00:10<00:07,  9.05it/s]\u001b[A\n",
            " 59%|█████▉    | 98/165 [00:10<00:07,  9.06it/s]\u001b[A\n",
            " 60%|██████    | 99/165 [00:10<00:07,  9.08it/s]\u001b[A\n",
            " 61%|██████    | 100/165 [00:11<00:07,  9.09it/s]\u001b[A\n",
            " 61%|██████    | 101/165 [00:11<00:07,  9.08it/s]\u001b[A\n",
            " 62%|██████▏   | 102/165 [00:11<00:06,  9.07it/s]\u001b[A\n",
            " 62%|██████▏   | 103/165 [00:11<00:06,  9.05it/s]\u001b[A\n",
            " 63%|██████▎   | 104/165 [00:11<00:06,  9.05it/s]\u001b[A\n",
            " 64%|██████▎   | 105/165 [00:11<00:06,  9.04it/s]\u001b[A\n",
            " 64%|██████▍   | 106/165 [00:11<00:06,  9.08it/s]\u001b[A\n",
            " 65%|██████▍   | 107/165 [00:11<00:06,  9.08it/s]\u001b[A\n",
            " 65%|██████▌   | 108/165 [00:11<00:06,  9.10it/s]\u001b[A\n",
            " 66%|██████▌   | 109/165 [00:12<00:06,  9.11it/s]\u001b[A\n",
            " 67%|██████▋   | 110/165 [00:12<00:06,  9.11it/s]\u001b[A\n",
            " 67%|██████▋   | 111/165 [00:12<00:05,  9.12it/s]\u001b[A\n",
            " 68%|██████▊   | 112/165 [00:12<00:05,  9.13it/s]\u001b[A\n",
            " 68%|██████▊   | 113/165 [00:12<00:05,  9.12it/s]\u001b[A\n",
            " 69%|██████▉   | 114/165 [00:12<00:05,  9.11it/s]\u001b[A\n",
            " 70%|██████▉   | 115/165 [00:12<00:05,  9.10it/s]\u001b[A\n",
            " 70%|███████   | 116/165 [00:12<00:05,  9.09it/s]\u001b[A\n",
            " 71%|███████   | 117/165 [00:12<00:05,  9.09it/s]\u001b[A\n",
            " 72%|███████▏  | 118/165 [00:12<00:05,  9.06it/s]\u001b[A\n",
            " 72%|███████▏  | 119/165 [00:13<00:05,  9.08it/s]\u001b[A\n",
            " 73%|███████▎  | 120/165 [00:13<00:04,  9.08it/s]\u001b[A\n",
            " 73%|███████▎  | 121/165 [00:13<00:04,  9.10it/s]\u001b[A\n",
            " 74%|███████▍  | 122/165 [00:13<00:04,  9.11it/s]\u001b[A\n",
            " 75%|███████▍  | 123/165 [00:13<00:04,  9.11it/s]\u001b[A\n",
            " 75%|███████▌  | 124/165 [00:13<00:04,  9.10it/s]\u001b[A\n",
            " 76%|███████▌  | 125/165 [00:13<00:04,  9.09it/s]\u001b[A\n",
            " 76%|███████▋  | 126/165 [00:13<00:04,  9.09it/s]\u001b[A\n",
            " 77%|███████▋  | 127/165 [00:13<00:04,  9.09it/s]\u001b[A\n",
            " 78%|███████▊  | 128/165 [00:14<00:04,  9.09it/s]\u001b[A\n",
            " 78%|███████▊  | 129/165 [00:14<00:03,  9.06it/s]\u001b[A\n",
            " 79%|███████▉  | 130/165 [00:14<00:03,  9.05it/s]\u001b[A\n",
            " 79%|███████▉  | 131/165 [00:14<00:03,  9.06it/s]\u001b[A\n",
            " 80%|████████  | 132/165 [00:14<00:03,  9.07it/s]\u001b[A\n",
            " 81%|████████  | 133/165 [00:14<00:03,  9.09it/s]\u001b[A\n",
            " 81%|████████  | 134/165 [00:14<00:03,  9.10it/s]\u001b[A\n",
            " 82%|████████▏ | 135/165 [00:14<00:03,  9.11it/s]\u001b[A\n",
            " 82%|████████▏ | 136/165 [00:14<00:03,  9.09it/s]\u001b[A\n",
            " 83%|████████▎ | 137/165 [00:15<00:03,  9.09it/s]\u001b[A\n",
            " 84%|████████▎ | 138/165 [00:15<00:02,  9.09it/s]\u001b[A\n",
            " 84%|████████▍ | 139/165 [00:15<00:02,  9.10it/s]\u001b[A\n",
            " 85%|████████▍ | 140/165 [00:15<00:02,  9.12it/s]\u001b[A\n",
            " 85%|████████▌ | 141/165 [00:15<00:02,  9.12it/s]\u001b[A\n",
            " 86%|████████▌ | 142/165 [00:15<00:02,  9.12it/s]\u001b[A\n",
            " 87%|████████▋ | 143/165 [00:15<00:02,  9.08it/s]\u001b[A\n",
            " 87%|████████▋ | 144/165 [00:15<00:02,  9.08it/s]\u001b[A\n",
            " 88%|████████▊ | 145/165 [00:15<00:02,  9.08it/s]\u001b[A\n",
            " 88%|████████▊ | 146/165 [00:16<00:02,  9.07it/s]\u001b[A\n",
            " 89%|████████▉ | 147/165 [00:16<00:01,  9.08it/s]\u001b[A\n",
            " 90%|████████▉ | 148/165 [00:16<00:01,  9.08it/s]\u001b[A\n",
            " 90%|█████████ | 149/165 [00:16<00:01,  9.08it/s]\u001b[A\n",
            " 91%|█████████ | 150/165 [00:16<00:01,  9.08it/s]\u001b[A\n",
            " 92%|█████████▏| 151/165 [00:16<00:01,  9.10it/s]\u001b[A\n",
            " 92%|█████████▏| 152/165 [00:16<00:01,  9.11it/s]\u001b[A\n",
            " 93%|█████████▎| 153/165 [00:16<00:01,  9.12it/s]\u001b[A\n",
            " 93%|█████████▎| 154/165 [00:16<00:01,  9.13it/s]\u001b[A\n",
            " 94%|█████████▍| 155/165 [00:17<00:01,  9.12it/s]\u001b[A\n",
            " 95%|█████████▍| 156/165 [00:17<00:00,  9.09it/s]\u001b[A\n",
            " 95%|█████████▌| 157/165 [00:17<00:00,  9.07it/s]\u001b[A\n",
            " 96%|█████████▌| 158/165 [00:17<00:00,  9.09it/s]\u001b[A\n",
            " 96%|█████████▋| 159/165 [00:17<00:00,  9.10it/s]\u001b[A\n",
            " 97%|█████████▋| 160/165 [00:17<00:00,  9.12it/s]\u001b[A\n",
            " 98%|█████████▊| 161/165 [00:17<00:00,  9.14it/s]\u001b[A\n",
            " 98%|█████████▊| 162/165 [00:17<00:00,  9.16it/s]\u001b[A\n",
            " 99%|█████████▉| 163/165 [00:17<00:00,  9.16it/s]\u001b[A\n",
            "100%|██████████| 165/165 [00:18<00:00,  9.08it/s]\n",
            "========== model: bert-base-uncased fold: 4 inference ==========\n",
            "========== model: bert-base-uncased fold: 4 inference ==========\n",
            "========== model: bert-base-uncased fold: 4 inference ==========\n",
            "========== model: bert-base-uncased fold: 4 inference ==========\n",
            "loading configuration file /content/activebus_bert_final/config.json\n",
            "Model config BertConfig {\n",
            "  \"_name_or_path\": \"activebus/BERT-XD_Review\",\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_ids\": 0,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.15.0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "loading weights file /content/activebus_bert_final/pytorch_model.bin\n",
            "Some weights of the model checkpoint at /content/activebus_bert_final were not used when initializing BertForSequenceClassification: ['cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /content/activebus_bert_final and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "\n",
            "  0%|          | 0/165 [00:00<?, ?it/s]\u001b[A\n",
            "  1%|          | 1/165 [00:00<00:19,  8.25it/s]\u001b[A\n",
            "  1%|          | 2/165 [00:00<00:18,  8.67it/s]\u001b[A\n",
            "  2%|▏         | 3/165 [00:00<00:18,  8.87it/s]\u001b[A\n",
            "  2%|▏         | 4/165 [00:00<00:17,  8.95it/s]\u001b[A\n",
            "  3%|▎         | 5/165 [00:00<00:17,  9.00it/s]\u001b[A\n",
            "  4%|▎         | 6/165 [00:00<00:17,  9.03it/s]\u001b[A\n",
            "  4%|▍         | 7/165 [00:00<00:17,  9.06it/s]\u001b[A\n",
            "  5%|▍         | 8/165 [00:00<00:17,  9.09it/s]\u001b[A\n",
            "  5%|▌         | 9/165 [00:01<00:17,  9.10it/s]\u001b[A\n",
            "  6%|▌         | 10/165 [00:01<00:17,  9.11it/s]\u001b[A\n",
            "  7%|▋         | 11/165 [00:01<00:16,  9.10it/s]\u001b[A\n",
            "  7%|▋         | 12/165 [00:01<00:16,  9.09it/s]\u001b[A\n",
            "  8%|▊         | 13/165 [00:01<00:16,  9.10it/s]\u001b[A\n",
            "  8%|▊         | 14/165 [00:01<00:16,  9.11it/s]\u001b[A\n",
            "  9%|▉         | 15/165 [00:01<00:16,  9.11it/s]\u001b[A\n",
            " 10%|▉         | 16/165 [00:01<00:16,  9.12it/s]\u001b[A\n",
            " 10%|█         | 17/165 [00:01<00:16,  9.13it/s]\u001b[A\n",
            " 11%|█         | 18/165 [00:01<00:16,  9.09it/s]\u001b[A\n",
            " 12%|█▏        | 19/165 [00:02<00:16,  9.11it/s]\u001b[A\n",
            " 12%|█▏        | 20/165 [00:02<00:15,  9.10it/s]\u001b[A\n",
            " 13%|█▎        | 21/165 [00:02<00:15,  9.10it/s]\u001b[A\n",
            " 13%|█▎        | 22/165 [00:02<00:15,  9.09it/s]\u001b[A\n",
            " 14%|█▍        | 23/165 [00:02<00:15,  9.07it/s]\u001b[A\n",
            " 15%|█▍        | 24/165 [00:02<00:15,  9.08it/s]\u001b[A\n",
            " 15%|█▌        | 25/165 [00:02<00:15,  9.09it/s]\u001b[A\n",
            " 16%|█▌        | 26/165 [00:02<00:15,  9.10it/s]\u001b[A\n",
            " 16%|█▋        | 27/165 [00:02<00:15,  9.10it/s]\u001b[A\n",
            " 17%|█▋        | 28/165 [00:03<00:15,  9.09it/s]\u001b[A\n",
            " 18%|█▊        | 29/165 [00:03<00:14,  9.11it/s]\u001b[A\n",
            " 18%|█▊        | 30/165 [00:03<00:14,  9.12it/s]\u001b[A\n",
            " 19%|█▉        | 31/165 [00:03<00:14,  9.11it/s]\u001b[A\n",
            " 19%|█▉        | 32/165 [00:03<00:14,  9.12it/s]\u001b[A\n",
            " 20%|██        | 33/165 [00:03<00:14,  9.11it/s]\u001b[A\n",
            " 21%|██        | 34/165 [00:03<00:14,  9.10it/s]\u001b[A\n",
            " 21%|██        | 35/165 [00:03<00:14,  9.10it/s]\u001b[A\n",
            " 22%|██▏       | 36/165 [00:03<00:14,  9.11it/s]\u001b[A\n",
            " 22%|██▏       | 37/165 [00:04<00:14,  9.11it/s]\u001b[A\n",
            " 23%|██▎       | 38/165 [00:04<00:13,  9.13it/s]\u001b[A\n",
            " 24%|██▎       | 39/165 [00:04<00:13,  9.11it/s]\u001b[A\n",
            " 24%|██▍       | 40/165 [00:04<00:13,  9.12it/s]\u001b[A\n",
            " 25%|██▍       | 41/165 [00:04<00:13,  9.10it/s]\u001b[A\n",
            " 25%|██▌       | 42/165 [00:04<00:13,  9.11it/s]\u001b[A\n",
            " 26%|██▌       | 43/165 [00:04<00:13,  9.10it/s]\u001b[A\n",
            " 27%|██▋       | 44/165 [00:04<00:13,  9.12it/s]\u001b[A\n",
            " 27%|██▋       | 45/165 [00:04<00:13,  9.12it/s]\u001b[A\n",
            " 28%|██▊       | 46/165 [00:05<00:13,  9.09it/s]\u001b[A\n",
            " 28%|██▊       | 47/165 [00:05<00:12,  9.08it/s]\u001b[A\n",
            " 29%|██▉       | 48/165 [00:05<00:12,  9.09it/s]\u001b[A\n",
            " 30%|██▉       | 49/165 [00:05<00:12,  9.10it/s]\u001b[A\n",
            " 30%|███       | 50/165 [00:05<00:12,  9.03it/s]\u001b[A\n",
            " 31%|███       | 51/165 [00:05<00:12,  9.06it/s]\u001b[A\n",
            " 32%|███▏      | 52/165 [00:05<00:12,  9.05it/s]\u001b[A\n",
            " 32%|███▏      | 53/165 [00:05<00:12,  9.06it/s]\u001b[A\n",
            " 33%|███▎      | 54/165 [00:05<00:12,  9.08it/s]\u001b[A\n",
            " 33%|███▎      | 55/165 [00:06<00:12,  9.10it/s]\u001b[A\n",
            " 34%|███▍      | 56/165 [00:06<00:11,  9.09it/s]\u001b[A\n",
            " 35%|███▍      | 57/165 [00:06<00:11,  9.10it/s]\u001b[A\n",
            " 35%|███▌      | 58/165 [00:06<00:11,  9.11it/s]\u001b[A\n",
            " 36%|███▌      | 59/165 [00:06<00:11,  9.11it/s]\u001b[A\n",
            " 36%|███▋      | 60/165 [00:06<00:11,  9.12it/s]\u001b[A\n",
            " 37%|███▋      | 61/165 [00:06<00:11,  9.08it/s]\u001b[A\n",
            " 38%|███▊      | 62/165 [00:06<00:11,  9.11it/s]\u001b[A\n",
            " 38%|███▊      | 63/165 [00:06<00:11,  9.11it/s]\u001b[A\n",
            " 39%|███▉      | 64/165 [00:07<00:11,  9.11it/s]\u001b[A\n",
            " 39%|███▉      | 65/165 [00:07<00:11,  9.09it/s]\u001b[A\n",
            " 40%|████      | 66/165 [00:07<00:10,  9.08it/s]\u001b[A\n",
            " 41%|████      | 67/165 [00:07<00:10,  9.09it/s]\u001b[A\n",
            " 41%|████      | 68/165 [00:07<00:10,  9.10it/s]\u001b[A\n",
            " 42%|████▏     | 69/165 [00:07<00:10,  9.08it/s]\u001b[A\n",
            " 42%|████▏     | 70/165 [00:07<00:10,  9.08it/s]\u001b[A\n",
            " 43%|████▎     | 71/165 [00:07<00:10,  9.08it/s]\u001b[A\n",
            " 44%|████▎     | 72/165 [00:07<00:10,  9.09it/s]\u001b[A\n",
            " 44%|████▍     | 73/165 [00:08<00:10,  9.09it/s]\u001b[A\n",
            " 45%|████▍     | 74/165 [00:08<00:09,  9.12it/s]\u001b[A\n",
            " 45%|████▌     | 75/165 [00:08<00:09,  9.12it/s]\u001b[A\n",
            " 46%|████▌     | 76/165 [00:08<00:09,  9.12it/s]\u001b[A\n",
            " 47%|████▋     | 77/165 [00:08<00:09,  9.13it/s]\u001b[A\n",
            " 47%|████▋     | 78/165 [00:08<00:09,  9.13it/s]\u001b[A\n",
            " 48%|████▊     | 79/165 [00:08<00:09,  9.13it/s]\u001b[A\n",
            " 48%|████▊     | 80/165 [00:08<00:09,  9.11it/s]\u001b[A\n",
            " 49%|████▉     | 81/165 [00:08<00:09,  9.10it/s]\u001b[A\n",
            " 50%|████▉     | 82/165 [00:09<00:09,  9.10it/s]\u001b[A\n",
            " 50%|█████     | 83/165 [00:09<00:09,  9.10it/s]\u001b[A\n",
            " 51%|█████     | 84/165 [00:09<00:08,  9.09it/s]\u001b[A\n",
            " 52%|█████▏    | 85/165 [00:09<00:08,  9.10it/s]\u001b[A\n",
            " 52%|█████▏    | 86/165 [00:09<00:08,  9.11it/s]\u001b[A\n",
            " 53%|█████▎    | 87/165 [00:09<00:08,  9.13it/s]\u001b[A\n",
            " 53%|█████▎    | 88/165 [00:09<00:08,  9.12it/s]\u001b[A\n",
            " 54%|█████▍    | 89/165 [00:09<00:08,  9.11it/s]\u001b[A\n",
            " 55%|█████▍    | 90/165 [00:09<00:08,  9.10it/s]\u001b[A\n",
            " 55%|█████▌    | 91/165 [00:10<00:08,  9.11it/s]\u001b[A\n",
            " 56%|█████▌    | 92/165 [00:10<00:08,  9.09it/s]\u001b[A\n",
            " 56%|█████▋    | 93/165 [00:10<00:07,  9.10it/s]\u001b[A\n",
            " 57%|█████▋    | 94/165 [00:10<00:07,  9.10it/s]\u001b[A\n",
            " 58%|█████▊    | 95/165 [00:10<00:07,  9.09it/s]\u001b[A\n",
            " 58%|█████▊    | 96/165 [00:10<00:07,  9.09it/s]\u001b[A\n",
            " 59%|█████▉    | 97/165 [00:10<00:07,  9.09it/s]\u001b[A\n",
            " 59%|█████▉    | 98/165 [00:10<00:07,  9.08it/s]\u001b[A\n",
            " 60%|██████    | 99/165 [00:10<00:07,  9.08it/s]\u001b[A\n",
            " 61%|██████    | 100/165 [00:11<00:07,  9.09it/s]\u001b[A\n",
            " 61%|██████    | 101/165 [00:11<00:07,  9.10it/s]\u001b[A\n",
            " 62%|██████▏   | 102/165 [00:11<00:06,  9.09it/s]\u001b[A\n",
            " 62%|██████▏   | 103/165 [00:11<00:06,  9.12it/s]\u001b[A\n",
            " 63%|██████▎   | 104/165 [00:11<00:06,  9.09it/s]\u001b[A\n",
            " 64%|██████▎   | 105/165 [00:11<00:06,  9.09it/s]\u001b[A\n",
            " 64%|██████▍   | 106/165 [00:11<00:06,  9.08it/s]\u001b[A\n",
            " 65%|██████▍   | 107/165 [00:11<00:06,  9.07it/s]\u001b[A\n",
            " 65%|██████▌   | 108/165 [00:11<00:06,  9.04it/s]\u001b[A\n",
            " 66%|██████▌   | 109/165 [00:11<00:06,  9.08it/s]\u001b[A\n",
            " 67%|██████▋   | 110/165 [00:12<00:06,  9.06it/s]\u001b[A\n",
            " 67%|██████▋   | 111/165 [00:12<00:05,  9.08it/s]\u001b[A\n",
            " 68%|██████▊   | 112/165 [00:12<00:05,  9.06it/s]\u001b[A\n",
            " 68%|██████▊   | 113/165 [00:12<00:05,  9.08it/s]\u001b[A\n",
            " 69%|██████▉   | 114/165 [00:12<00:05,  9.09it/s]\u001b[A\n",
            " 70%|██████▉   | 115/165 [00:12<00:05,  9.09it/s]\u001b[A\n",
            " 70%|███████   | 116/165 [00:12<00:05,  9.10it/s]\u001b[A\n",
            " 71%|███████   | 117/165 [00:12<00:05,  9.09it/s]\u001b[A\n",
            " 72%|███████▏  | 118/165 [00:12<00:05,  9.00it/s]\u001b[A\n",
            " 72%|███████▏  | 119/165 [00:13<00:05,  9.03it/s]\u001b[A\n",
            " 73%|███████▎  | 120/165 [00:13<00:04,  9.04it/s]\u001b[A\n",
            " 73%|███████▎  | 121/165 [00:13<00:04,  9.05it/s]\u001b[A\n",
            " 74%|███████▍  | 122/165 [00:13<00:04,  9.05it/s]\u001b[A\n",
            " 75%|███████▍  | 123/165 [00:13<00:04,  9.04it/s]\u001b[A\n",
            " 75%|███████▌  | 124/165 [00:13<00:04,  9.04it/s]\u001b[A\n",
            " 76%|███████▌  | 125/165 [00:13<00:04,  9.05it/s]\u001b[A\n",
            " 76%|███████▋  | 126/165 [00:13<00:04,  9.05it/s]\u001b[A\n",
            " 77%|███████▋  | 127/165 [00:13<00:04,  9.08it/s]\u001b[A\n",
            " 78%|███████▊  | 128/165 [00:14<00:04,  9.07it/s]\u001b[A\n",
            " 78%|███████▊  | 129/165 [00:14<00:03,  9.08it/s]\u001b[A\n",
            " 79%|███████▉  | 130/165 [00:14<00:03,  9.09it/s]\u001b[A\n",
            " 79%|███████▉  | 131/165 [00:14<00:03,  9.10it/s]\u001b[A\n",
            " 80%|████████  | 132/165 [00:14<00:03,  9.02it/s]\u001b[A\n",
            " 81%|████████  | 133/165 [00:14<00:03,  9.03it/s]\u001b[A\n",
            " 81%|████████  | 134/165 [00:14<00:03,  9.05it/s]\u001b[A\n",
            " 82%|████████▏ | 135/165 [00:14<00:03,  9.07it/s]\u001b[A\n",
            " 82%|████████▏ | 136/165 [00:14<00:03,  9.07it/s]\u001b[A\n",
            " 83%|████████▎ | 137/165 [00:15<00:03,  9.08it/s]\u001b[A\n",
            " 84%|████████▎ | 138/165 [00:15<00:02,  9.06it/s]\u001b[A\n",
            " 84%|████████▍ | 139/165 [00:15<00:02,  9.05it/s]\u001b[A\n",
            " 85%|████████▍ | 140/165 [00:15<00:02,  9.06it/s]\u001b[A\n",
            " 85%|████████▌ | 141/165 [00:15<00:02,  9.07it/s]\u001b[A\n",
            " 86%|████████▌ | 142/165 [00:15<00:02,  9.08it/s]\u001b[A\n",
            " 87%|████████▋ | 143/165 [00:15<00:02,  9.08it/s]\u001b[A\n",
            " 87%|████████▋ | 144/165 [00:15<00:02,  9.10it/s]\u001b[A\n",
            " 88%|████████▊ | 145/165 [00:15<00:02,  9.10it/s]\u001b[A\n",
            " 88%|████████▊ | 146/165 [00:16<00:02,  9.09it/s]\u001b[A\n",
            " 89%|████████▉ | 147/165 [00:16<00:01,  9.10it/s]\u001b[A\n",
            " 90%|████████▉ | 148/165 [00:16<00:01,  9.10it/s]\u001b[A\n",
            " 90%|█████████ | 149/165 [00:16<00:01,  9.10it/s]\u001b[A\n",
            " 91%|█████████ | 150/165 [00:16<00:01,  9.10it/s]\u001b[A\n",
            " 92%|█████████▏| 151/165 [00:16<00:01,  9.09it/s]\u001b[A\n",
            " 92%|█████████▏| 152/165 [00:16<00:01,  9.10it/s]\u001b[A\n",
            " 93%|█████████▎| 153/165 [00:16<00:01,  9.09it/s]\u001b[A\n",
            " 93%|█████████▎| 154/165 [00:16<00:01,  9.10it/s]\u001b[A\n",
            " 94%|█████████▍| 155/165 [00:17<00:01,  9.11it/s]\u001b[A\n",
            " 95%|█████████▍| 156/165 [00:17<00:00,  9.11it/s]\u001b[A\n",
            " 95%|█████████▌| 157/165 [00:17<00:00,  9.12it/s]\u001b[A\n",
            " 96%|█████████▌| 158/165 [00:17<00:00,  9.12it/s]\u001b[A\n",
            " 96%|█████████▋| 159/165 [00:17<00:00,  9.12it/s]\u001b[A\n",
            " 97%|█████████▋| 160/165 [00:17<00:00,  9.11it/s]\u001b[A\n",
            " 98%|█████████▊| 161/165 [00:17<00:00,  9.11it/s]\u001b[A\n",
            " 98%|█████████▊| 162/165 [00:17<00:00,  9.10it/s]\u001b[A\n",
            " 99%|█████████▉| 163/165 [00:17<00:00,  9.10it/s]\u001b[A\n",
            "100%|██████████| 165/165 [00:18<00:00,  9.07it/s]\n",
            " 67%|██████▋   | 4/6 [3:10:32<1:35:17, 2858.84s/it]Didn't find file /content/activebus_bert_final/added_tokens.json. We won't load it.\n",
            "loading file /content/activebus_bert_final/vocab.txt\n",
            "loading file /content/activebus_bert_final/tokenizer.json\n",
            "loading file None\n",
            "loading file /content/activebus_bert_final/special_tokens_map.json\n",
            "loading file /content/activebus_bert_final/tokenizer_config.json\n",
            "========== fold: 0 training ==========\n",
            "========== fold: 0 training ==========\n",
            "========== fold: 0 training ==========\n",
            "========== fold: 0 training ==========\n",
            "========== fold: 0 training ==========\n",
            "Didn't find file /content/activebus_bert_final/added_tokens.json. We won't load it.\n",
            "loading file /content/activebus_bert_final/vocab.txt\n",
            "loading file None\n",
            "loading file /content/activebus_bert_final/special_tokens_map.json\n",
            "loading file /content/activebus_bert_final/tokenizer_config.json\n",
            "loading file /content/activebus_bert_final/tokenizer.json\n",
            "Didn't find file /content/activebus_bert_final/added_tokens.json. We won't load it.\n",
            "loading file /content/activebus_bert_final/vocab.txt\n",
            "loading file None\n",
            "loading file /content/activebus_bert_final/special_tokens_map.json\n",
            "loading file /content/activebus_bert_final/tokenizer_config.json\n",
            "loading file /content/activebus_bert_final/tokenizer.json\n",
            "loading configuration file /content/activebus_bert_final/config.json\n",
            "Model config BertConfig {\n",
            "  \"_name_or_path\": \"activebus/BERT-XD_Review\",\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_ids\": 0,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.15.0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "loading weights file /content/activebus_bert_final/pytorch_model.bin\n",
            "Some weights of the model checkpoint at /content/activebus_bert_final were not used when initializing BertForSequenceClassification: ['cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /content/activebus_bert_final and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: [1][0/306] Elapsed 0m 0s (remain 3m 3s) Loss: 0.7010 \n",
            "Epoch: [1][100/306] Elapsed 0m 33s (remain 1m 7s) Loss: 0.4447 \n",
            "Epoch: [1][200/306] Elapsed 1m 5s (remain 0m 34s) Loss: 0.3580 \n",
            "Epoch: [1][300/306] Elapsed 1m 38s (remain 0m 1s) Loss: 0.3195 \n",
            "Epoch: [1][305/306] Elapsed 1m 39s (remain 0m 0s) Loss: 0.3180 \n",
            "EVAL: [0/77] Elapsed 0m 0s (remain 0m 27s) Loss: 0.0193 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1 - avg_train_loss: 0.3180  avg_val_loss: 0.2665  time: 108s\n",
            "Epoch 1 - avg_train_loss: 0.3180  avg_val_loss: 0.2665  time: 108s\n",
            "Epoch 1 - avg_train_loss: 0.3180  avg_val_loss: 0.2665  time: 108s\n",
            "Epoch 1 - avg_train_loss: 0.3180  avg_val_loss: 0.2665  time: 108s\n",
            "Epoch 1 - avg_train_loss: 0.3180  avg_val_loss: 0.2665  time: 108s\n",
            "Epoch 1 - Score: 0.2664532454374189\n",
            "Epoch 1 - Score: 0.2664532454374189\n",
            "Epoch 1 - Score: 0.2664532454374189\n",
            "Epoch 1 - Score: 0.2664532454374189\n",
            "Epoch 1 - Score: 0.2664532454374189\n",
            "Epoch 1 - Save Best Score: 0.2665 Model\n",
            "Epoch 1 - Save Best Score: 0.2665 Model\n",
            "Epoch 1 - Save Best Score: 0.2665 Model\n",
            "Epoch 1 - Save Best Score: 0.2665 Model\n",
            "Epoch 1 - Save Best Score: 0.2665 Model\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EVAL: [76/77] Elapsed 0m 8s (remain 0m 0s) Loss: 0.2665 \n",
            "Log loss for 0 is 0.2664532454374189\n",
            "Epoch: [2][0/306] Elapsed 0m 0s (remain 2m 55s) Loss: 0.1922 \n",
            "Epoch: [2][100/306] Elapsed 0m 33s (remain 1m 7s) Loss: 0.1484 \n",
            "Epoch: [2][200/306] Elapsed 1m 5s (remain 0m 34s) Loss: 0.1596 \n",
            "Epoch: [2][300/306] Elapsed 1m 38s (remain 0m 1s) Loss: 0.1581 \n",
            "Epoch: [2][305/306] Elapsed 1m 39s (remain 0m 0s) Loss: 0.1568 \n",
            "EVAL: [0/77] Elapsed 0m 0s (remain 0m 27s) Loss: 0.0862 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2 - avg_train_loss: 0.1568  avg_val_loss: 0.2491  time: 108s\n",
            "Epoch 2 - avg_train_loss: 0.1568  avg_val_loss: 0.2491  time: 108s\n",
            "Epoch 2 - avg_train_loss: 0.1568  avg_val_loss: 0.2491  time: 108s\n",
            "Epoch 2 - avg_train_loss: 0.1568  avg_val_loss: 0.2491  time: 108s\n",
            "Epoch 2 - avg_train_loss: 0.1568  avg_val_loss: 0.2491  time: 108s\n",
            "Epoch 2 - Score: 0.24911736933941458\n",
            "Epoch 2 - Score: 0.24911736933941458\n",
            "Epoch 2 - Score: 0.24911736933941458\n",
            "Epoch 2 - Score: 0.24911736933941458\n",
            "Epoch 2 - Score: 0.24911736933941458\n",
            "Epoch 2 - Save Best Score: 0.2491 Model\n",
            "Epoch 2 - Save Best Score: 0.2491 Model\n",
            "Epoch 2 - Save Best Score: 0.2491 Model\n",
            "Epoch 2 - Save Best Score: 0.2491 Model\n",
            "Epoch 2 - Save Best Score: 0.2491 Model\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EVAL: [76/77] Elapsed 0m 8s (remain 0m 0s) Loss: 0.2491 \n",
            "Log loss for 1 is 0.24911736933941458\n",
            "Epoch: [3][0/306] Elapsed 0m 0s (remain 2m 57s) Loss: 0.1222 \n",
            "Epoch: [3][100/306] Elapsed 0m 33s (remain 1m 7s) Loss: 0.0889 \n",
            "Epoch: [3][200/306] Elapsed 1m 5s (remain 0m 34s) Loss: 0.0868 \n",
            "Epoch: [3][300/306] Elapsed 1m 38s (remain 0m 1s) Loss: 0.0974 \n",
            "Epoch: [3][305/306] Elapsed 1m 39s (remain 0m 0s) Loss: 0.0974 \n",
            "EVAL: [0/77] Elapsed 0m 0s (remain 0m 26s) Loss: 0.0138 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3 - avg_train_loss: 0.0974  avg_val_loss: 0.2811  time: 108s\n",
            "Epoch 3 - avg_train_loss: 0.0974  avg_val_loss: 0.2811  time: 108s\n",
            "Epoch 3 - avg_train_loss: 0.0974  avg_val_loss: 0.2811  time: 108s\n",
            "Epoch 3 - avg_train_loss: 0.0974  avg_val_loss: 0.2811  time: 108s\n",
            "Epoch 3 - avg_train_loss: 0.0974  avg_val_loss: 0.2811  time: 108s\n",
            "Epoch 3 - Score: 0.2810755913761666\n",
            "Epoch 3 - Score: 0.2810755913761666\n",
            "Epoch 3 - Score: 0.2810755913761666\n",
            "Epoch 3 - Score: 0.2810755913761666\n",
            "Epoch 3 - Score: 0.2810755913761666\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EVAL: [76/77] Elapsed 0m 8s (remain 0m 0s) Loss: 0.2811 \n",
            "Log loss for 2 is 0.2810755913761666\n",
            "Epoch: [4][0/306] Elapsed 0m 0s (remain 3m 0s) Loss: 0.1699 \n",
            "Epoch: [4][100/306] Elapsed 0m 33s (remain 1m 7s) Loss: 0.0606 \n",
            "Epoch: [4][200/306] Elapsed 1m 5s (remain 0m 34s) Loss: 0.0582 \n",
            "Epoch: [4][300/306] Elapsed 1m 38s (remain 0m 1s) Loss: 0.0601 \n",
            "Epoch: [4][305/306] Elapsed 1m 39s (remain 0m 0s) Loss: 0.0600 \n",
            "EVAL: [0/77] Elapsed 0m 0s (remain 0m 27s) Loss: 0.1181 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4 - avg_train_loss: 0.0600  avg_val_loss: 0.3180  time: 108s\n",
            "Epoch 4 - avg_train_loss: 0.0600  avg_val_loss: 0.3180  time: 108s\n",
            "Epoch 4 - avg_train_loss: 0.0600  avg_val_loss: 0.3180  time: 108s\n",
            "Epoch 4 - avg_train_loss: 0.0600  avg_val_loss: 0.3180  time: 108s\n",
            "Epoch 4 - avg_train_loss: 0.0600  avg_val_loss: 0.3180  time: 108s\n",
            "Epoch 4 - Score: 0.31796695852951\n",
            "Epoch 4 - Score: 0.31796695852951\n",
            "Epoch 4 - Score: 0.31796695852951\n",
            "Epoch 4 - Score: 0.31796695852951\n",
            "Epoch 4 - Score: 0.31796695852951\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EVAL: [76/77] Elapsed 0m 8s (remain 0m 0s) Loss: 0.3180 \n",
            "Log loss for 3 is 0.31796695852951\n",
            "Epoch: [5][0/306] Elapsed 0m 0s (remain 3m 10s) Loss: 0.0075 \n",
            "Epoch: [5][100/306] Elapsed 0m 33s (remain 1m 7s) Loss: 0.0271 \n",
            "Epoch: [5][200/306] Elapsed 1m 5s (remain 0m 34s) Loss: 0.0336 \n",
            "Epoch: [5][300/306] Elapsed 1m 38s (remain 0m 1s) Loss: 0.0339 \n",
            "Epoch: [5][305/306] Elapsed 1m 39s (remain 0m 0s) Loss: 0.0341 \n",
            "EVAL: [0/77] Elapsed 0m 0s (remain 0m 27s) Loss: 0.0107 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5 - avg_train_loss: 0.0341  avg_val_loss: 0.3624  time: 108s\n",
            "Epoch 5 - avg_train_loss: 0.0341  avg_val_loss: 0.3624  time: 108s\n",
            "Epoch 5 - avg_train_loss: 0.0341  avg_val_loss: 0.3624  time: 108s\n",
            "Epoch 5 - avg_train_loss: 0.0341  avg_val_loss: 0.3624  time: 108s\n",
            "Epoch 5 - avg_train_loss: 0.0341  avg_val_loss: 0.3624  time: 108s\n",
            "Epoch 5 - Score: 0.3624342160460297\n",
            "Epoch 5 - Score: 0.3624342160460297\n",
            "Epoch 5 - Score: 0.3624342160460297\n",
            "Epoch 5 - Score: 0.3624342160460297\n",
            "Epoch 5 - Score: 0.3624342160460297\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EVAL: [76/77] Elapsed 0m 8s (remain 0m 0s) Loss: 0.3624 \n",
            "Log loss for 4 is 0.3624342160460297\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "========== fold: 0 result ==========\n",
            "========== fold: 0 result ==========\n",
            "========== fold: 0 result ==========\n",
            "========== fold: 0 result ==========\n",
            "========== fold: 0 result ==========\n",
            "Score: 0.24912\n",
            "Score: 0.24912\n",
            "Score: 0.24912\n",
            "Score: 0.24912\n",
            "Score: 0.24912\n",
            "========== fold: 1 training ==========\n",
            "========== fold: 1 training ==========\n",
            "========== fold: 1 training ==========\n",
            "========== fold: 1 training ==========\n",
            "========== fold: 1 training ==========\n",
            "Didn't find file /content/activebus_bert_final/added_tokens.json. We won't load it.\n",
            "loading file /content/activebus_bert_final/vocab.txt\n",
            "loading file None\n",
            "loading file /content/activebus_bert_final/special_tokens_map.json\n",
            "loading file /content/activebus_bert_final/tokenizer_config.json\n",
            "loading file /content/activebus_bert_final/tokenizer.json\n",
            "Didn't find file /content/activebus_bert_final/added_tokens.json. We won't load it.\n",
            "loading file /content/activebus_bert_final/vocab.txt\n",
            "loading file None\n",
            "loading file /content/activebus_bert_final/special_tokens_map.json\n",
            "loading file /content/activebus_bert_final/tokenizer_config.json\n",
            "loading file /content/activebus_bert_final/tokenizer.json\n",
            "loading configuration file /content/activebus_bert_final/config.json\n",
            "Model config BertConfig {\n",
            "  \"_name_or_path\": \"activebus/BERT-XD_Review\",\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_ids\": 0,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.15.0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "loading weights file /content/activebus_bert_final/pytorch_model.bin\n",
            "Some weights of the model checkpoint at /content/activebus_bert_final were not used when initializing BertForSequenceClassification: ['cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /content/activebus_bert_final and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: [1][0/306] Elapsed 0m 0s (remain 3m 3s) Loss: 0.6119 \n",
            "Epoch: [1][100/306] Elapsed 0m 33s (remain 1m 7s) Loss: 0.4453 \n",
            "Epoch: [1][200/306] Elapsed 1m 5s (remain 0m 34s) Loss: 0.3532 \n",
            "Epoch: [1][300/306] Elapsed 1m 37s (remain 0m 1s) Loss: 0.3121 \n",
            "Epoch: [1][305/306] Elapsed 1m 39s (remain 0m 0s) Loss: 0.3116 \n",
            "EVAL: [0/77] Elapsed 0m 0s (remain 0m 28s) Loss: 0.2055 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1 - avg_train_loss: 0.3116  avg_val_loss: 0.2251  time: 108s\n",
            "Epoch 1 - avg_train_loss: 0.3116  avg_val_loss: 0.2251  time: 108s\n",
            "Epoch 1 - avg_train_loss: 0.3116  avg_val_loss: 0.2251  time: 108s\n",
            "Epoch 1 - avg_train_loss: 0.3116  avg_val_loss: 0.2251  time: 108s\n",
            "Epoch 1 - avg_train_loss: 0.3116  avg_val_loss: 0.2251  time: 108s\n",
            "Epoch 1 - Score: 0.22508559842193962\n",
            "Epoch 1 - Score: 0.22508559842193962\n",
            "Epoch 1 - Score: 0.22508559842193962\n",
            "Epoch 1 - Score: 0.22508559842193962\n",
            "Epoch 1 - Score: 0.22508559842193962\n",
            "Epoch 1 - Save Best Score: 0.2251 Model\n",
            "Epoch 1 - Save Best Score: 0.2251 Model\n",
            "Epoch 1 - Save Best Score: 0.2251 Model\n",
            "Epoch 1 - Save Best Score: 0.2251 Model\n",
            "Epoch 1 - Save Best Score: 0.2251 Model\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EVAL: [76/77] Elapsed 0m 8s (remain 0m 0s) Loss: 0.2251 \n",
            "Log loss for 0 is 0.22508559842193962\n",
            "Epoch: [2][0/306] Elapsed 0m 0s (remain 3m 3s) Loss: 0.3584 \n",
            "Epoch: [2][100/306] Elapsed 0m 33s (remain 1m 7s) Loss: 0.1681 \n",
            "Epoch: [2][200/306] Elapsed 1m 5s (remain 0m 34s) Loss: 0.1725 \n",
            "Epoch: [2][300/306] Elapsed 1m 37s (remain 0m 1s) Loss: 0.1626 \n",
            "Epoch: [2][305/306] Elapsed 1m 39s (remain 0m 0s) Loss: 0.1610 \n",
            "EVAL: [0/77] Elapsed 0m 0s (remain 0m 26s) Loss: 0.1777 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2 - avg_train_loss: 0.1610  avg_val_loss: 0.2191  time: 108s\n",
            "Epoch 2 - avg_train_loss: 0.1610  avg_val_loss: 0.2191  time: 108s\n",
            "Epoch 2 - avg_train_loss: 0.1610  avg_val_loss: 0.2191  time: 108s\n",
            "Epoch 2 - avg_train_loss: 0.1610  avg_val_loss: 0.2191  time: 108s\n",
            "Epoch 2 - avg_train_loss: 0.1610  avg_val_loss: 0.2191  time: 108s\n",
            "Epoch 2 - Score: 0.21909793251298046\n",
            "Epoch 2 - Score: 0.21909793251298046\n",
            "Epoch 2 - Score: 0.21909793251298046\n",
            "Epoch 2 - Score: 0.21909793251298046\n",
            "Epoch 2 - Score: 0.21909793251298046\n",
            "Epoch 2 - Save Best Score: 0.2191 Model\n",
            "Epoch 2 - Save Best Score: 0.2191 Model\n",
            "Epoch 2 - Save Best Score: 0.2191 Model\n",
            "Epoch 2 - Save Best Score: 0.2191 Model\n",
            "Epoch 2 - Save Best Score: 0.2191 Model\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EVAL: [76/77] Elapsed 0m 8s (remain 0m 0s) Loss: 0.2191 \n",
            "Log loss for 1 is 0.21909793251298046\n",
            "Epoch: [3][0/306] Elapsed 0m 0s (remain 2m 56s) Loss: 0.0957 \n",
            "Epoch: [3][100/306] Elapsed 0m 33s (remain 1m 6s) Loss: 0.0682 \n",
            "Epoch: [3][200/306] Elapsed 1m 5s (remain 0m 34s) Loss: 0.0814 \n",
            "Epoch: [3][300/306] Elapsed 1m 37s (remain 0m 1s) Loss: 0.0918 \n",
            "Epoch: [3][305/306] Elapsed 1m 39s (remain 0m 0s) Loss: 0.0919 \n",
            "EVAL: [0/77] Elapsed 0m 0s (remain 0m 27s) Loss: 0.1853 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3 - avg_train_loss: 0.0919  avg_val_loss: 0.2023  time: 108s\n",
            "Epoch 3 - avg_train_loss: 0.0919  avg_val_loss: 0.2023  time: 108s\n",
            "Epoch 3 - avg_train_loss: 0.0919  avg_val_loss: 0.2023  time: 108s\n",
            "Epoch 3 - avg_train_loss: 0.0919  avg_val_loss: 0.2023  time: 108s\n",
            "Epoch 3 - avg_train_loss: 0.0919  avg_val_loss: 0.2023  time: 108s\n",
            "Epoch 3 - Score: 0.20228672050529395\n",
            "Epoch 3 - Score: 0.20228672050529395\n",
            "Epoch 3 - Score: 0.20228672050529395\n",
            "Epoch 3 - Score: 0.20228672050529395\n",
            "Epoch 3 - Score: 0.20228672050529395\n",
            "Epoch 3 - Save Best Score: 0.2023 Model\n",
            "Epoch 3 - Save Best Score: 0.2023 Model\n",
            "Epoch 3 - Save Best Score: 0.2023 Model\n",
            "Epoch 3 - Save Best Score: 0.2023 Model\n",
            "Epoch 3 - Save Best Score: 0.2023 Model\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EVAL: [76/77] Elapsed 0m 8s (remain 0m 0s) Loss: 0.2023 \n",
            "Log loss for 2 is 0.20228672050529395\n",
            "Epoch: [4][0/306] Elapsed 0m 0s (remain 2m 58s) Loss: 0.0322 \n",
            "Epoch: [4][100/306] Elapsed 0m 33s (remain 1m 7s) Loss: 0.0530 \n",
            "Epoch: [4][200/306] Elapsed 1m 5s (remain 0m 34s) Loss: 0.0575 \n",
            "Epoch: [4][300/306] Elapsed 1m 38s (remain 0m 1s) Loss: 0.0522 \n",
            "Epoch: [4][305/306] Elapsed 1m 39s (remain 0m 0s) Loss: 0.0524 \n",
            "EVAL: [0/77] Elapsed 0m 0s (remain 0m 27s) Loss: 0.3073 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4 - avg_train_loss: 0.0524  avg_val_loss: 0.2688  time: 108s\n",
            "Epoch 4 - avg_train_loss: 0.0524  avg_val_loss: 0.2688  time: 108s\n",
            "Epoch 4 - avg_train_loss: 0.0524  avg_val_loss: 0.2688  time: 108s\n",
            "Epoch 4 - avg_train_loss: 0.0524  avg_val_loss: 0.2688  time: 108s\n",
            "Epoch 4 - avg_train_loss: 0.0524  avg_val_loss: 0.2688  time: 108s\n",
            "Epoch 4 - Score: 0.2687620660569538\n",
            "Epoch 4 - Score: 0.2687620660569538\n",
            "Epoch 4 - Score: 0.2687620660569538\n",
            "Epoch 4 - Score: 0.2687620660569538\n",
            "Epoch 4 - Score: 0.2687620660569538\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EVAL: [76/77] Elapsed 0m 8s (remain 0m 0s) Loss: 0.2688 \n",
            "Log loss for 3 is 0.2687620660569538\n",
            "Epoch: [5][0/306] Elapsed 0m 0s (remain 3m 3s) Loss: 0.0043 \n",
            "Epoch: [5][100/306] Elapsed 0m 33s (remain 1m 7s) Loss: 0.0413 \n",
            "Epoch: [5][200/306] Elapsed 1m 5s (remain 0m 34s) Loss: 0.0433 \n",
            "Epoch: [5][300/306] Elapsed 1m 38s (remain 0m 1s) Loss: 0.0382 \n",
            "Epoch: [5][305/306] Elapsed 1m 39s (remain 0m 0s) Loss: 0.0378 \n",
            "EVAL: [0/77] Elapsed 0m 0s (remain 0m 27s) Loss: 0.3981 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5 - avg_train_loss: 0.0378  avg_val_loss: 0.3089  time: 108s\n",
            "Epoch 5 - avg_train_loss: 0.0378  avg_val_loss: 0.3089  time: 108s\n",
            "Epoch 5 - avg_train_loss: 0.0378  avg_val_loss: 0.3089  time: 108s\n",
            "Epoch 5 - avg_train_loss: 0.0378  avg_val_loss: 0.3089  time: 108s\n",
            "Epoch 5 - avg_train_loss: 0.0378  avg_val_loss: 0.3089  time: 108s\n",
            "Epoch 5 - Score: 0.3089326777340413\n",
            "Epoch 5 - Score: 0.3089326777340413\n",
            "Epoch 5 - Score: 0.3089326777340413\n",
            "Epoch 5 - Score: 0.3089326777340413\n",
            "Epoch 5 - Score: 0.3089326777340413\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EVAL: [76/77] Elapsed 0m 8s (remain 0m 0s) Loss: 0.3089 \n",
            "Log loss for 4 is 0.3089326777340413\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "========== fold: 1 result ==========\n",
            "========== fold: 1 result ==========\n",
            "========== fold: 1 result ==========\n",
            "========== fold: 1 result ==========\n",
            "========== fold: 1 result ==========\n",
            "Score: 0.20229\n",
            "Score: 0.20229\n",
            "Score: 0.20229\n",
            "Score: 0.20229\n",
            "Score: 0.20229\n",
            "========== fold: 2 training ==========\n",
            "========== fold: 2 training ==========\n",
            "========== fold: 2 training ==========\n",
            "========== fold: 2 training ==========\n",
            "========== fold: 2 training ==========\n",
            "Didn't find file /content/activebus_bert_final/added_tokens.json. We won't load it.\n",
            "loading file /content/activebus_bert_final/vocab.txt\n",
            "loading file None\n",
            "loading file /content/activebus_bert_final/special_tokens_map.json\n",
            "loading file /content/activebus_bert_final/tokenizer_config.json\n",
            "loading file /content/activebus_bert_final/tokenizer.json\n",
            "Didn't find file /content/activebus_bert_final/added_tokens.json. We won't load it.\n",
            "loading file /content/activebus_bert_final/vocab.txt\n",
            "loading file None\n",
            "loading file /content/activebus_bert_final/special_tokens_map.json\n",
            "loading file /content/activebus_bert_final/tokenizer_config.json\n",
            "loading file /content/activebus_bert_final/tokenizer.json\n",
            "loading configuration file /content/activebus_bert_final/config.json\n",
            "Model config BertConfig {\n",
            "  \"_name_or_path\": \"activebus/BERT-XD_Review\",\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_ids\": 0,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.15.0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "loading weights file /content/activebus_bert_final/pytorch_model.bin\n",
            "Some weights of the model checkpoint at /content/activebus_bert_final were not used when initializing BertForSequenceClassification: ['cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /content/activebus_bert_final and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: [1][0/306] Elapsed 0m 0s (remain 3m 11s) Loss: 0.6955 \n",
            "Epoch: [1][100/306] Elapsed 0m 33s (remain 1m 7s) Loss: 0.4537 \n",
            "Epoch: [1][200/306] Elapsed 1m 5s (remain 0m 34s) Loss: 0.3548 \n",
            "Epoch: [1][300/306] Elapsed 1m 38s (remain 0m 1s) Loss: 0.3135 \n",
            "Epoch: [1][305/306] Elapsed 1m 39s (remain 0m 0s) Loss: 0.3112 \n",
            "EVAL: [0/77] Elapsed 0m 0s (remain 0m 27s) Loss: 0.3737 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1 - avg_train_loss: 0.3112  avg_val_loss: 0.2170  time: 108s\n",
            "Epoch 1 - avg_train_loss: 0.3112  avg_val_loss: 0.2170  time: 108s\n",
            "Epoch 1 - avg_train_loss: 0.3112  avg_val_loss: 0.2170  time: 108s\n",
            "Epoch 1 - avg_train_loss: 0.3112  avg_val_loss: 0.2170  time: 108s\n",
            "Epoch 1 - avg_train_loss: 0.3112  avg_val_loss: 0.2170  time: 108s\n",
            "Epoch 1 - Score: 0.21699178295085525\n",
            "Epoch 1 - Score: 0.21699178295085525\n",
            "Epoch 1 - Score: 0.21699178295085525\n",
            "Epoch 1 - Score: 0.21699178295085525\n",
            "Epoch 1 - Score: 0.21699178295085525\n",
            "Epoch 1 - Save Best Score: 0.2170 Model\n",
            "Epoch 1 - Save Best Score: 0.2170 Model\n",
            "Epoch 1 - Save Best Score: 0.2170 Model\n",
            "Epoch 1 - Save Best Score: 0.2170 Model\n",
            "Epoch 1 - Save Best Score: 0.2170 Model\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EVAL: [76/77] Elapsed 0m 8s (remain 0m 0s) Loss: 0.2170 \n",
            "Log loss for 0 is 0.21699178295085525\n",
            "Epoch: [2][0/306] Elapsed 0m 0s (remain 2m 58s) Loss: 0.2592 \n",
            "Epoch: [2][100/306] Elapsed 0m 33s (remain 1m 7s) Loss: 0.1571 \n",
            "Epoch: [2][200/306] Elapsed 1m 5s (remain 0m 34s) Loss: 0.1629 \n",
            "Epoch: [2][300/306] Elapsed 1m 38s (remain 0m 1s) Loss: 0.1674 \n",
            "Epoch: [2][305/306] Elapsed 1m 39s (remain 0m 0s) Loss: 0.1681 \n",
            "EVAL: [0/77] Elapsed 0m 0s (remain 0m 26s) Loss: 0.2091 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2 - avg_train_loss: 0.1681  avg_val_loss: 0.1923  time: 108s\n",
            "Epoch 2 - avg_train_loss: 0.1681  avg_val_loss: 0.1923  time: 108s\n",
            "Epoch 2 - avg_train_loss: 0.1681  avg_val_loss: 0.1923  time: 108s\n",
            "Epoch 2 - avg_train_loss: 0.1681  avg_val_loss: 0.1923  time: 108s\n",
            "Epoch 2 - avg_train_loss: 0.1681  avg_val_loss: 0.1923  time: 108s\n",
            "Epoch 2 - Score: 0.19225669272066365\n",
            "Epoch 2 - Score: 0.19225669272066365\n",
            "Epoch 2 - Score: 0.19225669272066365\n",
            "Epoch 2 - Score: 0.19225669272066365\n",
            "Epoch 2 - Score: 0.19225669272066365\n",
            "Epoch 2 - Save Best Score: 0.1923 Model\n",
            "Epoch 2 - Save Best Score: 0.1923 Model\n",
            "Epoch 2 - Save Best Score: 0.1923 Model\n",
            "Epoch 2 - Save Best Score: 0.1923 Model\n",
            "Epoch 2 - Save Best Score: 0.1923 Model\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EVAL: [76/77] Elapsed 0m 8s (remain 0m 0s) Loss: 0.1923 \n",
            "Log loss for 1 is 0.19225669272066365\n",
            "Epoch: [3][0/306] Elapsed 0m 0s (remain 2m 59s) Loss: 0.0381 \n",
            "Epoch: [3][100/306] Elapsed 0m 33s (remain 1m 7s) Loss: 0.0962 \n",
            "Epoch: [3][200/306] Elapsed 1m 5s (remain 0m 34s) Loss: 0.0942 \n",
            "Epoch: [3][300/306] Elapsed 1m 38s (remain 0m 1s) Loss: 0.1020 \n",
            "Epoch: [3][305/306] Elapsed 1m 39s (remain 0m 0s) Loss: 0.1020 \n",
            "EVAL: [0/77] Elapsed 0m 0s (remain 0m 27s) Loss: 0.0403 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3 - avg_train_loss: 0.1020  avg_val_loss: 0.2381  time: 108s\n",
            "Epoch 3 - avg_train_loss: 0.1020  avg_val_loss: 0.2381  time: 108s\n",
            "Epoch 3 - avg_train_loss: 0.1020  avg_val_loss: 0.2381  time: 108s\n",
            "Epoch 3 - avg_train_loss: 0.1020  avg_val_loss: 0.2381  time: 108s\n",
            "Epoch 3 - avg_train_loss: 0.1020  avg_val_loss: 0.2381  time: 108s\n",
            "Epoch 3 - Score: 0.23814970658793772\n",
            "Epoch 3 - Score: 0.23814970658793772\n",
            "Epoch 3 - Score: 0.23814970658793772\n",
            "Epoch 3 - Score: 0.23814970658793772\n",
            "Epoch 3 - Score: 0.23814970658793772\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EVAL: [76/77] Elapsed 0m 8s (remain 0m 0s) Loss: 0.2381 \n",
            "Log loss for 2 is 0.23814970658793772\n",
            "Epoch: [4][0/306] Elapsed 0m 0s (remain 3m 2s) Loss: 0.0127 \n",
            "Epoch: [4][100/306] Elapsed 0m 33s (remain 1m 7s) Loss: 0.0498 \n",
            "Epoch: [4][200/306] Elapsed 1m 5s (remain 0m 34s) Loss: 0.0477 \n",
            "Epoch: [4][300/306] Elapsed 1m 38s (remain 0m 1s) Loss: 0.0535 \n",
            "Epoch: [4][305/306] Elapsed 1m 39s (remain 0m 0s) Loss: 0.0529 \n",
            "EVAL: [0/77] Elapsed 0m 0s (remain 0m 27s) Loss: 0.0164 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4 - avg_train_loss: 0.0529  avg_val_loss: 0.2909  time: 108s\n",
            "Epoch 4 - avg_train_loss: 0.0529  avg_val_loss: 0.2909  time: 108s\n",
            "Epoch 4 - avg_train_loss: 0.0529  avg_val_loss: 0.2909  time: 108s\n",
            "Epoch 4 - avg_train_loss: 0.0529  avg_val_loss: 0.2909  time: 108s\n",
            "Epoch 4 - avg_train_loss: 0.0529  avg_val_loss: 0.2909  time: 108s\n",
            "Epoch 4 - Score: 0.29085278742815374\n",
            "Epoch 4 - Score: 0.29085278742815374\n",
            "Epoch 4 - Score: 0.29085278742815374\n",
            "Epoch 4 - Score: 0.29085278742815374\n",
            "Epoch 4 - Score: 0.29085278742815374\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EVAL: [76/77] Elapsed 0m 8s (remain 0m 0s) Loss: 0.2909 \n",
            "Log loss for 3 is 0.29085278742815374\n",
            "Epoch: [5][0/306] Elapsed 0m 0s (remain 3m 8s) Loss: 0.0051 \n",
            "Epoch: [5][100/306] Elapsed 0m 33s (remain 1m 7s) Loss: 0.0362 \n",
            "Epoch: [5][200/306] Elapsed 1m 5s (remain 0m 34s) Loss: 0.0396 \n",
            "Epoch: [5][300/306] Elapsed 1m 38s (remain 0m 1s) Loss: 0.0384 \n",
            "Epoch: [5][305/306] Elapsed 1m 39s (remain 0m 0s) Loss: 0.0378 \n",
            "EVAL: [0/77] Elapsed 0m 0s (remain 0m 26s) Loss: 0.0460 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5 - avg_train_loss: 0.0378  avg_val_loss: 0.3005  time: 108s\n",
            "Epoch 5 - avg_train_loss: 0.0378  avg_val_loss: 0.3005  time: 108s\n",
            "Epoch 5 - avg_train_loss: 0.0378  avg_val_loss: 0.3005  time: 108s\n",
            "Epoch 5 - avg_train_loss: 0.0378  avg_val_loss: 0.3005  time: 108s\n",
            "Epoch 5 - avg_train_loss: 0.0378  avg_val_loss: 0.3005  time: 108s\n",
            "Epoch 5 - Score: 0.3005359455187499\n",
            "Epoch 5 - Score: 0.3005359455187499\n",
            "Epoch 5 - Score: 0.3005359455187499\n",
            "Epoch 5 - Score: 0.3005359455187499\n",
            "Epoch 5 - Score: 0.3005359455187499\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EVAL: [76/77] Elapsed 0m 8s (remain 0m 0s) Loss: 0.3005 \n",
            "Log loss for 4 is 0.3005359455187499\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "========== fold: 2 result ==========\n",
            "========== fold: 2 result ==========\n",
            "========== fold: 2 result ==========\n",
            "========== fold: 2 result ==========\n",
            "========== fold: 2 result ==========\n",
            "Score: 0.19226\n",
            "Score: 0.19226\n",
            "Score: 0.19226\n",
            "Score: 0.19226\n",
            "Score: 0.19226\n",
            "========== fold: 3 training ==========\n",
            "========== fold: 3 training ==========\n",
            "========== fold: 3 training ==========\n",
            "========== fold: 3 training ==========\n",
            "========== fold: 3 training ==========\n",
            "Didn't find file /content/activebus_bert_final/added_tokens.json. We won't load it.\n",
            "loading file /content/activebus_bert_final/vocab.txt\n",
            "loading file None\n",
            "loading file /content/activebus_bert_final/special_tokens_map.json\n",
            "loading file /content/activebus_bert_final/tokenizer_config.json\n",
            "loading file /content/activebus_bert_final/tokenizer.json\n",
            "Didn't find file /content/activebus_bert_final/added_tokens.json. We won't load it.\n",
            "loading file /content/activebus_bert_final/vocab.txt\n",
            "loading file None\n",
            "loading file /content/activebus_bert_final/special_tokens_map.json\n",
            "loading file /content/activebus_bert_final/tokenizer_config.json\n",
            "loading file /content/activebus_bert_final/tokenizer.json\n",
            "loading configuration file /content/activebus_bert_final/config.json\n",
            "Model config BertConfig {\n",
            "  \"_name_or_path\": \"activebus/BERT-XD_Review\",\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_ids\": 0,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.15.0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "loading weights file /content/activebus_bert_final/pytorch_model.bin\n",
            "Some weights of the model checkpoint at /content/activebus_bert_final were not used when initializing BertForSequenceClassification: ['cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /content/activebus_bert_final and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: [1][0/306] Elapsed 0m 0s (remain 3m 4s) Loss: 0.6756 \n",
            "Epoch: [1][100/306] Elapsed 0m 33s (remain 1m 7s) Loss: 0.4580 \n",
            "Epoch: [1][200/306] Elapsed 1m 5s (remain 0m 34s) Loss: 0.3682 \n",
            "Epoch: [1][300/306] Elapsed 1m 38s (remain 0m 1s) Loss: 0.3251 \n",
            "Epoch: [1][305/306] Elapsed 1m 39s (remain 0m 0s) Loss: 0.3218 \n",
            "EVAL: [0/77] Elapsed 0m 0s (remain 0m 27s) Loss: 0.2292 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1 - avg_train_loss: 0.3218  avg_val_loss: 0.2077  time: 108s\n",
            "Epoch 1 - avg_train_loss: 0.3218  avg_val_loss: 0.2077  time: 108s\n",
            "Epoch 1 - avg_train_loss: 0.3218  avg_val_loss: 0.2077  time: 108s\n",
            "Epoch 1 - avg_train_loss: 0.3218  avg_val_loss: 0.2077  time: 108s\n",
            "Epoch 1 - avg_train_loss: 0.3218  avg_val_loss: 0.2077  time: 108s\n",
            "Epoch 1 - Score: 0.20768337129116568\n",
            "Epoch 1 - Score: 0.20768337129116568\n",
            "Epoch 1 - Score: 0.20768337129116568\n",
            "Epoch 1 - Score: 0.20768337129116568\n",
            "Epoch 1 - Score: 0.20768337129116568\n",
            "Epoch 1 - Save Best Score: 0.2077 Model\n",
            "Epoch 1 - Save Best Score: 0.2077 Model\n",
            "Epoch 1 - Save Best Score: 0.2077 Model\n",
            "Epoch 1 - Save Best Score: 0.2077 Model\n",
            "Epoch 1 - Save Best Score: 0.2077 Model\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EVAL: [76/77] Elapsed 0m 8s (remain 0m 0s) Loss: 0.2077 \n",
            "Log loss for 0 is 0.20768337129116568\n",
            "Epoch: [2][0/306] Elapsed 0m 0s (remain 3m 0s) Loss: 0.1723 \n",
            "Epoch: [2][100/306] Elapsed 0m 33s (remain 1m 7s) Loss: 0.1741 \n",
            "Epoch: [2][200/306] Elapsed 1m 5s (remain 0m 34s) Loss: 0.1689 \n",
            "Epoch: [2][300/306] Elapsed 1m 38s (remain 0m 1s) Loss: 0.1635 \n",
            "Epoch: [2][305/306] Elapsed 1m 39s (remain 0m 0s) Loss: 0.1624 \n",
            "EVAL: [0/77] Elapsed 0m 0s (remain 0m 28s) Loss: 0.1655 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2 - avg_train_loss: 0.1624  avg_val_loss: 0.1891  time: 108s\n",
            "Epoch 2 - avg_train_loss: 0.1624  avg_val_loss: 0.1891  time: 108s\n",
            "Epoch 2 - avg_train_loss: 0.1624  avg_val_loss: 0.1891  time: 108s\n",
            "Epoch 2 - avg_train_loss: 0.1624  avg_val_loss: 0.1891  time: 108s\n",
            "Epoch 2 - avg_train_loss: 0.1624  avg_val_loss: 0.1891  time: 108s\n",
            "Epoch 2 - Score: 0.18907079778967642\n",
            "Epoch 2 - Score: 0.18907079778967642\n",
            "Epoch 2 - Score: 0.18907079778967642\n",
            "Epoch 2 - Score: 0.18907079778967642\n",
            "Epoch 2 - Score: 0.18907079778967642\n",
            "Epoch 2 - Save Best Score: 0.1891 Model\n",
            "Epoch 2 - Save Best Score: 0.1891 Model\n",
            "Epoch 2 - Save Best Score: 0.1891 Model\n",
            "Epoch 2 - Save Best Score: 0.1891 Model\n",
            "Epoch 2 - Save Best Score: 0.1891 Model\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EVAL: [76/77] Elapsed 0m 8s (remain 0m 0s) Loss: 0.1891 \n",
            "Log loss for 1 is 0.18907079778967642\n",
            "Epoch: [3][0/306] Elapsed 0m 0s (remain 3m 0s) Loss: 0.0397 \n",
            "Epoch: [3][100/306] Elapsed 0m 33s (remain 1m 7s) Loss: 0.0860 \n",
            "Epoch: [3][200/306] Elapsed 1m 5s (remain 0m 34s) Loss: 0.0922 \n",
            "Epoch: [3][300/306] Elapsed 1m 38s (remain 0m 1s) Loss: 0.0900 \n",
            "Epoch: [3][305/306] Elapsed 1m 39s (remain 0m 0s) Loss: 0.0891 \n",
            "EVAL: [0/77] Elapsed 0m 0s (remain 0m 27s) Loss: 0.3332 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3 - avg_train_loss: 0.0891  avg_val_loss: 0.2841  time: 108s\n",
            "Epoch 3 - avg_train_loss: 0.0891  avg_val_loss: 0.2841  time: 108s\n",
            "Epoch 3 - avg_train_loss: 0.0891  avg_val_loss: 0.2841  time: 108s\n",
            "Epoch 3 - avg_train_loss: 0.0891  avg_val_loss: 0.2841  time: 108s\n",
            "Epoch 3 - avg_train_loss: 0.0891  avg_val_loss: 0.2841  time: 108s\n",
            "Epoch 3 - Score: 0.2840609219344219\n",
            "Epoch 3 - Score: 0.2840609219344219\n",
            "Epoch 3 - Score: 0.2840609219344219\n",
            "Epoch 3 - Score: 0.2840609219344219\n",
            "Epoch 3 - Score: 0.2840609219344219\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EVAL: [76/77] Elapsed 0m 8s (remain 0m 0s) Loss: 0.2841 \n",
            "Log loss for 2 is 0.2840609219344219\n",
            "Epoch: [4][0/306] Elapsed 0m 0s (remain 3m 2s) Loss: 0.0107 \n",
            "Epoch: [4][100/306] Elapsed 0m 33s (remain 1m 7s) Loss: 0.0407 \n",
            "Epoch: [4][200/306] Elapsed 1m 5s (remain 0m 34s) Loss: 0.0440 \n",
            "Epoch: [4][300/306] Elapsed 1m 38s (remain 0m 1s) Loss: 0.0518 \n",
            "Epoch: [4][305/306] Elapsed 1m 39s (remain 0m 0s) Loss: 0.0516 \n",
            "EVAL: [0/77] Elapsed 0m 0s (remain 0m 27s) Loss: 0.4959 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4 - avg_train_loss: 0.0516  avg_val_loss: 0.2993  time: 108s\n",
            "Epoch 4 - avg_train_loss: 0.0516  avg_val_loss: 0.2993  time: 108s\n",
            "Epoch 4 - avg_train_loss: 0.0516  avg_val_loss: 0.2993  time: 108s\n",
            "Epoch 4 - avg_train_loss: 0.0516  avg_val_loss: 0.2993  time: 108s\n",
            "Epoch 4 - avg_train_loss: 0.0516  avg_val_loss: 0.2993  time: 108s\n",
            "Epoch 4 - Score: 0.2993170218053122\n",
            "Epoch 4 - Score: 0.2993170218053122\n",
            "Epoch 4 - Score: 0.2993170218053122\n",
            "Epoch 4 - Score: 0.2993170218053122\n",
            "Epoch 4 - Score: 0.2993170218053122\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EVAL: [76/77] Elapsed 0m 8s (remain 0m 0s) Loss: 0.2993 \n",
            "Log loss for 3 is 0.2993170218053122\n",
            "Epoch: [5][0/306] Elapsed 0m 0s (remain 3m 3s) Loss: 0.0210 \n",
            "Epoch: [5][100/306] Elapsed 0m 33s (remain 1m 7s) Loss: 0.0434 \n",
            "Epoch: [5][200/306] Elapsed 1m 5s (remain 0m 34s) Loss: 0.0435 \n",
            "Epoch: [5][300/306] Elapsed 1m 38s (remain 0m 1s) Loss: 0.0385 \n",
            "Epoch: [5][305/306] Elapsed 1m 39s (remain 0m 0s) Loss: 0.0386 \n",
            "EVAL: [0/77] Elapsed 0m 0s (remain 0m 26s) Loss: 0.0698 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5 - avg_train_loss: 0.0386  avg_val_loss: 0.3245  time: 108s\n",
            "Epoch 5 - avg_train_loss: 0.0386  avg_val_loss: 0.3245  time: 108s\n",
            "Epoch 5 - avg_train_loss: 0.0386  avg_val_loss: 0.3245  time: 108s\n",
            "Epoch 5 - avg_train_loss: 0.0386  avg_val_loss: 0.3245  time: 108s\n",
            "Epoch 5 - avg_train_loss: 0.0386  avg_val_loss: 0.3245  time: 108s\n",
            "Epoch 5 - Score: 0.32451902496091073\n",
            "Epoch 5 - Score: 0.32451902496091073\n",
            "Epoch 5 - Score: 0.32451902496091073\n",
            "Epoch 5 - Score: 0.32451902496091073\n",
            "Epoch 5 - Score: 0.32451902496091073\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EVAL: [76/77] Elapsed 0m 8s (remain 0m 0s) Loss: 0.3245 \n",
            "Log loss for 4 is 0.32451902496091073\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "========== fold: 3 result ==========\n",
            "========== fold: 3 result ==========\n",
            "========== fold: 3 result ==========\n",
            "========== fold: 3 result ==========\n",
            "========== fold: 3 result ==========\n",
            "Score: 0.18907\n",
            "Score: 0.18907\n",
            "Score: 0.18907\n",
            "Score: 0.18907\n",
            "Score: 0.18907\n",
            "========== fold: 4 training ==========\n",
            "========== fold: 4 training ==========\n",
            "========== fold: 4 training ==========\n",
            "========== fold: 4 training ==========\n",
            "========== fold: 4 training ==========\n",
            "Didn't find file /content/activebus_bert_final/added_tokens.json. We won't load it.\n",
            "loading file /content/activebus_bert_final/vocab.txt\n",
            "loading file None\n",
            "loading file /content/activebus_bert_final/special_tokens_map.json\n",
            "loading file /content/activebus_bert_final/tokenizer_config.json\n",
            "loading file /content/activebus_bert_final/tokenizer.json\n",
            "Didn't find file /content/activebus_bert_final/added_tokens.json. We won't load it.\n",
            "loading file /content/activebus_bert_final/vocab.txt\n",
            "loading file None\n",
            "loading file /content/activebus_bert_final/special_tokens_map.json\n",
            "loading file /content/activebus_bert_final/tokenizer_config.json\n",
            "loading file /content/activebus_bert_final/tokenizer.json\n",
            "loading configuration file /content/activebus_bert_final/config.json\n",
            "Model config BertConfig {\n",
            "  \"_name_or_path\": \"activebus/BERT-XD_Review\",\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_ids\": 0,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.15.0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "loading weights file /content/activebus_bert_final/pytorch_model.bin\n",
            "Some weights of the model checkpoint at /content/activebus_bert_final were not used when initializing BertForSequenceClassification: ['cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /content/activebus_bert_final and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: [1][0/306] Elapsed 0m 0s (remain 2m 58s) Loss: 0.7086 \n",
            "Epoch: [1][100/306] Elapsed 0m 33s (remain 1m 7s) Loss: 0.4769 \n",
            "Epoch: [1][200/306] Elapsed 1m 5s (remain 0m 34s) Loss: 0.3763 \n",
            "Epoch: [1][300/306] Elapsed 1m 38s (remain 0m 1s) Loss: 0.3354 \n",
            "Epoch: [1][305/306] Elapsed 1m 39s (remain 0m 0s) Loss: 0.3337 \n",
            "EVAL: [0/77] Elapsed 0m 0s (remain 0m 26s) Loss: 0.3189 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1 - avg_train_loss: 0.3337  avg_val_loss: 0.2270  time: 108s\n",
            "Epoch 1 - avg_train_loss: 0.3337  avg_val_loss: 0.2270  time: 108s\n",
            "Epoch 1 - avg_train_loss: 0.3337  avg_val_loss: 0.2270  time: 108s\n",
            "Epoch 1 - avg_train_loss: 0.3337  avg_val_loss: 0.2270  time: 108s\n",
            "Epoch 1 - avg_train_loss: 0.3337  avg_val_loss: 0.2270  time: 108s\n",
            "Epoch 1 - Score: 0.22704494367987405\n",
            "Epoch 1 - Score: 0.22704494367987405\n",
            "Epoch 1 - Score: 0.22704494367987405\n",
            "Epoch 1 - Score: 0.22704494367987405\n",
            "Epoch 1 - Score: 0.22704494367987405\n",
            "Epoch 1 - Save Best Score: 0.2270 Model\n",
            "Epoch 1 - Save Best Score: 0.2270 Model\n",
            "Epoch 1 - Save Best Score: 0.2270 Model\n",
            "Epoch 1 - Save Best Score: 0.2270 Model\n",
            "Epoch 1 - Save Best Score: 0.2270 Model\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EVAL: [76/77] Elapsed 0m 8s (remain 0m 0s) Loss: 0.2270 \n",
            "Log loss for 0 is 0.22704494367987405\n",
            "Epoch: [2][0/306] Elapsed 0m 0s (remain 2m 57s) Loss: 0.1112 \n",
            "Epoch: [2][100/306] Elapsed 0m 33s (remain 1m 7s) Loss: 0.1544 \n",
            "Epoch: [2][200/306] Elapsed 1m 5s (remain 0m 34s) Loss: 0.1673 \n",
            "Epoch: [2][300/306] Elapsed 1m 37s (remain 0m 1s) Loss: 0.1646 \n",
            "Epoch: [2][305/306] Elapsed 1m 39s (remain 0m 0s) Loss: 0.1669 \n",
            "EVAL: [0/77] Elapsed 0m 0s (remain 0m 26s) Loss: 0.3295 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2 - avg_train_loss: 0.1669  avg_val_loss: 0.1893  time: 108s\n",
            "Epoch 2 - avg_train_loss: 0.1669  avg_val_loss: 0.1893  time: 108s\n",
            "Epoch 2 - avg_train_loss: 0.1669  avg_val_loss: 0.1893  time: 108s\n",
            "Epoch 2 - avg_train_loss: 0.1669  avg_val_loss: 0.1893  time: 108s\n",
            "Epoch 2 - avg_train_loss: 0.1669  avg_val_loss: 0.1893  time: 108s\n",
            "Epoch 2 - Score: 0.18930521055019195\n",
            "Epoch 2 - Score: 0.18930521055019195\n",
            "Epoch 2 - Score: 0.18930521055019195\n",
            "Epoch 2 - Score: 0.18930521055019195\n",
            "Epoch 2 - Score: 0.18930521055019195\n",
            "Epoch 2 - Save Best Score: 0.1893 Model\n",
            "Epoch 2 - Save Best Score: 0.1893 Model\n",
            "Epoch 2 - Save Best Score: 0.1893 Model\n",
            "Epoch 2 - Save Best Score: 0.1893 Model\n",
            "Epoch 2 - Save Best Score: 0.1893 Model\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EVAL: [76/77] Elapsed 0m 8s (remain 0m 0s) Loss: 0.1893 \n",
            "Log loss for 1 is 0.18930521055019195\n",
            "Epoch: [3][0/306] Elapsed 0m 0s (remain 3m 2s) Loss: 0.1279 \n",
            "Epoch: [3][100/306] Elapsed 0m 33s (remain 1m 7s) Loss: 0.0910 \n",
            "Epoch: [3][200/306] Elapsed 1m 5s (remain 0m 34s) Loss: 0.0983 \n",
            "Epoch: [3][300/306] Elapsed 1m 38s (remain 0m 1s) Loss: 0.0943 \n",
            "Epoch: [3][305/306] Elapsed 1m 39s (remain 0m 0s) Loss: 0.0955 \n",
            "EVAL: [0/77] Elapsed 0m 0s (remain 0m 26s) Loss: 0.3812 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3 - avg_train_loss: 0.0955  avg_val_loss: 0.2203  time: 108s\n",
            "Epoch 3 - avg_train_loss: 0.0955  avg_val_loss: 0.2203  time: 108s\n",
            "Epoch 3 - avg_train_loss: 0.0955  avg_val_loss: 0.2203  time: 108s\n",
            "Epoch 3 - avg_train_loss: 0.0955  avg_val_loss: 0.2203  time: 108s\n",
            "Epoch 3 - avg_train_loss: 0.0955  avg_val_loss: 0.2203  time: 108s\n",
            "Epoch 3 - Score: 0.22034996912963445\n",
            "Epoch 3 - Score: 0.22034996912963445\n",
            "Epoch 3 - Score: 0.22034996912963445\n",
            "Epoch 3 - Score: 0.22034996912963445\n",
            "Epoch 3 - Score: 0.22034996912963445\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EVAL: [76/77] Elapsed 0m 8s (remain 0m 0s) Loss: 0.2203 \n",
            "Log loss for 2 is 0.22034996912963445\n",
            "Epoch: [4][0/306] Elapsed 0m 0s (remain 3m 0s) Loss: 0.0149 \n",
            "Epoch: [4][100/306] Elapsed 0m 33s (remain 1m 7s) Loss: 0.0480 \n",
            "Epoch: [4][200/306] Elapsed 1m 5s (remain 0m 34s) Loss: 0.0618 \n",
            "Epoch: [4][300/306] Elapsed 1m 38s (remain 0m 1s) Loss: 0.0671 \n",
            "Epoch: [4][305/306] Elapsed 1m 39s (remain 0m 0s) Loss: 0.0664 \n",
            "EVAL: [0/77] Elapsed 0m 0s (remain 0m 27s) Loss: 0.7080 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4 - avg_train_loss: 0.0664  avg_val_loss: 0.2648  time: 108s\n",
            "Epoch 4 - avg_train_loss: 0.0664  avg_val_loss: 0.2648  time: 108s\n",
            "Epoch 4 - avg_train_loss: 0.0664  avg_val_loss: 0.2648  time: 108s\n",
            "Epoch 4 - avg_train_loss: 0.0664  avg_val_loss: 0.2648  time: 108s\n",
            "Epoch 4 - avg_train_loss: 0.0664  avg_val_loss: 0.2648  time: 108s\n",
            "Epoch 4 - Score: 0.26477657609530775\n",
            "Epoch 4 - Score: 0.26477657609530775\n",
            "Epoch 4 - Score: 0.26477657609530775\n",
            "Epoch 4 - Score: 0.26477657609530775\n",
            "Epoch 4 - Score: 0.26477657609530775\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EVAL: [76/77] Elapsed 0m 8s (remain 0m 0s) Loss: 0.2648 \n",
            "Log loss for 3 is 0.26477657609530775\n",
            "Epoch: [5][0/306] Elapsed 0m 0s (remain 2m 59s) Loss: 0.0081 \n",
            "Epoch: [5][100/306] Elapsed 0m 33s (remain 1m 7s) Loss: 0.0311 \n",
            "Epoch: [5][200/306] Elapsed 1m 5s (remain 0m 34s) Loss: 0.0426 \n",
            "Epoch: [5][300/306] Elapsed 1m 38s (remain 0m 1s) Loss: 0.0427 \n",
            "Epoch: [5][305/306] Elapsed 1m 39s (remain 0m 0s) Loss: 0.0430 \n",
            "EVAL: [0/77] Elapsed 0m 0s (remain 0m 27s) Loss: 0.8330 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5 - avg_train_loss: 0.0430  avg_val_loss: 0.2671  time: 108s\n",
            "Epoch 5 - avg_train_loss: 0.0430  avg_val_loss: 0.2671  time: 108s\n",
            "Epoch 5 - avg_train_loss: 0.0430  avg_val_loss: 0.2671  time: 108s\n",
            "Epoch 5 - avg_train_loss: 0.0430  avg_val_loss: 0.2671  time: 108s\n",
            "Epoch 5 - avg_train_loss: 0.0430  avg_val_loss: 0.2671  time: 108s\n",
            "Epoch 5 - Score: 0.26705876598424017\n",
            "Epoch 5 - Score: 0.26705876598424017\n",
            "Epoch 5 - Score: 0.26705876598424017\n",
            "Epoch 5 - Score: 0.26705876598424017\n",
            "Epoch 5 - Score: 0.26705876598424017\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EVAL: [76/77] Elapsed 0m 8s (remain 0m 0s) Loss: 0.2671 \n",
            "Log loss for 4 is 0.26705876598424017\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "========== fold: 4 result ==========\n",
            "========== fold: 4 result ==========\n",
            "========== fold: 4 result ==========\n",
            "========== fold: 4 result ==========\n",
            "========== fold: 4 result ==========\n",
            "Score: 0.18931\n",
            "Score: 0.18931\n",
            "Score: 0.18931\n",
            "Score: 0.18931\n",
            "Score: 0.18931\n",
            "========== CV ==========\n",
            "========== CV ==========\n",
            "========== CV ==========\n",
            "========== CV ==========\n",
            "========== CV ==========\n",
            "Score: 0.20441\n",
            "Score: 0.20441\n",
            "Score: 0.20441\n",
            "Score: 0.20441\n",
            "Score: 0.20441\n",
            "Didn't find file /content/activebus_bert_final/added_tokens.json. We won't load it.\n",
            "loading file /content/activebus_bert_final/vocab.txt\n",
            "loading file None\n",
            "loading file /content/activebus_bert_final/special_tokens_map.json\n",
            "loading file /content/activebus_bert_final/tokenizer_config.json\n",
            "loading file /content/activebus_bert_final/tokenizer.json\n",
            "========== model: bert-base-uncased fold: 0 inference ==========\n",
            "========== model: bert-base-uncased fold: 0 inference ==========\n",
            "========== model: bert-base-uncased fold: 0 inference ==========\n",
            "========== model: bert-base-uncased fold: 0 inference ==========\n",
            "========== model: bert-base-uncased fold: 0 inference ==========\n",
            "loading configuration file /content/activebus_bert_final/config.json\n",
            "Model config BertConfig {\n",
            "  \"_name_or_path\": \"activebus/BERT-XD_Review\",\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_ids\": 0,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.15.0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "loading weights file /content/activebus_bert_final/pytorch_model.bin\n",
            "Some weights of the model checkpoint at /content/activebus_bert_final were not used when initializing BertForSequenceClassification: ['cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /content/activebus_bert_final and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "\n",
            "  0%|          | 0/165 [00:00<?, ?it/s]\u001b[A\n",
            "  1%|          | 1/165 [00:00<00:21,  7.46it/s]\u001b[A\n",
            "  1%|          | 2/165 [00:00<00:20,  7.96it/s]\u001b[A\n",
            "  2%|▏         | 3/165 [00:00<00:19,  8.39it/s]\u001b[A\n",
            "  2%|▏         | 4/165 [00:00<00:18,  8.56it/s]\u001b[A\n",
            "  3%|▎         | 5/165 [00:00<00:18,  8.73it/s]\u001b[A\n",
            "  4%|▎         | 6/165 [00:00<00:17,  8.87it/s]\u001b[A\n",
            "  4%|▍         | 7/165 [00:00<00:17,  8.96it/s]\u001b[A\n",
            "  5%|▍         | 8/165 [00:00<00:17,  9.00it/s]\u001b[A\n",
            "  5%|▌         | 9/165 [00:01<00:17,  9.03it/s]\u001b[A\n",
            "  6%|▌         | 10/165 [00:01<00:17,  9.05it/s]\u001b[A\n",
            "  7%|▋         | 11/165 [00:01<00:16,  9.10it/s]\u001b[A\n",
            "  7%|▋         | 12/165 [00:01<00:16,  9.12it/s]\u001b[A\n",
            "  8%|▊         | 13/165 [00:01<00:16,  9.04it/s]\u001b[A\n",
            "  8%|▊         | 14/165 [00:01<00:16,  9.07it/s]\u001b[A\n",
            "  9%|▉         | 15/165 [00:01<00:16,  9.08it/s]\u001b[A\n",
            " 10%|▉         | 16/165 [00:01<00:16,  9.09it/s]\u001b[A\n",
            " 10%|█         | 17/165 [00:01<00:16,  9.11it/s]\u001b[A\n",
            " 11%|█         | 18/165 [00:02<00:16,  9.11it/s]\u001b[A\n",
            " 12%|█▏        | 19/165 [00:02<00:16,  9.12it/s]\u001b[A\n",
            " 12%|█▏        | 20/165 [00:02<00:15,  9.13it/s]\u001b[A\n",
            " 13%|█▎        | 21/165 [00:02<00:15,  9.12it/s]\u001b[A\n",
            " 13%|█▎        | 22/165 [00:02<00:15,  9.09it/s]\u001b[A\n",
            " 14%|█▍        | 23/165 [00:02<00:15,  9.07it/s]\u001b[A\n",
            " 15%|█▍        | 24/165 [00:02<00:15,  9.05it/s]\u001b[A\n",
            " 15%|█▌        | 25/165 [00:02<00:15,  9.06it/s]\u001b[A\n",
            " 16%|█▌        | 26/165 [00:02<00:15,  9.08it/s]\u001b[A\n",
            " 16%|█▋        | 27/165 [00:03<00:15,  9.08it/s]\u001b[A\n",
            " 17%|█▋        | 28/165 [00:03<00:15,  9.11it/s]\u001b[A\n",
            " 18%|█▊        | 29/165 [00:03<00:14,  9.11it/s]\u001b[A\n",
            " 18%|█▊        | 30/165 [00:03<00:14,  9.10it/s]\u001b[A\n",
            " 19%|█▉        | 31/165 [00:03<00:14,  9.09it/s]\u001b[A\n",
            " 19%|█▉        | 32/165 [00:03<00:14,  9.09it/s]\u001b[A\n",
            " 20%|██        | 33/165 [00:03<00:14,  9.10it/s]\u001b[A\n",
            " 21%|██        | 34/165 [00:03<00:14,  9.09it/s]\u001b[A\n",
            " 21%|██        | 35/165 [00:03<00:14,  9.10it/s]\u001b[A\n",
            " 22%|██▏       | 36/165 [00:03<00:14,  9.11it/s]\u001b[A\n",
            " 22%|██▏       | 37/165 [00:04<00:14,  9.11it/s]\u001b[A\n",
            " 23%|██▎       | 38/165 [00:04<00:13,  9.12it/s]\u001b[A\n",
            " 24%|██▎       | 39/165 [00:04<00:13,  9.13it/s]\u001b[A\n",
            " 24%|██▍       | 40/165 [00:04<00:13,  9.12it/s]\u001b[A\n",
            " 25%|██▍       | 41/165 [00:04<00:13,  9.12it/s]\u001b[A\n",
            " 25%|██▌       | 42/165 [00:04<00:13,  9.11it/s]\u001b[A\n",
            " 26%|██▌       | 43/165 [00:04<00:13,  9.10it/s]\u001b[A\n",
            " 27%|██▋       | 44/165 [00:04<00:13,  9.09it/s]\u001b[A\n",
            " 27%|██▋       | 45/165 [00:04<00:13,  9.11it/s]\u001b[A\n",
            " 28%|██▊       | 46/165 [00:05<00:13,  9.12it/s]\u001b[A\n",
            " 28%|██▊       | 47/165 [00:05<00:12,  9.09it/s]\u001b[A\n",
            " 29%|██▉       | 48/165 [00:05<00:12,  9.11it/s]\u001b[A\n",
            " 30%|██▉       | 49/165 [00:05<00:12,  9.10it/s]\u001b[A\n",
            " 30%|███       | 50/165 [00:05<00:12,  9.08it/s]\u001b[A\n",
            " 31%|███       | 51/165 [00:05<00:12,  9.08it/s]\u001b[A\n",
            " 32%|███▏      | 52/165 [00:05<00:12,  9.10it/s]\u001b[A\n",
            " 32%|███▏      | 53/165 [00:05<00:12,  9.09it/s]\u001b[A\n",
            " 33%|███▎      | 54/165 [00:05<00:12,  9.08it/s]\u001b[A\n",
            " 33%|███▎      | 55/165 [00:06<00:12,  9.11it/s]\u001b[A\n",
            " 34%|███▍      | 56/165 [00:06<00:11,  9.12it/s]\u001b[A\n",
            " 35%|███▍      | 57/165 [00:06<00:11,  9.11it/s]\u001b[A\n",
            " 35%|███▌      | 58/165 [00:06<00:11,  9.12it/s]\u001b[A\n",
            " 36%|███▌      | 59/165 [00:06<00:11,  9.12it/s]\u001b[A\n",
            " 36%|███▋      | 60/165 [00:06<00:11,  9.13it/s]\u001b[A\n",
            " 37%|███▋      | 61/165 [00:06<00:11,  9.13it/s]\u001b[A\n",
            " 38%|███▊      | 62/165 [00:06<00:11,  9.14it/s]\u001b[A\n",
            " 38%|███▊      | 63/165 [00:06<00:11,  9.12it/s]\u001b[A\n",
            " 39%|███▉      | 64/165 [00:07<00:11,  9.12it/s]\u001b[A\n",
            " 39%|███▉      | 65/165 [00:07<00:10,  9.13it/s]\u001b[A\n",
            " 40%|████      | 66/165 [00:07<00:10,  9.10it/s]\u001b[A\n",
            " 41%|████      | 67/165 [00:07<00:10,  9.06it/s]\u001b[A\n",
            " 41%|████      | 68/165 [00:07<00:10,  9.06it/s]\u001b[A\n",
            " 42%|████▏     | 69/165 [00:07<00:10,  9.08it/s]\u001b[A\n",
            " 42%|████▏     | 70/165 [00:07<00:10,  9.10it/s]\u001b[A\n",
            " 43%|████▎     | 71/165 [00:07<00:10,  9.09it/s]\u001b[A\n",
            " 44%|████▎     | 72/165 [00:07<00:10,  9.07it/s]\u001b[A\n",
            " 44%|████▍     | 73/165 [00:08<00:10,  9.09it/s]\u001b[A\n",
            " 45%|████▍     | 74/165 [00:08<00:09,  9.11it/s]\u001b[A\n",
            " 45%|████▌     | 75/165 [00:08<00:09,  9.12it/s]\u001b[A\n",
            " 46%|████▌     | 76/165 [00:08<00:09,  9.10it/s]\u001b[A\n",
            " 47%|████▋     | 77/165 [00:08<00:09,  9.10it/s]\u001b[A\n",
            " 47%|████▋     | 78/165 [00:08<00:09,  9.10it/s]\u001b[A\n",
            " 48%|████▊     | 79/165 [00:08<00:09,  9.08it/s]\u001b[A\n",
            " 48%|████▊     | 80/165 [00:08<00:09,  9.08it/s]\u001b[A\n",
            " 49%|████▉     | 81/165 [00:08<00:09,  9.05it/s]\u001b[A\n",
            " 50%|████▉     | 82/165 [00:09<00:17,  4.69it/s]\u001b[A\n",
            " 50%|█████     | 83/165 [00:09<00:14,  5.49it/s]\u001b[A\n",
            " 51%|█████     | 84/165 [00:09<00:13,  6.23it/s]\u001b[A\n",
            " 52%|█████▏    | 85/165 [00:09<00:11,  6.89it/s]\u001b[A\n",
            " 52%|█████▏    | 86/165 [00:09<00:10,  7.43it/s]\u001b[A\n",
            " 53%|█████▎    | 87/165 [00:09<00:09,  7.86it/s]\u001b[A\n",
            " 53%|█████▎    | 88/165 [00:10<00:09,  8.19it/s]\u001b[A\n",
            " 54%|█████▍    | 89/165 [00:10<00:09,  8.44it/s]\u001b[A\n",
            " 55%|█████▍    | 90/165 [00:10<00:08,  8.62it/s]\u001b[A\n",
            " 55%|█████▌    | 91/165 [00:10<00:08,  8.76it/s]\u001b[A\n",
            " 56%|█████▌    | 92/165 [00:10<00:08,  8.87it/s]\u001b[A\n",
            " 56%|█████▋    | 93/165 [00:10<00:08,  8.95it/s]\u001b[A\n",
            " 57%|█████▋    | 94/165 [00:10<00:07,  9.01it/s]\u001b[A\n",
            " 58%|█████▊    | 95/165 [00:10<00:07,  9.02it/s]\u001b[A\n",
            " 58%|█████▊    | 96/165 [00:10<00:07,  9.02it/s]\u001b[A\n",
            " 59%|█████▉    | 97/165 [00:11<00:07,  9.02it/s]\u001b[A\n",
            " 59%|█████▉    | 98/165 [00:11<00:07,  9.06it/s]\u001b[A\n",
            " 60%|██████    | 99/165 [00:11<00:07,  9.07it/s]\u001b[A\n",
            " 61%|██████    | 100/165 [00:11<00:07,  9.09it/s]\u001b[A\n",
            " 61%|██████    | 101/165 [00:11<00:07,  9.10it/s]\u001b[A\n",
            " 62%|██████▏   | 102/165 [00:11<00:06,  9.09it/s]\u001b[A\n",
            " 62%|██████▏   | 103/165 [00:11<00:06,  9.07it/s]\u001b[A\n",
            " 63%|██████▎   | 104/165 [00:11<00:06,  9.05it/s]\u001b[A\n",
            " 64%|██████▎   | 105/165 [00:11<00:06,  9.07it/s]\u001b[A\n",
            " 64%|██████▍   | 106/165 [00:12<00:06,  9.07it/s]\u001b[A\n",
            " 65%|██████▍   | 107/165 [00:12<00:06,  9.08it/s]\u001b[A\n",
            " 65%|██████▌   | 108/165 [00:12<00:06,  9.10it/s]\u001b[A\n",
            " 66%|██████▌   | 109/165 [00:12<00:06,  9.11it/s]\u001b[A\n",
            " 67%|██████▋   | 110/165 [00:12<00:06,  9.11it/s]\u001b[A\n",
            " 67%|██████▋   | 111/165 [00:12<00:05,  9.11it/s]\u001b[A\n",
            " 68%|██████▊   | 112/165 [00:12<00:05,  9.08it/s]\u001b[A\n",
            " 68%|██████▊   | 113/165 [00:12<00:05,  9.04it/s]\u001b[A\n",
            " 69%|██████▉   | 114/165 [00:12<00:05,  9.05it/s]\u001b[A\n",
            " 70%|██████▉   | 115/165 [00:13<00:05,  9.07it/s]\u001b[A\n",
            " 70%|███████   | 116/165 [00:13<00:05,  9.09it/s]\u001b[A\n",
            " 71%|███████   | 117/165 [00:13<00:05,  9.10it/s]\u001b[A\n",
            " 72%|███████▏  | 118/165 [00:13<00:05,  9.11it/s]\u001b[A\n",
            " 72%|███████▏  | 119/165 [00:13<00:05,  9.12it/s]\u001b[A\n",
            " 73%|███████▎  | 120/165 [00:13<00:04,  9.08it/s]\u001b[A\n",
            " 73%|███████▎  | 121/165 [00:13<00:04,  9.09it/s]\u001b[A\n",
            " 74%|███████▍  | 122/165 [00:13<00:04,  9.11it/s]\u001b[A\n",
            " 75%|███████▍  | 123/165 [00:13<00:04,  9.11it/s]\u001b[A\n",
            " 75%|███████▌  | 124/165 [00:14<00:04,  9.11it/s]\u001b[A\n",
            " 76%|███████▌  | 125/165 [00:14<00:04,  9.09it/s]\u001b[A\n",
            " 76%|███████▋  | 126/165 [00:14<00:04,  9.11it/s]\u001b[A\n",
            " 77%|███████▋  | 127/165 [00:14<00:04,  9.12it/s]\u001b[A\n",
            " 78%|███████▊  | 128/165 [00:14<00:04,  9.12it/s]\u001b[A\n",
            " 78%|███████▊  | 129/165 [00:14<00:03,  9.11it/s]\u001b[A\n",
            " 79%|███████▉  | 130/165 [00:14<00:03,  9.12it/s]\u001b[A\n",
            " 79%|███████▉  | 131/165 [00:14<00:03,  9.13it/s]\u001b[A\n",
            " 80%|████████  | 132/165 [00:14<00:03,  9.12it/s]\u001b[A\n",
            " 81%|████████  | 133/165 [00:15<00:03,  9.11it/s]\u001b[A\n",
            " 81%|████████  | 134/165 [00:15<00:03,  9.10it/s]\u001b[A\n",
            " 82%|████████▏ | 135/165 [00:15<00:03,  9.10it/s]\u001b[A\n",
            " 82%|████████▏ | 136/165 [00:15<00:03,  9.12it/s]\u001b[A\n",
            " 83%|████████▎ | 137/165 [00:15<00:03,  9.09it/s]\u001b[A\n",
            " 84%|████████▎ | 138/165 [00:15<00:02,  9.11it/s]\u001b[A\n",
            " 84%|████████▍ | 139/165 [00:15<00:02,  9.07it/s]\u001b[A\n",
            " 85%|████████▍ | 140/165 [00:15<00:02,  9.07it/s]\u001b[A\n",
            " 85%|████████▌ | 141/165 [00:15<00:02,  9.10it/s]\u001b[A\n",
            " 86%|████████▌ | 142/165 [00:15<00:02,  9.11it/s]\u001b[A\n",
            " 87%|████████▋ | 143/165 [00:16<00:02,  9.09it/s]\u001b[A\n",
            " 87%|████████▋ | 144/165 [00:16<00:02,  9.08it/s]\u001b[A\n",
            " 88%|████████▊ | 145/165 [00:16<00:02,  9.05it/s]\u001b[A\n",
            " 88%|████████▊ | 146/165 [00:16<00:02,  9.06it/s]\u001b[A\n",
            " 89%|████████▉ | 147/165 [00:16<00:01,  9.09it/s]\u001b[A\n",
            " 90%|████████▉ | 148/165 [00:16<00:01,  9.09it/s]\u001b[A\n",
            " 90%|█████████ | 149/165 [00:16<00:01,  9.11it/s]\u001b[A\n",
            " 91%|█████████ | 150/165 [00:16<00:01,  9.11it/s]\u001b[A\n",
            " 92%|█████████▏| 151/165 [00:16<00:01,  9.12it/s]\u001b[A\n",
            " 92%|█████████▏| 152/165 [00:17<00:01,  9.11it/s]\u001b[A\n",
            " 93%|█████████▎| 153/165 [00:17<00:01,  9.07it/s]\u001b[A\n",
            " 93%|█████████▎| 154/165 [00:17<00:01,  9.10it/s]\u001b[A\n",
            " 94%|█████████▍| 155/165 [00:17<00:01,  9.09it/s]\u001b[A\n",
            " 95%|█████████▍| 156/165 [00:17<00:00,  9.02it/s]\u001b[A\n",
            " 95%|█████████▌| 157/165 [00:17<00:00,  9.04it/s]\u001b[A\n",
            " 96%|█████████▌| 158/165 [00:17<00:00,  9.05it/s]\u001b[A\n",
            " 96%|█████████▋| 159/165 [00:17<00:00,  9.07it/s]\u001b[A\n",
            " 97%|█████████▋| 160/165 [00:17<00:00,  9.10it/s]\u001b[A\n",
            " 98%|█████████▊| 161/165 [00:18<00:00,  9.13it/s]\u001b[A\n",
            " 98%|█████████▊| 162/165 [00:18<00:00,  9.13it/s]\u001b[A\n",
            " 99%|█████████▉| 163/165 [00:18<00:00,  9.14it/s]\u001b[A\n",
            "100%|██████████| 165/165 [00:18<00:00,  8.89it/s]\n",
            "========== model: bert-base-uncased fold: 1 inference ==========\n",
            "========== model: bert-base-uncased fold: 1 inference ==========\n",
            "========== model: bert-base-uncased fold: 1 inference ==========\n",
            "========== model: bert-base-uncased fold: 1 inference ==========\n",
            "========== model: bert-base-uncased fold: 1 inference ==========\n",
            "loading configuration file /content/activebus_bert_final/config.json\n",
            "Model config BertConfig {\n",
            "  \"_name_or_path\": \"activebus/BERT-XD_Review\",\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_ids\": 0,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.15.0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "loading weights file /content/activebus_bert_final/pytorch_model.bin\n",
            "Some weights of the model checkpoint at /content/activebus_bert_final were not used when initializing BertForSequenceClassification: ['cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /content/activebus_bert_final and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "\n",
            "  0%|          | 0/165 [00:00<?, ?it/s]\u001b[A\n",
            "  1%|          | 1/165 [00:00<00:18,  8.79it/s]\u001b[A\n",
            "  1%|          | 2/165 [00:00<00:18,  8.95it/s]\u001b[A\n",
            "  2%|▏         | 3/165 [00:00<00:18,  8.99it/s]\u001b[A\n",
            "  2%|▏         | 4/165 [00:00<00:17,  9.04it/s]\u001b[A\n",
            "  3%|▎         | 5/165 [00:00<00:17,  9.05it/s]\u001b[A\n",
            "  4%|▎         | 6/165 [00:00<00:17,  9.05it/s]\u001b[A\n",
            "  4%|▍         | 7/165 [00:00<00:17,  9.09it/s]\u001b[A\n",
            "  5%|▍         | 8/165 [00:00<00:17,  9.11it/s]\u001b[A\n",
            "  5%|▌         | 9/165 [00:00<00:17,  9.12it/s]\u001b[A\n",
            "  6%|▌         | 10/165 [00:01<00:17,  9.11it/s]\u001b[A\n",
            "  7%|▋         | 11/165 [00:01<00:16,  9.08it/s]\u001b[A\n",
            "  7%|▋         | 12/165 [00:01<00:16,  9.07it/s]\u001b[A\n",
            "  8%|▊         | 13/165 [00:01<00:16,  9.07it/s]\u001b[A\n",
            "  8%|▊         | 14/165 [00:01<00:16,  9.07it/s]\u001b[A\n",
            "  9%|▉         | 15/165 [00:01<00:16,  9.02it/s]\u001b[A\n",
            " 10%|▉         | 16/165 [00:01<00:16,  9.03it/s]\u001b[A\n",
            " 10%|█         | 17/165 [00:01<00:16,  9.06it/s]\u001b[A\n",
            " 11%|█         | 18/165 [00:01<00:16,  9.08it/s]\u001b[A\n",
            " 12%|█▏        | 19/165 [00:02<00:16,  9.08it/s]\u001b[A\n",
            " 12%|█▏        | 20/165 [00:02<00:15,  9.07it/s]\u001b[A\n",
            " 13%|█▎        | 21/165 [00:02<00:15,  9.07it/s]\u001b[A\n",
            " 13%|█▎        | 22/165 [00:02<00:15,  9.08it/s]\u001b[A\n",
            " 14%|█▍        | 23/165 [00:02<00:15,  9.11it/s]\u001b[A\n",
            " 15%|█▍        | 24/165 [00:02<00:15,  9.11it/s]\u001b[A\n",
            " 15%|█▌        | 25/165 [00:02<00:15,  9.10it/s]\u001b[A\n",
            " 16%|█▌        | 26/165 [00:02<00:15,  9.08it/s]\u001b[A\n",
            " 16%|█▋        | 27/165 [00:02<00:15,  9.10it/s]\u001b[A\n",
            " 17%|█▋        | 28/165 [00:03<00:15,  9.06it/s]\u001b[A\n",
            " 18%|█▊        | 29/165 [00:03<00:14,  9.08it/s]\u001b[A\n",
            " 18%|█▊        | 30/165 [00:03<00:14,  9.08it/s]\u001b[A\n",
            " 19%|█▉        | 31/165 [00:03<00:14,  9.07it/s]\u001b[A\n",
            " 19%|█▉        | 32/165 [00:03<00:14,  9.09it/s]\u001b[A\n",
            " 20%|██        | 33/165 [00:03<00:14,  9.04it/s]\u001b[A\n",
            " 21%|██        | 34/165 [00:03<00:14,  9.04it/s]\u001b[A\n",
            " 21%|██        | 35/165 [00:03<00:14,  9.05it/s]\u001b[A\n",
            " 22%|██▏       | 36/165 [00:03<00:14,  9.07it/s]\u001b[A\n",
            " 22%|██▏       | 37/165 [00:04<00:14,  9.07it/s]\u001b[A\n",
            " 23%|██▎       | 38/165 [00:04<00:13,  9.07it/s]\u001b[A\n",
            " 24%|██▎       | 39/165 [00:04<00:13,  9.09it/s]\u001b[A\n",
            " 24%|██▍       | 40/165 [00:04<00:13,  9.09it/s]\u001b[A\n",
            " 25%|██▍       | 41/165 [00:04<00:13,  9.08it/s]\u001b[A\n",
            " 25%|██▌       | 42/165 [00:04<00:13,  9.10it/s]\u001b[A\n",
            " 26%|██▌       | 43/165 [00:04<00:13,  9.13it/s]\u001b[A\n",
            " 27%|██▋       | 44/165 [00:04<00:13,  9.14it/s]\u001b[A\n",
            " 27%|██▋       | 45/165 [00:04<00:13,  9.13it/s]\u001b[A\n",
            " 28%|██▊       | 46/165 [00:05<00:13,  9.13it/s]\u001b[A\n",
            " 28%|██▊       | 47/165 [00:05<00:12,  9.13it/s]\u001b[A\n",
            " 29%|██▉       | 48/165 [00:05<00:12,  9.10it/s]\u001b[A\n",
            " 30%|██▉       | 49/165 [00:05<00:12,  9.07it/s]\u001b[A\n",
            " 30%|███       | 50/165 [00:05<00:12,  9.06it/s]\u001b[A\n",
            " 31%|███       | 51/165 [00:05<00:12,  9.04it/s]\u001b[A\n",
            " 32%|███▏      | 52/165 [00:05<00:12,  9.04it/s]\u001b[A\n",
            " 32%|███▏      | 53/165 [00:05<00:12,  9.05it/s]\u001b[A\n",
            " 33%|███▎      | 54/165 [00:05<00:12,  9.07it/s]\u001b[A\n",
            " 33%|███▎      | 55/165 [00:06<00:12,  9.07it/s]\u001b[A\n",
            " 34%|███▍      | 56/165 [00:06<00:12,  9.08it/s]\u001b[A\n",
            " 35%|███▍      | 57/165 [00:06<00:11,  9.08it/s]\u001b[A\n",
            " 35%|███▌      | 58/165 [00:06<00:11,  9.09it/s]\u001b[A\n",
            " 36%|███▌      | 59/165 [00:06<00:11,  9.09it/s]\u001b[A\n",
            " 36%|███▋      | 60/165 [00:06<00:11,  9.04it/s]\u001b[A\n",
            " 37%|███▋      | 61/165 [00:06<00:11,  9.06it/s]\u001b[A\n",
            " 38%|███▊      | 62/165 [00:06<00:11,  9.08it/s]\u001b[A\n",
            " 38%|███▊      | 63/165 [00:06<00:11,  9.11it/s]\u001b[A\n",
            " 39%|███▉      | 64/165 [00:07<00:11,  9.11it/s]\u001b[A\n",
            " 39%|███▉      | 65/165 [00:07<00:10,  9.11it/s]\u001b[A\n",
            " 40%|████      | 66/165 [00:07<00:10,  9.12it/s]\u001b[A\n",
            " 41%|████      | 67/165 [00:07<00:10,  9.10it/s]\u001b[A\n",
            " 41%|████      | 68/165 [00:07<00:10,  9.08it/s]\u001b[A\n",
            " 42%|████▏     | 69/165 [00:07<00:10,  9.08it/s]\u001b[A\n",
            " 42%|████▏     | 70/165 [00:07<00:10,  9.11it/s]\u001b[A\n",
            " 43%|████▎     | 71/165 [00:07<00:10,  9.10it/s]\u001b[A\n",
            " 44%|████▎     | 72/165 [00:07<00:10,  9.09it/s]\u001b[A\n",
            " 44%|████▍     | 73/165 [00:08<00:10,  9.09it/s]\u001b[A\n",
            " 45%|████▍     | 74/165 [00:08<00:10,  9.09it/s]\u001b[A\n",
            " 45%|████▌     | 75/165 [00:08<00:09,  9.10it/s]\u001b[A\n",
            " 46%|████▌     | 76/165 [00:08<00:09,  9.11it/s]\u001b[A\n",
            " 47%|████▋     | 77/165 [00:08<00:09,  9.09it/s]\u001b[A\n",
            " 47%|████▋     | 78/165 [00:08<00:09,  9.08it/s]\u001b[A\n",
            " 48%|████▊     | 79/165 [00:08<00:09,  9.06it/s]\u001b[A\n",
            " 48%|████▊     | 80/165 [00:08<00:09,  9.08it/s]\u001b[A\n",
            " 49%|████▉     | 81/165 [00:08<00:09,  9.10it/s]\u001b[A\n",
            " 50%|████▉     | 82/165 [00:09<00:09,  9.10it/s]\u001b[A\n",
            " 50%|█████     | 83/165 [00:09<00:08,  9.11it/s]\u001b[A\n",
            " 51%|█████     | 84/165 [00:09<00:08,  9.11it/s]\u001b[A\n",
            " 52%|█████▏    | 85/165 [00:09<00:08,  9.09it/s]\u001b[A\n",
            " 52%|█████▏    | 86/165 [00:09<00:08,  9.09it/s]\u001b[A\n",
            " 53%|█████▎    | 87/165 [00:09<00:08,  9.12it/s]\u001b[A\n",
            " 53%|█████▎    | 88/165 [00:09<00:08,  9.12it/s]\u001b[A\n",
            " 54%|█████▍    | 89/165 [00:09<00:08,  9.11it/s]\u001b[A\n",
            " 55%|█████▍    | 90/165 [00:09<00:08,  9.09it/s]\u001b[A\n",
            " 55%|█████▌    | 91/165 [00:10<00:08,  9.10it/s]\u001b[A\n",
            " 56%|█████▌    | 92/165 [00:10<00:08,  9.10it/s]\u001b[A\n",
            " 56%|█████▋    | 93/165 [00:10<00:07,  9.12it/s]\u001b[A\n",
            " 57%|█████▋    | 94/165 [00:10<00:07,  9.12it/s]\u001b[A\n",
            " 58%|█████▊    | 95/165 [00:10<00:07,  9.12it/s]\u001b[A\n",
            " 58%|█████▊    | 96/165 [00:10<00:07,  9.11it/s]\u001b[A\n",
            " 59%|█████▉    | 97/165 [00:10<00:07,  9.04it/s]\u001b[A\n",
            " 59%|█████▉    | 98/165 [00:10<00:07,  9.05it/s]\u001b[A\n",
            " 60%|██████    | 99/165 [00:10<00:07,  9.07it/s]\u001b[A\n",
            " 61%|██████    | 100/165 [00:11<00:07,  9.09it/s]\u001b[A\n",
            " 61%|██████    | 101/165 [00:11<00:07,  9.09it/s]\u001b[A\n",
            " 62%|██████▏   | 102/165 [00:11<00:06,  9.10it/s]\u001b[A\n",
            " 62%|██████▏   | 103/165 [00:11<00:06,  9.06it/s]\u001b[A\n",
            " 63%|██████▎   | 104/165 [00:11<00:06,  9.09it/s]\u001b[A\n",
            " 64%|██████▎   | 105/165 [00:11<00:06,  9.10it/s]\u001b[A\n",
            " 64%|██████▍   | 106/165 [00:11<00:06,  9.11it/s]\u001b[A\n",
            " 65%|██████▍   | 107/165 [00:11<00:06,  9.11it/s]\u001b[A\n",
            " 65%|██████▌   | 108/165 [00:11<00:06,  9.12it/s]\u001b[A\n",
            " 66%|██████▌   | 109/165 [00:11<00:06,  9.13it/s]\u001b[A\n",
            " 67%|██████▋   | 110/165 [00:12<00:06,  9.12it/s]\u001b[A\n",
            " 67%|██████▋   | 111/165 [00:12<00:05,  9.10it/s]\u001b[A\n",
            " 68%|██████▊   | 112/165 [00:12<00:05,  9.07it/s]\u001b[A\n",
            " 68%|██████▊   | 113/165 [00:12<00:05,  9.09it/s]\u001b[A\n",
            " 69%|██████▉   | 114/165 [00:12<00:05,  9.10it/s]\u001b[A\n",
            " 70%|██████▉   | 115/165 [00:12<00:05,  9.11it/s]\u001b[A\n",
            " 70%|███████   | 116/165 [00:12<00:05,  9.12it/s]\u001b[A\n",
            " 71%|███████   | 117/165 [00:12<00:05,  9.11it/s]\u001b[A\n",
            " 72%|███████▏  | 118/165 [00:12<00:05,  9.12it/s]\u001b[A\n",
            " 72%|███████▏  | 119/165 [00:13<00:05,  9.10it/s]\u001b[A\n",
            " 73%|███████▎  | 120/165 [00:13<00:04,  9.11it/s]\u001b[A\n",
            " 73%|███████▎  | 121/165 [00:13<00:04,  9.10it/s]\u001b[A\n",
            " 74%|███████▍  | 122/165 [00:13<00:04,  9.10it/s]\u001b[A\n",
            " 75%|███████▍  | 123/165 [00:13<00:04,  9.12it/s]\u001b[A\n",
            " 75%|███████▌  | 124/165 [00:13<00:04,  9.10it/s]\u001b[A\n",
            " 76%|███████▌  | 125/165 [00:13<00:04,  9.11it/s]\u001b[A\n",
            " 76%|███████▋  | 126/165 [00:13<00:04,  9.11it/s]\u001b[A\n",
            " 77%|███████▋  | 127/165 [00:13<00:04,  9.11it/s]\u001b[A\n",
            " 78%|███████▊  | 128/165 [00:14<00:04,  9.06it/s]\u001b[A\n",
            " 78%|███████▊  | 129/165 [00:14<00:03,  9.08it/s]\u001b[A\n",
            " 79%|███████▉  | 130/165 [00:14<00:03,  9.10it/s]\u001b[A\n",
            " 79%|███████▉  | 131/165 [00:14<00:03,  9.12it/s]\u001b[A\n",
            " 80%|████████  | 132/165 [00:14<00:03,  9.12it/s]\u001b[A\n",
            " 81%|████████  | 133/165 [00:14<00:03,  9.12it/s]\u001b[A\n",
            " 81%|████████  | 134/165 [00:14<00:03,  9.11it/s]\u001b[A\n",
            " 82%|████████▏ | 135/165 [00:14<00:03,  9.11it/s]\u001b[A\n",
            " 82%|████████▏ | 136/165 [00:14<00:03,  9.12it/s]\u001b[A\n",
            " 83%|████████▎ | 137/165 [00:15<00:03,  9.12it/s]\u001b[A\n",
            " 84%|████████▎ | 138/165 [00:15<00:02,  9.10it/s]\u001b[A\n",
            " 84%|████████▍ | 139/165 [00:15<00:02,  9.07it/s]\u001b[A\n",
            " 85%|████████▍ | 140/165 [00:15<00:02,  9.05it/s]\u001b[A\n",
            " 85%|████████▌ | 141/165 [00:15<00:02,  9.08it/s]\u001b[A\n",
            " 86%|████████▌ | 142/165 [00:15<00:02,  9.07it/s]\u001b[A\n",
            " 87%|████████▋ | 143/165 [00:15<00:02,  9.06it/s]\u001b[A\n",
            " 87%|████████▋ | 144/165 [00:15<00:02,  9.07it/s]\u001b[A\n",
            " 88%|████████▊ | 145/165 [00:15<00:02,  9.09it/s]\u001b[A\n",
            " 88%|████████▊ | 146/165 [00:16<00:02,  9.09it/s]\u001b[A\n",
            " 89%|████████▉ | 147/165 [00:16<00:01,  9.08it/s]\u001b[A\n",
            " 90%|████████▉ | 148/165 [00:16<00:01,  9.08it/s]\u001b[A\n",
            " 90%|█████████ | 149/165 [00:16<00:01,  9.07it/s]\u001b[A\n",
            " 91%|█████████ | 150/165 [00:16<00:01,  9.08it/s]\u001b[A\n",
            " 92%|█████████▏| 151/165 [00:16<00:01,  9.10it/s]\u001b[A\n",
            " 92%|█████████▏| 152/165 [00:16<00:01,  9.10it/s]\u001b[A\n",
            " 93%|█████████▎| 153/165 [00:16<00:01,  9.10it/s]\u001b[A\n",
            " 93%|█████████▎| 154/165 [00:16<00:01,  9.11it/s]\u001b[A\n",
            " 94%|█████████▍| 155/165 [00:17<00:01,  9.10it/s]\u001b[A\n",
            " 95%|█████████▍| 156/165 [00:17<00:00,  9.09it/s]\u001b[A\n",
            " 95%|█████████▌| 157/165 [00:17<00:00,  9.10it/s]\u001b[A\n",
            " 96%|█████████▌| 158/165 [00:17<00:00,  9.12it/s]\u001b[A\n",
            " 96%|█████████▋| 159/165 [00:17<00:00,  9.13it/s]\u001b[A\n",
            " 97%|█████████▋| 160/165 [00:17<00:00,  9.07it/s]\u001b[A\n",
            " 98%|█████████▊| 161/165 [00:17<00:00,  9.10it/s]\u001b[A\n",
            " 98%|█████████▊| 162/165 [00:17<00:00,  9.10it/s]\u001b[A\n",
            " 99%|█████████▉| 163/165 [00:17<00:00,  9.10it/s]\u001b[A\n",
            "100%|██████████| 165/165 [00:18<00:00,  9.08it/s]\n",
            "========== model: bert-base-uncased fold: 2 inference ==========\n",
            "========== model: bert-base-uncased fold: 2 inference ==========\n",
            "========== model: bert-base-uncased fold: 2 inference ==========\n",
            "========== model: bert-base-uncased fold: 2 inference ==========\n",
            "========== model: bert-base-uncased fold: 2 inference ==========\n",
            "loading configuration file /content/activebus_bert_final/config.json\n",
            "Model config BertConfig {\n",
            "  \"_name_or_path\": \"activebus/BERT-XD_Review\",\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_ids\": 0,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.15.0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "loading weights file /content/activebus_bert_final/pytorch_model.bin\n",
            "Some weights of the model checkpoint at /content/activebus_bert_final were not used when initializing BertForSequenceClassification: ['cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /content/activebus_bert_final and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "\n",
            "  0%|          | 0/165 [00:00<?, ?it/s]\u001b[A\n",
            "  1%|          | 1/165 [00:00<00:19,  8.30it/s]\u001b[A\n",
            "  1%|          | 2/165 [00:00<00:18,  8.75it/s]\u001b[A\n",
            "  2%|▏         | 3/165 [00:00<00:18,  8.66it/s]\u001b[A\n",
            "  2%|▏         | 4/165 [00:00<00:18,  8.83it/s]\u001b[A\n",
            "  3%|▎         | 5/165 [00:00<00:17,  8.91it/s]\u001b[A\n",
            "  4%|▎         | 6/165 [00:00<00:17,  8.98it/s]\u001b[A\n",
            "  4%|▍         | 7/165 [00:00<00:17,  9.00it/s]\u001b[A\n",
            "  5%|▍         | 8/165 [00:00<00:17,  9.02it/s]\u001b[A\n",
            "  5%|▌         | 9/165 [00:01<00:17,  9.08it/s]\u001b[A\n",
            "  6%|▌         | 10/165 [00:01<00:17,  9.06it/s]\u001b[A\n",
            "  7%|▋         | 11/165 [00:01<00:16,  9.09it/s]\u001b[A\n",
            "  7%|▋         | 12/165 [00:01<00:16,  9.09it/s]\u001b[A\n",
            "  8%|▊         | 13/165 [00:01<00:16,  9.10it/s]\u001b[A\n",
            "  8%|▊         | 14/165 [00:01<00:16,  9.10it/s]\u001b[A\n",
            "  9%|▉         | 15/165 [00:01<00:16,  9.09it/s]\u001b[A\n",
            " 10%|▉         | 16/165 [00:01<00:16,  9.10it/s]\u001b[A\n",
            " 10%|█         | 17/165 [00:01<00:16,  9.11it/s]\u001b[A\n",
            " 11%|█         | 18/165 [00:01<00:16,  9.10it/s]\u001b[A\n",
            " 12%|█▏        | 19/165 [00:02<00:16,  9.08it/s]\u001b[A\n",
            " 12%|█▏        | 20/165 [00:02<00:15,  9.07it/s]\u001b[A\n",
            " 13%|█▎        | 21/165 [00:02<00:15,  9.06it/s]\u001b[A\n",
            " 13%|█▎        | 22/165 [00:02<00:15,  9.06it/s]\u001b[A\n",
            " 14%|█▍        | 23/165 [00:02<00:15,  9.06it/s]\u001b[A\n",
            " 15%|█▍        | 24/165 [00:02<00:15,  9.08it/s]\u001b[A\n",
            " 15%|█▌        | 25/165 [00:02<00:15,  9.08it/s]\u001b[A\n",
            " 16%|█▌        | 26/165 [00:02<00:15,  9.09it/s]\u001b[A\n",
            " 16%|█▋        | 27/165 [00:02<00:15,  9.09it/s]\u001b[A\n",
            " 17%|█▋        | 28/165 [00:03<00:15,  9.10it/s]\u001b[A\n",
            " 18%|█▊        | 29/165 [00:03<00:14,  9.11it/s]\u001b[A\n",
            " 18%|█▊        | 30/165 [00:03<00:14,  9.09it/s]\u001b[A\n",
            " 19%|█▉        | 31/165 [00:03<00:14,  9.08it/s]\u001b[A\n",
            " 19%|█▉        | 32/165 [00:03<00:14,  9.06it/s]\u001b[A\n",
            " 20%|██        | 33/165 [00:03<00:14,  9.04it/s]\u001b[A\n",
            " 21%|██        | 34/165 [00:03<00:14,  9.02it/s]\u001b[A\n",
            " 21%|██        | 35/165 [00:03<00:14,  9.02it/s]\u001b[A\n",
            " 22%|██▏       | 36/165 [00:03<00:14,  9.05it/s]\u001b[A\n",
            " 22%|██▏       | 37/165 [00:04<00:14,  9.06it/s]\u001b[A\n",
            " 23%|██▎       | 38/165 [00:04<00:13,  9.08it/s]\u001b[A\n",
            " 24%|██▎       | 39/165 [00:04<00:13,  9.08it/s]\u001b[A\n",
            " 24%|██▍       | 40/165 [00:04<00:13,  9.07it/s]\u001b[A\n",
            " 25%|██▍       | 41/165 [00:04<00:13,  9.09it/s]\u001b[A\n",
            " 25%|██▌       | 42/165 [00:04<00:13,  9.10it/s]\u001b[A\n",
            " 26%|██▌       | 43/165 [00:04<00:13,  9.09it/s]\u001b[A\n",
            " 27%|██▋       | 44/165 [00:04<00:13,  9.09it/s]\u001b[A\n",
            " 27%|██▋       | 45/165 [00:04<00:13,  9.11it/s]\u001b[A\n",
            " 28%|██▊       | 46/165 [00:05<00:13,  9.09it/s]\u001b[A\n",
            " 28%|██▊       | 47/165 [00:05<00:13,  9.07it/s]\u001b[A\n",
            " 29%|██▉       | 48/165 [00:05<00:12,  9.08it/s]\u001b[A\n",
            " 30%|██▉       | 49/165 [00:05<00:12,  9.08it/s]\u001b[A\n",
            " 30%|███       | 50/165 [00:05<00:12,  9.08it/s]\u001b[A\n",
            " 31%|███       | 51/165 [00:05<00:12,  9.10it/s]\u001b[A\n",
            " 32%|███▏      | 52/165 [00:05<00:12,  9.12it/s]\u001b[A\n",
            " 32%|███▏      | 53/165 [00:05<00:12,  9.12it/s]\u001b[A\n",
            " 33%|███▎      | 54/165 [00:05<00:12,  9.11it/s]\u001b[A\n",
            " 33%|███▎      | 55/165 [00:06<00:12,  9.11it/s]\u001b[A\n",
            " 34%|███▍      | 56/165 [00:06<00:11,  9.10it/s]\u001b[A\n",
            " 35%|███▍      | 57/165 [00:06<00:11,  9.10it/s]\u001b[A\n",
            " 35%|███▌      | 58/165 [00:06<00:11,  9.08it/s]\u001b[A\n",
            " 36%|███▌      | 59/165 [00:06<00:11,  9.08it/s]\u001b[A\n",
            " 36%|███▋      | 60/165 [00:06<00:11,  9.10it/s]\u001b[A\n",
            " 37%|███▋      | 61/165 [00:06<00:11,  9.10it/s]\u001b[A\n",
            " 38%|███▊      | 62/165 [00:06<00:11,  9.07it/s]\u001b[A\n",
            " 38%|███▊      | 63/165 [00:06<00:11,  9.08it/s]\u001b[A\n",
            " 39%|███▉      | 64/165 [00:07<00:11,  9.08it/s]\u001b[A\n",
            " 39%|███▉      | 65/165 [00:07<00:10,  9.09it/s]\u001b[A\n",
            " 40%|████      | 66/165 [00:07<00:10,  9.10it/s]\u001b[A\n",
            " 41%|████      | 67/165 [00:07<00:10,  9.10it/s]\u001b[A\n",
            " 41%|████      | 68/165 [00:07<00:10,  9.11it/s]\u001b[A\n",
            " 42%|████▏     | 69/165 [00:07<00:10,  9.09it/s]\u001b[A\n",
            " 42%|████▏     | 70/165 [00:07<00:10,  9.10it/s]\u001b[A\n",
            " 43%|████▎     | 71/165 [00:07<00:10,  9.11it/s]\u001b[A\n",
            " 44%|████▎     | 72/165 [00:07<00:10,  9.09it/s]\u001b[A\n",
            " 44%|████▍     | 73/165 [00:08<00:10,  9.08it/s]\u001b[A\n",
            " 45%|████▍     | 74/165 [00:08<00:09,  9.11it/s]\u001b[A\n",
            " 45%|████▌     | 75/165 [00:08<00:09,  9.09it/s]\u001b[A\n",
            " 46%|████▌     | 76/165 [00:08<00:09,  9.10it/s]\u001b[A\n",
            " 47%|████▋     | 77/165 [00:08<00:09,  9.12it/s]\u001b[A\n",
            " 47%|████▋     | 78/165 [00:08<00:09,  9.12it/s]\u001b[A\n",
            " 48%|████▊     | 79/165 [00:08<00:09,  9.12it/s]\u001b[A\n",
            " 48%|████▊     | 80/165 [00:08<00:09,  9.11it/s]\u001b[A\n",
            " 49%|████▉     | 81/165 [00:08<00:09,  9.11it/s]\u001b[A\n",
            " 50%|████▉     | 82/165 [00:09<00:09,  9.11it/s]\u001b[A\n",
            " 50%|█████     | 83/165 [00:09<00:08,  9.12it/s]\u001b[A\n",
            " 51%|█████     | 84/165 [00:09<00:08,  9.11it/s]\u001b[A\n",
            " 52%|█████▏    | 85/165 [00:09<00:08,  9.12it/s]\u001b[A\n",
            " 52%|█████▏    | 86/165 [00:09<00:08,  9.09it/s]\u001b[A\n",
            " 53%|█████▎    | 87/165 [00:09<00:08,  9.08it/s]\u001b[A\n",
            " 53%|█████▎    | 88/165 [00:09<00:08,  9.10it/s]\u001b[A\n",
            " 54%|█████▍    | 89/165 [00:09<00:08,  9.11it/s]\u001b[A\n",
            " 55%|█████▍    | 90/165 [00:09<00:08,  9.11it/s]\u001b[A\n",
            " 55%|█████▌    | 91/165 [00:10<00:08,  9.10it/s]\u001b[A\n",
            " 56%|█████▌    | 92/165 [00:10<00:08,  9.09it/s]\u001b[A\n",
            " 56%|█████▋    | 93/165 [00:10<00:07,  9.10it/s]\u001b[A\n",
            " 57%|█████▋    | 94/165 [00:10<00:07,  9.10it/s]\u001b[A\n",
            " 58%|█████▊    | 95/165 [00:10<00:07,  9.09it/s]\u001b[A\n",
            " 58%|█████▊    | 96/165 [00:10<00:07,  8.97it/s]\u001b[A\n",
            " 59%|█████▉    | 97/165 [00:10<00:07,  9.02it/s]\u001b[A\n",
            " 59%|█████▉    | 98/165 [00:10<00:07,  9.00it/s]\u001b[A\n",
            " 60%|██████    | 99/165 [00:10<00:07,  9.02it/s]\u001b[A\n",
            " 61%|██████    | 100/165 [00:11<00:07,  9.06it/s]\u001b[A\n",
            " 61%|██████    | 101/165 [00:11<00:07,  9.07it/s]\u001b[A\n",
            " 62%|██████▏   | 102/165 [00:11<00:06,  9.10it/s]\u001b[A\n",
            " 62%|██████▏   | 103/165 [00:11<00:06,  9.11it/s]\u001b[A\n",
            " 63%|██████▎   | 104/165 [00:11<00:06,  9.11it/s]\u001b[A\n",
            " 64%|██████▎   | 105/165 [00:11<00:06,  9.10it/s]\u001b[A\n",
            " 64%|██████▍   | 106/165 [00:11<00:06,  9.11it/s]\u001b[A\n",
            " 65%|██████▍   | 107/165 [00:11<00:06,  9.07it/s]\u001b[A\n",
            " 65%|██████▌   | 108/165 [00:11<00:06,  9.07it/s]\u001b[A\n",
            " 66%|██████▌   | 109/165 [00:12<00:06,  9.09it/s]\u001b[A\n",
            " 67%|██████▋   | 110/165 [00:12<00:06,  9.09it/s]\u001b[A\n",
            " 67%|██████▋   | 111/165 [00:12<00:05,  9.08it/s]\u001b[A\n",
            " 68%|██████▊   | 112/165 [00:12<00:05,  9.09it/s]\u001b[A\n",
            " 68%|██████▊   | 113/165 [00:12<00:05,  9.10it/s]\u001b[A\n",
            " 69%|██████▉   | 114/165 [00:12<00:05,  9.09it/s]\u001b[A\n",
            " 70%|██████▉   | 115/165 [00:12<00:05,  9.11it/s]\u001b[A\n",
            " 70%|███████   | 116/165 [00:12<00:05,  9.12it/s]\u001b[A\n",
            " 71%|███████   | 117/165 [00:12<00:05,  9.12it/s]\u001b[A\n",
            " 72%|███████▏  | 118/165 [00:13<00:05,  9.10it/s]\u001b[A\n",
            " 72%|███████▏  | 119/165 [00:13<00:05,  9.06it/s]\u001b[A\n",
            " 73%|███████▎  | 120/165 [00:13<00:04,  9.08it/s]\u001b[A\n",
            " 73%|███████▎  | 121/165 [00:13<00:04,  9.05it/s]\u001b[A\n",
            " 74%|███████▍  | 122/165 [00:13<00:04,  9.06it/s]\u001b[A\n",
            " 75%|███████▍  | 123/165 [00:13<00:04,  9.07it/s]\u001b[A\n",
            " 75%|███████▌  | 124/165 [00:13<00:04,  9.07it/s]\u001b[A\n",
            " 76%|███████▌  | 125/165 [00:13<00:04,  9.08it/s]\u001b[A\n",
            " 76%|███████▋  | 126/165 [00:13<00:04,  9.08it/s]\u001b[A\n",
            " 77%|███████▋  | 127/165 [00:13<00:04,  9.09it/s]\u001b[A\n",
            " 78%|███████▊  | 128/165 [00:14<00:04,  9.11it/s]\u001b[A\n",
            " 78%|███████▊  | 129/165 [00:14<00:03,  9.12it/s]\u001b[A\n",
            " 79%|███████▉  | 130/165 [00:14<00:03,  9.11it/s]\u001b[A\n",
            " 79%|███████▉  | 131/165 [00:14<00:03,  9.12it/s]\u001b[A\n",
            " 80%|████████  | 132/165 [00:14<00:03,  9.13it/s]\u001b[A\n",
            " 81%|████████  | 133/165 [00:14<00:03,  9.14it/s]\u001b[A\n",
            " 81%|████████  | 134/165 [00:14<00:03,  9.11it/s]\u001b[A\n",
            " 82%|████████▏ | 135/165 [00:14<00:03,  9.11it/s]\u001b[A\n",
            " 82%|████████▏ | 136/165 [00:14<00:03,  9.12it/s]\u001b[A\n",
            " 83%|████████▎ | 137/165 [00:15<00:03,  9.10it/s]\u001b[A\n",
            " 84%|████████▎ | 138/165 [00:15<00:02,  9.07it/s]\u001b[A\n",
            " 84%|████████▍ | 139/165 [00:15<00:02,  9.09it/s]\u001b[A\n",
            " 85%|████████▍ | 140/165 [00:15<00:02,  9.10it/s]\u001b[A\n",
            " 85%|████████▌ | 141/165 [00:15<00:02,  9.12it/s]\u001b[A\n",
            " 86%|████████▌ | 142/165 [00:15<00:02,  9.11it/s]\u001b[A\n",
            " 87%|████████▋ | 143/165 [00:15<00:02,  9.10it/s]\u001b[A\n",
            " 87%|████████▋ | 144/165 [00:15<00:02,  9.10it/s]\u001b[A\n",
            " 88%|████████▊ | 145/165 [00:15<00:02,  9.06it/s]\u001b[A\n",
            " 88%|████████▊ | 146/165 [00:16<00:02,  9.07it/s]\u001b[A\n",
            " 89%|████████▉ | 147/165 [00:16<00:01,  9.10it/s]\u001b[A\n",
            " 90%|████████▉ | 148/165 [00:16<00:01,  9.10it/s]\u001b[A\n",
            " 90%|█████████ | 149/165 [00:16<00:01,  9.12it/s]\u001b[A\n",
            " 91%|█████████ | 150/165 [00:16<00:01,  9.13it/s]\u001b[A\n",
            " 92%|█████████▏| 151/165 [00:16<00:01,  9.14it/s]\u001b[A\n",
            " 92%|█████████▏| 152/165 [00:16<00:01,  9.15it/s]\u001b[A\n",
            " 93%|█████████▎| 153/165 [00:16<00:01,  9.15it/s]\u001b[A\n",
            " 93%|█████████▎| 154/165 [00:16<00:01,  9.13it/s]\u001b[A\n",
            " 94%|█████████▍| 155/165 [00:17<00:01,  9.13it/s]\u001b[A\n",
            " 95%|█████████▍| 156/165 [00:17<00:00,  9.14it/s]\u001b[A\n",
            " 95%|█████████▌| 157/165 [00:17<00:00,  9.14it/s]\u001b[A\n",
            " 96%|█████████▌| 158/165 [00:17<00:00,  9.15it/s]\u001b[A\n",
            " 96%|█████████▋| 159/165 [00:17<00:00,  9.15it/s]\u001b[A\n",
            " 97%|█████████▋| 160/165 [00:17<00:00,  9.16it/s]\u001b[A\n",
            " 98%|█████████▊| 161/165 [00:17<00:00,  9.16it/s]\u001b[A\n",
            " 98%|█████████▊| 162/165 [00:17<00:00,  9.14it/s]\u001b[A\n",
            " 99%|█████████▉| 163/165 [00:17<00:00,  9.15it/s]\u001b[A\n",
            "100%|██████████| 165/165 [00:18<00:00,  9.07it/s]\n",
            "========== model: bert-base-uncased fold: 3 inference ==========\n",
            "========== model: bert-base-uncased fold: 3 inference ==========\n",
            "========== model: bert-base-uncased fold: 3 inference ==========\n",
            "========== model: bert-base-uncased fold: 3 inference ==========\n",
            "========== model: bert-base-uncased fold: 3 inference ==========\n",
            "loading configuration file /content/activebus_bert_final/config.json\n",
            "Model config BertConfig {\n",
            "  \"_name_or_path\": \"activebus/BERT-XD_Review\",\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_ids\": 0,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.15.0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "loading weights file /content/activebus_bert_final/pytorch_model.bin\n",
            "Some weights of the model checkpoint at /content/activebus_bert_final were not used when initializing BertForSequenceClassification: ['cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /content/activebus_bert_final and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "\n",
            "  0%|          | 0/165 [00:00<?, ?it/s]\u001b[A\n",
            "  1%|          | 1/165 [00:00<00:19,  8.55it/s]\u001b[A\n",
            "  1%|          | 2/165 [00:00<00:18,  8.86it/s]\u001b[A\n",
            "  2%|▏         | 3/165 [00:00<00:18,  8.95it/s]\u001b[A\n",
            "  2%|▏         | 4/165 [00:00<00:17,  9.00it/s]\u001b[A\n",
            "  3%|▎         | 5/165 [00:00<00:17,  9.04it/s]\u001b[A\n",
            "  4%|▎         | 6/165 [00:00<00:17,  9.04it/s]\u001b[A\n",
            "  4%|▍         | 7/165 [00:00<00:17,  9.08it/s]\u001b[A\n",
            "  5%|▍         | 8/165 [00:00<00:17,  9.09it/s]\u001b[A\n",
            "  5%|▌         | 9/165 [00:00<00:17,  9.09it/s]\u001b[A\n",
            "  6%|▌         | 10/165 [00:01<00:17,  9.08it/s]\u001b[A\n",
            "  7%|▋         | 11/165 [00:01<00:16,  9.09it/s]\u001b[A\n",
            "  7%|▋         | 12/165 [00:01<00:16,  9.10it/s]\u001b[A\n",
            "  8%|▊         | 13/165 [00:01<00:16,  9.11it/s]\u001b[A\n",
            "  8%|▊         | 14/165 [00:01<00:16,  9.11it/s]\u001b[A\n",
            "  9%|▉         | 15/165 [00:01<00:16,  9.12it/s]\u001b[A\n",
            " 10%|▉         | 16/165 [00:01<00:16,  9.12it/s]\u001b[A\n",
            " 10%|█         | 17/165 [00:01<00:16,  9.11it/s]\u001b[A\n",
            " 11%|█         | 18/165 [00:01<00:16,  9.08it/s]\u001b[A\n",
            " 12%|█▏        | 19/165 [00:02<00:16,  9.08it/s]\u001b[A\n",
            " 12%|█▏        | 20/165 [00:02<00:15,  9.10it/s]\u001b[A\n",
            " 13%|█▎        | 21/165 [00:02<00:15,  9.10it/s]\u001b[A\n",
            " 13%|█▎        | 22/165 [00:02<00:15,  9.07it/s]\u001b[A\n",
            " 14%|█▍        | 23/165 [00:02<00:15,  9.10it/s]\u001b[A\n",
            " 15%|█▍        | 24/165 [00:02<00:19,  7.24it/s]\u001b[A\n",
            " 15%|█▌        | 25/165 [00:02<00:18,  7.70it/s]\u001b[A\n",
            " 16%|█▌        | 26/165 [00:02<00:17,  8.07it/s]\u001b[A\n",
            " 16%|█▋        | 27/165 [00:03<00:16,  8.34it/s]\u001b[A\n",
            " 17%|█▋        | 28/165 [00:03<00:15,  8.57it/s]\u001b[A\n",
            " 18%|█▊        | 29/165 [00:03<00:15,  8.69it/s]\u001b[A\n",
            " 18%|█▊        | 30/165 [00:03<00:15,  8.81it/s]\u001b[A\n",
            " 19%|█▉        | 31/165 [00:03<00:15,  8.89it/s]\u001b[A\n",
            " 19%|█▉        | 32/165 [00:03<00:14,  8.94it/s]\u001b[A\n",
            " 20%|██        | 33/165 [00:03<00:14,  8.99it/s]\u001b[A\n",
            " 21%|██        | 34/165 [00:03<00:14,  9.02it/s]\u001b[A\n",
            " 21%|██        | 35/165 [00:03<00:14,  9.05it/s]\u001b[A\n",
            " 22%|██▏       | 36/165 [00:04<00:14,  9.09it/s]\u001b[A\n",
            " 22%|██▏       | 37/165 [00:04<00:14,  9.09it/s]\u001b[A\n",
            " 23%|██▎       | 38/165 [00:04<00:13,  9.10it/s]\u001b[A\n",
            " 24%|██▎       | 39/165 [00:04<00:13,  9.08it/s]\u001b[A\n",
            " 24%|██▍       | 40/165 [00:04<00:13,  9.09it/s]\u001b[A\n",
            " 25%|██▍       | 41/165 [00:04<00:13,  9.08it/s]\u001b[A\n",
            " 25%|██▌       | 42/165 [00:04<00:13,  9.08it/s]\u001b[A\n",
            " 26%|██▌       | 43/165 [00:04<00:13,  9.09it/s]\u001b[A\n",
            " 27%|██▋       | 44/165 [00:04<00:13,  9.09it/s]\u001b[A\n",
            " 27%|██▋       | 45/165 [00:05<00:13,  9.11it/s]\u001b[A\n",
            " 28%|██▊       | 46/165 [00:05<00:13,  9.11it/s]\u001b[A\n",
            " 28%|██▊       | 47/165 [00:05<00:13,  9.07it/s]\u001b[A\n",
            " 29%|██▉       | 48/165 [00:05<00:12,  9.08it/s]\u001b[A\n",
            " 30%|██▉       | 49/165 [00:05<00:12,  9.07it/s]\u001b[A\n",
            " 30%|███       | 50/165 [00:05<00:12,  9.09it/s]\u001b[A\n",
            " 31%|███       | 51/165 [00:05<00:12,  9.11it/s]\u001b[A\n",
            " 32%|███▏      | 52/165 [00:05<00:12,  9.12it/s]\u001b[A\n",
            " 32%|███▏      | 53/165 [00:05<00:12,  9.12it/s]\u001b[A\n",
            " 33%|███▎      | 54/165 [00:06<00:12,  9.14it/s]\u001b[A\n",
            " 33%|███▎      | 55/165 [00:06<00:12,  9.13it/s]\u001b[A\n",
            " 34%|███▍      | 56/165 [00:06<00:11,  9.11it/s]\u001b[A\n",
            " 35%|███▍      | 57/165 [00:06<00:11,  9.12it/s]\u001b[A\n",
            " 35%|███▌      | 58/165 [00:06<00:11,  9.11it/s]\u001b[A\n",
            " 36%|███▌      | 59/165 [00:06<00:11,  9.06it/s]\u001b[A\n",
            " 36%|███▋      | 60/165 [00:06<00:11,  9.08it/s]\u001b[A\n",
            " 37%|███▋      | 61/165 [00:06<00:11,  9.09it/s]\u001b[A\n",
            " 38%|███▊      | 62/165 [00:06<00:11,  9.11it/s]\u001b[A\n",
            " 38%|███▊      | 63/165 [00:07<00:11,  9.11it/s]\u001b[A\n",
            " 39%|███▉      | 64/165 [00:07<00:11,  9.10it/s]\u001b[A\n",
            " 39%|███▉      | 65/165 [00:07<00:11,  9.07it/s]\u001b[A\n",
            " 40%|████      | 66/165 [00:07<00:10,  9.06it/s]\u001b[A\n",
            " 41%|████      | 67/165 [00:07<00:10,  9.08it/s]\u001b[A\n",
            " 41%|████      | 68/165 [00:07<00:10,  9.09it/s]\u001b[A\n",
            " 42%|████▏     | 69/165 [00:07<00:10,  9.10it/s]\u001b[A\n",
            " 42%|████▏     | 70/165 [00:07<00:10,  9.11it/s]\u001b[A\n",
            " 43%|████▎     | 71/165 [00:07<00:10,  9.13it/s]\u001b[A\n",
            " 44%|████▎     | 72/165 [00:08<00:10,  9.13it/s]\u001b[A\n",
            " 44%|████▍     | 73/165 [00:08<00:10,  9.13it/s]\u001b[A\n",
            " 45%|████▍     | 74/165 [00:08<00:09,  9.14it/s]\u001b[A\n",
            " 45%|████▌     | 75/165 [00:08<00:09,  9.12it/s]\u001b[A\n",
            " 46%|████▌     | 76/165 [00:08<00:09,  9.12it/s]\u001b[A\n",
            " 47%|████▋     | 77/165 [00:08<00:09,  9.12it/s]\u001b[A\n",
            " 47%|████▋     | 78/165 [00:08<00:09,  9.13it/s]\u001b[A\n",
            " 48%|████▊     | 79/165 [00:08<00:09,  9.13it/s]\u001b[A\n",
            " 48%|████▊     | 80/165 [00:08<00:09,  9.14it/s]\u001b[A\n",
            " 49%|████▉     | 81/165 [00:09<00:09,  9.15it/s]\u001b[A\n",
            " 50%|████▉     | 82/165 [00:09<00:09,  9.15it/s]\u001b[A\n",
            " 50%|█████     | 83/165 [00:09<00:08,  9.13it/s]\u001b[A\n",
            " 51%|█████     | 84/165 [00:09<00:08,  9.11it/s]\u001b[A\n",
            " 52%|█████▏    | 85/165 [00:09<00:08,  9.09it/s]\u001b[A\n",
            " 52%|█████▏    | 86/165 [00:09<00:08,  9.10it/s]\u001b[A\n",
            " 53%|█████▎    | 87/165 [00:09<00:08,  9.11it/s]\u001b[A\n",
            " 53%|█████▎    | 88/165 [00:09<00:08,  9.11it/s]\u001b[A\n",
            " 54%|█████▍    | 89/165 [00:09<00:08,  9.09it/s]\u001b[A\n",
            " 55%|█████▍    | 90/165 [00:09<00:08,  9.08it/s]\u001b[A\n",
            " 55%|█████▌    | 91/165 [00:10<00:08,  9.10it/s]\u001b[A\n",
            " 56%|█████▌    | 92/165 [00:10<00:08,  9.09it/s]\u001b[A\n",
            " 56%|█████▋    | 93/165 [00:10<00:07,  9.09it/s]\u001b[A\n",
            " 57%|█████▋    | 94/165 [00:10<00:07,  9.10it/s]\u001b[A\n",
            " 58%|█████▊    | 95/165 [00:10<00:07,  9.11it/s]\u001b[A\n",
            " 58%|█████▊    | 96/165 [00:10<00:07,  9.10it/s]\u001b[A\n",
            " 59%|█████▉    | 97/165 [00:10<00:07,  9.09it/s]\u001b[A\n",
            " 59%|█████▉    | 98/165 [00:10<00:07,  9.10it/s]\u001b[A\n",
            " 60%|██████    | 99/165 [00:10<00:07,  9.11it/s]\u001b[A\n",
            " 61%|██████    | 100/165 [00:11<00:07,  9.09it/s]\u001b[A\n",
            " 61%|██████    | 101/165 [00:11<00:07,  9.07it/s]\u001b[A\n",
            " 62%|██████▏   | 102/165 [00:11<00:06,  9.08it/s]\u001b[A\n",
            " 62%|██████▏   | 103/165 [00:11<00:06,  9.08it/s]\u001b[A\n",
            " 63%|██████▎   | 104/165 [00:11<00:06,  9.09it/s]\u001b[A\n",
            " 64%|██████▎   | 105/165 [00:11<00:06,  9.09it/s]\u001b[A\n",
            " 64%|██████▍   | 106/165 [00:11<00:06,  9.10it/s]\u001b[A\n",
            " 65%|██████▍   | 107/165 [00:11<00:06,  9.11it/s]\u001b[A\n",
            " 65%|██████▌   | 108/165 [00:11<00:06,  9.09it/s]\u001b[A\n",
            " 66%|██████▌   | 109/165 [00:12<00:06,  9.09it/s]\u001b[A\n",
            " 67%|██████▋   | 110/165 [00:12<00:06,  9.10it/s]\u001b[A\n",
            " 67%|██████▋   | 111/165 [00:12<00:05,  9.10it/s]\u001b[A\n",
            " 68%|██████▊   | 112/165 [00:12<00:05,  9.11it/s]\u001b[A\n",
            " 68%|██████▊   | 113/165 [00:12<00:05,  9.10it/s]\u001b[A\n",
            " 69%|██████▉   | 114/165 [00:12<00:05,  9.11it/s]\u001b[A\n",
            " 70%|██████▉   | 115/165 [00:12<00:05,  9.12it/s]\u001b[A\n",
            " 70%|███████   | 116/165 [00:12<00:05,  9.13it/s]\u001b[A\n",
            " 71%|███████   | 117/165 [00:12<00:05,  9.13it/s]\u001b[A\n",
            " 72%|███████▏  | 118/165 [00:13<00:05,  9.12it/s]\u001b[A\n",
            " 72%|███████▏  | 119/165 [00:13<00:05,  9.13it/s]\u001b[A\n",
            " 73%|███████▎  | 120/165 [00:13<00:04,  9.12it/s]\u001b[A\n",
            " 73%|███████▎  | 121/165 [00:13<00:04,  9.11it/s]\u001b[A\n",
            " 74%|███████▍  | 122/165 [00:13<00:04,  9.12it/s]\u001b[A\n",
            " 75%|███████▍  | 123/165 [00:13<00:04,  9.12it/s]\u001b[A\n",
            " 75%|███████▌  | 124/165 [00:13<00:04,  9.11it/s]\u001b[A\n",
            " 76%|███████▌  | 125/165 [00:13<00:04,  9.10it/s]\u001b[A\n",
            " 76%|███████▋  | 126/165 [00:13<00:04,  9.10it/s]\u001b[A\n",
            " 77%|███████▋  | 127/165 [00:14<00:04,  9.12it/s]\u001b[A\n",
            " 78%|███████▊  | 128/165 [00:14<00:04,  9.12it/s]\u001b[A\n",
            " 78%|███████▊  | 129/165 [00:14<00:03,  9.13it/s]\u001b[A\n",
            " 79%|███████▉  | 130/165 [00:14<00:03,  9.12it/s]\u001b[A\n",
            " 79%|███████▉  | 131/165 [00:14<00:03,  9.12it/s]\u001b[A\n",
            " 80%|████████  | 132/165 [00:14<00:03,  9.12it/s]\u001b[A\n",
            " 81%|████████  | 133/165 [00:14<00:03,  9.12it/s]\u001b[A\n",
            " 81%|████████  | 134/165 [00:14<00:03,  9.11it/s]\u001b[A\n",
            " 82%|████████▏ | 135/165 [00:14<00:03,  9.09it/s]\u001b[A\n",
            " 82%|████████▏ | 136/165 [00:15<00:03,  9.09it/s]\u001b[A\n",
            " 83%|████████▎ | 137/165 [00:15<00:03,  9.08it/s]\u001b[A\n",
            " 84%|████████▎ | 138/165 [00:15<00:02,  9.07it/s]\u001b[A\n",
            " 84%|████████▍ | 139/165 [00:15<00:02,  9.08it/s]\u001b[A\n",
            " 85%|████████▍ | 140/165 [00:15<00:02,  9.08it/s]\u001b[A\n",
            " 85%|████████▌ | 141/165 [00:15<00:02,  9.07it/s]\u001b[A\n",
            " 86%|████████▌ | 142/165 [00:15<00:02,  9.09it/s]\u001b[A\n",
            " 87%|████████▋ | 143/165 [00:15<00:02,  9.10it/s]\u001b[A\n",
            " 87%|████████▋ | 144/165 [00:15<00:02,  9.10it/s]\u001b[A\n",
            " 88%|████████▊ | 145/165 [00:16<00:02,  9.10it/s]\u001b[A\n",
            " 88%|████████▊ | 146/165 [00:16<00:02,  9.11it/s]\u001b[A\n",
            " 89%|████████▉ | 147/165 [00:16<00:01,  9.11it/s]\u001b[A\n",
            " 90%|████████▉ | 148/165 [00:16<00:01,  9.12it/s]\u001b[A\n",
            " 90%|█████████ | 149/165 [00:16<00:01,  9.09it/s]\u001b[A\n",
            " 91%|█████████ | 150/165 [00:16<00:01,  9.07it/s]\u001b[A\n",
            " 92%|█████████▏| 151/165 [00:16<00:01,  9.05it/s]\u001b[A\n",
            " 92%|█████████▏| 152/165 [00:16<00:01,  9.09it/s]\u001b[A\n",
            " 93%|█████████▎| 153/165 [00:16<00:01,  9.11it/s]\u001b[A\n",
            " 93%|█████████▎| 154/165 [00:17<00:01,  9.11it/s]\u001b[A\n",
            " 94%|█████████▍| 155/165 [00:17<00:01,  9.12it/s]\u001b[A\n",
            " 95%|█████████▍| 156/165 [00:17<00:00,  9.12it/s]\u001b[A\n",
            " 95%|█████████▌| 157/165 [00:17<00:00,  9.09it/s]\u001b[A\n",
            " 96%|█████████▌| 158/165 [00:17<00:00,  9.09it/s]\u001b[A\n",
            " 96%|█████████▋| 159/165 [00:17<00:00,  9.11it/s]\u001b[A\n",
            " 97%|█████████▋| 160/165 [00:17<00:00,  9.13it/s]\u001b[A\n",
            " 98%|█████████▊| 161/165 [00:17<00:00,  9.12it/s]\u001b[A\n",
            " 98%|█████████▊| 162/165 [00:17<00:00,  9.13it/s]\u001b[A\n",
            " 99%|█████████▉| 163/165 [00:18<00:00,  9.15it/s]\u001b[A\n",
            "100%|██████████| 165/165 [00:18<00:00,  9.04it/s]\n",
            "========== model: bert-base-uncased fold: 4 inference ==========\n",
            "========== model: bert-base-uncased fold: 4 inference ==========\n",
            "========== model: bert-base-uncased fold: 4 inference ==========\n",
            "========== model: bert-base-uncased fold: 4 inference ==========\n",
            "========== model: bert-base-uncased fold: 4 inference ==========\n",
            "loading configuration file /content/activebus_bert_final/config.json\n",
            "Model config BertConfig {\n",
            "  \"_name_or_path\": \"activebus/BERT-XD_Review\",\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_ids\": 0,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.15.0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "loading weights file /content/activebus_bert_final/pytorch_model.bin\n",
            "Some weights of the model checkpoint at /content/activebus_bert_final were not used when initializing BertForSequenceClassification: ['cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /content/activebus_bert_final and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "\n",
            "  0%|          | 0/165 [00:00<?, ?it/s]\u001b[A\n",
            "  1%|          | 1/165 [00:00<00:18,  8.72it/s]\u001b[A\n",
            "  1%|          | 2/165 [00:00<00:18,  8.89it/s]\u001b[A\n",
            "  2%|▏         | 3/165 [00:00<00:18,  8.97it/s]\u001b[A\n",
            "  2%|▏         | 4/165 [00:00<00:17,  9.01it/s]\u001b[A\n",
            "  3%|▎         | 5/165 [00:00<00:17,  9.04it/s]\u001b[A\n",
            "  4%|▎         | 6/165 [00:00<00:17,  9.09it/s]\u001b[A\n",
            "  4%|▍         | 7/165 [00:00<00:17,  9.08it/s]\u001b[A\n",
            "  5%|▍         | 8/165 [00:00<00:17,  8.95it/s]\u001b[A\n",
            "  5%|▌         | 9/165 [00:01<00:17,  9.00it/s]\u001b[A\n",
            "  6%|▌         | 10/165 [00:01<00:17,  9.04it/s]\u001b[A\n",
            "  7%|▋         | 11/165 [00:01<00:16,  9.06it/s]\u001b[A\n",
            "  7%|▋         | 12/165 [00:01<00:16,  9.09it/s]\u001b[A\n",
            "  8%|▊         | 13/165 [00:01<00:16,  9.09it/s]\u001b[A\n",
            "  8%|▊         | 14/165 [00:01<00:16,  9.08it/s]\u001b[A\n",
            "  9%|▉         | 15/165 [00:01<00:16,  9.07it/s]\u001b[A\n",
            " 10%|▉         | 16/165 [00:01<00:16,  9.08it/s]\u001b[A\n",
            " 10%|█         | 17/165 [00:01<00:16,  9.08it/s]\u001b[A\n",
            " 11%|█         | 18/165 [00:01<00:16,  9.09it/s]\u001b[A\n",
            " 12%|█▏        | 19/165 [00:02<00:16,  9.04it/s]\u001b[A\n",
            " 12%|█▏        | 20/165 [00:02<00:16,  9.05it/s]\u001b[A\n",
            " 13%|█▎        | 21/165 [00:02<00:15,  9.08it/s]\u001b[A\n",
            " 13%|█▎        | 22/165 [00:02<00:15,  9.10it/s]\u001b[A\n",
            " 14%|█▍        | 23/165 [00:02<00:15,  9.09it/s]\u001b[A\n",
            " 15%|█▍        | 24/165 [00:02<00:15,  9.10it/s]\u001b[A\n",
            " 15%|█▌        | 25/165 [00:02<00:15,  9.10it/s]\u001b[A\n",
            " 16%|█▌        | 26/165 [00:02<00:15,  9.10it/s]\u001b[A\n",
            " 16%|█▋        | 27/165 [00:02<00:15,  9.12it/s]\u001b[A\n",
            " 17%|█▋        | 28/165 [00:03<00:15,  9.12it/s]\u001b[A\n",
            " 18%|█▊        | 29/165 [00:03<00:14,  9.11it/s]\u001b[A\n",
            " 18%|█▊        | 30/165 [00:03<00:14,  9.11it/s]\u001b[A\n",
            " 19%|█▉        | 31/165 [00:03<00:14,  9.12it/s]\u001b[A\n",
            " 19%|█▉        | 32/165 [00:03<00:14,  9.11it/s]\u001b[A\n",
            " 20%|██        | 33/165 [00:03<00:14,  9.11it/s]\u001b[A\n",
            " 21%|██        | 34/165 [00:03<00:14,  9.12it/s]\u001b[A\n",
            " 21%|██        | 35/165 [00:03<00:14,  9.13it/s]\u001b[A\n",
            " 22%|██▏       | 36/165 [00:03<00:14,  9.12it/s]\u001b[A\n",
            " 22%|██▏       | 37/165 [00:04<00:14,  9.12it/s]\u001b[A\n",
            " 23%|██▎       | 38/165 [00:04<00:13,  9.12it/s]\u001b[A\n",
            " 24%|██▎       | 39/165 [00:04<00:13,  9.10it/s]\u001b[A\n",
            " 24%|██▍       | 40/165 [00:04<00:13,  9.09it/s]\u001b[A\n",
            " 25%|██▍       | 41/165 [00:04<00:13,  9.10it/s]\u001b[A\n",
            " 25%|██▌       | 42/165 [00:04<00:13,  9.12it/s]\u001b[A\n",
            " 26%|██▌       | 43/165 [00:04<00:13,  9.13it/s]\u001b[A\n",
            " 27%|██▋       | 44/165 [00:04<00:13,  9.09it/s]\u001b[A\n",
            " 27%|██▋       | 45/165 [00:04<00:13,  9.08it/s]\u001b[A\n",
            " 28%|██▊       | 46/165 [00:05<00:13,  9.08it/s]\u001b[A\n",
            " 28%|██▊       | 47/165 [00:05<00:12,  9.08it/s]\u001b[A\n",
            " 29%|██▉       | 48/165 [00:05<00:12,  9.10it/s]\u001b[A\n",
            " 30%|██▉       | 49/165 [00:05<00:12,  9.11it/s]\u001b[A\n",
            " 30%|███       | 50/165 [00:05<00:12,  9.11it/s]\u001b[A\n",
            " 31%|███       | 51/165 [00:05<00:12,  9.11it/s]\u001b[A\n",
            " 32%|███▏      | 52/165 [00:05<00:12,  9.10it/s]\u001b[A\n",
            " 32%|███▏      | 53/165 [00:05<00:12,  9.09it/s]\u001b[A\n",
            " 33%|███▎      | 54/165 [00:05<00:12,  9.08it/s]\u001b[A\n",
            " 33%|███▎      | 55/165 [00:06<00:12,  9.09it/s]\u001b[A\n",
            " 34%|███▍      | 56/165 [00:06<00:11,  9.10it/s]\u001b[A\n",
            " 35%|███▍      | 57/165 [00:06<00:11,  9.10it/s]\u001b[A\n",
            " 35%|███▌      | 58/165 [00:06<00:11,  9.11it/s]\u001b[A\n",
            " 36%|███▌      | 59/165 [00:06<00:11,  9.11it/s]\u001b[A\n",
            " 36%|███▋      | 60/165 [00:06<00:11,  9.00it/s]\u001b[A\n",
            " 37%|███▋      | 61/165 [00:06<00:11,  9.02it/s]\u001b[A\n",
            " 38%|███▊      | 62/165 [00:06<00:11,  9.04it/s]\u001b[A\n",
            " 38%|███▊      | 63/165 [00:06<00:11,  9.06it/s]\u001b[A\n",
            " 39%|███▉      | 64/165 [00:07<00:11,  9.05it/s]\u001b[A\n",
            " 39%|███▉      | 65/165 [00:07<00:11,  9.05it/s]\u001b[A\n",
            " 40%|████      | 66/165 [00:07<00:10,  9.05it/s]\u001b[A\n",
            " 41%|████      | 67/165 [00:07<00:10,  9.09it/s]\u001b[A\n",
            " 41%|████      | 68/165 [00:07<00:10,  9.08it/s]\u001b[A\n",
            " 42%|████▏     | 69/165 [00:07<00:10,  9.10it/s]\u001b[A\n",
            " 42%|████▏     | 70/165 [00:07<00:10,  9.12it/s]\u001b[A\n",
            " 43%|████▎     | 71/165 [00:07<00:10,  9.12it/s]\u001b[A\n",
            " 44%|████▎     | 72/165 [00:07<00:10,  9.12it/s]\u001b[A\n",
            " 44%|████▍     | 73/165 [00:08<00:10,  9.13it/s]\u001b[A\n",
            " 45%|████▍     | 74/165 [00:08<00:09,  9.12it/s]\u001b[A\n",
            " 45%|████▌     | 75/165 [00:08<00:09,  9.10it/s]\u001b[A\n",
            " 46%|████▌     | 76/165 [00:08<00:09,  9.08it/s]\u001b[A\n",
            " 47%|████▋     | 77/165 [00:08<00:09,  9.08it/s]\u001b[A\n",
            " 47%|████▋     | 78/165 [00:08<00:09,  9.10it/s]\u001b[A\n",
            " 48%|████▊     | 79/165 [00:08<00:09,  9.11it/s]\u001b[A\n",
            " 48%|████▊     | 80/165 [00:08<00:09,  9.10it/s]\u001b[A\n",
            " 49%|████▉     | 81/165 [00:08<00:09,  9.10it/s]\u001b[A\n",
            " 50%|████▉     | 82/165 [00:09<00:09,  9.10it/s]\u001b[A\n",
            " 50%|█████     | 83/165 [00:09<00:09,  9.09it/s]\u001b[A\n",
            " 51%|█████     | 84/165 [00:09<00:08,  9.09it/s]\u001b[A\n",
            " 52%|█████▏    | 85/165 [00:09<00:08,  9.11it/s]\u001b[A\n",
            " 52%|█████▏    | 86/165 [00:09<00:08,  9.12it/s]\u001b[A\n",
            " 53%|█████▎    | 87/165 [00:09<00:08,  9.11it/s]\u001b[A\n",
            " 53%|█████▎    | 88/165 [00:09<00:08,  9.09it/s]\u001b[A\n",
            " 54%|█████▍    | 89/165 [00:09<00:08,  9.09it/s]\u001b[A\n",
            " 55%|█████▍    | 90/165 [00:09<00:08,  9.09it/s]\u001b[A\n",
            " 55%|█████▌    | 91/165 [00:10<00:08,  9.08it/s]\u001b[A\n",
            " 56%|█████▌    | 92/165 [00:10<00:08,  9.09it/s]\u001b[A\n",
            " 56%|█████▋    | 93/165 [00:10<00:07,  9.10it/s]\u001b[A\n",
            " 57%|█████▋    | 94/165 [00:10<00:07,  9.10it/s]\u001b[A\n",
            " 58%|█████▊    | 95/165 [00:10<00:07,  9.11it/s]\u001b[A\n",
            " 58%|█████▊    | 96/165 [00:10<00:07,  9.10it/s]\u001b[A\n",
            " 59%|█████▉    | 97/165 [00:10<00:07,  9.11it/s]\u001b[A\n",
            " 59%|█████▉    | 98/165 [00:10<00:07,  9.10it/s]\u001b[A\n",
            " 60%|██████    | 99/165 [00:10<00:07,  9.10it/s]\u001b[A\n",
            " 61%|██████    | 100/165 [00:11<00:07,  9.10it/s]\u001b[A\n",
            " 61%|██████    | 101/165 [00:11<00:07,  9.11it/s]\u001b[A\n",
            " 62%|██████▏   | 102/165 [00:11<00:06,  9.11it/s]\u001b[A\n",
            " 62%|██████▏   | 103/165 [00:11<00:06,  9.10it/s]\u001b[A\n",
            " 63%|██████▎   | 104/165 [00:11<00:06,  9.11it/s]\u001b[A\n",
            " 64%|██████▎   | 105/165 [00:11<00:06,  9.11it/s]\u001b[A\n",
            " 64%|██████▍   | 106/165 [00:11<00:06,  9.12it/s]\u001b[A\n",
            " 65%|██████▍   | 107/165 [00:11<00:06,  9.12it/s]\u001b[A\n",
            " 65%|██████▌   | 108/165 [00:11<00:06,  9.12it/s]\u001b[A\n",
            " 66%|██████▌   | 109/165 [00:11<00:06,  9.13it/s]\u001b[A\n",
            " 67%|██████▋   | 110/165 [00:12<00:06,  9.11it/s]\u001b[A\n",
            " 67%|██████▋   | 111/165 [00:12<00:05,  9.12it/s]\u001b[A\n",
            " 68%|██████▊   | 112/165 [00:12<00:05,  9.12it/s]\u001b[A\n",
            " 68%|██████▊   | 113/165 [00:12<00:05,  9.11it/s]\u001b[A\n",
            " 69%|██████▉   | 114/165 [00:12<00:05,  9.12it/s]\u001b[A\n",
            " 70%|██████▉   | 115/165 [00:12<00:05,  9.07it/s]\u001b[A\n",
            " 70%|███████   | 116/165 [00:12<00:05,  9.09it/s]\u001b[A\n",
            " 71%|███████   | 117/165 [00:12<00:05,  9.09it/s]\u001b[A\n",
            " 72%|███████▏  | 118/165 [00:12<00:05,  9.10it/s]\u001b[A\n",
            " 72%|███████▏  | 119/165 [00:13<00:05,  9.09it/s]\u001b[A\n",
            " 73%|███████▎  | 120/165 [00:13<00:04,  9.11it/s]\u001b[A\n",
            " 73%|███████▎  | 121/165 [00:13<00:04,  9.11it/s]\u001b[A\n",
            " 74%|███████▍  | 122/165 [00:13<00:04,  9.07it/s]\u001b[A\n",
            " 75%|███████▍  | 123/165 [00:13<00:04,  9.09it/s]\u001b[A\n",
            " 75%|███████▌  | 124/165 [00:13<00:04,  9.07it/s]\u001b[A\n",
            " 76%|███████▌  | 125/165 [00:13<00:04,  9.09it/s]\u001b[A\n",
            " 76%|███████▋  | 126/165 [00:13<00:04,  9.07it/s]\u001b[A\n",
            " 77%|███████▋  | 127/165 [00:13<00:04,  9.07it/s]\u001b[A\n",
            " 78%|███████▊  | 128/165 [00:14<00:04,  9.10it/s]\u001b[A\n",
            " 78%|███████▊  | 129/165 [00:14<00:03,  9.09it/s]\u001b[A\n",
            " 79%|███████▉  | 130/165 [00:14<00:03,  9.09it/s]\u001b[A\n",
            " 79%|███████▉  | 131/165 [00:14<00:03,  9.08it/s]\u001b[A\n",
            " 80%|████████  | 132/165 [00:14<00:03,  9.10it/s]\u001b[A\n",
            " 81%|████████  | 133/165 [00:14<00:03,  9.10it/s]\u001b[A\n",
            " 81%|████████  | 134/165 [00:14<00:03,  9.10it/s]\u001b[A\n",
            " 82%|████████▏ | 135/165 [00:14<00:03,  9.12it/s]\u001b[A\n",
            " 82%|████████▏ | 136/165 [00:14<00:03,  9.11it/s]\u001b[A\n",
            " 83%|████████▎ | 137/165 [00:15<00:03,  9.10it/s]\u001b[A\n",
            " 84%|████████▎ | 138/165 [00:15<00:02,  9.11it/s]\u001b[A\n",
            " 84%|████████▍ | 139/165 [00:15<00:02,  9.10it/s]\u001b[A\n",
            " 85%|████████▍ | 140/165 [00:15<00:02,  9.10it/s]\u001b[A\n",
            " 85%|████████▌ | 141/165 [00:15<00:02,  9.11it/s]\u001b[A\n",
            " 86%|████████▌ | 142/165 [00:15<00:02,  9.09it/s]\u001b[A\n",
            " 87%|████████▋ | 143/165 [00:15<00:02,  9.07it/s]\u001b[A\n",
            " 87%|████████▋ | 144/165 [00:15<00:02,  9.09it/s]\u001b[A\n",
            " 88%|████████▊ | 145/165 [00:15<00:02,  9.10it/s]\u001b[A\n",
            " 88%|████████▊ | 146/165 [00:16<00:02,  9.09it/s]\u001b[A\n",
            " 89%|████████▉ | 147/165 [00:16<00:01,  9.08it/s]\u001b[A\n",
            " 90%|████████▉ | 148/165 [00:16<00:01,  9.08it/s]\u001b[A\n",
            " 90%|█████████ | 149/165 [00:16<00:01,  9.10it/s]\u001b[A\n",
            " 91%|█████████ | 150/165 [00:16<00:01,  9.12it/s]\u001b[A\n",
            " 92%|█████████▏| 151/165 [00:16<00:01,  9.13it/s]\u001b[A\n",
            " 92%|█████████▏| 152/165 [00:16<00:01,  9.14it/s]\u001b[A\n",
            " 93%|█████████▎| 153/165 [00:16<00:01,  9.12it/s]\u001b[A\n",
            " 93%|█████████▎| 154/165 [00:16<00:01,  9.09it/s]\u001b[A\n",
            " 94%|█████████▍| 155/165 [00:17<00:01,  9.09it/s]\u001b[A\n",
            " 95%|█████████▍| 156/165 [00:17<00:00,  9.04it/s]\u001b[A\n",
            " 95%|█████████▌| 157/165 [00:17<00:00,  9.04it/s]\u001b[A\n",
            " 96%|█████████▌| 158/165 [00:17<00:00,  9.05it/s]\u001b[A\n",
            " 96%|█████████▋| 159/165 [00:17<00:00,  9.07it/s]\u001b[A\n",
            " 97%|█████████▋| 160/165 [00:17<00:00,  9.09it/s]\u001b[A\n",
            " 98%|█████████▊| 161/165 [00:17<00:00,  9.12it/s]\u001b[A\n",
            " 98%|█████████▊| 162/165 [00:17<00:00,  9.12it/s]\u001b[A\n",
            " 99%|█████████▉| 163/165 [00:17<00:00,  9.13it/s]\u001b[A\n",
            "100%|██████████| 165/165 [00:18<00:00,  9.08it/s]\n",
            " 83%|████████▎ | 5/6 [3:58:19<47:41, 2861.70s/it]  Didn't find file /content/activebus_bert_final/added_tokens.json. We won't load it.\n",
            "loading file /content/activebus_bert_final/vocab.txt\n",
            "loading file /content/activebus_bert_final/tokenizer.json\n",
            "loading file None\n",
            "loading file /content/activebus_bert_final/special_tokens_map.json\n",
            "loading file /content/activebus_bert_final/tokenizer_config.json\n",
            "========== fold: 0 training ==========\n",
            "========== fold: 0 training ==========\n",
            "========== fold: 0 training ==========\n",
            "========== fold: 0 training ==========\n",
            "========== fold: 0 training ==========\n",
            "========== fold: 0 training ==========\n",
            "Didn't find file /content/activebus_bert_final/added_tokens.json. We won't load it.\n",
            "loading file /content/activebus_bert_final/vocab.txt\n",
            "loading file None\n",
            "loading file /content/activebus_bert_final/special_tokens_map.json\n",
            "loading file /content/activebus_bert_final/tokenizer_config.json\n",
            "loading file /content/activebus_bert_final/tokenizer.json\n",
            "Didn't find file /content/activebus_bert_final/added_tokens.json. We won't load it.\n",
            "loading file /content/activebus_bert_final/vocab.txt\n",
            "loading file None\n",
            "loading file /content/activebus_bert_final/special_tokens_map.json\n",
            "loading file /content/activebus_bert_final/tokenizer_config.json\n",
            "loading file /content/activebus_bert_final/tokenizer.json\n",
            "loading configuration file /content/activebus_bert_final/config.json\n",
            "Model config BertConfig {\n",
            "  \"_name_or_path\": \"activebus/BERT-XD_Review\",\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_ids\": 0,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.15.0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "loading weights file /content/activebus_bert_final/pytorch_model.bin\n",
            "Some weights of the model checkpoint at /content/activebus_bert_final were not used when initializing BertForSequenceClassification: ['cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /content/activebus_bert_final and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: [1][0/306] Elapsed 0m 0s (remain 3m 10s) Loss: 0.7031 \n",
            "Epoch: [1][100/306] Elapsed 0m 33s (remain 1m 7s) Loss: 0.2453 \n",
            "Epoch: [1][200/306] Elapsed 1m 5s (remain 0m 34s) Loss: 0.1920 \n",
            "Epoch: [1][300/306] Elapsed 1m 38s (remain 0m 1s) Loss: 0.1665 \n",
            "Epoch: [1][305/306] Elapsed 1m 39s (remain 0m 0s) Loss: 0.1663 \n",
            "EVAL: [0/77] Elapsed 0m 0s (remain 0m 26s) Loss: 0.1760 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1 - avg_train_loss: 0.1663  avg_val_loss: 0.1222  time: 108s\n",
            "Epoch 1 - avg_train_loss: 0.1663  avg_val_loss: 0.1222  time: 108s\n",
            "Epoch 1 - avg_train_loss: 0.1663  avg_val_loss: 0.1222  time: 108s\n",
            "Epoch 1 - avg_train_loss: 0.1663  avg_val_loss: 0.1222  time: 108s\n",
            "Epoch 1 - avg_train_loss: 0.1663  avg_val_loss: 0.1222  time: 108s\n",
            "Epoch 1 - avg_train_loss: 0.1663  avg_val_loss: 0.1222  time: 108s\n",
            "Epoch 1 - Score: 0.1221830845334194\n",
            "Epoch 1 - Score: 0.1221830845334194\n",
            "Epoch 1 - Score: 0.1221830845334194\n",
            "Epoch 1 - Score: 0.1221830845334194\n",
            "Epoch 1 - Score: 0.1221830845334194\n",
            "Epoch 1 - Score: 0.1221830845334194\n",
            "Epoch 1 - Save Best Score: 0.1222 Model\n",
            "Epoch 1 - Save Best Score: 0.1222 Model\n",
            "Epoch 1 - Save Best Score: 0.1222 Model\n",
            "Epoch 1 - Save Best Score: 0.1222 Model\n",
            "Epoch 1 - Save Best Score: 0.1222 Model\n",
            "Epoch 1 - Save Best Score: 0.1222 Model\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EVAL: [76/77] Elapsed 0m 8s (remain 0m 0s) Loss: 0.1222 \n",
            "Log loss for 0 is 0.1221830845334194\n",
            "Epoch: [2][0/306] Elapsed 0m 0s (remain 3m 3s) Loss: 0.0227 \n",
            "Epoch: [2][100/306] Elapsed 0m 33s (remain 1m 7s) Loss: 0.0820 \n",
            "Epoch: [2][200/306] Elapsed 1m 5s (remain 0m 34s) Loss: 0.0773 \n",
            "Epoch: [2][300/306] Elapsed 1m 38s (remain 0m 1s) Loss: 0.0773 \n",
            "Epoch: [2][305/306] Elapsed 1m 39s (remain 0m 0s) Loss: 0.0765 \n",
            "EVAL: [0/77] Elapsed 0m 0s (remain 0m 28s) Loss: 0.1057 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2 - avg_train_loss: 0.0765  avg_val_loss: 0.1311  time: 108s\n",
            "Epoch 2 - avg_train_loss: 0.0765  avg_val_loss: 0.1311  time: 108s\n",
            "Epoch 2 - avg_train_loss: 0.0765  avg_val_loss: 0.1311  time: 108s\n",
            "Epoch 2 - avg_train_loss: 0.0765  avg_val_loss: 0.1311  time: 108s\n",
            "Epoch 2 - avg_train_loss: 0.0765  avg_val_loss: 0.1311  time: 108s\n",
            "Epoch 2 - avg_train_loss: 0.0765  avg_val_loss: 0.1311  time: 108s\n",
            "Epoch 2 - Score: 0.13105151351220476\n",
            "Epoch 2 - Score: 0.13105151351220476\n",
            "Epoch 2 - Score: 0.13105151351220476\n",
            "Epoch 2 - Score: 0.13105151351220476\n",
            "Epoch 2 - Score: 0.13105151351220476\n",
            "Epoch 2 - Score: 0.13105151351220476\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EVAL: [76/77] Elapsed 0m 8s (remain 0m 0s) Loss: 0.1311 \n",
            "Log loss for 1 is 0.13105151351220476\n",
            "Epoch: [3][0/306] Elapsed 0m 0s (remain 3m 7s) Loss: 0.0110 \n",
            "Epoch: [3][100/306] Elapsed 0m 33s (remain 1m 7s) Loss: 0.0444 \n",
            "Epoch: [3][200/306] Elapsed 1m 5s (remain 0m 34s) Loss: 0.0487 \n",
            "Epoch: [3][300/306] Elapsed 1m 38s (remain 0m 1s) Loss: 0.0455 \n",
            "Epoch: [3][305/306] Elapsed 1m 39s (remain 0m 0s) Loss: 0.0455 \n",
            "EVAL: [0/77] Elapsed 0m 0s (remain 0m 26s) Loss: 0.0510 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3 - avg_train_loss: 0.0455  avg_val_loss: 0.1471  time: 108s\n",
            "Epoch 3 - avg_train_loss: 0.0455  avg_val_loss: 0.1471  time: 108s\n",
            "Epoch 3 - avg_train_loss: 0.0455  avg_val_loss: 0.1471  time: 108s\n",
            "Epoch 3 - avg_train_loss: 0.0455  avg_val_loss: 0.1471  time: 108s\n",
            "Epoch 3 - avg_train_loss: 0.0455  avg_val_loss: 0.1471  time: 108s\n",
            "Epoch 3 - avg_train_loss: 0.0455  avg_val_loss: 0.1471  time: 108s\n",
            "Epoch 3 - Score: 0.1471312848432255\n",
            "Epoch 3 - Score: 0.1471312848432255\n",
            "Epoch 3 - Score: 0.1471312848432255\n",
            "Epoch 3 - Score: 0.1471312848432255\n",
            "Epoch 3 - Score: 0.1471312848432255\n",
            "Epoch 3 - Score: 0.1471312848432255\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EVAL: [76/77] Elapsed 0m 8s (remain 0m 0s) Loss: 0.1471 \n",
            "Log loss for 2 is 0.1471312848432255\n",
            "Epoch: [4][0/306] Elapsed 0m 0s (remain 3m 6s) Loss: 0.0059 \n",
            "Epoch: [4][100/306] Elapsed 0m 33s (remain 1m 7s) Loss: 0.0169 \n",
            "Epoch: [4][200/306] Elapsed 1m 5s (remain 0m 34s) Loss: 0.0166 \n",
            "Epoch: [4][300/306] Elapsed 1m 38s (remain 0m 1s) Loss: 0.0218 \n",
            "Epoch: [4][305/306] Elapsed 1m 39s (remain 0m 0s) Loss: 0.0235 \n",
            "EVAL: [0/77] Elapsed 0m 0s (remain 0m 26s) Loss: 0.2074 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4 - avg_train_loss: 0.0235  avg_val_loss: 0.1790  time: 108s\n",
            "Epoch 4 - avg_train_loss: 0.0235  avg_val_loss: 0.1790  time: 108s\n",
            "Epoch 4 - avg_train_loss: 0.0235  avg_val_loss: 0.1790  time: 108s\n",
            "Epoch 4 - avg_train_loss: 0.0235  avg_val_loss: 0.1790  time: 108s\n",
            "Epoch 4 - avg_train_loss: 0.0235  avg_val_loss: 0.1790  time: 108s\n",
            "Epoch 4 - avg_train_loss: 0.0235  avg_val_loss: 0.1790  time: 108s\n",
            "Epoch 4 - Score: 0.17903134823470196\n",
            "Epoch 4 - Score: 0.17903134823470196\n",
            "Epoch 4 - Score: 0.17903134823470196\n",
            "Epoch 4 - Score: 0.17903134823470196\n",
            "Epoch 4 - Score: 0.17903134823470196\n",
            "Epoch 4 - Score: 0.17903134823470196\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EVAL: [76/77] Elapsed 0m 8s (remain 0m 0s) Loss: 0.1790 \n",
            "Log loss for 3 is 0.17903134823470196\n",
            "Epoch: [5][0/306] Elapsed 0m 0s (remain 3m 2s) Loss: 0.0091 \n",
            "Epoch: [5][100/306] Elapsed 0m 33s (remain 1m 7s) Loss: 0.0224 \n",
            "Epoch: [5][200/306] Elapsed 1m 5s (remain 0m 34s) Loss: 0.0194 \n",
            "Epoch: [5][300/306] Elapsed 1m 38s (remain 0m 1s) Loss: 0.0204 \n",
            "Epoch: [5][305/306] Elapsed 1m 39s (remain 0m 0s) Loss: 0.0203 \n",
            "EVAL: [0/77] Elapsed 0m 0s (remain 0m 28s) Loss: 0.0436 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5 - avg_train_loss: 0.0203  avg_val_loss: 0.1922  time: 108s\n",
            "Epoch 5 - avg_train_loss: 0.0203  avg_val_loss: 0.1922  time: 108s\n",
            "Epoch 5 - avg_train_loss: 0.0203  avg_val_loss: 0.1922  time: 108s\n",
            "Epoch 5 - avg_train_loss: 0.0203  avg_val_loss: 0.1922  time: 108s\n",
            "Epoch 5 - avg_train_loss: 0.0203  avg_val_loss: 0.1922  time: 108s\n",
            "Epoch 5 - avg_train_loss: 0.0203  avg_val_loss: 0.1922  time: 108s\n",
            "Epoch 5 - Score: 0.192224858711624\n",
            "Epoch 5 - Score: 0.192224858711624\n",
            "Epoch 5 - Score: 0.192224858711624\n",
            "Epoch 5 - Score: 0.192224858711624\n",
            "Epoch 5 - Score: 0.192224858711624\n",
            "Epoch 5 - Score: 0.192224858711624\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EVAL: [76/77] Elapsed 0m 8s (remain 0m 0s) Loss: 0.1922 \n",
            "Log loss for 4 is 0.192224858711624\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "========== fold: 0 result ==========\n",
            "========== fold: 0 result ==========\n",
            "========== fold: 0 result ==========\n",
            "========== fold: 0 result ==========\n",
            "========== fold: 0 result ==========\n",
            "========== fold: 0 result ==========\n",
            "Score: 0.12218\n",
            "Score: 0.12218\n",
            "Score: 0.12218\n",
            "Score: 0.12218\n",
            "Score: 0.12218\n",
            "Score: 0.12218\n",
            "========== fold: 1 training ==========\n",
            "========== fold: 1 training ==========\n",
            "========== fold: 1 training ==========\n",
            "========== fold: 1 training ==========\n",
            "========== fold: 1 training ==========\n",
            "========== fold: 1 training ==========\n",
            "Didn't find file /content/activebus_bert_final/added_tokens.json. We won't load it.\n",
            "loading file /content/activebus_bert_final/vocab.txt\n",
            "loading file None\n",
            "loading file /content/activebus_bert_final/special_tokens_map.json\n",
            "loading file /content/activebus_bert_final/tokenizer_config.json\n",
            "loading file /content/activebus_bert_final/tokenizer.json\n",
            "Didn't find file /content/activebus_bert_final/added_tokens.json. We won't load it.\n",
            "loading file /content/activebus_bert_final/vocab.txt\n",
            "loading file None\n",
            "loading file /content/activebus_bert_final/special_tokens_map.json\n",
            "loading file /content/activebus_bert_final/tokenizer_config.json\n",
            "loading file /content/activebus_bert_final/tokenizer.json\n",
            "loading configuration file /content/activebus_bert_final/config.json\n",
            "Model config BertConfig {\n",
            "  \"_name_or_path\": \"activebus/BERT-XD_Review\",\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_ids\": 0,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.15.0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "loading weights file /content/activebus_bert_final/pytorch_model.bin\n",
            "Some weights of the model checkpoint at /content/activebus_bert_final were not used when initializing BertForSequenceClassification: ['cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /content/activebus_bert_final and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: [1][0/306] Elapsed 0m 0s (remain 3m 28s) Loss: 0.7725 \n",
            "Epoch: [1][100/306] Elapsed 0m 33s (remain 1m 7s) Loss: 0.2654 \n",
            "Epoch: [1][200/306] Elapsed 1m 5s (remain 0m 34s) Loss: 0.1952 \n",
            "Epoch: [1][300/306] Elapsed 1m 38s (remain 0m 1s) Loss: 0.1678 \n",
            "Epoch: [1][305/306] Elapsed 1m 39s (remain 0m 0s) Loss: 0.1674 \n",
            "EVAL: [0/77] Elapsed 0m 0s (remain 0m 26s) Loss: 0.0125 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1 - avg_train_loss: 0.1674  avg_val_loss: 0.1430  time: 108s\n",
            "Epoch 1 - avg_train_loss: 0.1674  avg_val_loss: 0.1430  time: 108s\n",
            "Epoch 1 - avg_train_loss: 0.1674  avg_val_loss: 0.1430  time: 108s\n",
            "Epoch 1 - avg_train_loss: 0.1674  avg_val_loss: 0.1430  time: 108s\n",
            "Epoch 1 - avg_train_loss: 0.1674  avg_val_loss: 0.1430  time: 108s\n",
            "Epoch 1 - avg_train_loss: 0.1674  avg_val_loss: 0.1430  time: 108s\n",
            "Epoch 1 - Score: 0.14302176350106036\n",
            "Epoch 1 - Score: 0.14302176350106036\n",
            "Epoch 1 - Score: 0.14302176350106036\n",
            "Epoch 1 - Score: 0.14302176350106036\n",
            "Epoch 1 - Score: 0.14302176350106036\n",
            "Epoch 1 - Score: 0.14302176350106036\n",
            "Epoch 1 - Save Best Score: 0.1430 Model\n",
            "Epoch 1 - Save Best Score: 0.1430 Model\n",
            "Epoch 1 - Save Best Score: 0.1430 Model\n",
            "Epoch 1 - Save Best Score: 0.1430 Model\n",
            "Epoch 1 - Save Best Score: 0.1430 Model\n",
            "Epoch 1 - Save Best Score: 0.1430 Model\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EVAL: [76/77] Elapsed 0m 8s (remain 0m 0s) Loss: 0.1430 \n",
            "Log loss for 0 is 0.14302176350106036\n",
            "Epoch: [2][0/306] Elapsed 0m 0s (remain 3m 4s) Loss: 0.0182 \n",
            "Epoch: [2][100/306] Elapsed 0m 33s (remain 1m 7s) Loss: 0.0906 \n",
            "Epoch: [2][200/306] Elapsed 1m 5s (remain 0m 34s) Loss: 0.0785 \n",
            "Epoch: [2][300/306] Elapsed 1m 38s (remain 0m 1s) Loss: 0.0761 \n",
            "Epoch: [2][305/306] Elapsed 1m 39s (remain 0m 0s) Loss: 0.0760 \n",
            "EVAL: [0/77] Elapsed 0m 0s (remain 0m 28s) Loss: 0.0146 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2 - avg_train_loss: 0.0760  avg_val_loss: 0.1172  time: 108s\n",
            "Epoch 2 - avg_train_loss: 0.0760  avg_val_loss: 0.1172  time: 108s\n",
            "Epoch 2 - avg_train_loss: 0.0760  avg_val_loss: 0.1172  time: 108s\n",
            "Epoch 2 - avg_train_loss: 0.0760  avg_val_loss: 0.1172  time: 108s\n",
            "Epoch 2 - avg_train_loss: 0.0760  avg_val_loss: 0.1172  time: 108s\n",
            "Epoch 2 - avg_train_loss: 0.0760  avg_val_loss: 0.1172  time: 108s\n",
            "Epoch 2 - Score: 0.11721657571440126\n",
            "Epoch 2 - Score: 0.11721657571440126\n",
            "Epoch 2 - Score: 0.11721657571440126\n",
            "Epoch 2 - Score: 0.11721657571440126\n",
            "Epoch 2 - Score: 0.11721657571440126\n",
            "Epoch 2 - Score: 0.11721657571440126\n",
            "Epoch 2 - Save Best Score: 0.1172 Model\n",
            "Epoch 2 - Save Best Score: 0.1172 Model\n",
            "Epoch 2 - Save Best Score: 0.1172 Model\n",
            "Epoch 2 - Save Best Score: 0.1172 Model\n",
            "Epoch 2 - Save Best Score: 0.1172 Model\n",
            "Epoch 2 - Save Best Score: 0.1172 Model\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EVAL: [76/77] Elapsed 0m 8s (remain 0m 0s) Loss: 0.1172 \n",
            "Log loss for 1 is 0.11721657571440126\n",
            "Epoch: [3][0/306] Elapsed 0m 0s (remain 3m 1s) Loss: 0.0077 \n",
            "Epoch: [3][100/306] Elapsed 0m 33s (remain 1m 7s) Loss: 0.0235 \n",
            "Epoch: [3][200/306] Elapsed 1m 5s (remain 0m 34s) Loss: 0.0326 \n",
            "Epoch: [3][300/306] Elapsed 1m 38s (remain 0m 1s) Loss: 0.0306 \n",
            "Epoch: [3][305/306] Elapsed 1m 39s (remain 0m 0s) Loss: 0.0303 \n",
            "EVAL: [0/77] Elapsed 0m 0s (remain 0m 26s) Loss: 0.0117 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3 - avg_train_loss: 0.0303  avg_val_loss: 0.1347  time: 108s\n",
            "Epoch 3 - avg_train_loss: 0.0303  avg_val_loss: 0.1347  time: 108s\n",
            "Epoch 3 - avg_train_loss: 0.0303  avg_val_loss: 0.1347  time: 108s\n",
            "Epoch 3 - avg_train_loss: 0.0303  avg_val_loss: 0.1347  time: 108s\n",
            "Epoch 3 - avg_train_loss: 0.0303  avg_val_loss: 0.1347  time: 108s\n",
            "Epoch 3 - avg_train_loss: 0.0303  avg_val_loss: 0.1347  time: 108s\n",
            "Epoch 3 - Score: 0.13470947339145214\n",
            "Epoch 3 - Score: 0.13470947339145214\n",
            "Epoch 3 - Score: 0.13470947339145214\n",
            "Epoch 3 - Score: 0.13470947339145214\n",
            "Epoch 3 - Score: 0.13470947339145214\n",
            "Epoch 3 - Score: 0.13470947339145214\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EVAL: [76/77] Elapsed 0m 8s (remain 0m 0s) Loss: 0.1347 \n",
            "Log loss for 2 is 0.13470947339145214\n",
            "Epoch: [4][0/306] Elapsed 0m 0s (remain 3m 6s) Loss: 0.0124 \n",
            "Epoch: [4][100/306] Elapsed 0m 33s (remain 1m 7s) Loss: 0.0146 \n",
            "Epoch: [4][200/306] Elapsed 1m 5s (remain 0m 34s) Loss: 0.0183 \n",
            "Epoch: [4][300/306] Elapsed 1m 38s (remain 0m 1s) Loss: 0.0227 \n",
            "Epoch: [4][305/306] Elapsed 1m 39s (remain 0m 0s) Loss: 0.0226 \n",
            "EVAL: [0/77] Elapsed 0m 0s (remain 0m 25s) Loss: 0.1255 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4 - avg_train_loss: 0.0226  avg_val_loss: 0.2141  time: 108s\n",
            "Epoch 4 - avg_train_loss: 0.0226  avg_val_loss: 0.2141  time: 108s\n",
            "Epoch 4 - avg_train_loss: 0.0226  avg_val_loss: 0.2141  time: 108s\n",
            "Epoch 4 - avg_train_loss: 0.0226  avg_val_loss: 0.2141  time: 108s\n",
            "Epoch 4 - avg_train_loss: 0.0226  avg_val_loss: 0.2141  time: 108s\n",
            "Epoch 4 - avg_train_loss: 0.0226  avg_val_loss: 0.2141  time: 108s\n",
            "Epoch 4 - Score: 0.21414349561231932\n",
            "Epoch 4 - Score: 0.21414349561231932\n",
            "Epoch 4 - Score: 0.21414349561231932\n",
            "Epoch 4 - Score: 0.21414349561231932\n",
            "Epoch 4 - Score: 0.21414349561231932\n",
            "Epoch 4 - Score: 0.21414349561231932\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EVAL: [76/77] Elapsed 0m 8s (remain 0m 0s) Loss: 0.2141 \n",
            "Log loss for 3 is 0.21414349561231932\n",
            "Epoch: [5][0/306] Elapsed 0m 0s (remain 3m 5s) Loss: 0.0076 \n",
            "Epoch: [5][100/306] Elapsed 0m 33s (remain 1m 7s) Loss: 0.0351 \n",
            "Epoch: [5][200/306] Elapsed 1m 5s (remain 0m 34s) Loss: 0.0316 \n",
            "Epoch: [5][300/306] Elapsed 1m 38s (remain 0m 1s) Loss: 0.0271 \n",
            "Epoch: [5][305/306] Elapsed 1m 39s (remain 0m 0s) Loss: 0.0266 \n",
            "EVAL: [0/77] Elapsed 0m 0s (remain 0m 27s) Loss: 0.0019 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5 - avg_train_loss: 0.0266  avg_val_loss: 0.1955  time: 108s\n",
            "Epoch 5 - avg_train_loss: 0.0266  avg_val_loss: 0.1955  time: 108s\n",
            "Epoch 5 - avg_train_loss: 0.0266  avg_val_loss: 0.1955  time: 108s\n",
            "Epoch 5 - avg_train_loss: 0.0266  avg_val_loss: 0.1955  time: 108s\n",
            "Epoch 5 - avg_train_loss: 0.0266  avg_val_loss: 0.1955  time: 108s\n",
            "Epoch 5 - avg_train_loss: 0.0266  avg_val_loss: 0.1955  time: 108s\n",
            "Epoch 5 - Score: 0.19554908538615112\n",
            "Epoch 5 - Score: 0.19554908538615112\n",
            "Epoch 5 - Score: 0.19554908538615112\n",
            "Epoch 5 - Score: 0.19554908538615112\n",
            "Epoch 5 - Score: 0.19554908538615112\n",
            "Epoch 5 - Score: 0.19554908538615112\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EVAL: [76/77] Elapsed 0m 8s (remain 0m 0s) Loss: 0.1955 \n",
            "Log loss for 4 is 0.19554908538615112\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "========== fold: 1 result ==========\n",
            "========== fold: 1 result ==========\n",
            "========== fold: 1 result ==========\n",
            "========== fold: 1 result ==========\n",
            "========== fold: 1 result ==========\n",
            "========== fold: 1 result ==========\n",
            "Score: 0.11722\n",
            "Score: 0.11722\n",
            "Score: 0.11722\n",
            "Score: 0.11722\n",
            "Score: 0.11722\n",
            "Score: 0.11722\n",
            "========== fold: 2 training ==========\n",
            "========== fold: 2 training ==========\n",
            "========== fold: 2 training ==========\n",
            "========== fold: 2 training ==========\n",
            "========== fold: 2 training ==========\n",
            "========== fold: 2 training ==========\n",
            "Didn't find file /content/activebus_bert_final/added_tokens.json. We won't load it.\n",
            "loading file /content/activebus_bert_final/vocab.txt\n",
            "loading file None\n",
            "loading file /content/activebus_bert_final/special_tokens_map.json\n",
            "loading file /content/activebus_bert_final/tokenizer_config.json\n",
            "loading file /content/activebus_bert_final/tokenizer.json\n",
            "Didn't find file /content/activebus_bert_final/added_tokens.json. We won't load it.\n",
            "loading file /content/activebus_bert_final/vocab.txt\n",
            "loading file None\n",
            "loading file /content/activebus_bert_final/special_tokens_map.json\n",
            "loading file /content/activebus_bert_final/tokenizer_config.json\n",
            "loading file /content/activebus_bert_final/tokenizer.json\n",
            "loading configuration file /content/activebus_bert_final/config.json\n",
            "Model config BertConfig {\n",
            "  \"_name_or_path\": \"activebus/BERT-XD_Review\",\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_ids\": 0,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.15.0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "loading weights file /content/activebus_bert_final/pytorch_model.bin\n",
            "Some weights of the model checkpoint at /content/activebus_bert_final were not used when initializing BertForSequenceClassification: ['cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /content/activebus_bert_final and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: [1][0/306] Elapsed 0m 0s (remain 3m 17s) Loss: 0.7038 \n",
            "Epoch: [1][100/306] Elapsed 0m 33s (remain 1m 7s) Loss: 0.2626 \n",
            "Epoch: [1][200/306] Elapsed 1m 5s (remain 0m 34s) Loss: 0.2027 \n",
            "Epoch: [1][300/306] Elapsed 1m 38s (remain 0m 1s) Loss: 0.1824 \n",
            "Epoch: [1][305/306] Elapsed 1m 39s (remain 0m 0s) Loss: 0.1818 \n",
            "EVAL: [0/77] Elapsed 0m 0s (remain 0m 28s) Loss: 0.1872 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1 - avg_train_loss: 0.1818  avg_val_loss: 0.0883  time: 108s\n",
            "Epoch 1 - avg_train_loss: 0.1818  avg_val_loss: 0.0883  time: 108s\n",
            "Epoch 1 - avg_train_loss: 0.1818  avg_val_loss: 0.0883  time: 108s\n",
            "Epoch 1 - avg_train_loss: 0.1818  avg_val_loss: 0.0883  time: 108s\n",
            "Epoch 1 - avg_train_loss: 0.1818  avg_val_loss: 0.0883  time: 108s\n",
            "Epoch 1 - avg_train_loss: 0.1818  avg_val_loss: 0.0883  time: 108s\n",
            "Epoch 1 - Score: 0.08829035968637396\n",
            "Epoch 1 - Score: 0.08829035968637396\n",
            "Epoch 1 - Score: 0.08829035968637396\n",
            "Epoch 1 - Score: 0.08829035968637396\n",
            "Epoch 1 - Score: 0.08829035968637396\n",
            "Epoch 1 - Score: 0.08829035968637396\n",
            "Epoch 1 - Save Best Score: 0.0883 Model\n",
            "Epoch 1 - Save Best Score: 0.0883 Model\n",
            "Epoch 1 - Save Best Score: 0.0883 Model\n",
            "Epoch 1 - Save Best Score: 0.0883 Model\n",
            "Epoch 1 - Save Best Score: 0.0883 Model\n",
            "Epoch 1 - Save Best Score: 0.0883 Model\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EVAL: [76/77] Elapsed 0m 8s (remain 0m 0s) Loss: 0.0883 \n",
            "Log loss for 0 is 0.08829035968637396\n",
            "Epoch: [2][0/306] Elapsed 0m 0s (remain 2m 59s) Loss: 0.0925 \n",
            "Epoch: [2][100/306] Elapsed 0m 33s (remain 1m 7s) Loss: 0.0841 \n",
            "Epoch: [2][200/306] Elapsed 1m 5s (remain 0m 34s) Loss: 0.0737 \n",
            "Epoch: [2][300/306] Elapsed 1m 38s (remain 0m 1s) Loss: 0.0739 \n",
            "Epoch: [2][305/306] Elapsed 1m 39s (remain 0m 0s) Loss: 0.0747 \n",
            "EVAL: [0/77] Elapsed 0m 0s (remain 0m 26s) Loss: 0.2105 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2 - avg_train_loss: 0.0747  avg_val_loss: 0.0957  time: 108s\n",
            "Epoch 2 - avg_train_loss: 0.0747  avg_val_loss: 0.0957  time: 108s\n",
            "Epoch 2 - avg_train_loss: 0.0747  avg_val_loss: 0.0957  time: 108s\n",
            "Epoch 2 - avg_train_loss: 0.0747  avg_val_loss: 0.0957  time: 108s\n",
            "Epoch 2 - avg_train_loss: 0.0747  avg_val_loss: 0.0957  time: 108s\n",
            "Epoch 2 - avg_train_loss: 0.0747  avg_val_loss: 0.0957  time: 108s\n",
            "Epoch 2 - Score: 0.09571113803482006\n",
            "Epoch 2 - Score: 0.09571113803482006\n",
            "Epoch 2 - Score: 0.09571113803482006\n",
            "Epoch 2 - Score: 0.09571113803482006\n",
            "Epoch 2 - Score: 0.09571113803482006\n",
            "Epoch 2 - Score: 0.09571113803482006\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EVAL: [76/77] Elapsed 0m 8s (remain 0m 0s) Loss: 0.0957 \n",
            "Log loss for 1 is 0.09571113803482006\n",
            "Epoch: [3][0/306] Elapsed 0m 0s (remain 3m 15s) Loss: 0.0071 \n",
            "Epoch: [3][100/306] Elapsed 0m 33s (remain 1m 7s) Loss: 0.0328 \n",
            "Epoch: [3][200/306] Elapsed 1m 5s (remain 0m 34s) Loss: 0.0387 \n",
            "Epoch: [3][300/306] Elapsed 1m 38s (remain 0m 1s) Loss: 0.0403 \n",
            "Epoch: [3][305/306] Elapsed 1m 39s (remain 0m 0s) Loss: 0.0404 \n",
            "EVAL: [0/77] Elapsed 0m 0s (remain 0m 26s) Loss: 0.1206 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3 - avg_train_loss: 0.0404  avg_val_loss: 0.1274  time: 108s\n",
            "Epoch 3 - avg_train_loss: 0.0404  avg_val_loss: 0.1274  time: 108s\n",
            "Epoch 3 - avg_train_loss: 0.0404  avg_val_loss: 0.1274  time: 108s\n",
            "Epoch 3 - avg_train_loss: 0.0404  avg_val_loss: 0.1274  time: 108s\n",
            "Epoch 3 - avg_train_loss: 0.0404  avg_val_loss: 0.1274  time: 108s\n",
            "Epoch 3 - avg_train_loss: 0.0404  avg_val_loss: 0.1274  time: 108s\n",
            "Epoch 3 - Score: 0.12740917297839374\n",
            "Epoch 3 - Score: 0.12740917297839374\n",
            "Epoch 3 - Score: 0.12740917297839374\n",
            "Epoch 3 - Score: 0.12740917297839374\n",
            "Epoch 3 - Score: 0.12740917297839374\n",
            "Epoch 3 - Score: 0.12740917297839374\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EVAL: [76/77] Elapsed 0m 8s (remain 0m 0s) Loss: 0.1274 \n",
            "Log loss for 2 is 0.12740917297839374\n",
            "Epoch: [4][0/306] Elapsed 0m 0s (remain 2m 56s) Loss: 0.0229 \n",
            "Epoch: [4][100/306] Elapsed 0m 33s (remain 1m 7s) Loss: 0.0309 \n",
            "Epoch: [4][200/306] Elapsed 1m 5s (remain 0m 34s) Loss: 0.0285 \n",
            "Epoch: [4][300/306] Elapsed 1m 38s (remain 0m 1s) Loss: 0.0281 \n",
            "Epoch: [4][305/306] Elapsed 1m 39s (remain 0m 0s) Loss: 0.0277 \n",
            "EVAL: [0/77] Elapsed 0m 0s (remain 0m 25s) Loss: 0.6392 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4 - avg_train_loss: 0.0277  avg_val_loss: 0.1545  time: 108s\n",
            "Epoch 4 - avg_train_loss: 0.0277  avg_val_loss: 0.1545  time: 108s\n",
            "Epoch 4 - avg_train_loss: 0.0277  avg_val_loss: 0.1545  time: 108s\n",
            "Epoch 4 - avg_train_loss: 0.0277  avg_val_loss: 0.1545  time: 108s\n",
            "Epoch 4 - avg_train_loss: 0.0277  avg_val_loss: 0.1545  time: 108s\n",
            "Epoch 4 - avg_train_loss: 0.0277  avg_val_loss: 0.1545  time: 108s\n",
            "Epoch 4 - Score: 0.15449143334017867\n",
            "Epoch 4 - Score: 0.15449143334017867\n",
            "Epoch 4 - Score: 0.15449143334017867\n",
            "Epoch 4 - Score: 0.15449143334017867\n",
            "Epoch 4 - Score: 0.15449143334017867\n",
            "Epoch 4 - Score: 0.15449143334017867\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EVAL: [76/77] Elapsed 0m 8s (remain 0m 0s) Loss: 0.1545 \n",
            "Log loss for 3 is 0.15449143334017867\n",
            "Epoch: [5][0/306] Elapsed 0m 0s (remain 3m 3s) Loss: 0.0014 \n",
            "Epoch: [5][100/306] Elapsed 0m 33s (remain 1m 7s) Loss: 0.0176 \n",
            "Epoch: [5][200/306] Elapsed 1m 5s (remain 0m 34s) Loss: 0.0177 \n",
            "Epoch: [5][300/306] Elapsed 1m 38s (remain 0m 1s) Loss: 0.0160 \n",
            "Epoch: [5][305/306] Elapsed 1m 39s (remain 0m 0s) Loss: 0.0173 \n",
            "EVAL: [0/77] Elapsed 0m 0s (remain 0m 27s) Loss: 0.4336 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5 - avg_train_loss: 0.0173  avg_val_loss: 0.1682  time: 108s\n",
            "Epoch 5 - avg_train_loss: 0.0173  avg_val_loss: 0.1682  time: 108s\n",
            "Epoch 5 - avg_train_loss: 0.0173  avg_val_loss: 0.1682  time: 108s\n",
            "Epoch 5 - avg_train_loss: 0.0173  avg_val_loss: 0.1682  time: 108s\n",
            "Epoch 5 - avg_train_loss: 0.0173  avg_val_loss: 0.1682  time: 108s\n",
            "Epoch 5 - avg_train_loss: 0.0173  avg_val_loss: 0.1682  time: 108s\n",
            "Epoch 5 - Score: 0.16819779448218403\n",
            "Epoch 5 - Score: 0.16819779448218403\n",
            "Epoch 5 - Score: 0.16819779448218403\n",
            "Epoch 5 - Score: 0.16819779448218403\n",
            "Epoch 5 - Score: 0.16819779448218403\n",
            "Epoch 5 - Score: 0.16819779448218403\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EVAL: [76/77] Elapsed 0m 8s (remain 0m 0s) Loss: 0.1682 \n",
            "Log loss for 4 is 0.16819779448218403\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "========== fold: 2 result ==========\n",
            "========== fold: 2 result ==========\n",
            "========== fold: 2 result ==========\n",
            "========== fold: 2 result ==========\n",
            "========== fold: 2 result ==========\n",
            "========== fold: 2 result ==========\n",
            "Score: 0.08829\n",
            "Score: 0.08829\n",
            "Score: 0.08829\n",
            "Score: 0.08829\n",
            "Score: 0.08829\n",
            "Score: 0.08829\n",
            "========== fold: 3 training ==========\n",
            "========== fold: 3 training ==========\n",
            "========== fold: 3 training ==========\n",
            "========== fold: 3 training ==========\n",
            "========== fold: 3 training ==========\n",
            "========== fold: 3 training ==========\n",
            "Didn't find file /content/activebus_bert_final/added_tokens.json. We won't load it.\n",
            "loading file /content/activebus_bert_final/vocab.txt\n",
            "loading file None\n",
            "loading file /content/activebus_bert_final/special_tokens_map.json\n",
            "loading file /content/activebus_bert_final/tokenizer_config.json\n",
            "loading file /content/activebus_bert_final/tokenizer.json\n",
            "Didn't find file /content/activebus_bert_final/added_tokens.json. We won't load it.\n",
            "loading file /content/activebus_bert_final/vocab.txt\n",
            "loading file None\n",
            "loading file /content/activebus_bert_final/special_tokens_map.json\n",
            "loading file /content/activebus_bert_final/tokenizer_config.json\n",
            "loading file /content/activebus_bert_final/tokenizer.json\n",
            "loading configuration file /content/activebus_bert_final/config.json\n",
            "Model config BertConfig {\n",
            "  \"_name_or_path\": \"activebus/BERT-XD_Review\",\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_ids\": 0,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.15.0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "loading weights file /content/activebus_bert_final/pytorch_model.bin\n",
            "Some weights of the model checkpoint at /content/activebus_bert_final were not used when initializing BertForSequenceClassification: ['cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /content/activebus_bert_final and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: [1][0/306] Elapsed 0m 0s (remain 3m 7s) Loss: 0.6899 \n",
            "Epoch: [1][100/306] Elapsed 0m 33s (remain 1m 7s) Loss: 0.2424 \n",
            "Epoch: [1][200/306] Elapsed 1m 5s (remain 0m 34s) Loss: 0.1840 \n",
            "Epoch: [1][300/306] Elapsed 1m 38s (remain 0m 1s) Loss: 0.1620 \n",
            "Epoch: [1][305/306] Elapsed 1m 39s (remain 0m 0s) Loss: 0.1615 \n",
            "EVAL: [0/77] Elapsed 0m 0s (remain 0m 26s) Loss: 0.0122 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1 - avg_train_loss: 0.1615  avg_val_loss: 0.1197  time: 108s\n",
            "Epoch 1 - avg_train_loss: 0.1615  avg_val_loss: 0.1197  time: 108s\n",
            "Epoch 1 - avg_train_loss: 0.1615  avg_val_loss: 0.1197  time: 108s\n",
            "Epoch 1 - avg_train_loss: 0.1615  avg_val_loss: 0.1197  time: 108s\n",
            "Epoch 1 - avg_train_loss: 0.1615  avg_val_loss: 0.1197  time: 108s\n",
            "Epoch 1 - avg_train_loss: 0.1615  avg_val_loss: 0.1197  time: 108s\n",
            "Epoch 1 - Score: 0.11965136847059533\n",
            "Epoch 1 - Score: 0.11965136847059533\n",
            "Epoch 1 - Score: 0.11965136847059533\n",
            "Epoch 1 - Score: 0.11965136847059533\n",
            "Epoch 1 - Score: 0.11965136847059533\n",
            "Epoch 1 - Score: 0.11965136847059533\n",
            "Epoch 1 - Save Best Score: 0.1197 Model\n",
            "Epoch 1 - Save Best Score: 0.1197 Model\n",
            "Epoch 1 - Save Best Score: 0.1197 Model\n",
            "Epoch 1 - Save Best Score: 0.1197 Model\n",
            "Epoch 1 - Save Best Score: 0.1197 Model\n",
            "Epoch 1 - Save Best Score: 0.1197 Model\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EVAL: [76/77] Elapsed 0m 8s (remain 0m 0s) Loss: 0.1197 \n",
            "Log loss for 0 is 0.11965136847059533\n",
            "Epoch: [2][0/306] Elapsed 0m 0s (remain 2m 58s) Loss: 0.0515 \n",
            "Epoch: [2][100/306] Elapsed 0m 33s (remain 1m 7s) Loss: 0.0831 \n",
            "Epoch: [2][200/306] Elapsed 1m 5s (remain 0m 34s) Loss: 0.0765 \n",
            "Epoch: [2][300/306] Elapsed 1m 38s (remain 0m 1s) Loss: 0.0783 \n",
            "Epoch: [2][305/306] Elapsed 1m 39s (remain 0m 0s) Loss: 0.0782 \n",
            "EVAL: [0/77] Elapsed 0m 0s (remain 0m 25s) Loss: 0.0117 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2 - avg_train_loss: 0.0782  avg_val_loss: 0.1574  time: 108s\n",
            "Epoch 2 - avg_train_loss: 0.0782  avg_val_loss: 0.1574  time: 108s\n",
            "Epoch 2 - avg_train_loss: 0.0782  avg_val_loss: 0.1574  time: 108s\n",
            "Epoch 2 - avg_train_loss: 0.0782  avg_val_loss: 0.1574  time: 108s\n",
            "Epoch 2 - avg_train_loss: 0.0782  avg_val_loss: 0.1574  time: 108s\n",
            "Epoch 2 - avg_train_loss: 0.0782  avg_val_loss: 0.1574  time: 108s\n",
            "Epoch 2 - Score: 0.15744377300076828\n",
            "Epoch 2 - Score: 0.15744377300076828\n",
            "Epoch 2 - Score: 0.15744377300076828\n",
            "Epoch 2 - Score: 0.15744377300076828\n",
            "Epoch 2 - Score: 0.15744377300076828\n",
            "Epoch 2 - Score: 0.15744377300076828\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EVAL: [76/77] Elapsed 0m 8s (remain 0m 0s) Loss: 0.1574 \n",
            "Log loss for 1 is 0.15744377300076828\n",
            "Epoch: [3][0/306] Elapsed 0m 0s (remain 2m 55s) Loss: 0.0487 \n",
            "Epoch: [3][100/306] Elapsed 0m 33s (remain 1m 7s) Loss: 0.0404 \n",
            "Epoch: [3][200/306] Elapsed 1m 5s (remain 0m 34s) Loss: 0.0416 \n",
            "Epoch: [3][300/306] Elapsed 1m 38s (remain 0m 1s) Loss: 0.0390 \n",
            "Epoch: [3][305/306] Elapsed 1m 39s (remain 0m 0s) Loss: 0.0394 \n",
            "EVAL: [0/77] Elapsed 0m 0s (remain 0m 27s) Loss: 0.0021 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3 - avg_train_loss: 0.0394  avg_val_loss: 0.1959  time: 108s\n",
            "Epoch 3 - avg_train_loss: 0.0394  avg_val_loss: 0.1959  time: 108s\n",
            "Epoch 3 - avg_train_loss: 0.0394  avg_val_loss: 0.1959  time: 108s\n",
            "Epoch 3 - avg_train_loss: 0.0394  avg_val_loss: 0.1959  time: 108s\n",
            "Epoch 3 - avg_train_loss: 0.0394  avg_val_loss: 0.1959  time: 108s\n",
            "Epoch 3 - avg_train_loss: 0.0394  avg_val_loss: 0.1959  time: 108s\n",
            "Epoch 3 - Score: 0.1958806047403157\n",
            "Epoch 3 - Score: 0.1958806047403157\n",
            "Epoch 3 - Score: 0.1958806047403157\n",
            "Epoch 3 - Score: 0.1958806047403157\n",
            "Epoch 3 - Score: 0.1958806047403157\n",
            "Epoch 3 - Score: 0.1958806047403157\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EVAL: [76/77] Elapsed 0m 8s (remain 0m 0s) Loss: 0.1959 \n",
            "Log loss for 2 is 0.1958806047403157\n",
            "Epoch: [4][0/306] Elapsed 0m 0s (remain 3m 8s) Loss: 0.0861 \n",
            "Epoch: [4][100/306] Elapsed 0m 33s (remain 1m 7s) Loss: 0.0149 \n",
            "Epoch: [4][200/306] Elapsed 1m 5s (remain 0m 34s) Loss: 0.0228 \n",
            "Epoch: [4][300/306] Elapsed 1m 38s (remain 0m 1s) Loss: 0.0266 \n",
            "Epoch: [4][305/306] Elapsed 1m 39s (remain 0m 0s) Loss: 0.0264 \n",
            "EVAL: [0/77] Elapsed 0m 0s (remain 0m 26s) Loss: 0.0506 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4 - avg_train_loss: 0.0264  avg_val_loss: 0.1511  time: 108s\n",
            "Epoch 4 - avg_train_loss: 0.0264  avg_val_loss: 0.1511  time: 108s\n",
            "Epoch 4 - avg_train_loss: 0.0264  avg_val_loss: 0.1511  time: 108s\n",
            "Epoch 4 - avg_train_loss: 0.0264  avg_val_loss: 0.1511  time: 108s\n",
            "Epoch 4 - avg_train_loss: 0.0264  avg_val_loss: 0.1511  time: 108s\n",
            "Epoch 4 - avg_train_loss: 0.0264  avg_val_loss: 0.1511  time: 108s\n",
            "Epoch 4 - Score: 0.1511348767458733\n",
            "Epoch 4 - Score: 0.1511348767458733\n",
            "Epoch 4 - Score: 0.1511348767458733\n",
            "Epoch 4 - Score: 0.1511348767458733\n",
            "Epoch 4 - Score: 0.1511348767458733\n",
            "Epoch 4 - Score: 0.1511348767458733\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EVAL: [76/77] Elapsed 0m 8s (remain 0m 0s) Loss: 0.1511 \n",
            "Log loss for 3 is 0.1511348767458733\n",
            "Epoch: [5][0/306] Elapsed 0m 0s (remain 3m 1s) Loss: 0.0121 \n",
            "Epoch: [5][100/306] Elapsed 0m 33s (remain 1m 7s) Loss: 0.0099 \n",
            "Epoch: [5][200/306] Elapsed 1m 5s (remain 0m 34s) Loss: 0.0134 \n",
            "Epoch: [5][300/306] Elapsed 1m 38s (remain 0m 1s) Loss: 0.0184 \n",
            "Epoch: [5][305/306] Elapsed 1m 39s (remain 0m 0s) Loss: 0.0183 \n",
            "EVAL: [0/77] Elapsed 0m 0s (remain 0m 26s) Loss: 0.0038 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5 - avg_train_loss: 0.0183  avg_val_loss: 0.1748  time: 108s\n",
            "Epoch 5 - avg_train_loss: 0.0183  avg_val_loss: 0.1748  time: 108s\n",
            "Epoch 5 - avg_train_loss: 0.0183  avg_val_loss: 0.1748  time: 108s\n",
            "Epoch 5 - avg_train_loss: 0.0183  avg_val_loss: 0.1748  time: 108s\n",
            "Epoch 5 - avg_train_loss: 0.0183  avg_val_loss: 0.1748  time: 108s\n",
            "Epoch 5 - avg_train_loss: 0.0183  avg_val_loss: 0.1748  time: 108s\n",
            "Epoch 5 - Score: 0.17478113034486767\n",
            "Epoch 5 - Score: 0.17478113034486767\n",
            "Epoch 5 - Score: 0.17478113034486767\n",
            "Epoch 5 - Score: 0.17478113034486767\n",
            "Epoch 5 - Score: 0.17478113034486767\n",
            "Epoch 5 - Score: 0.17478113034486767\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EVAL: [76/77] Elapsed 0m 8s (remain 0m 0s) Loss: 0.1748 \n",
            "Log loss for 4 is 0.17478113034486767\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "========== fold: 3 result ==========\n",
            "========== fold: 3 result ==========\n",
            "========== fold: 3 result ==========\n",
            "========== fold: 3 result ==========\n",
            "========== fold: 3 result ==========\n",
            "========== fold: 3 result ==========\n",
            "Score: 0.11965\n",
            "Score: 0.11965\n",
            "Score: 0.11965\n",
            "Score: 0.11965\n",
            "Score: 0.11965\n",
            "Score: 0.11965\n",
            "========== fold: 4 training ==========\n",
            "========== fold: 4 training ==========\n",
            "========== fold: 4 training ==========\n",
            "========== fold: 4 training ==========\n",
            "========== fold: 4 training ==========\n",
            "========== fold: 4 training ==========\n",
            "Didn't find file /content/activebus_bert_final/added_tokens.json. We won't load it.\n",
            "loading file /content/activebus_bert_final/vocab.txt\n",
            "loading file None\n",
            "loading file /content/activebus_bert_final/special_tokens_map.json\n",
            "loading file /content/activebus_bert_final/tokenizer_config.json\n",
            "loading file /content/activebus_bert_final/tokenizer.json\n",
            "Didn't find file /content/activebus_bert_final/added_tokens.json. We won't load it.\n",
            "loading file /content/activebus_bert_final/vocab.txt\n",
            "loading file None\n",
            "loading file /content/activebus_bert_final/special_tokens_map.json\n",
            "loading file /content/activebus_bert_final/tokenizer_config.json\n",
            "loading file /content/activebus_bert_final/tokenizer.json\n",
            "loading configuration file /content/activebus_bert_final/config.json\n",
            "Model config BertConfig {\n",
            "  \"_name_or_path\": \"activebus/BERT-XD_Review\",\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_ids\": 0,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.15.0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "loading weights file /content/activebus_bert_final/pytorch_model.bin\n",
            "Some weights of the model checkpoint at /content/activebus_bert_final were not used when initializing BertForSequenceClassification: ['cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /content/activebus_bert_final and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: [1][0/306] Elapsed 0m 0s (remain 3m 1s) Loss: 0.7284 \n",
            "Epoch: [1][100/306] Elapsed 0m 33s (remain 1m 7s) Loss: 0.2391 \n",
            "Epoch: [1][200/306] Elapsed 1m 5s (remain 0m 34s) Loss: 0.1853 \n",
            "Epoch: [1][300/306] Elapsed 1m 38s (remain 0m 1s) Loss: 0.1655 \n",
            "Epoch: [1][305/306] Elapsed 1m 39s (remain 0m 0s) Loss: 0.1651 \n",
            "EVAL: [0/77] Elapsed 0m 0s (remain 0m 25s) Loss: 0.1035 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1 - avg_train_loss: 0.1651  avg_val_loss: 0.1000  time: 108s\n",
            "Epoch 1 - avg_train_loss: 0.1651  avg_val_loss: 0.1000  time: 108s\n",
            "Epoch 1 - avg_train_loss: 0.1651  avg_val_loss: 0.1000  time: 108s\n",
            "Epoch 1 - avg_train_loss: 0.1651  avg_val_loss: 0.1000  time: 108s\n",
            "Epoch 1 - avg_train_loss: 0.1651  avg_val_loss: 0.1000  time: 108s\n",
            "Epoch 1 - avg_train_loss: 0.1651  avg_val_loss: 0.1000  time: 108s\n",
            "Epoch 1 - Score: 0.10000640755658172\n",
            "Epoch 1 - Score: 0.10000640755658172\n",
            "Epoch 1 - Score: 0.10000640755658172\n",
            "Epoch 1 - Score: 0.10000640755658172\n",
            "Epoch 1 - Score: 0.10000640755658172\n",
            "Epoch 1 - Score: 0.10000640755658172\n",
            "Epoch 1 - Save Best Score: 0.1000 Model\n",
            "Epoch 1 - Save Best Score: 0.1000 Model\n",
            "Epoch 1 - Save Best Score: 0.1000 Model\n",
            "Epoch 1 - Save Best Score: 0.1000 Model\n",
            "Epoch 1 - Save Best Score: 0.1000 Model\n",
            "Epoch 1 - Save Best Score: 0.1000 Model\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EVAL: [76/77] Elapsed 0m 8s (remain 0m 0s) Loss: 0.1000 \n",
            "Log loss for 0 is 0.10000640755658172\n",
            "Epoch: [2][0/306] Elapsed 0m 0s (remain 2m 57s) Loss: 0.0341 \n",
            "Epoch: [2][100/306] Elapsed 0m 33s (remain 1m 7s) Loss: 0.0602 \n",
            "Epoch: [2][200/306] Elapsed 1m 5s (remain 0m 34s) Loss: 0.0695 \n",
            "Epoch: [2][300/306] Elapsed 1m 38s (remain 0m 1s) Loss: 0.0719 \n",
            "Epoch: [2][305/306] Elapsed 1m 39s (remain 0m 0s) Loss: 0.0721 \n",
            "EVAL: [0/77] Elapsed 0m 0s (remain 0m 27s) Loss: 0.0847 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2 - avg_train_loss: 0.0721  avg_val_loss: 0.1071  time: 108s\n",
            "Epoch 2 - avg_train_loss: 0.0721  avg_val_loss: 0.1071  time: 108s\n",
            "Epoch 2 - avg_train_loss: 0.0721  avg_val_loss: 0.1071  time: 108s\n",
            "Epoch 2 - avg_train_loss: 0.0721  avg_val_loss: 0.1071  time: 108s\n",
            "Epoch 2 - avg_train_loss: 0.0721  avg_val_loss: 0.1071  time: 108s\n",
            "Epoch 2 - avg_train_loss: 0.0721  avg_val_loss: 0.1071  time: 108s\n",
            "Epoch 2 - Score: 0.10714814345078723\n",
            "Epoch 2 - Score: 0.10714814345078723\n",
            "Epoch 2 - Score: 0.10714814345078723\n",
            "Epoch 2 - Score: 0.10714814345078723\n",
            "Epoch 2 - Score: 0.10714814345078723\n",
            "Epoch 2 - Score: 0.10714814345078723\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EVAL: [76/77] Elapsed 0m 8s (remain 0m 0s) Loss: 0.1071 \n",
            "Log loss for 1 is 0.10714814345078723\n",
            "Epoch: [3][0/306] Elapsed 0m 0s (remain 3m 0s) Loss: 0.0065 \n",
            "Epoch: [3][100/306] Elapsed 0m 33s (remain 1m 7s) Loss: 0.0344 \n",
            "Epoch: [3][200/306] Elapsed 1m 5s (remain 0m 34s) Loss: 0.0474 \n",
            "Epoch: [3][300/306] Elapsed 1m 38s (remain 0m 1s) Loss: 0.0421 \n",
            "Epoch: [3][305/306] Elapsed 1m 39s (remain 0m 0s) Loss: 0.0416 \n",
            "EVAL: [0/77] Elapsed 0m 0s (remain 0m 26s) Loss: 0.1398 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3 - avg_train_loss: 0.0416  avg_val_loss: 0.1421  time: 108s\n",
            "Epoch 3 - avg_train_loss: 0.0416  avg_val_loss: 0.1421  time: 108s\n",
            "Epoch 3 - avg_train_loss: 0.0416  avg_val_loss: 0.1421  time: 108s\n",
            "Epoch 3 - avg_train_loss: 0.0416  avg_val_loss: 0.1421  time: 108s\n",
            "Epoch 3 - avg_train_loss: 0.0416  avg_val_loss: 0.1421  time: 108s\n",
            "Epoch 3 - avg_train_loss: 0.0416  avg_val_loss: 0.1421  time: 108s\n",
            "Epoch 3 - Score: 0.14213300990407562\n",
            "Epoch 3 - Score: 0.14213300990407562\n",
            "Epoch 3 - Score: 0.14213300990407562\n",
            "Epoch 3 - Score: 0.14213300990407562\n",
            "Epoch 3 - Score: 0.14213300990407562\n",
            "Epoch 3 - Score: 0.14213300990407562\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EVAL: [76/77] Elapsed 0m 8s (remain 0m 0s) Loss: 0.1421 \n",
            "Log loss for 2 is 0.14213300990407562\n",
            "Epoch: [4][0/306] Elapsed 0m 0s (remain 3m 9s) Loss: 0.0841 \n",
            "Epoch: [4][100/306] Elapsed 0m 33s (remain 1m 7s) Loss: 0.0202 \n",
            "Epoch: [4][200/306] Elapsed 1m 5s (remain 0m 34s) Loss: 0.0215 \n",
            "Epoch: [4][300/306] Elapsed 1m 38s (remain 0m 1s) Loss: 0.0254 \n",
            "Epoch: [4][305/306] Elapsed 1m 39s (remain 0m 0s) Loss: 0.0252 \n",
            "EVAL: [0/77] Elapsed 0m 0s (remain 0m 27s) Loss: 0.0330 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4 - avg_train_loss: 0.0252  avg_val_loss: 0.1407  time: 108s\n",
            "Epoch 4 - avg_train_loss: 0.0252  avg_val_loss: 0.1407  time: 108s\n",
            "Epoch 4 - avg_train_loss: 0.0252  avg_val_loss: 0.1407  time: 108s\n",
            "Epoch 4 - avg_train_loss: 0.0252  avg_val_loss: 0.1407  time: 108s\n",
            "Epoch 4 - avg_train_loss: 0.0252  avg_val_loss: 0.1407  time: 108s\n",
            "Epoch 4 - avg_train_loss: 0.0252  avg_val_loss: 0.1407  time: 108s\n",
            "Epoch 4 - Score: 0.14066506236158394\n",
            "Epoch 4 - Score: 0.14066506236158394\n",
            "Epoch 4 - Score: 0.14066506236158394\n",
            "Epoch 4 - Score: 0.14066506236158394\n",
            "Epoch 4 - Score: 0.14066506236158394\n",
            "Epoch 4 - Score: 0.14066506236158394\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EVAL: [76/77] Elapsed 0m 8s (remain 0m 0s) Loss: 0.1407 \n",
            "Log loss for 3 is 0.14066506236158394\n",
            "Epoch: [5][0/306] Elapsed 0m 0s (remain 3m 3s) Loss: 0.0080 \n",
            "Epoch: [5][100/306] Elapsed 0m 33s (remain 1m 7s) Loss: 0.0135 \n",
            "Epoch: [5][200/306] Elapsed 1m 5s (remain 0m 34s) Loss: 0.0169 \n",
            "Epoch: [5][300/306] Elapsed 1m 38s (remain 0m 1s) Loss: 0.0170 \n",
            "Epoch: [5][305/306] Elapsed 1m 39s (remain 0m 0s) Loss: 0.0181 \n",
            "EVAL: [0/77] Elapsed 0m 0s (remain 0m 26s) Loss: 0.0046 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5 - avg_train_loss: 0.0181  avg_val_loss: 0.2122  time: 108s\n",
            "Epoch 5 - avg_train_loss: 0.0181  avg_val_loss: 0.2122  time: 108s\n",
            "Epoch 5 - avg_train_loss: 0.0181  avg_val_loss: 0.2122  time: 108s\n",
            "Epoch 5 - avg_train_loss: 0.0181  avg_val_loss: 0.2122  time: 108s\n",
            "Epoch 5 - avg_train_loss: 0.0181  avg_val_loss: 0.2122  time: 108s\n",
            "Epoch 5 - avg_train_loss: 0.0181  avg_val_loss: 0.2122  time: 108s\n",
            "Epoch 5 - Score: 0.21217511663388042\n",
            "Epoch 5 - Score: 0.21217511663388042\n",
            "Epoch 5 - Score: 0.21217511663388042\n",
            "Epoch 5 - Score: 0.21217511663388042\n",
            "Epoch 5 - Score: 0.21217511663388042\n",
            "Epoch 5 - Score: 0.21217511663388042\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EVAL: [76/77] Elapsed 0m 8s (remain 0m 0s) Loss: 0.2122 \n",
            "Log loss for 4 is 0.21217511663388042\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "========== fold: 4 result ==========\n",
            "========== fold: 4 result ==========\n",
            "========== fold: 4 result ==========\n",
            "========== fold: 4 result ==========\n",
            "========== fold: 4 result ==========\n",
            "========== fold: 4 result ==========\n",
            "Score: 0.10001\n",
            "Score: 0.10001\n",
            "Score: 0.10001\n",
            "Score: 0.10001\n",
            "Score: 0.10001\n",
            "Score: 0.10001\n",
            "========== CV ==========\n",
            "========== CV ==========\n",
            "========== CV ==========\n",
            "========== CV ==========\n",
            "========== CV ==========\n",
            "========== CV ==========\n",
            "Score: 0.10947\n",
            "Score: 0.10947\n",
            "Score: 0.10947\n",
            "Score: 0.10947\n",
            "Score: 0.10947\n",
            "Score: 0.10947\n",
            "Didn't find file /content/activebus_bert_final/added_tokens.json. We won't load it.\n",
            "loading file /content/activebus_bert_final/vocab.txt\n",
            "loading file None\n",
            "loading file /content/activebus_bert_final/special_tokens_map.json\n",
            "loading file /content/activebus_bert_final/tokenizer_config.json\n",
            "loading file /content/activebus_bert_final/tokenizer.json\n",
            "========== model: bert-base-uncased fold: 0 inference ==========\n",
            "========== model: bert-base-uncased fold: 0 inference ==========\n",
            "========== model: bert-base-uncased fold: 0 inference ==========\n",
            "========== model: bert-base-uncased fold: 0 inference ==========\n",
            "========== model: bert-base-uncased fold: 0 inference ==========\n",
            "========== model: bert-base-uncased fold: 0 inference ==========\n",
            "loading configuration file /content/activebus_bert_final/config.json\n",
            "Model config BertConfig {\n",
            "  \"_name_or_path\": \"activebus/BERT-XD_Review\",\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_ids\": 0,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.15.0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "loading weights file /content/activebus_bert_final/pytorch_model.bin\n",
            "Some weights of the model checkpoint at /content/activebus_bert_final were not used when initializing BertForSequenceClassification: ['cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /content/activebus_bert_final and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "\n",
            "  0%|          | 0/165 [00:00<?, ?it/s]\u001b[A\n",
            "  1%|          | 1/165 [00:00<00:21,  7.62it/s]\u001b[A\n",
            "  1%|          | 2/165 [00:00<00:20,  8.03it/s]\u001b[A\n",
            "  2%|▏         | 3/165 [00:00<00:19,  8.32it/s]\u001b[A\n",
            "  2%|▏         | 4/165 [00:00<00:18,  8.60it/s]\u001b[A\n",
            "  3%|▎         | 5/165 [00:00<00:18,  8.78it/s]\u001b[A\n",
            "  4%|▎         | 6/165 [00:00<00:17,  8.88it/s]\u001b[A\n",
            "  4%|▍         | 7/165 [00:00<00:17,  8.94it/s]\u001b[A\n",
            "  5%|▍         | 8/165 [00:00<00:17,  8.99it/s]\u001b[A\n",
            "  5%|▌         | 9/165 [00:01<00:17,  9.00it/s]\u001b[A\n",
            "  6%|▌         | 10/165 [00:01<00:17,  9.02it/s]\u001b[A\n",
            "  7%|▋         | 11/165 [00:01<00:17,  9.03it/s]\u001b[A\n",
            "  7%|▋         | 12/165 [00:01<00:16,  9.03it/s]\u001b[A\n",
            "  8%|▊         | 13/165 [00:01<00:16,  9.04it/s]\u001b[A\n",
            "  8%|▊         | 14/165 [00:01<00:16,  9.05it/s]\u001b[A\n",
            "  9%|▉         | 15/165 [00:01<00:16,  9.07it/s]\u001b[A\n",
            " 10%|▉         | 16/165 [00:01<00:16,  9.08it/s]\u001b[A\n",
            " 10%|█         | 17/165 [00:01<00:16,  9.09it/s]\u001b[A\n",
            " 11%|█         | 18/165 [00:02<00:16,  9.09it/s]\u001b[A\n",
            " 12%|█▏        | 19/165 [00:02<00:16,  9.07it/s]\u001b[A\n",
            " 12%|█▏        | 20/165 [00:02<00:15,  9.07it/s]\u001b[A\n",
            " 13%|█▎        | 21/165 [00:02<00:15,  9.07it/s]\u001b[A\n",
            " 13%|█▎        | 22/165 [00:02<00:15,  9.08it/s]\u001b[A\n",
            " 14%|█▍        | 23/165 [00:02<00:15,  9.07it/s]\u001b[A\n",
            " 15%|█▍        | 24/165 [00:02<00:15,  9.09it/s]\u001b[A\n",
            " 15%|█▌        | 25/165 [00:02<00:15,  9.08it/s]\u001b[A\n",
            " 16%|█▌        | 26/165 [00:02<00:15,  9.08it/s]\u001b[A\n",
            " 16%|█▋        | 27/165 [00:03<00:15,  9.09it/s]\u001b[A\n",
            " 17%|█▋        | 28/165 [00:03<00:15,  9.10it/s]\u001b[A\n",
            " 18%|█▊        | 29/165 [00:03<00:14,  9.10it/s]\u001b[A\n",
            " 18%|█▊        | 30/165 [00:03<00:14,  9.09it/s]\u001b[A\n",
            " 19%|█▉        | 31/165 [00:03<00:14,  9.06it/s]\u001b[A\n",
            " 19%|█▉        | 32/165 [00:03<00:14,  9.07it/s]\u001b[A\n",
            " 20%|██        | 33/165 [00:03<00:14,  9.08it/s]\u001b[A\n",
            " 21%|██        | 34/165 [00:03<00:16,  7.86it/s]\u001b[A\n",
            " 21%|██        | 35/165 [00:03<00:15,  8.19it/s]\u001b[A\n",
            " 22%|██▏       | 36/165 [00:04<00:15,  8.44it/s]\u001b[A\n",
            " 22%|██▏       | 37/165 [00:04<00:14,  8.62it/s]\u001b[A\n",
            " 23%|██▎       | 38/165 [00:04<00:14,  8.76it/s]\u001b[A\n",
            " 24%|██▎       | 39/165 [00:04<00:14,  8.86it/s]\u001b[A\n",
            " 24%|██▍       | 40/165 [00:04<00:14,  8.92it/s]\u001b[A\n",
            " 25%|██▍       | 41/165 [00:04<00:13,  8.98it/s]\u001b[A\n",
            " 25%|██▌       | 42/165 [00:04<00:13,  8.99it/s]\u001b[A\n",
            " 26%|██▌       | 43/165 [00:04<00:13,  8.98it/s]\u001b[A\n",
            " 27%|██▋       | 44/165 [00:04<00:13,  9.01it/s]\u001b[A\n",
            " 27%|██▋       | 45/165 [00:05<00:13,  9.04it/s]\u001b[A\n",
            " 28%|██▊       | 46/165 [00:05<00:13,  9.05it/s]\u001b[A\n",
            " 28%|██▊       | 47/165 [00:05<00:13,  9.07it/s]\u001b[A\n",
            " 29%|██▉       | 48/165 [00:05<00:12,  9.06it/s]\u001b[A\n",
            " 30%|██▉       | 49/165 [00:05<00:12,  9.05it/s]\u001b[A\n",
            " 30%|███       | 50/165 [00:05<00:12,  9.07it/s]\u001b[A\n",
            " 31%|███       | 51/165 [00:05<00:12,  9.07it/s]\u001b[A\n",
            " 32%|███▏      | 52/165 [00:05<00:12,  9.08it/s]\u001b[A\n",
            " 32%|███▏      | 53/165 [00:05<00:12,  9.07it/s]\u001b[A\n",
            " 33%|███▎      | 54/165 [00:06<00:12,  9.08it/s]\u001b[A\n",
            " 33%|███▎      | 55/165 [00:06<00:12,  9.10it/s]\u001b[A\n",
            " 34%|███▍      | 56/165 [00:06<00:11,  9.09it/s]\u001b[A\n",
            " 35%|███▍      | 57/165 [00:06<00:11,  9.09it/s]\u001b[A\n",
            " 35%|███▌      | 58/165 [00:06<00:11,  9.09it/s]\u001b[A\n",
            " 36%|███▌      | 59/165 [00:06<00:11,  9.09it/s]\u001b[A\n",
            " 36%|███▋      | 60/165 [00:06<00:11,  9.09it/s]\u001b[A\n",
            " 37%|███▋      | 61/165 [00:06<00:11,  9.06it/s]\u001b[A\n",
            " 38%|███▊      | 62/165 [00:06<00:11,  9.07it/s]\u001b[A\n",
            " 38%|███▊      | 63/165 [00:07<00:11,  9.05it/s]\u001b[A\n",
            " 39%|███▉      | 64/165 [00:07<00:11,  9.05it/s]\u001b[A\n",
            " 39%|███▉      | 65/165 [00:07<00:11,  9.07it/s]\u001b[A\n",
            " 40%|████      | 66/165 [00:07<00:10,  9.08it/s]\u001b[A\n",
            " 41%|████      | 67/165 [00:07<00:10,  9.07it/s]\u001b[A\n",
            " 41%|████      | 68/165 [00:07<00:10,  9.05it/s]\u001b[A\n",
            " 42%|████▏     | 69/165 [00:07<00:10,  9.05it/s]\u001b[A\n",
            " 42%|████▏     | 70/165 [00:07<00:10,  9.06it/s]\u001b[A\n",
            " 43%|████▎     | 71/165 [00:07<00:10,  9.06it/s]\u001b[A\n",
            " 44%|████▎     | 72/165 [00:08<00:10,  9.08it/s]\u001b[A\n",
            " 44%|████▍     | 73/165 [00:08<00:10,  9.07it/s]\u001b[A\n",
            " 45%|████▍     | 74/165 [00:08<00:10,  9.09it/s]\u001b[A\n",
            " 45%|████▌     | 75/165 [00:08<00:09,  9.10it/s]\u001b[A\n",
            " 46%|████▌     | 76/165 [00:08<00:09,  9.09it/s]\u001b[A\n",
            " 47%|████▋     | 77/165 [00:08<00:09,  9.10it/s]\u001b[A\n",
            " 47%|████▋     | 78/165 [00:08<00:09,  9.08it/s]\u001b[A\n",
            " 48%|████▊     | 79/165 [00:08<00:09,  9.08it/s]\u001b[A\n",
            " 48%|████▊     | 80/165 [00:08<00:09,  9.07it/s]\u001b[A\n",
            " 49%|████▉     | 81/165 [00:09<00:09,  9.08it/s]\u001b[A\n",
            " 50%|████▉     | 82/165 [00:09<00:09,  9.10it/s]\u001b[A\n",
            " 50%|█████     | 83/165 [00:09<00:09,  9.09it/s]\u001b[A\n",
            " 51%|█████     | 84/165 [00:09<00:08,  9.08it/s]\u001b[A\n",
            " 52%|█████▏    | 85/165 [00:09<00:08,  9.09it/s]\u001b[A\n",
            " 52%|█████▏    | 86/165 [00:09<00:08,  9.07it/s]\u001b[A\n",
            " 53%|█████▎    | 87/165 [00:09<00:08,  9.05it/s]\u001b[A\n",
            " 53%|█████▎    | 88/165 [00:09<00:08,  9.07it/s]\u001b[A\n",
            " 54%|█████▍    | 89/165 [00:09<00:08,  9.06it/s]\u001b[A\n",
            " 55%|█████▍    | 90/165 [00:10<00:08,  9.05it/s]\u001b[A\n",
            " 55%|█████▌    | 91/165 [00:10<00:08,  9.06it/s]\u001b[A\n",
            " 56%|█████▌    | 92/165 [00:10<00:08,  9.07it/s]\u001b[A\n",
            " 56%|█████▋    | 93/165 [00:10<00:07,  9.08it/s]\u001b[A\n",
            " 57%|█████▋    | 94/165 [00:10<00:07,  9.08it/s]\u001b[A\n",
            " 58%|█████▊    | 95/165 [00:10<00:07,  9.09it/s]\u001b[A\n",
            " 58%|█████▊    | 96/165 [00:10<00:07,  9.09it/s]\u001b[A\n",
            " 59%|█████▉    | 97/165 [00:10<00:07,  9.10it/s]\u001b[A\n",
            " 59%|█████▉    | 98/165 [00:10<00:07,  9.09it/s]\u001b[A\n",
            " 60%|██████    | 99/165 [00:10<00:07,  9.09it/s]\u001b[A\n",
            " 61%|██████    | 100/165 [00:11<00:07,  9.09it/s]\u001b[A\n",
            " 61%|██████    | 101/165 [00:11<00:07,  9.05it/s]\u001b[A\n",
            " 62%|██████▏   | 102/165 [00:11<00:06,  9.06it/s]\u001b[A\n",
            " 62%|██████▏   | 103/165 [00:11<00:06,  9.05it/s]\u001b[A\n",
            " 63%|██████▎   | 104/165 [00:11<00:06,  9.06it/s]\u001b[A\n",
            " 64%|██████▎   | 105/165 [00:11<00:06,  9.07it/s]\u001b[A\n",
            " 64%|██████▍   | 106/165 [00:11<00:06,  9.08it/s]\u001b[A\n",
            " 65%|██████▍   | 107/165 [00:11<00:06,  9.04it/s]\u001b[A\n",
            " 65%|██████▌   | 108/165 [00:11<00:06,  9.05it/s]\u001b[A\n",
            " 66%|██████▌   | 109/165 [00:12<00:06,  9.08it/s]\u001b[A\n",
            " 67%|██████▋   | 110/165 [00:12<00:06,  9.10it/s]\u001b[A\n",
            " 67%|██████▋   | 111/165 [00:12<00:05,  9.09it/s]\u001b[A\n",
            " 68%|██████▊   | 112/165 [00:12<00:05,  9.09it/s]\u001b[A\n",
            " 68%|██████▊   | 113/165 [00:12<00:05,  9.09it/s]\u001b[A\n",
            " 69%|██████▉   | 114/165 [00:12<00:05,  9.10it/s]\u001b[A\n",
            " 70%|██████▉   | 115/165 [00:12<00:05,  9.11it/s]\u001b[A\n",
            " 70%|███████   | 116/165 [00:12<00:05,  9.10it/s]\u001b[A\n",
            " 71%|███████   | 117/165 [00:12<00:05,  9.10it/s]\u001b[A\n",
            " 72%|███████▏  | 118/165 [00:13<00:05,  9.11it/s]\u001b[A\n",
            " 72%|███████▏  | 119/165 [00:13<00:05,  9.12it/s]\u001b[A\n",
            " 73%|███████▎  | 120/165 [00:13<00:04,  9.11it/s]\u001b[A\n",
            " 73%|███████▎  | 121/165 [00:13<00:04,  9.10it/s]\u001b[A\n",
            " 74%|███████▍  | 122/165 [00:13<00:04,  9.06it/s]\u001b[A\n",
            " 75%|███████▍  | 123/165 [00:13<00:04,  9.07it/s]\u001b[A\n",
            " 75%|███████▌  | 124/165 [00:13<00:04,  9.08it/s]\u001b[A\n",
            " 76%|███████▌  | 125/165 [00:13<00:04,  9.09it/s]\u001b[A\n",
            " 76%|███████▋  | 126/165 [00:13<00:04,  9.09it/s]\u001b[A\n",
            " 77%|███████▋  | 127/165 [00:14<00:04,  9.10it/s]\u001b[A\n",
            " 78%|███████▊  | 128/165 [00:14<00:04,  9.11it/s]\u001b[A\n",
            " 78%|███████▊  | 129/165 [00:14<00:03,  9.11it/s]\u001b[A\n",
            " 79%|███████▉  | 130/165 [00:14<00:03,  9.11it/s]\u001b[A\n",
            " 79%|███████▉  | 131/165 [00:14<00:03,  9.12it/s]\u001b[A\n",
            " 80%|████████  | 132/165 [00:14<00:03,  9.11it/s]\u001b[A\n",
            " 81%|████████  | 133/165 [00:14<00:03,  9.11it/s]\u001b[A\n",
            " 81%|████████  | 134/165 [00:14<00:03,  9.11it/s]\u001b[A\n",
            " 82%|████████▏ | 135/165 [00:14<00:03,  9.11it/s]\u001b[A\n",
            " 82%|████████▏ | 136/165 [00:15<00:03,  9.12it/s]\u001b[A\n",
            " 83%|████████▎ | 137/165 [00:15<00:03,  9.10it/s]\u001b[A\n",
            " 84%|████████▎ | 138/165 [00:15<00:02,  9.10it/s]\u001b[A\n",
            " 84%|████████▍ | 139/165 [00:15<00:02,  9.12it/s]\u001b[A\n",
            " 85%|████████▍ | 140/165 [00:15<00:02,  9.13it/s]\u001b[A\n",
            " 85%|████████▌ | 141/165 [00:15<00:02,  9.13it/s]\u001b[A\n",
            " 86%|████████▌ | 142/165 [00:15<00:02,  9.12it/s]\u001b[A\n",
            " 87%|████████▋ | 143/165 [00:15<00:02,  9.10it/s]\u001b[A\n",
            " 87%|████████▋ | 144/165 [00:15<00:02,  9.10it/s]\u001b[A\n",
            " 88%|████████▊ | 145/165 [00:16<00:02,  9.11it/s]\u001b[A\n",
            " 88%|████████▊ | 146/165 [00:16<00:02,  9.09it/s]\u001b[A\n",
            " 89%|████████▉ | 147/165 [00:16<00:01,  9.11it/s]\u001b[A\n",
            " 90%|████████▉ | 148/165 [00:16<00:01,  9.11it/s]\u001b[A\n",
            " 90%|█████████ | 149/165 [00:16<00:01,  9.09it/s]\u001b[A\n",
            " 91%|█████████ | 150/165 [00:16<00:01,  9.08it/s]\u001b[A\n",
            " 92%|█████████▏| 151/165 [00:16<00:01,  9.10it/s]\u001b[A\n",
            " 92%|█████████▏| 152/165 [00:16<00:01,  9.10it/s]\u001b[A\n",
            " 93%|█████████▎| 153/165 [00:16<00:01,  9.10it/s]\u001b[A\n",
            " 93%|█████████▎| 154/165 [00:17<00:01,  9.10it/s]\u001b[A\n",
            " 94%|█████████▍| 155/165 [00:17<00:01,  9.10it/s]\u001b[A\n",
            " 95%|█████████▍| 156/165 [00:17<00:00,  9.11it/s]\u001b[A\n",
            " 95%|█████████▌| 157/165 [00:17<00:00,  9.08it/s]\u001b[A\n",
            " 96%|█████████▌| 158/165 [00:17<00:00,  9.11it/s]\u001b[A\n",
            " 96%|█████████▋| 159/165 [00:17<00:00,  9.12it/s]\u001b[A\n",
            " 97%|█████████▋| 160/165 [00:17<00:00,  9.13it/s]\u001b[A\n",
            " 98%|█████████▊| 161/165 [00:17<00:00,  9.14it/s]\u001b[A\n",
            " 98%|█████████▊| 162/165 [00:17<00:00,  9.15it/s]\u001b[A\n",
            " 99%|█████████▉| 163/165 [00:18<00:00,  9.15it/s]\u001b[A\n",
            "100%|██████████| 165/165 [00:18<00:00,  9.03it/s]\n",
            "========== model: bert-base-uncased fold: 1 inference ==========\n",
            "========== model: bert-base-uncased fold: 1 inference ==========\n",
            "========== model: bert-base-uncased fold: 1 inference ==========\n",
            "========== model: bert-base-uncased fold: 1 inference ==========\n",
            "========== model: bert-base-uncased fold: 1 inference ==========\n",
            "========== model: bert-base-uncased fold: 1 inference ==========\n",
            "loading configuration file /content/activebus_bert_final/config.json\n",
            "Model config BertConfig {\n",
            "  \"_name_or_path\": \"activebus/BERT-XD_Review\",\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_ids\": 0,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.15.0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "loading weights file /content/activebus_bert_final/pytorch_model.bin\n",
            "Some weights of the model checkpoint at /content/activebus_bert_final were not used when initializing BertForSequenceClassification: ['cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /content/activebus_bert_final and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "\n",
            "  0%|          | 0/165 [00:00<?, ?it/s]\u001b[A\n",
            "  1%|          | 1/165 [00:00<00:19,  8.22it/s]\u001b[A\n",
            "  1%|          | 2/165 [00:00<00:18,  8.71it/s]\u001b[A\n",
            "  2%|▏         | 3/165 [00:00<00:18,  8.88it/s]\u001b[A\n",
            "  2%|▏         | 4/165 [00:00<00:17,  8.97it/s]\u001b[A\n",
            "  3%|▎         | 5/165 [00:00<00:17,  9.01it/s]\u001b[A\n",
            "  4%|▎         | 6/165 [00:00<00:17,  9.06it/s]\u001b[A\n",
            "  4%|▍         | 7/165 [00:00<00:17,  9.08it/s]\u001b[A\n",
            "  5%|▍         | 8/165 [00:00<00:17,  9.08it/s]\u001b[A\n",
            "  5%|▌         | 9/165 [00:01<00:17,  9.07it/s]\u001b[A\n",
            "  6%|▌         | 10/165 [00:01<00:17,  9.08it/s]\u001b[A\n",
            "  7%|▋         | 11/165 [00:01<00:16,  9.09it/s]\u001b[A\n",
            "  7%|▋         | 12/165 [00:01<00:16,  9.01it/s]\u001b[A\n",
            "  8%|▊         | 13/165 [00:01<00:16,  9.04it/s]\u001b[A\n",
            "  8%|▊         | 14/165 [00:01<00:16,  9.06it/s]\u001b[A\n",
            "  9%|▉         | 15/165 [00:01<00:16,  9.08it/s]\u001b[A\n",
            " 10%|▉         | 16/165 [00:01<00:16,  9.07it/s]\u001b[A\n",
            " 10%|█         | 17/165 [00:01<00:16,  9.08it/s]\u001b[A\n",
            " 11%|█         | 18/165 [00:01<00:16,  9.07it/s]\u001b[A\n",
            " 12%|█▏        | 19/165 [00:02<00:16,  9.08it/s]\u001b[A\n",
            " 12%|█▏        | 20/165 [00:02<00:15,  9.09it/s]\u001b[A\n",
            " 13%|█▎        | 21/165 [00:02<00:15,  9.09it/s]\u001b[A\n",
            " 13%|█▎        | 22/165 [00:02<00:15,  9.09it/s]\u001b[A\n",
            " 14%|█▍        | 23/165 [00:02<00:15,  9.07it/s]\u001b[A\n",
            " 15%|█▍        | 24/165 [00:02<00:15,  9.08it/s]\u001b[A\n",
            " 15%|█▌        | 25/165 [00:02<00:15,  9.10it/s]\u001b[A\n",
            " 16%|█▌        | 26/165 [00:02<00:15,  9.07it/s]\u001b[A\n",
            " 16%|█▋        | 27/165 [00:02<00:15,  9.07it/s]\u001b[A\n",
            " 17%|█▋        | 28/165 [00:03<00:15,  9.08it/s]\u001b[A\n",
            " 18%|█▊        | 29/165 [00:03<00:15,  9.07it/s]\u001b[A\n",
            " 18%|█▊        | 30/165 [00:03<00:14,  9.05it/s]\u001b[A\n",
            " 19%|█▉        | 31/165 [00:03<00:14,  9.06it/s]\u001b[A\n",
            " 19%|█▉        | 32/165 [00:03<00:14,  9.07it/s]\u001b[A\n",
            " 20%|██        | 33/165 [00:03<00:14,  9.08it/s]\u001b[A\n",
            " 21%|██        | 34/165 [00:03<00:14,  9.08it/s]\u001b[A\n",
            " 21%|██        | 35/165 [00:03<00:14,  9.09it/s]\u001b[A\n",
            " 22%|██▏       | 36/165 [00:03<00:14,  9.11it/s]\u001b[A\n",
            " 22%|██▏       | 37/165 [00:04<00:14,  9.11it/s]\u001b[A\n",
            " 23%|██▎       | 38/165 [00:04<00:13,  9.11it/s]\u001b[A\n",
            " 24%|██▎       | 39/165 [00:04<00:13,  9.11it/s]\u001b[A\n",
            " 24%|██▍       | 40/165 [00:04<00:13,  9.11it/s]\u001b[A\n",
            " 25%|██▍       | 41/165 [00:04<00:13,  9.11it/s]\u001b[A\n",
            " 25%|██▌       | 42/165 [00:04<00:13,  9.12it/s]\u001b[A\n",
            " 26%|██▌       | 43/165 [00:04<00:13,  9.11it/s]\u001b[A\n",
            " 27%|██▋       | 44/165 [00:04<00:13,  9.11it/s]\u001b[A\n",
            " 27%|██▋       | 45/165 [00:04<00:13,  9.10it/s]\u001b[A\n",
            " 28%|██▊       | 46/165 [00:05<00:13,  9.11it/s]\u001b[A\n",
            " 28%|██▊       | 47/165 [00:05<00:12,  9.10it/s]\u001b[A\n",
            " 29%|██▉       | 48/165 [00:05<00:12,  9.10it/s]\u001b[A\n",
            " 30%|██▉       | 49/165 [00:05<00:12,  9.12it/s]\u001b[A\n",
            " 30%|███       | 50/165 [00:05<00:12,  9.11it/s]\u001b[A\n",
            " 31%|███       | 51/165 [00:05<00:12,  9.11it/s]\u001b[A\n",
            " 32%|███▏      | 52/165 [00:05<00:12,  9.09it/s]\u001b[A\n",
            " 32%|███▏      | 53/165 [00:05<00:12,  9.10it/s]\u001b[A\n",
            " 33%|███▎      | 54/165 [00:05<00:12,  9.11it/s]\u001b[A\n",
            " 33%|███▎      | 55/165 [00:06<00:12,  9.11it/s]\u001b[A\n",
            " 34%|███▍      | 56/165 [00:06<00:11,  9.11it/s]\u001b[A\n",
            " 35%|███▍      | 57/165 [00:06<00:11,  9.13it/s]\u001b[A\n",
            " 35%|███▌      | 58/165 [00:06<00:11,  9.10it/s]\u001b[A\n",
            " 36%|███▌      | 59/165 [00:06<00:11,  9.09it/s]\u001b[A\n",
            " 36%|███▋      | 60/165 [00:06<00:11,  9.11it/s]\u001b[A\n",
            " 37%|███▋      | 61/165 [00:06<00:11,  9.08it/s]\u001b[A\n",
            " 38%|███▊      | 62/165 [00:06<00:11,  9.10it/s]\u001b[A\n",
            " 38%|███▊      | 63/165 [00:06<00:11,  9.10it/s]\u001b[A\n",
            " 39%|███▉      | 64/165 [00:07<00:11,  9.11it/s]\u001b[A\n",
            " 39%|███▉      | 65/165 [00:07<00:11,  9.09it/s]\u001b[A\n",
            " 40%|████      | 66/165 [00:07<00:10,  9.11it/s]\u001b[A\n",
            " 41%|████      | 67/165 [00:07<00:10,  9.11it/s]\u001b[A\n",
            " 41%|████      | 68/165 [00:07<00:10,  9.12it/s]\u001b[A\n",
            " 42%|████▏     | 69/165 [00:07<00:10,  9.13it/s]\u001b[A\n",
            " 42%|████▏     | 70/165 [00:07<00:10,  9.14it/s]\u001b[A\n",
            " 43%|████▎     | 71/165 [00:07<00:10,  9.13it/s]\u001b[A\n",
            " 44%|████▎     | 72/165 [00:07<00:10,  9.12it/s]\u001b[A\n",
            " 44%|████▍     | 73/165 [00:08<00:10,  9.10it/s]\u001b[A\n",
            " 45%|████▍     | 74/165 [00:08<00:09,  9.11it/s]\u001b[A\n",
            " 45%|████▌     | 75/165 [00:08<00:09,  9.11it/s]\u001b[A\n",
            " 46%|████▌     | 76/165 [00:08<00:09,  9.12it/s]\u001b[A\n",
            " 47%|████▋     | 77/165 [00:08<00:09,  9.12it/s]\u001b[A\n",
            " 47%|████▋     | 78/165 [00:08<00:09,  9.13it/s]\u001b[A\n",
            " 48%|████▊     | 79/165 [00:08<00:09,  9.10it/s]\u001b[A\n",
            " 48%|████▊     | 80/165 [00:08<00:09,  9.10it/s]\u001b[A\n",
            " 49%|████▉     | 81/165 [00:08<00:09,  9.11it/s]\u001b[A\n",
            " 50%|████▉     | 82/165 [00:09<00:09,  9.11it/s]\u001b[A\n",
            " 50%|█████     | 83/165 [00:09<00:09,  9.11it/s]\u001b[A\n",
            " 51%|█████     | 84/165 [00:09<00:08,  9.08it/s]\u001b[A\n",
            " 52%|█████▏    | 85/165 [00:09<00:08,  9.09it/s]\u001b[A\n",
            " 52%|█████▏    | 86/165 [00:09<00:08,  9.11it/s]\u001b[A\n",
            " 53%|█████▎    | 87/165 [00:09<00:08,  9.09it/s]\u001b[A\n",
            " 53%|█████▎    | 88/165 [00:09<00:08,  9.11it/s]\u001b[A\n",
            " 54%|█████▍    | 89/165 [00:09<00:08,  9.09it/s]\u001b[A\n",
            " 55%|█████▍    | 90/165 [00:09<00:08,  9.11it/s]\u001b[A\n",
            " 55%|█████▌    | 91/165 [00:10<00:08,  9.11it/s]\u001b[A\n",
            " 56%|█████▌    | 92/165 [00:10<00:08,  9.11it/s]\u001b[A\n",
            " 56%|█████▋    | 93/165 [00:10<00:07,  9.12it/s]\u001b[A\n",
            " 57%|█████▋    | 94/165 [00:10<00:07,  9.12it/s]\u001b[A\n",
            " 58%|█████▊    | 95/165 [00:10<00:07,  9.12it/s]\u001b[A\n",
            " 58%|█████▊    | 96/165 [00:10<00:07,  9.11it/s]\u001b[A\n",
            " 59%|█████▉    | 97/165 [00:10<00:07,  9.09it/s]\u001b[A\n",
            " 59%|█████▉    | 98/165 [00:10<00:07,  9.08it/s]\u001b[A\n",
            " 60%|██████    | 99/165 [00:10<00:07,  9.07it/s]\u001b[A\n",
            " 61%|██████    | 100/165 [00:11<00:07,  9.07it/s]\u001b[A\n",
            " 61%|██████    | 101/165 [00:11<00:07,  9.08it/s]\u001b[A\n",
            " 62%|██████▏   | 102/165 [00:11<00:06,  9.08it/s]\u001b[A\n",
            " 62%|██████▏   | 103/165 [00:11<00:06,  9.09it/s]\u001b[A\n",
            " 63%|██████▎   | 104/165 [00:11<00:06,  9.07it/s]\u001b[A\n",
            " 64%|██████▎   | 105/165 [00:11<00:06,  9.06it/s]\u001b[A\n",
            " 64%|██████▍   | 106/165 [00:11<00:06,  9.05it/s]\u001b[A\n",
            " 65%|██████▍   | 107/165 [00:11<00:06,  9.05it/s]\u001b[A\n",
            " 65%|██████▌   | 108/165 [00:11<00:06,  9.06it/s]\u001b[A\n",
            " 66%|██████▌   | 109/165 [00:11<00:06,  9.07it/s]\u001b[A\n",
            " 67%|██████▋   | 110/165 [00:12<00:06,  9.06it/s]\u001b[A\n",
            " 67%|██████▋   | 111/165 [00:12<00:05,  9.07it/s]\u001b[A\n",
            " 68%|██████▊   | 112/165 [00:12<00:05,  9.08it/s]\u001b[A\n",
            " 68%|██████▊   | 113/165 [00:12<00:05,  9.09it/s]\u001b[A\n",
            " 69%|██████▉   | 114/165 [00:12<00:05,  9.09it/s]\u001b[A\n",
            " 70%|██████▉   | 115/165 [00:12<00:05,  9.11it/s]\u001b[A\n",
            " 70%|███████   | 116/165 [00:12<00:05,  9.10it/s]\u001b[A\n",
            " 71%|███████   | 117/165 [00:12<00:05,  9.09it/s]\u001b[A\n",
            " 72%|███████▏  | 118/165 [00:12<00:05,  9.10it/s]\u001b[A\n",
            " 72%|███████▏  | 119/165 [00:13<00:05,  9.11it/s]\u001b[A\n",
            " 73%|███████▎  | 120/165 [00:13<00:04,  9.11it/s]\u001b[A\n",
            " 73%|███████▎  | 121/165 [00:13<00:04,  9.11it/s]\u001b[A\n",
            " 74%|███████▍  | 122/165 [00:13<00:04,  9.12it/s]\u001b[A\n",
            " 75%|███████▍  | 123/165 [00:13<00:04,  9.10it/s]\u001b[A\n",
            " 75%|███████▌  | 124/165 [00:13<00:04,  9.09it/s]\u001b[A\n",
            " 76%|███████▌  | 125/165 [00:13<00:04,  9.08it/s]\u001b[A\n",
            " 76%|███████▋  | 126/165 [00:13<00:04,  9.09it/s]\u001b[A\n",
            " 77%|███████▋  | 127/165 [00:13<00:04,  9.09it/s]\u001b[A\n",
            " 78%|███████▊  | 128/165 [00:14<00:04,  9.09it/s]\u001b[A\n",
            " 78%|███████▊  | 129/165 [00:14<00:03,  9.09it/s]\u001b[A\n",
            " 79%|███████▉  | 130/165 [00:14<00:03,  9.07it/s]\u001b[A\n",
            " 79%|███████▉  | 131/165 [00:14<00:03,  9.09it/s]\u001b[A\n",
            " 80%|████████  | 132/165 [00:14<00:03,  9.11it/s]\u001b[A\n",
            " 81%|████████  | 133/165 [00:14<00:03,  9.12it/s]\u001b[A\n",
            " 81%|████████  | 134/165 [00:14<00:03,  9.12it/s]\u001b[A\n",
            " 82%|████████▏ | 135/165 [00:14<00:03,  9.13it/s]\u001b[A\n",
            " 82%|████████▏ | 136/165 [00:14<00:03,  9.12it/s]\u001b[A\n",
            " 83%|████████▎ | 137/165 [00:15<00:03,  9.12it/s]\u001b[A\n",
            " 84%|████████▎ | 138/165 [00:15<00:02,  9.01it/s]\u001b[A\n",
            " 84%|████████▍ | 139/165 [00:15<00:02,  9.02it/s]\u001b[A\n",
            " 85%|████████▍ | 140/165 [00:15<00:02,  9.05it/s]\u001b[A\n",
            " 85%|████████▌ | 141/165 [00:15<00:02,  9.06it/s]\u001b[A\n",
            " 86%|████████▌ | 142/165 [00:15<00:02,  9.08it/s]\u001b[A\n",
            " 87%|████████▋ | 143/165 [00:15<00:02,  9.05it/s]\u001b[A\n",
            " 87%|████████▋ | 144/165 [00:15<00:02,  9.05it/s]\u001b[A\n",
            " 88%|████████▊ | 145/165 [00:15<00:02,  9.07it/s]\u001b[A\n",
            " 88%|████████▊ | 146/165 [00:16<00:02,  9.06it/s]\u001b[A\n",
            " 89%|████████▉ | 147/165 [00:16<00:01,  9.08it/s]\u001b[A\n",
            " 90%|████████▉ | 148/165 [00:16<00:01,  9.06it/s]\u001b[A\n",
            " 90%|█████████ | 149/165 [00:16<00:01,  9.08it/s]\u001b[A\n",
            " 91%|█████████ | 150/165 [00:16<00:01,  9.07it/s]\u001b[A\n",
            " 92%|█████████▏| 151/165 [00:16<00:01,  9.08it/s]\u001b[A\n",
            " 92%|█████████▏| 152/165 [00:16<00:01,  9.10it/s]\u001b[A\n",
            " 93%|█████████▎| 153/165 [00:16<00:01,  9.11it/s]\u001b[A\n",
            " 93%|█████████▎| 154/165 [00:16<00:01,  9.11it/s]\u001b[A\n",
            " 94%|█████████▍| 155/165 [00:17<00:01,  9.10it/s]\u001b[A\n",
            " 95%|█████████▍| 156/165 [00:17<00:00,  9.10it/s]\u001b[A\n",
            " 95%|█████████▌| 157/165 [00:17<00:00,  9.11it/s]\u001b[A\n",
            " 96%|█████████▌| 158/165 [00:17<00:00,  9.09it/s]\u001b[A\n",
            " 96%|█████████▋| 159/165 [00:17<00:00,  9.09it/s]\u001b[A\n",
            " 97%|█████████▋| 160/165 [00:17<00:00,  9.08it/s]\u001b[A\n",
            " 98%|█████████▊| 161/165 [00:17<00:00,  9.08it/s]\u001b[A\n",
            " 98%|█████████▊| 162/165 [00:17<00:00,  9.09it/s]\u001b[A\n",
            " 99%|█████████▉| 163/165 [00:17<00:00,  9.09it/s]\u001b[A\n",
            "100%|██████████| 165/165 [00:18<00:00,  9.08it/s]\n",
            "========== model: bert-base-uncased fold: 2 inference ==========\n",
            "========== model: bert-base-uncased fold: 2 inference ==========\n",
            "========== model: bert-base-uncased fold: 2 inference ==========\n",
            "========== model: bert-base-uncased fold: 2 inference ==========\n",
            "========== model: bert-base-uncased fold: 2 inference ==========\n",
            "========== model: bert-base-uncased fold: 2 inference ==========\n",
            "loading configuration file /content/activebus_bert_final/config.json\n",
            "Model config BertConfig {\n",
            "  \"_name_or_path\": \"activebus/BERT-XD_Review\",\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_ids\": 0,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.15.0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "loading weights file /content/activebus_bert_final/pytorch_model.bin\n",
            "Some weights of the model checkpoint at /content/activebus_bert_final were not used when initializing BertForSequenceClassification: ['cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /content/activebus_bert_final and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "\n",
            "  0%|          | 0/165 [00:00<?, ?it/s]\u001b[A\n",
            "  1%|          | 1/165 [00:00<00:18,  8.74it/s]\u001b[A\n",
            "  1%|          | 2/165 [00:00<00:18,  8.93it/s]\u001b[A\n",
            "  2%|▏         | 3/165 [00:00<00:18,  8.98it/s]\u001b[A\n",
            "  2%|▏         | 4/165 [00:00<00:17,  9.03it/s]\u001b[A\n",
            "  3%|▎         | 5/165 [00:00<00:17,  9.05it/s]\u001b[A\n",
            "  4%|▎         | 6/165 [00:00<00:17,  9.06it/s]\u001b[A\n",
            "  4%|▍         | 7/165 [00:00<00:17,  9.06it/s]\u001b[A\n",
            "  5%|▍         | 8/165 [00:00<00:17,  9.06it/s]\u001b[A\n",
            "  5%|▌         | 9/165 [00:00<00:17,  9.06it/s]\u001b[A\n",
            "  6%|▌         | 10/165 [00:01<00:17,  9.07it/s]\u001b[A\n",
            "  7%|▋         | 11/165 [00:01<00:16,  9.09it/s]\u001b[A\n",
            "  7%|▋         | 12/165 [00:01<00:16,  9.09it/s]\u001b[A\n",
            "  8%|▊         | 13/165 [00:01<00:16,  9.10it/s]\u001b[A\n",
            "  8%|▊         | 14/165 [00:01<00:16,  9.10it/s]\u001b[A\n",
            "  9%|▉         | 15/165 [00:01<00:16,  9.10it/s]\u001b[A\n",
            " 10%|▉         | 16/165 [00:01<00:16,  9.12it/s]\u001b[A\n",
            " 10%|█         | 17/165 [00:01<00:16,  9.01it/s]\u001b[A\n",
            " 11%|█         | 18/165 [00:01<00:16,  9.05it/s]\u001b[A\n",
            " 12%|█▏        | 19/165 [00:02<00:16,  9.06it/s]\u001b[A\n",
            " 12%|█▏        | 20/165 [00:02<00:15,  9.08it/s]\u001b[A\n",
            " 13%|█▎        | 21/165 [00:02<00:15,  9.09it/s]\u001b[A\n",
            " 13%|█▎        | 22/165 [00:02<00:15,  9.08it/s]\u001b[A\n",
            " 14%|█▍        | 23/165 [00:02<00:15,  9.09it/s]\u001b[A\n",
            " 15%|█▍        | 24/165 [00:02<00:15,  9.10it/s]\u001b[A\n",
            " 15%|█▌        | 25/165 [00:02<00:15,  9.10it/s]\u001b[A\n",
            " 16%|█▌        | 26/165 [00:02<00:15,  9.10it/s]\u001b[A\n",
            " 16%|█▋        | 27/165 [00:02<00:15,  9.09it/s]\u001b[A\n",
            " 17%|█▋        | 28/165 [00:03<00:15,  9.09it/s]\u001b[A\n",
            " 18%|█▊        | 29/165 [00:03<00:14,  9.11it/s]\u001b[A\n",
            " 18%|█▊        | 30/165 [00:03<00:14,  9.10it/s]\u001b[A\n",
            " 19%|█▉        | 31/165 [00:03<00:14,  9.11it/s]\u001b[A\n",
            " 19%|█▉        | 32/165 [00:03<00:14,  9.12it/s]\u001b[A\n",
            " 20%|██        | 33/165 [00:03<00:14,  9.13it/s]\u001b[A\n",
            " 21%|██        | 34/165 [00:03<00:14,  9.13it/s]\u001b[A\n",
            " 21%|██        | 35/165 [00:03<00:14,  9.12it/s]\u001b[A\n",
            " 22%|██▏       | 36/165 [00:03<00:14,  9.12it/s]\u001b[A\n",
            " 22%|██▏       | 37/165 [00:04<00:14,  9.10it/s]\u001b[A\n",
            " 23%|██▎       | 38/165 [00:04<00:13,  9.10it/s]\u001b[A\n",
            " 24%|██▎       | 39/165 [00:04<00:13,  9.10it/s]\u001b[A\n",
            " 24%|██▍       | 40/165 [00:04<00:13,  9.10it/s]\u001b[A\n",
            " 25%|██▍       | 41/165 [00:04<00:13,  9.12it/s]\u001b[A\n",
            " 25%|██▌       | 42/165 [00:04<00:13,  9.12it/s]\u001b[A\n",
            " 26%|██▌       | 43/165 [00:04<00:13,  9.11it/s]\u001b[A\n",
            " 27%|██▋       | 44/165 [00:04<00:13,  9.13it/s]\u001b[A\n",
            " 27%|██▋       | 45/165 [00:04<00:13,  9.11it/s]\u001b[A\n",
            " 28%|██▊       | 46/165 [00:05<00:13,  9.10it/s]\u001b[A\n",
            " 28%|██▊       | 47/165 [00:05<00:12,  9.11it/s]\u001b[A\n",
            " 29%|██▉       | 48/165 [00:05<00:12,  9.10it/s]\u001b[A\n",
            " 30%|██▉       | 49/165 [00:05<00:12,  9.09it/s]\u001b[A\n",
            " 30%|███       | 50/165 [00:05<00:12,  9.10it/s]\u001b[A\n",
            " 31%|███       | 51/165 [00:05<00:12,  9.11it/s]\u001b[A\n",
            " 32%|███▏      | 52/165 [00:05<00:12,  9.11it/s]\u001b[A\n",
            " 32%|███▏      | 53/165 [00:05<00:12,  9.10it/s]\u001b[A\n",
            " 33%|███▎      | 54/165 [00:05<00:12,  9.07it/s]\u001b[A\n",
            " 33%|███▎      | 55/165 [00:06<00:12,  9.07it/s]\u001b[A\n",
            " 34%|███▍      | 56/165 [00:06<00:12,  9.07it/s]\u001b[A\n",
            " 35%|███▍      | 57/165 [00:06<00:11,  9.06it/s]\u001b[A\n",
            " 35%|███▌      | 58/165 [00:06<00:11,  9.06it/s]\u001b[A\n",
            " 36%|███▌      | 59/165 [00:06<00:11,  9.05it/s]\u001b[A\n",
            " 36%|███▋      | 60/165 [00:06<00:11,  9.08it/s]\u001b[A\n",
            " 37%|███▋      | 61/165 [00:06<00:11,  9.06it/s]\u001b[A\n",
            " 38%|███▊      | 62/165 [00:06<00:11,  9.08it/s]\u001b[A\n",
            " 38%|███▊      | 63/165 [00:06<00:11,  9.05it/s]\u001b[A\n",
            " 39%|███▉      | 64/165 [00:07<00:11,  9.06it/s]\u001b[A\n",
            " 39%|███▉      | 65/165 [00:07<00:11,  9.07it/s]\u001b[A\n",
            " 40%|████      | 66/165 [00:07<00:10,  9.09it/s]\u001b[A\n",
            " 41%|████      | 67/165 [00:07<00:10,  9.10it/s]\u001b[A\n",
            " 41%|████      | 68/165 [00:07<00:10,  9.10it/s]\u001b[A\n",
            " 42%|████▏     | 69/165 [00:07<00:10,  9.12it/s]\u001b[A\n",
            " 42%|████▏     | 70/165 [00:07<00:10,  9.10it/s]\u001b[A\n",
            " 43%|████▎     | 71/165 [00:07<00:10,  9.09it/s]\u001b[A\n",
            " 44%|████▎     | 72/165 [00:07<00:10,  9.09it/s]\u001b[A\n",
            " 44%|████▍     | 73/165 [00:08<00:10,  9.08it/s]\u001b[A\n",
            " 45%|████▍     | 74/165 [00:08<00:10,  9.09it/s]\u001b[A\n",
            " 45%|████▌     | 75/165 [00:08<00:09,  9.10it/s]\u001b[A\n",
            " 46%|████▌     | 76/165 [00:08<00:09,  9.09it/s]\u001b[A\n",
            " 47%|████▋     | 77/165 [00:08<00:09,  9.08it/s]\u001b[A\n",
            " 47%|████▋     | 78/165 [00:08<00:09,  9.04it/s]\u001b[A\n",
            " 48%|████▊     | 79/165 [00:08<00:09,  9.06it/s]\u001b[A\n",
            " 48%|████▊     | 80/165 [00:08<00:09,  9.07it/s]\u001b[A\n",
            " 49%|████▉     | 81/165 [00:08<00:09,  9.08it/s]\u001b[A\n",
            " 50%|████▉     | 82/165 [00:09<00:09,  9.08it/s]\u001b[A\n",
            " 50%|█████     | 83/165 [00:09<00:09,  9.08it/s]\u001b[A\n",
            " 51%|█████     | 84/165 [00:09<00:08,  9.09it/s]\u001b[A\n",
            " 52%|█████▏    | 85/165 [00:09<00:08,  9.09it/s]\u001b[A\n",
            " 52%|█████▏    | 86/165 [00:09<00:08,  9.11it/s]\u001b[A\n",
            " 53%|█████▎    | 87/165 [00:09<00:08,  9.09it/s]\u001b[A\n",
            " 53%|█████▎    | 88/165 [00:09<00:08,  9.09it/s]\u001b[A\n",
            " 54%|█████▍    | 89/165 [00:09<00:08,  8.96it/s]\u001b[A\n",
            " 55%|█████▍    | 90/165 [00:09<00:08,  8.99it/s]\u001b[A\n",
            " 55%|█████▌    | 91/165 [00:10<00:08,  9.02it/s]\u001b[A\n",
            " 56%|█████▌    | 92/165 [00:10<00:08,  9.03it/s]\u001b[A\n",
            " 56%|█████▋    | 93/165 [00:10<00:07,  9.05it/s]\u001b[A\n",
            " 57%|█████▋    | 94/165 [00:10<00:07,  9.07it/s]\u001b[A\n",
            " 58%|█████▊    | 95/165 [00:10<00:07,  9.08it/s]\u001b[A\n",
            " 58%|█████▊    | 96/165 [00:10<00:07,  9.10it/s]\u001b[A\n",
            " 59%|█████▉    | 97/165 [00:10<00:07,  9.06it/s]\u001b[A\n",
            " 59%|█████▉    | 98/165 [00:10<00:07,  9.05it/s]\u001b[A\n",
            " 60%|██████    | 99/165 [00:10<00:07,  9.05it/s]\u001b[A\n",
            " 61%|██████    | 100/165 [00:11<00:07,  9.07it/s]\u001b[A\n",
            " 61%|██████    | 101/165 [00:11<00:07,  9.07it/s]\u001b[A\n",
            " 62%|██████▏   | 102/165 [00:11<00:06,  9.08it/s]\u001b[A\n",
            " 62%|██████▏   | 103/165 [00:11<00:06,  9.08it/s]\u001b[A\n",
            " 63%|██████▎   | 104/165 [00:11<00:06,  9.09it/s]\u001b[A\n",
            " 64%|██████▎   | 105/165 [00:11<00:06,  9.11it/s]\u001b[A\n",
            " 64%|██████▍   | 106/165 [00:11<00:06,  9.10it/s]\u001b[A\n",
            " 65%|██████▍   | 107/165 [00:11<00:06,  9.10it/s]\u001b[A\n",
            " 65%|██████▌   | 108/165 [00:11<00:06,  9.11it/s]\u001b[A\n",
            " 66%|██████▌   | 109/165 [00:12<00:06,  9.12it/s]\u001b[A\n",
            " 67%|██████▋   | 110/165 [00:12<00:06,  9.10it/s]\u001b[A\n",
            " 67%|██████▋   | 111/165 [00:12<00:05,  9.09it/s]\u001b[A\n",
            " 68%|██████▊   | 112/165 [00:12<00:05,  9.10it/s]\u001b[A\n",
            " 68%|██████▊   | 113/165 [00:12<00:05,  9.10it/s]\u001b[A\n",
            " 69%|██████▉   | 114/165 [00:12<00:05,  9.11it/s]\u001b[A\n",
            " 70%|██████▉   | 115/165 [00:12<00:05,  9.08it/s]\u001b[A\n",
            " 70%|███████   | 116/165 [00:12<00:05,  9.05it/s]\u001b[A\n",
            " 71%|███████   | 117/165 [00:12<00:05,  9.05it/s]\u001b[A\n",
            " 72%|███████▏  | 118/165 [00:12<00:05,  9.07it/s]\u001b[A\n",
            " 72%|███████▏  | 119/165 [00:13<00:05,  9.07it/s]\u001b[A\n",
            " 73%|███████▎  | 120/165 [00:13<00:04,  9.07it/s]\u001b[A\n",
            " 73%|███████▎  | 121/165 [00:13<00:04,  9.09it/s]\u001b[A\n",
            " 74%|███████▍  | 122/165 [00:13<00:04,  9.11it/s]\u001b[A\n",
            " 75%|███████▍  | 123/165 [00:13<00:04,  9.12it/s]\u001b[A\n",
            " 75%|███████▌  | 124/165 [00:13<00:04,  9.12it/s]\u001b[A\n",
            " 76%|███████▌  | 125/165 [00:13<00:04,  9.09it/s]\u001b[A\n",
            " 76%|███████▋  | 126/165 [00:13<00:04,  9.09it/s]\u001b[A\n",
            " 77%|███████▋  | 127/165 [00:13<00:04,  9.10it/s]\u001b[A\n",
            " 78%|███████▊  | 128/165 [00:14<00:04,  9.08it/s]\u001b[A\n",
            " 78%|███████▊  | 129/165 [00:14<00:03,  9.09it/s]\u001b[A\n",
            " 79%|███████▉  | 130/165 [00:14<00:03,  9.08it/s]\u001b[A\n",
            " 79%|███████▉  | 131/165 [00:14<00:03,  9.11it/s]\u001b[A\n",
            " 80%|████████  | 132/165 [00:14<00:03,  9.11it/s]\u001b[A\n",
            " 81%|████████  | 133/165 [00:14<00:03,  9.11it/s]\u001b[A\n",
            " 81%|████████  | 134/165 [00:14<00:03,  9.10it/s]\u001b[A\n",
            " 82%|████████▏ | 135/165 [00:14<00:03,  9.08it/s]\u001b[A\n",
            " 82%|████████▏ | 136/165 [00:14<00:03,  9.08it/s]\u001b[A\n",
            " 83%|████████▎ | 137/165 [00:15<00:03,  9.09it/s]\u001b[A\n",
            " 84%|████████▎ | 138/165 [00:15<00:02,  9.10it/s]\u001b[A\n",
            " 84%|████████▍ | 139/165 [00:15<00:02,  9.11it/s]\u001b[A\n",
            " 85%|████████▍ | 140/165 [00:15<00:02,  9.09it/s]\u001b[A\n",
            " 85%|████████▌ | 141/165 [00:15<00:02,  9.09it/s]\u001b[A\n",
            " 86%|████████▌ | 142/165 [00:15<00:02,  9.09it/s]\u001b[A\n",
            " 87%|████████▋ | 143/165 [00:15<00:02,  9.10it/s]\u001b[A\n",
            " 87%|████████▋ | 144/165 [00:15<00:02,  9.10it/s]\u001b[A\n",
            " 88%|████████▊ | 145/165 [00:15<00:02,  9.10it/s]\u001b[A\n",
            " 88%|████████▊ | 146/165 [00:16<00:02,  9.07it/s]\u001b[A\n",
            " 89%|████████▉ | 147/165 [00:16<00:01,  9.08it/s]\u001b[A\n",
            " 90%|████████▉ | 148/165 [00:16<00:01,  9.08it/s]\u001b[A\n",
            " 90%|█████████ | 149/165 [00:16<00:01,  9.09it/s]\u001b[A\n",
            " 91%|█████████ | 150/165 [00:16<00:01,  9.11it/s]\u001b[A\n",
            " 92%|█████████▏| 151/165 [00:16<00:01,  9.13it/s]\u001b[A\n",
            " 92%|█████████▏| 152/165 [00:16<00:01,  9.13it/s]\u001b[A\n",
            " 93%|█████████▎| 153/165 [00:16<00:01,  9.12it/s]\u001b[A\n",
            " 93%|█████████▎| 154/165 [00:16<00:01,  9.13it/s]\u001b[A\n",
            " 94%|█████████▍| 155/165 [00:17<00:01,  9.08it/s]\u001b[A\n",
            " 95%|█████████▍| 156/165 [00:17<00:00,  9.10it/s]\u001b[A\n",
            " 95%|█████████▌| 157/165 [00:17<00:00,  9.08it/s]\u001b[A\n",
            " 96%|█████████▌| 158/165 [00:17<00:00,  9.08it/s]\u001b[A\n",
            " 96%|█████████▋| 159/165 [00:17<00:00,  9.10it/s]\u001b[A\n",
            " 97%|█████████▋| 160/165 [00:17<00:00,  9.12it/s]\u001b[A\n",
            " 98%|█████████▊| 161/165 [00:17<00:00,  9.12it/s]\u001b[A\n",
            " 98%|█████████▊| 162/165 [00:17<00:00,  9.13it/s]\u001b[A\n",
            " 99%|█████████▉| 163/165 [00:17<00:00,  9.13it/s]\u001b[A\n",
            "100%|██████████| 165/165 [00:18<00:00,  9.07it/s]\n",
            "========== model: bert-base-uncased fold: 3 inference ==========\n",
            "========== model: bert-base-uncased fold: 3 inference ==========\n",
            "========== model: bert-base-uncased fold: 3 inference ==========\n",
            "========== model: bert-base-uncased fold: 3 inference ==========\n",
            "========== model: bert-base-uncased fold: 3 inference ==========\n",
            "========== model: bert-base-uncased fold: 3 inference ==========\n",
            "loading configuration file /content/activebus_bert_final/config.json\n",
            "Model config BertConfig {\n",
            "  \"_name_or_path\": \"activebus/BERT-XD_Review\",\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_ids\": 0,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.15.0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "loading weights file /content/activebus_bert_final/pytorch_model.bin\n",
            "Some weights of the model checkpoint at /content/activebus_bert_final were not used when initializing BertForSequenceClassification: ['cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /content/activebus_bert_final and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "\n",
            "  0%|          | 0/165 [00:00<?, ?it/s]\u001b[A\n",
            "  1%|          | 1/165 [00:00<00:19,  8.35it/s]\u001b[A\n",
            "  1%|          | 2/165 [00:00<00:18,  8.76it/s]\u001b[A\n",
            "  2%|▏         | 3/165 [00:00<00:18,  8.87it/s]\u001b[A\n",
            "  2%|▏         | 4/165 [00:00<00:18,  8.94it/s]\u001b[A\n",
            "  3%|▎         | 5/165 [00:00<00:17,  9.00it/s]\u001b[A\n",
            "  4%|▎         | 6/165 [00:00<00:17,  9.03it/s]\u001b[A\n",
            "  4%|▍         | 7/165 [00:00<00:17,  9.06it/s]\u001b[A\n",
            "  5%|▍         | 8/165 [00:00<00:17,  9.04it/s]\u001b[A\n",
            "  5%|▌         | 9/165 [00:01<00:17,  9.07it/s]\u001b[A\n",
            "  6%|▌         | 10/165 [00:01<00:17,  9.07it/s]\u001b[A\n",
            "  7%|▋         | 11/165 [00:01<00:16,  9.08it/s]\u001b[A\n",
            "  7%|▋         | 12/165 [00:01<00:16,  9.10it/s]\u001b[A\n",
            "  8%|▊         | 13/165 [00:01<00:16,  9.10it/s]\u001b[A\n",
            "  8%|▊         | 14/165 [00:01<00:16,  9.09it/s]\u001b[A\n",
            "  9%|▉         | 15/165 [00:01<00:16,  9.10it/s]\u001b[A\n",
            " 10%|▉         | 16/165 [00:01<00:16,  9.07it/s]\u001b[A\n",
            " 10%|█         | 17/165 [00:01<00:16,  9.09it/s]\u001b[A\n",
            " 11%|█         | 18/165 [00:01<00:16,  9.08it/s]\u001b[A\n",
            " 12%|█▏        | 19/165 [00:02<00:16,  9.10it/s]\u001b[A\n",
            " 12%|█▏        | 20/165 [00:02<00:15,  9.10it/s]\u001b[A\n",
            " 13%|█▎        | 21/165 [00:02<00:15,  9.10it/s]\u001b[A\n",
            " 13%|█▎        | 22/165 [00:02<00:15,  9.10it/s]\u001b[A\n",
            " 14%|█▍        | 23/165 [00:02<00:15,  9.11it/s]\u001b[A\n",
            " 15%|█▍        | 24/165 [00:02<00:15,  9.12it/s]\u001b[A\n",
            " 15%|█▌        | 25/165 [00:02<00:15,  9.11it/s]\u001b[A\n",
            " 16%|█▌        | 26/165 [00:02<00:15,  9.11it/s]\u001b[A\n",
            " 16%|█▋        | 27/165 [00:02<00:15,  9.11it/s]\u001b[A\n",
            " 17%|█▋        | 28/165 [00:03<00:15,  9.11it/s]\u001b[A\n",
            " 18%|█▊        | 29/165 [00:03<00:14,  9.12it/s]\u001b[A\n",
            " 18%|█▊        | 30/165 [00:03<00:14,  9.12it/s]\u001b[A\n",
            " 19%|█▉        | 31/165 [00:03<00:14,  9.12it/s]\u001b[A\n",
            " 19%|█▉        | 32/165 [00:03<00:14,  9.13it/s]\u001b[A\n",
            " 20%|██        | 33/165 [00:03<00:14,  9.13it/s]\u001b[A\n",
            " 21%|██        | 34/165 [00:03<00:14,  9.14it/s]\u001b[A\n",
            " 21%|██        | 35/165 [00:03<00:14,  9.14it/s]\u001b[A\n",
            " 22%|██▏       | 36/165 [00:03<00:14,  9.13it/s]\u001b[A\n",
            " 22%|██▏       | 37/165 [00:04<00:14,  9.13it/s]\u001b[A\n",
            " 23%|██▎       | 38/165 [00:04<00:13,  9.11it/s]\u001b[A\n",
            " 24%|██▎       | 39/165 [00:04<00:13,  9.12it/s]\u001b[A\n",
            " 24%|██▍       | 40/165 [00:04<00:13,  9.13it/s]\u001b[A\n",
            " 25%|██▍       | 41/165 [00:04<00:13,  9.12it/s]\u001b[A\n",
            " 25%|██▌       | 42/165 [00:04<00:13,  9.12it/s]\u001b[A\n",
            " 26%|██▌       | 43/165 [00:04<00:13,  9.11it/s]\u001b[A\n",
            " 27%|██▋       | 44/165 [00:04<00:13,  9.12it/s]\u001b[A\n",
            " 27%|██▋       | 45/165 [00:04<00:13,  9.13it/s]\u001b[A\n",
            " 28%|██▊       | 46/165 [00:05<00:13,  9.13it/s]\u001b[A\n",
            " 28%|██▊       | 47/165 [00:05<00:12,  9.11it/s]\u001b[A\n",
            " 29%|██▉       | 48/165 [00:05<00:12,  9.10it/s]\u001b[A\n",
            " 30%|██▉       | 49/165 [00:05<00:12,  9.10it/s]\u001b[A\n",
            " 30%|███       | 50/165 [00:05<00:12,  9.08it/s]\u001b[A\n",
            " 31%|███       | 51/165 [00:05<00:12,  9.09it/s]\u001b[A\n",
            " 32%|███▏      | 52/165 [00:05<00:12,  9.11it/s]\u001b[A\n",
            " 32%|███▏      | 53/165 [00:05<00:12,  9.11it/s]\u001b[A\n",
            " 33%|███▎      | 54/165 [00:05<00:12,  9.09it/s]\u001b[A\n",
            " 33%|███▎      | 55/165 [00:06<00:12,  9.10it/s]\u001b[A\n",
            " 34%|███▍      | 56/165 [00:06<00:11,  9.10it/s]\u001b[A\n",
            " 35%|███▍      | 57/165 [00:06<00:11,  9.11it/s]\u001b[A\n",
            " 35%|███▌      | 58/165 [00:06<00:11,  9.11it/s]\u001b[A\n",
            " 36%|███▌      | 59/165 [00:06<00:11,  9.11it/s]\u001b[A\n",
            " 36%|███▋      | 60/165 [00:06<00:11,  9.10it/s]\u001b[A\n",
            " 37%|███▋      | 61/165 [00:06<00:11,  9.09it/s]\u001b[A\n",
            " 38%|███▊      | 62/165 [00:06<00:11,  9.11it/s]\u001b[A\n",
            " 38%|███▊      | 63/165 [00:06<00:11,  9.11it/s]\u001b[A\n",
            " 39%|███▉      | 64/165 [00:07<00:11,  9.11it/s]\u001b[A\n",
            " 39%|███▉      | 65/165 [00:07<00:10,  9.13it/s]\u001b[A\n",
            " 40%|████      | 66/165 [00:07<00:10,  9.12it/s]\u001b[A\n",
            " 41%|████      | 67/165 [00:07<00:10,  9.12it/s]\u001b[A\n",
            " 41%|████      | 68/165 [00:07<00:10,  9.12it/s]\u001b[A\n",
            " 42%|████▏     | 69/165 [00:07<00:10,  9.12it/s]\u001b[A\n",
            " 42%|████▏     | 70/165 [00:07<00:10,  9.10it/s]\u001b[A\n",
            " 43%|████▎     | 71/165 [00:07<00:10,  9.10it/s]\u001b[A\n",
            " 44%|████▎     | 72/165 [00:07<00:10,  9.12it/s]\u001b[A\n",
            " 44%|████▍     | 73/165 [00:08<00:10,  9.11it/s]\u001b[A\n",
            " 45%|████▍     | 74/165 [00:08<00:10,  9.09it/s]\u001b[A\n",
            " 45%|████▌     | 75/165 [00:08<00:09,  9.10it/s]\u001b[A\n",
            " 46%|████▌     | 76/165 [00:08<00:09,  9.11it/s]\u001b[A\n",
            " 47%|████▋     | 77/165 [00:08<00:09,  9.10it/s]\u001b[A\n",
            " 47%|████▋     | 78/165 [00:08<00:09,  9.11it/s]\u001b[A\n",
            " 48%|████▊     | 79/165 [00:08<00:09,  9.12it/s]\u001b[A\n",
            " 48%|████▊     | 80/165 [00:08<00:09,  9.12it/s]\u001b[A\n",
            " 49%|████▉     | 81/165 [00:08<00:09,  9.10it/s]\u001b[A\n",
            " 50%|████▉     | 82/165 [00:09<00:09,  9.08it/s]\u001b[A\n",
            " 50%|█████     | 83/165 [00:09<00:09,  9.09it/s]\u001b[A\n",
            " 51%|█████     | 84/165 [00:09<00:08,  9.08it/s]\u001b[A\n",
            " 52%|█████▏    | 85/165 [00:09<00:08,  9.09it/s]\u001b[A\n",
            " 52%|█████▏    | 86/165 [00:09<00:08,  9.07it/s]\u001b[A\n",
            " 53%|█████▎    | 87/165 [00:09<00:08,  9.09it/s]\u001b[A\n",
            " 53%|█████▎    | 88/165 [00:09<00:08,  9.09it/s]\u001b[A\n",
            " 54%|█████▍    | 89/165 [00:09<00:08,  9.11it/s]\u001b[A\n",
            " 55%|█████▍    | 90/165 [00:09<00:08,  9.08it/s]\u001b[A\n",
            " 55%|█████▌    | 91/165 [00:10<00:08,  9.08it/s]\u001b[A\n",
            " 56%|█████▌    | 92/165 [00:10<00:08,  9.09it/s]\u001b[A\n",
            " 56%|█████▋    | 93/165 [00:10<00:07,  9.10it/s]\u001b[A\n",
            " 57%|█████▋    | 94/165 [00:10<00:07,  9.09it/s]\u001b[A\n",
            " 58%|█████▊    | 95/165 [00:10<00:07,  9.08it/s]\u001b[A\n",
            " 58%|█████▊    | 96/165 [00:10<00:07,  9.07it/s]\u001b[A\n",
            " 59%|█████▉    | 97/165 [00:10<00:07,  9.09it/s]\u001b[A\n",
            " 59%|█████▉    | 98/165 [00:10<00:07,  9.11it/s]\u001b[A\n",
            " 60%|██████    | 99/165 [00:10<00:07,  9.10it/s]\u001b[A\n",
            " 61%|██████    | 100/165 [00:10<00:07,  9.11it/s]\u001b[A\n",
            " 61%|██████    | 101/165 [00:11<00:07,  9.09it/s]\u001b[A\n",
            " 62%|██████▏   | 102/165 [00:11<00:06,  9.12it/s]\u001b[A\n",
            " 62%|██████▏   | 103/165 [00:11<00:06,  9.12it/s]\u001b[A\n",
            " 63%|██████▎   | 104/165 [00:11<00:06,  9.12it/s]\u001b[A\n",
            " 64%|██████▎   | 105/165 [00:11<00:06,  9.11it/s]\u001b[A\n",
            " 64%|██████▍   | 106/165 [00:11<00:06,  9.11it/s]\u001b[A\n",
            " 65%|██████▍   | 107/165 [00:11<00:06,  9.10it/s]\u001b[A\n",
            " 65%|██████▌   | 108/165 [00:11<00:06,  9.12it/s]\u001b[A\n",
            " 66%|██████▌   | 109/165 [00:11<00:06,  9.13it/s]\u001b[A\n",
            " 67%|██████▋   | 110/165 [00:12<00:06,  9.11it/s]\u001b[A\n",
            " 67%|██████▋   | 111/165 [00:12<00:05,  9.10it/s]\u001b[A\n",
            " 68%|██████▊   | 112/165 [00:12<00:05,  9.11it/s]\u001b[A\n",
            " 68%|██████▊   | 113/165 [00:12<00:05,  9.03it/s]\u001b[A\n",
            " 69%|██████▉   | 114/165 [00:12<00:05,  9.06it/s]\u001b[A\n",
            " 70%|██████▉   | 115/165 [00:12<00:05,  9.07it/s]\u001b[A\n",
            " 70%|███████   | 116/165 [00:12<00:05,  9.09it/s]\u001b[A\n",
            " 71%|███████   | 117/165 [00:12<00:05,  9.10it/s]\u001b[A\n",
            " 72%|███████▏  | 118/165 [00:12<00:05,  9.07it/s]\u001b[A\n",
            " 72%|███████▏  | 119/165 [00:13<00:05,  9.06it/s]\u001b[A\n",
            " 73%|███████▎  | 120/165 [00:13<00:04,  9.08it/s]\u001b[A\n",
            " 73%|███████▎  | 121/165 [00:13<00:04,  9.10it/s]\u001b[A\n",
            " 74%|███████▍  | 122/165 [00:13<00:04,  9.11it/s]\u001b[A\n",
            " 75%|███████▍  | 123/165 [00:13<00:04,  9.08it/s]\u001b[A\n",
            " 75%|███████▌  | 124/165 [00:13<00:04,  9.10it/s]\u001b[A\n",
            " 76%|███████▌  | 125/165 [00:13<00:04,  9.10it/s]\u001b[A\n",
            " 76%|███████▋  | 126/165 [00:13<00:04,  9.12it/s]\u001b[A\n",
            " 77%|███████▋  | 127/165 [00:13<00:04,  9.11it/s]\u001b[A\n",
            " 78%|███████▊  | 128/165 [00:14<00:04,  9.12it/s]\u001b[A\n",
            " 78%|███████▊  | 129/165 [00:14<00:03,  9.11it/s]\u001b[A\n",
            " 79%|███████▉  | 130/165 [00:14<00:03,  9.11it/s]\u001b[A\n",
            " 79%|███████▉  | 131/165 [00:14<00:03,  9.12it/s]\u001b[A\n",
            " 80%|████████  | 132/165 [00:14<00:03,  9.11it/s]\u001b[A\n",
            " 81%|████████  | 133/165 [00:14<00:03,  9.11it/s]\u001b[A\n",
            " 81%|████████  | 134/165 [00:14<00:03,  9.10it/s]\u001b[A\n",
            " 82%|████████▏ | 135/165 [00:14<00:03,  9.10it/s]\u001b[A\n",
            " 82%|████████▏ | 136/165 [00:14<00:03,  9.09it/s]\u001b[A\n",
            " 83%|████████▎ | 137/165 [00:15<00:03,  9.08it/s]\u001b[A\n",
            " 84%|████████▎ | 138/165 [00:15<00:02,  9.06it/s]\u001b[A\n",
            " 84%|████████▍ | 139/165 [00:15<00:02,  9.09it/s]\u001b[A\n",
            " 85%|████████▍ | 140/165 [00:15<00:02,  9.11it/s]\u001b[A\n",
            " 85%|████████▌ | 141/165 [00:15<00:02,  9.12it/s]\u001b[A\n",
            " 86%|████████▌ | 142/165 [00:15<00:02,  9.10it/s]\u001b[A\n",
            " 87%|████████▋ | 143/165 [00:15<00:02,  9.10it/s]\u001b[A\n",
            " 87%|████████▋ | 144/165 [00:15<00:02,  9.05it/s]\u001b[A\n",
            " 88%|████████▊ | 145/165 [00:15<00:02,  9.08it/s]\u001b[A\n",
            " 88%|████████▊ | 146/165 [00:16<00:02,  9.08it/s]\u001b[A\n",
            " 89%|████████▉ | 147/165 [00:16<00:01,  9.07it/s]\u001b[A\n",
            " 90%|████████▉ | 148/165 [00:16<00:01,  9.10it/s]\u001b[A\n",
            " 90%|█████████ | 149/165 [00:16<00:01,  9.09it/s]\u001b[A\n",
            " 91%|█████████ | 150/165 [00:16<00:01,  9.09it/s]\u001b[A\n",
            " 92%|█████████▏| 151/165 [00:16<00:01,  9.09it/s]\u001b[A\n",
            " 92%|█████████▏| 152/165 [00:16<00:01,  9.04it/s]\u001b[A\n",
            " 93%|█████████▎| 153/165 [00:16<00:01,  9.06it/s]\u001b[A\n",
            " 93%|█████████▎| 154/165 [00:16<00:01,  9.06it/s]\u001b[A\n",
            " 94%|█████████▍| 155/165 [00:17<00:01,  9.07it/s]\u001b[A\n",
            " 95%|█████████▍| 156/165 [00:17<00:00,  9.06it/s]\u001b[A\n",
            " 95%|█████████▌| 157/165 [00:17<00:00,  9.09it/s]\u001b[A\n",
            " 96%|█████████▌| 158/165 [00:17<00:00,  9.11it/s]\u001b[A\n",
            " 96%|█████████▋| 159/165 [00:17<00:00,  9.13it/s]\u001b[A\n",
            " 97%|█████████▋| 160/165 [00:17<00:00,  9.14it/s]\u001b[A\n",
            " 98%|█████████▊| 161/165 [00:17<00:00,  9.14it/s]\u001b[A\n",
            " 98%|█████████▊| 162/165 [00:17<00:00,  9.14it/s]\u001b[A\n",
            " 99%|█████████▉| 163/165 [00:17<00:00,  9.14it/s]\u001b[A\n",
            "100%|██████████| 165/165 [00:18<00:00,  9.08it/s]\n",
            "========== model: bert-base-uncased fold: 4 inference ==========\n",
            "========== model: bert-base-uncased fold: 4 inference ==========\n",
            "========== model: bert-base-uncased fold: 4 inference ==========\n",
            "========== model: bert-base-uncased fold: 4 inference ==========\n",
            "========== model: bert-base-uncased fold: 4 inference ==========\n",
            "========== model: bert-base-uncased fold: 4 inference ==========\n",
            "loading configuration file /content/activebus_bert_final/config.json\n",
            "Model config BertConfig {\n",
            "  \"_name_or_path\": \"activebus/BERT-XD_Review\",\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_ids\": 0,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.15.0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "loading weights file /content/activebus_bert_final/pytorch_model.bin\n",
            "Some weights of the model checkpoint at /content/activebus_bert_final were not used when initializing BertForSequenceClassification: ['cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /content/activebus_bert_final and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "\n",
            "  0%|          | 0/165 [00:00<?, ?it/s]\u001b[A\n",
            "  1%|          | 1/165 [00:00<00:20,  8.16it/s]\u001b[A\n",
            "  1%|          | 2/165 [00:00<00:18,  8.71it/s]\u001b[A\n",
            "  2%|▏         | 3/165 [00:00<00:18,  8.89it/s]\u001b[A\n",
            "  2%|▏         | 4/165 [00:00<00:18,  8.90it/s]\u001b[A\n",
            "  3%|▎         | 5/165 [00:00<00:17,  8.97it/s]\u001b[A\n",
            "  4%|▎         | 6/165 [00:00<00:17,  9.03it/s]\u001b[A\n",
            "  4%|▍         | 7/165 [00:00<00:17,  9.06it/s]\u001b[A\n",
            "  5%|▍         | 8/165 [00:00<00:17,  9.07it/s]\u001b[A\n",
            "  5%|▌         | 9/165 [00:01<00:17,  9.08it/s]\u001b[A\n",
            "  6%|▌         | 10/165 [00:01<00:17,  9.10it/s]\u001b[A\n",
            "  7%|▋         | 11/165 [00:01<00:16,  9.10it/s]\u001b[A\n",
            "  7%|▋         | 12/165 [00:01<00:16,  9.08it/s]\u001b[A\n",
            "  8%|▊         | 13/165 [00:01<00:16,  9.10it/s]\u001b[A\n",
            "  8%|▊         | 14/165 [00:01<00:16,  9.12it/s]\u001b[A\n",
            "  9%|▉         | 15/165 [00:01<00:16,  9.12it/s]\u001b[A\n",
            " 10%|▉         | 16/165 [00:01<00:16,  9.10it/s]\u001b[A\n",
            " 10%|█         | 17/165 [00:01<00:16,  9.11it/s]\u001b[A\n",
            " 11%|█         | 18/165 [00:01<00:16,  9.13it/s]\u001b[A\n",
            " 12%|█▏        | 19/165 [00:02<00:15,  9.15it/s]\u001b[A\n",
            " 12%|█▏        | 20/165 [00:02<00:15,  9.14it/s]\u001b[A\n",
            " 13%|█▎        | 21/165 [00:02<00:15,  9.12it/s]\u001b[A\n",
            " 13%|█▎        | 22/165 [00:02<00:15,  9.12it/s]\u001b[A\n",
            " 14%|█▍        | 23/165 [00:02<00:15,  9.12it/s]\u001b[A\n",
            " 15%|█▍        | 24/165 [00:02<00:15,  9.12it/s]\u001b[A\n",
            " 15%|█▌        | 25/165 [00:02<00:15,  9.12it/s]\u001b[A\n",
            " 16%|█▌        | 26/165 [00:02<00:15,  9.13it/s]\u001b[A\n",
            " 16%|█▋        | 27/165 [00:02<00:15,  9.13it/s]\u001b[A\n",
            " 17%|█▋        | 28/165 [00:03<00:15,  9.12it/s]\u001b[A\n",
            " 18%|█▊        | 29/165 [00:03<00:14,  9.11it/s]\u001b[A\n",
            " 18%|█▊        | 30/165 [00:03<00:14,  9.09it/s]\u001b[A\n",
            " 19%|█▉        | 31/165 [00:03<00:14,  9.08it/s]\u001b[A\n",
            " 19%|█▉        | 32/165 [00:03<00:14,  9.09it/s]\u001b[A\n",
            " 20%|██        | 33/165 [00:03<00:14,  9.10it/s]\u001b[A\n",
            " 21%|██        | 34/165 [00:03<00:14,  9.11it/s]\u001b[A\n",
            " 21%|██        | 35/165 [00:03<00:14,  9.11it/s]\u001b[A\n",
            " 22%|██▏       | 36/165 [00:03<00:14,  9.10it/s]\u001b[A\n",
            " 22%|██▏       | 37/165 [00:04<00:14,  9.10it/s]\u001b[A\n",
            " 23%|██▎       | 38/165 [00:04<00:13,  9.09it/s]\u001b[A\n",
            " 24%|██▎       | 39/165 [00:04<00:13,  9.09it/s]\u001b[A\n",
            " 24%|██▍       | 40/165 [00:04<00:13,  9.10it/s]\u001b[A\n",
            " 25%|██▍       | 41/165 [00:04<00:13,  9.11it/s]\u001b[A\n",
            " 25%|██▌       | 42/165 [00:04<00:13,  9.11it/s]\u001b[A\n",
            " 26%|██▌       | 43/165 [00:04<00:13,  9.12it/s]\u001b[A\n",
            " 27%|██▋       | 44/165 [00:04<00:13,  9.12it/s]\u001b[A\n",
            " 27%|██▋       | 45/165 [00:04<00:13,  9.12it/s]\u001b[A\n",
            " 28%|██▊       | 46/165 [00:05<00:13,  9.13it/s]\u001b[A\n",
            " 28%|██▊       | 47/165 [00:05<00:12,  9.13it/s]\u001b[A\n",
            " 29%|██▉       | 48/165 [00:05<00:12,  9.13it/s]\u001b[A\n",
            " 30%|██▉       | 49/165 [00:05<00:12,  9.12it/s]\u001b[A\n",
            " 30%|███       | 50/165 [00:05<00:12,  9.10it/s]\u001b[A\n",
            " 31%|███       | 51/165 [00:05<00:12,  9.10it/s]\u001b[A\n",
            " 32%|███▏      | 52/165 [00:05<00:12,  9.10it/s]\u001b[A\n",
            " 32%|███▏      | 53/165 [00:05<00:12,  9.12it/s]\u001b[A\n",
            " 33%|███▎      | 54/165 [00:05<00:12,  9.13it/s]\u001b[A\n",
            " 33%|███▎      | 55/165 [00:06<00:12,  9.12it/s]\u001b[A\n",
            " 34%|███▍      | 56/165 [00:06<00:11,  9.11it/s]\u001b[A\n",
            " 35%|███▍      | 57/165 [00:06<00:11,  9.10it/s]\u001b[A\n",
            " 35%|███▌      | 58/165 [00:06<00:11,  9.10it/s]\u001b[A\n",
            " 36%|███▌      | 59/165 [00:06<00:11,  9.09it/s]\u001b[A\n",
            " 36%|███▋      | 60/165 [00:06<00:11,  9.10it/s]\u001b[A\n",
            " 37%|███▋      | 61/165 [00:06<00:11,  9.10it/s]\u001b[A\n",
            " 38%|███▊      | 62/165 [00:06<00:11,  9.09it/s]\u001b[A\n",
            " 38%|███▊      | 63/165 [00:06<00:11,  9.10it/s]\u001b[A\n",
            " 39%|███▉      | 64/165 [00:07<00:11,  9.11it/s]\u001b[A\n",
            " 39%|███▉      | 65/165 [00:07<00:11,  9.09it/s]\u001b[A\n",
            " 40%|████      | 66/165 [00:07<00:10,  9.10it/s]\u001b[A\n",
            " 41%|████      | 67/165 [00:07<00:10,  9.10it/s]\u001b[A\n",
            " 41%|████      | 68/165 [00:07<00:10,  9.11it/s]\u001b[A\n",
            " 42%|████▏     | 69/165 [00:07<00:10,  9.11it/s]\u001b[A\n",
            " 42%|████▏     | 70/165 [00:07<00:10,  9.11it/s]\u001b[A\n",
            " 43%|████▎     | 71/165 [00:07<00:10,  9.09it/s]\u001b[A\n",
            " 44%|████▎     | 72/165 [00:07<00:10,  9.08it/s]\u001b[A\n",
            " 44%|████▍     | 73/165 [00:08<00:10,  9.09it/s]\u001b[A\n",
            " 45%|████▍     | 74/165 [00:08<00:09,  9.10it/s]\u001b[A\n",
            " 45%|████▌     | 75/165 [00:08<00:09,  9.11it/s]\u001b[A\n",
            " 46%|████▌     | 76/165 [00:08<00:09,  9.10it/s]\u001b[A\n",
            " 47%|████▋     | 77/165 [00:08<00:09,  9.11it/s]\u001b[A\n",
            " 47%|████▋     | 78/165 [00:08<00:09,  9.10it/s]\u001b[A\n",
            " 48%|████▊     | 79/165 [00:08<00:09,  9.10it/s]\u001b[A\n",
            " 48%|████▊     | 80/165 [00:08<00:09,  9.09it/s]\u001b[A\n",
            " 49%|████▉     | 81/165 [00:08<00:09,  9.11it/s]\u001b[A\n",
            " 50%|████▉     | 82/165 [00:09<00:09,  9.11it/s]\u001b[A\n",
            " 50%|█████     | 83/165 [00:09<00:08,  9.11it/s]\u001b[A\n",
            " 51%|█████     | 84/165 [00:09<00:08,  9.08it/s]\u001b[A\n",
            " 52%|█████▏    | 85/165 [00:09<00:08,  9.07it/s]\u001b[A\n",
            " 52%|█████▏    | 86/165 [00:09<00:08,  9.09it/s]\u001b[A\n",
            " 53%|█████▎    | 87/165 [00:09<00:08,  9.09it/s]\u001b[A\n",
            " 53%|█████▎    | 88/165 [00:09<00:08,  9.07it/s]\u001b[A\n",
            " 54%|█████▍    | 89/165 [00:09<00:08,  9.08it/s]\u001b[A\n",
            " 55%|█████▍    | 90/165 [00:09<00:08,  9.08it/s]\u001b[A\n",
            " 55%|█████▌    | 91/165 [00:10<00:08,  9.09it/s]\u001b[A\n",
            " 56%|█████▌    | 92/165 [00:10<00:08,  9.10it/s]\u001b[A\n",
            " 56%|█████▋    | 93/165 [00:10<00:07,  9.10it/s]\u001b[A\n",
            " 57%|█████▋    | 94/165 [00:10<00:07,  9.11it/s]\u001b[A\n",
            " 58%|█████▊    | 95/165 [00:10<00:07,  9.10it/s]\u001b[A\n",
            " 58%|█████▊    | 96/165 [00:10<00:07,  9.09it/s]\u001b[A\n",
            " 59%|█████▉    | 97/165 [00:10<00:07,  9.04it/s]\u001b[A\n",
            " 59%|█████▉    | 98/165 [00:10<00:07,  9.06it/s]\u001b[A\n",
            " 60%|██████    | 99/165 [00:10<00:07,  9.06it/s]\u001b[A\n",
            " 61%|██████    | 100/165 [00:10<00:07,  9.08it/s]\u001b[A\n",
            " 61%|██████    | 101/165 [00:11<00:07,  9.08it/s]\u001b[A\n",
            " 62%|██████▏   | 102/165 [00:11<00:06,  9.07it/s]\u001b[A\n",
            " 62%|██████▏   | 103/165 [00:11<00:06,  9.05it/s]\u001b[A\n",
            " 63%|██████▎   | 104/165 [00:11<00:06,  9.07it/s]\u001b[A\n",
            " 64%|██████▎   | 105/165 [00:11<00:06,  9.08it/s]\u001b[A\n",
            " 64%|██████▍   | 106/165 [00:11<00:06,  9.08it/s]\u001b[A\n",
            " 65%|██████▍   | 107/165 [00:11<00:06,  9.10it/s]\u001b[A\n",
            " 65%|██████▌   | 108/165 [00:11<00:06,  9.10it/s]\u001b[A\n",
            " 66%|██████▌   | 109/165 [00:11<00:06,  9.10it/s]\u001b[A\n",
            " 67%|██████▋   | 110/165 [00:12<00:06,  9.10it/s]\u001b[A\n",
            " 67%|██████▋   | 111/165 [00:12<00:05,  9.09it/s]\u001b[A\n",
            " 68%|██████▊   | 112/165 [00:12<00:05,  9.08it/s]\u001b[A\n",
            " 68%|██████▊   | 113/165 [00:12<00:05,  9.10it/s]\u001b[A\n",
            " 69%|██████▉   | 114/165 [00:12<00:05,  9.09it/s]\u001b[A\n",
            " 70%|██████▉   | 115/165 [00:12<00:05,  9.10it/s]\u001b[A\n",
            " 70%|███████   | 116/165 [00:12<00:05,  9.08it/s]\u001b[A\n",
            " 71%|███████   | 117/165 [00:12<00:05,  9.07it/s]\u001b[A\n",
            " 72%|███████▏  | 118/165 [00:12<00:05,  9.09it/s]\u001b[A\n",
            " 72%|███████▏  | 119/165 [00:13<00:05,  9.08it/s]\u001b[A\n",
            " 73%|███████▎  | 120/165 [00:13<00:04,  9.08it/s]\u001b[A\n",
            " 73%|███████▎  | 121/165 [00:13<00:04,  9.09it/s]\u001b[A\n",
            " 74%|███████▍  | 122/165 [00:13<00:04,  9.10it/s]\u001b[A\n",
            " 75%|███████▍  | 123/165 [00:13<00:04,  9.11it/s]\u001b[A\n",
            " 75%|███████▌  | 124/165 [00:13<00:04,  9.12it/s]\u001b[A\n",
            " 76%|███████▌  | 125/165 [00:13<00:04,  9.12it/s]\u001b[A\n",
            " 76%|███████▋  | 126/165 [00:13<00:04,  9.09it/s]\u001b[A\n",
            " 77%|███████▋  | 127/165 [00:13<00:04,  9.09it/s]\u001b[A\n",
            " 78%|███████▊  | 128/165 [00:14<00:04,  9.08it/s]\u001b[A\n",
            " 78%|███████▊  | 129/165 [00:14<00:03,  9.11it/s]\u001b[A\n",
            " 79%|███████▉  | 130/165 [00:14<00:03,  9.08it/s]\u001b[A\n",
            " 79%|███████▉  | 131/165 [00:14<00:03,  9.10it/s]\u001b[A\n",
            " 80%|████████  | 132/165 [00:14<00:03,  9.08it/s]\u001b[A\n",
            " 81%|████████  | 133/165 [00:14<00:03,  9.08it/s]\u001b[A\n",
            " 81%|████████  | 134/165 [00:14<00:03,  9.08it/s]\u001b[A\n",
            " 82%|████████▏ | 135/165 [00:14<00:03,  9.09it/s]\u001b[A\n",
            " 82%|████████▏ | 136/165 [00:14<00:03,  9.11it/s]\u001b[A\n",
            " 83%|████████▎ | 137/165 [00:15<00:03,  9.08it/s]\u001b[A\n",
            " 84%|████████▎ | 138/165 [00:15<00:03,  8.97it/s]\u001b[A\n",
            " 84%|████████▍ | 139/165 [00:15<00:02,  9.02it/s]\u001b[A\n",
            " 85%|████████▍ | 140/165 [00:15<00:02,  9.06it/s]\u001b[A\n",
            " 85%|████████▌ | 141/165 [00:15<00:02,  9.07it/s]\u001b[A\n",
            " 86%|████████▌ | 142/165 [00:15<00:02,  9.09it/s]\u001b[A\n",
            " 87%|████████▋ | 143/165 [00:15<00:02,  9.10it/s]\u001b[A\n",
            " 87%|████████▋ | 144/165 [00:15<00:02,  9.09it/s]\u001b[A\n",
            " 88%|████████▊ | 145/165 [00:15<00:02,  9.08it/s]\u001b[A\n",
            " 88%|████████▊ | 146/165 [00:16<00:02,  9.09it/s]\u001b[A\n",
            " 89%|████████▉ | 147/165 [00:16<00:01,  9.09it/s]\u001b[A\n",
            " 90%|████████▉ | 148/165 [00:16<00:01,  9.09it/s]\u001b[A\n",
            " 90%|█████████ | 149/165 [00:16<00:01,  9.10it/s]\u001b[A\n",
            " 91%|█████████ | 150/165 [00:16<00:01,  9.09it/s]\u001b[A\n",
            " 92%|█████████▏| 151/165 [00:16<00:01,  9.09it/s]\u001b[A\n",
            " 92%|█████████▏| 152/165 [00:16<00:01,  9.10it/s]\u001b[A\n",
            " 93%|█████████▎| 153/165 [00:16<00:01,  9.10it/s]\u001b[A\n",
            " 93%|█████████▎| 154/165 [00:16<00:01,  9.10it/s]\u001b[A\n",
            " 94%|█████████▍| 155/165 [00:17<00:01,  9.09it/s]\u001b[A\n",
            " 95%|█████████▍| 156/165 [00:17<00:00,  9.08it/s]\u001b[A\n",
            " 95%|█████████▌| 157/165 [00:17<00:00,  9.10it/s]\u001b[A\n",
            " 96%|█████████▌| 158/165 [00:17<00:00,  9.12it/s]\u001b[A\n",
            " 96%|█████████▋| 159/165 [00:17<00:00,  9.14it/s]\u001b[A\n",
            " 97%|█████████▋| 160/165 [00:17<00:00,  9.14it/s]\u001b[A\n",
            " 98%|█████████▊| 161/165 [00:17<00:00,  9.14it/s]\u001b[A\n",
            " 98%|█████████▊| 162/165 [00:17<00:00,  9.14it/s]\u001b[A\n",
            " 99%|█████████▉| 163/165 [00:17<00:00,  9.14it/s]\u001b[A\n",
            "100%|██████████| 165/165 [00:18<00:00,  9.08it/s]\n",
            "100%|██████████| 6/6 [4:45:58<00:00, 2859.83s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "KdrYQ-ivV5zi"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "TARGETS=['Components',\t'Design and Aesthetics',\t'Installation']"
      ],
      "metadata": {
        "id": "zY0qeUJ7HOnl"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for Target in tqdm(TARGETS):\n",
        "    def init_logger(log_file=OUTPUT_DIR + \"train.log\"):\n",
        "        from logging import INFO, FileHandler, Formatter, StreamHandler, getLogger\n",
        "\n",
        "        logger = getLogger(__name__)\n",
        "        logger.setLevel(INFO)\n",
        "        handler1 = StreamHandler()\n",
        "        handler1.setFormatter(Formatter(\"%(message)s\"))\n",
        "        handler2 = FileHandler(filename=log_file)\n",
        "        handler2.setFormatter(Formatter(\"%(message)s\"))\n",
        "        logger.addHandler(handler1)\n",
        "        logger.addHandler(handler2)\n",
        "        return logger\n",
        "\n",
        "    LOGGER = init_logger()\n",
        "\n",
        "    def seed_torch(seed=13):\n",
        "        random.seed(seed)\n",
        "        os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
        "        np.random.seed(seed)\n",
        "        torch.manual_seed(seed)\n",
        "        torch.cuda.manual_seed(seed)\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "        torch.backends.cudnn.deterministic = True\n",
        "\n",
        "    seed = 13\n",
        "    seed_torch(seed)\n",
        "\n",
        "    \n",
        "    train = pd.read_csv(DATA_DIR + \"train.csv\",usecols=['Review',Target])\n",
        "    test = pd.read_csv(DATA_DIR + \"test.csv\",usecols=['Review',Target])\n",
        "    train['Review'] = train['Review'].map(lambda x: re.sub(r'\\W+', ' ', x))\n",
        "    test['Review'] = test['Review'].map(lambda x: re.sub(r'\\W+', ' ', x))\n",
        "    train['Review']=train['Review'].str.lower()\n",
        "    test['Review']=test['Review'].str.lower()\n",
        "    sub = pd.read_csv(\"/content/submission.csv\")\n",
        "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "    \n",
        "\n",
        "\n",
        "    def get_train_data(train):\n",
        "\n",
        "        Fold = StratifiedKFold(n_splits=5, shuffle=True, random_state=seed)\n",
        "        for n, (train_index, val_index) in enumerate(Fold.split(train, train[Target])):\n",
        "            train.loc[val_index, \"fold\"] = int(n)\n",
        "        train[\"fold\"] = train[\"fold\"].astype(np.uint8)\n",
        "\n",
        "        return train\n",
        "\n",
        "    def get_test_data(test):\n",
        "        return test\n",
        "\n",
        "    train = get_train_data(train)\n",
        "\n",
        "    class BaseDataset(Dataset):\n",
        "        def __init__(self, df, model_name, include_labels=True):\n",
        "            tokenizer = T.BertTokenizer.from_pretrained(model_name)\n",
        "\n",
        "            self.df = df\n",
        "            self.include_labels = include_labels\n",
        "\n",
        "            self.title = df[\"Review\"].tolist()\n",
        "            self.encoded = tokenizer.batch_encode_plus(\n",
        "                self.title,\n",
        "                padding = 'max_length',            \n",
        "                max_length = 192,\n",
        "                truncation = True,\n",
        "                return_attention_mask=True\n",
        "            )\n",
        "            \n",
        "            if self.include_labels:\n",
        "                self.labels = df[Target].values\n",
        "\n",
        "        def __len__(self):\n",
        "            return len(self.df)\n",
        "\n",
        "        def __getitem__(self, idx):\n",
        "            input_ids = torch.tensor(self.encoded['input_ids'][idx])\n",
        "            attention_mask = torch.tensor(self.encoded['attention_mask'][idx])\n",
        "\n",
        "            if self.include_labels:\n",
        "                label = torch.tensor(self.labels[idx]).float()\n",
        "                return input_ids, attention_mask, label\n",
        "\n",
        "            return input_ids, attention_mask\n",
        "\n",
        "    class BaseModel(nn.Module):\n",
        "        def __init__(self, model_name):\n",
        "            super().__init__()\n",
        "\n",
        "            self.model = T.BertForSequenceClassification.from_pretrained(model_name, num_labels=1)\n",
        "            self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "        def forward(self, input_ids, attention_mask):\n",
        "            out = self.model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "            out = self.sigmoid(out.logits).squeeze()\n",
        "\n",
        "            return out\n",
        "\n",
        "    class AverageMeter(object):\n",
        "        \"\"\"Computes and stores the average and current value\"\"\"\n",
        "\n",
        "        def __init__(self):\n",
        "            self.reset()\n",
        "\n",
        "        def reset(self):\n",
        "            self.val = 0\n",
        "            self.avg = 0\n",
        "            self.sum = 0\n",
        "            self.count = 0\n",
        "\n",
        "        def update(self, val, n=1):\n",
        "            self.val = val\n",
        "            self.sum += val * n\n",
        "            self.count += n\n",
        "            self.avg = self.sum / self.count\n",
        "\n",
        "\n",
        "    def asMinutes(s):\n",
        "        m = math.floor(s / 60)\n",
        "        s -= m * 60\n",
        "        return \"%dm %ds\" % (m, s)\n",
        "\n",
        "\n",
        "    def timeSince(since, percent):\n",
        "        now = time.time()\n",
        "        s = now - since\n",
        "        es = s / (percent)\n",
        "        rs = es - s\n",
        "        return \"%s (remain %s)\" % (asMinutes(s), asMinutes(rs))\n",
        "\n",
        "    def train_fn(train_loader, model, criterion, optimizer, epoch, device):\n",
        "        start = end = time.time()\n",
        "        losses = AverageMeter()\n",
        "\n",
        "        # switch to train mode\n",
        "        model.train()\n",
        "\n",
        "        for step, (input_ids, attention_mask, labels) in enumerate(train_loader):\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            input_ids = input_ids.to(device)\n",
        "            attention_mask = attention_mask.to(device)\n",
        "            labels = labels.to(device)\n",
        "            batch_size = labels.size(0)\n",
        "\n",
        "            y_preds = model(input_ids, attention_mask)\n",
        "\n",
        "            loss = criterion(y_preds, labels)\n",
        "\n",
        "            # record loss\n",
        "            losses.update(loss.item(), batch_size)\n",
        "            loss.backward()\n",
        "\n",
        "            optimizer.step()\n",
        "\n",
        "            if step % 100 == 0 or step == (len(train_loader) - 1):\n",
        "                print(\n",
        "                    f\"Epoch: [{epoch + 1}][{step}/{len(train_loader)}] \"\n",
        "                    f\"Elapsed {timeSince(start, float(step + 1) / len(train_loader)):s} \"\n",
        "                    f\"Loss: {losses.avg:.4f} \"\n",
        "                )\n",
        "\n",
        "        return losses.avg\n",
        "\n",
        "    def valid_fn(valid_loader, model, criterion, device):\n",
        "        start = end = time.time()\n",
        "        losses = AverageMeter()\n",
        "\n",
        "        # switch to evaluation mode\n",
        "        model.eval()\n",
        "        preds = []\n",
        "\n",
        "        for step, (input_ids, attention_mask, labels) in enumerate(valid_loader):\n",
        "            input_ids = input_ids.to(device)\n",
        "            attention_mask = attention_mask.to(device)\n",
        "            labels = labels.to(device)\n",
        "            batch_size = labels.size(0)\n",
        "            # compute loss\n",
        "            with torch.no_grad():\n",
        "                y_preds = model(input_ids, attention_mask)\n",
        "            \n",
        "            loss = criterion(y_preds, labels)\n",
        "            losses.update(loss.item(), batch_size)\n",
        "\n",
        "            # record score\n",
        "            preds.append(y_preds.to(\"cpu\").numpy())\n",
        "\n",
        "            if step % 100 == 0 or step == (len(valid_loader) - 1):\n",
        "                print(\n",
        "                    f\"EVAL: [{step}/{len(valid_loader)}] \"\n",
        "                    f\"Elapsed {timeSince(start, float(step + 1) / len(valid_loader)):s} \"\n",
        "                    f\"Loss: {losses.avg:.4f} \"\n",
        "                )\n",
        "\n",
        "        predictions = np.concatenate(preds)\n",
        "        return losses.avg, predictions\n",
        "\n",
        "    def inference():\n",
        "        predictions = []\n",
        "\n",
        "        test_dataset = BaseDataset(test, MODEL_NAME, include_labels=False)\n",
        "        test_loader = DataLoader(\n",
        "            test_dataset, batch_size=16, shuffle=False, num_workers=4, pin_memory=True\n",
        "        )\n",
        "\n",
        "        for fold in range(5):\n",
        "            LOGGER.info(f\"========== model: bert-base-uncased fold: {fold} inference ==========\")\n",
        "            model = BaseModel(MODEL_NAME)\n",
        "            model.to(device)\n",
        "            model.load_state_dict(torch.load(OUTPUT_DIR + f\"bert-base-uncased_fold{fold}_best.pth\")[\"model\"])\n",
        "            model.eval()\n",
        "            preds = []\n",
        "            for i, (input_ids, attention_mask) in tqdm(enumerate(test_loader), total=len(test_loader)):\n",
        "                input_ids = input_ids.to(device)\n",
        "                attention_mask = attention_mask.to(device)\n",
        "                with torch.no_grad():\n",
        "                    y_preds = model(input_ids, attention_mask)\n",
        "                preds.append(y_preds.to(\"cpu\").numpy())\n",
        "            preds = np.concatenate(preds)\n",
        "            predictions.append(preds)\n",
        "        predictions = np.mean(predictions, axis=0)\n",
        "\n",
        "        return predictions\n",
        "\n",
        "    def train_loop(train, fold):\n",
        "\n",
        "        LOGGER.info(f\"========== fold: {fold} training ==========\")\n",
        "\n",
        "        # ====================================================\n",
        "        # Data Loader\n",
        "        # ====================================================\n",
        "        trn_idx = train[train[\"fold\"] != fold].index\n",
        "        val_idx = train[train[\"fold\"] == fold].index\n",
        "\n",
        "        train_folds = train.loc[trn_idx].reset_index(drop=True)\n",
        "        valid_folds = train.loc[val_idx].reset_index(drop=True)\n",
        "\n",
        "        train_dataset = BaseDataset(train_folds,MODEL_NAME)\n",
        "        valid_dataset = BaseDataset(valid_folds, MODEL_NAME)\n",
        "\n",
        "        train_loader = DataLoader(\n",
        "            train_dataset,\n",
        "            batch_size=16,\n",
        "            shuffle=True,\n",
        "            num_workers=4,\n",
        "            pin_memory=True,\n",
        "            drop_last=True,\n",
        "        )\n",
        "        valid_loader = DataLoader(\n",
        "            valid_dataset,\n",
        "            batch_size=16,\n",
        "            shuffle=False,\n",
        "            num_workers=4,\n",
        "            pin_memory=True,\n",
        "            drop_last=False,\n",
        "        )\n",
        "\n",
        "        # ====================================================\n",
        "        # Model\n",
        "        # ====================================================\n",
        "        model = BaseModel(MODEL_NAME)\n",
        "        model.to(device)\n",
        "\n",
        "        optimizer = T.AdamW(model.parameters(),lr=2e-5)\n",
        "\n",
        "        criterion = nn.BCELoss()\n",
        "\n",
        "        # ====================================================\n",
        "        # Loop\n",
        "        # ====================================================\n",
        "        best_score = 100\n",
        "        best_loss = np.inf\n",
        "\n",
        "        for epoch in range(5):\n",
        "            start_time = time.time()\n",
        "            \n",
        "            # train\n",
        "            avg_loss = train_fn(train_loader, model, criterion, optimizer, epoch, device)\n",
        "\n",
        "            # eval\n",
        "            avg_val_loss, preds = valid_fn(valid_loader, model, criterion, device)\n",
        "            valid_labels = valid_folds[Target].values\n",
        "\n",
        "            # scoring\n",
        "            score = log_loss(valid_labels, preds)\n",
        "            print(f\"Log loss for {epoch} is {score}\")\n",
        "            elapsed = time.time() - start_time\n",
        "            LOGGER.info(\n",
        "                f\"Epoch {epoch+1} - avg_train_loss: {avg_loss:.4f}  avg_val_loss: {avg_val_loss:.4f}  time: {elapsed:.0f}s\"\n",
        "            )\n",
        "            LOGGER.info(f\"Epoch {epoch+1} - Score: {score}\")\n",
        "\n",
        "            if score < best_score:\n",
        "                best_score = score\n",
        "                LOGGER.info(f\"Epoch {epoch+1} - Save Best Score: {best_score:.4f} Model\")\n",
        "                torch.save(\n",
        "                    {\"model\": model.state_dict(), \"preds\": preds}, OUTPUT_DIR + f\"bert-base-uncased_fold{fold}_best.pth\"\n",
        "                )\n",
        "\n",
        "        check_point = torch.load(OUTPUT_DIR + f\"bert-base-uncased_fold{fold}_best.pth\")\n",
        "\n",
        "        valid_folds[\"preds\"] = check_point[\"preds\"]\n",
        "\n",
        "        return valid_folds\n",
        "\n",
        "    from sklearn.metrics import log_loss\n",
        "    def get_result(result_df):\n",
        "        preds = result_df[\"preds\"].values\n",
        "        labels = result_df[Target].values\n",
        "        score = log_loss(labels, preds)\n",
        "        LOGGER.info(f\"Score: {score:<.5f}\")\n",
        "\n",
        "    def main():\n",
        "        # Training\n",
        "        oof_df = pd.DataFrame()\n",
        "        for fold in range(5):\n",
        "            _oof_df = train_loop(train, fold)\n",
        "            oof_df = pd.concat([oof_df, _oof_df])\n",
        "            LOGGER.info(f\"========== fold: {fold} result ==========\")\n",
        "            get_result(_oof_df)\n",
        "            \n",
        "        # CV result\n",
        "        LOGGER.info(f\"========== CV ==========\")\n",
        "        get_result(oof_df)\n",
        "        \n",
        "        # Save OOF result\n",
        "        oof_df.to_csv(OUTPUT_DIR + \"oof_df.csv\", index=False)\n",
        "\n",
        "        # Inference\n",
        "        predictions = inference()\n",
        "        \n",
        "        # submission\n",
        "        sub[Target] = predictions\n",
        "        sub.to_csv('/content/submission.csv', index=False)\n",
        "\n",
        "    if __name__ == \"__main__\":\n",
        "        main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KMLhcSENHj-r",
        "outputId": "43d494b9-799c-420f-a3d3-8197344470a8"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/3 [00:00<?, ?it/s]Didn't find file /content/activebus_bert_final/added_tokens.json. We won't load it.\n",
            "loading file /content/activebus_bert_final/vocab.txt\n",
            "loading file /content/activebus_bert_final/tokenizer.json\n",
            "loading file None\n",
            "loading file /content/activebus_bert_final/special_tokens_map.json\n",
            "loading file /content/activebus_bert_final/tokenizer_config.json\n",
            "========== fold: 0 training ==========\n",
            "========== fold: 0 training ==========\n",
            "========== fold: 0 training ==========\n",
            "========== fold: 0 training ==========\n",
            "========== fold: 0 training ==========\n",
            "========== fold: 0 training ==========\n",
            "========== fold: 0 training ==========\n",
            "Didn't find file /content/activebus_bert_final/added_tokens.json. We won't load it.\n",
            "loading file /content/activebus_bert_final/vocab.txt\n",
            "loading file None\n",
            "loading file /content/activebus_bert_final/special_tokens_map.json\n",
            "loading file /content/activebus_bert_final/tokenizer_config.json\n",
            "loading file /content/activebus_bert_final/tokenizer.json\n",
            "Didn't find file /content/activebus_bert_final/added_tokens.json. We won't load it.\n",
            "loading file /content/activebus_bert_final/vocab.txt\n",
            "loading file None\n",
            "loading file /content/activebus_bert_final/special_tokens_map.json\n",
            "loading file /content/activebus_bert_final/tokenizer_config.json\n",
            "loading file /content/activebus_bert_final/tokenizer.json\n",
            "loading configuration file /content/activebus_bert_final/config.json\n",
            "Model config BertConfig {\n",
            "  \"_name_or_path\": \"activebus/BERT-XD_Review\",\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_ids\": 0,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.15.0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "loading weights file /content/activebus_bert_final/pytorch_model.bin\n",
            "Some weights of the model checkpoint at /content/activebus_bert_final were not used when initializing BertForSequenceClassification: ['cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /content/activebus_bert_final and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: [1][0/306] Elapsed 0m 0s (remain 2m 56s) Loss: 0.7026 \n",
            "Epoch: [1][100/306] Elapsed 0m 31s (remain 1m 3s) Loss: 0.2203 \n",
            "Epoch: [1][200/306] Elapsed 1m 2s (remain 0m 32s) Loss: 0.1701 \n",
            "Epoch: [1][300/306] Elapsed 1m 33s (remain 0m 1s) Loss: 0.1530 \n",
            "Epoch: [1][305/306] Elapsed 1m 34s (remain 0m 0s) Loss: 0.1520 \n",
            "EVAL: [0/77] Elapsed 0m 0s (remain 0m 25s) Loss: 0.1294 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1 - avg_train_loss: 0.1520  avg_val_loss: 0.0830  time: 103s\n",
            "Epoch 1 - avg_train_loss: 0.1520  avg_val_loss: 0.0830  time: 103s\n",
            "Epoch 1 - avg_train_loss: 0.1520  avg_val_loss: 0.0830  time: 103s\n",
            "Epoch 1 - avg_train_loss: 0.1520  avg_val_loss: 0.0830  time: 103s\n",
            "Epoch 1 - avg_train_loss: 0.1520  avg_val_loss: 0.0830  time: 103s\n",
            "Epoch 1 - avg_train_loss: 0.1520  avg_val_loss: 0.0830  time: 103s\n",
            "Epoch 1 - avg_train_loss: 0.1520  avg_val_loss: 0.0830  time: 103s\n",
            "Epoch 1 - Score: 0.08297779627374892\n",
            "Epoch 1 - Score: 0.08297779627374892\n",
            "Epoch 1 - Score: 0.08297779627374892\n",
            "Epoch 1 - Score: 0.08297779627374892\n",
            "Epoch 1 - Score: 0.08297779627374892\n",
            "Epoch 1 - Score: 0.08297779627374892\n",
            "Epoch 1 - Score: 0.08297779627374892\n",
            "Epoch 1 - Save Best Score: 0.0830 Model\n",
            "Epoch 1 - Save Best Score: 0.0830 Model\n",
            "Epoch 1 - Save Best Score: 0.0830 Model\n",
            "Epoch 1 - Save Best Score: 0.0830 Model\n",
            "Epoch 1 - Save Best Score: 0.0830 Model\n",
            "Epoch 1 - Save Best Score: 0.0830 Model\n",
            "Epoch 1 - Save Best Score: 0.0830 Model\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EVAL: [76/77] Elapsed 0m 8s (remain 0m 0s) Loss: 0.0830 \n",
            "Log loss for 0 is 0.08297779627374892\n",
            "Epoch: [2][0/306] Elapsed 0m 0s (remain 2m 54s) Loss: 0.0139 \n",
            "Epoch: [2][100/306] Elapsed 0m 31s (remain 1m 3s) Loss: 0.0688 \n",
            "Epoch: [2][200/306] Elapsed 1m 2s (remain 0m 32s) Loss: 0.0769 \n",
            "Epoch: [2][300/306] Elapsed 1m 33s (remain 0m 1s) Loss: 0.0783 \n",
            "Epoch: [2][305/306] Elapsed 1m 34s (remain 0m 0s) Loss: 0.0801 \n",
            "EVAL: [0/77] Elapsed 0m 0s (remain 0m 28s) Loss: 0.1317 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2 - avg_train_loss: 0.0801  avg_val_loss: 0.0908  time: 103s\n",
            "Epoch 2 - avg_train_loss: 0.0801  avg_val_loss: 0.0908  time: 103s\n",
            "Epoch 2 - avg_train_loss: 0.0801  avg_val_loss: 0.0908  time: 103s\n",
            "Epoch 2 - avg_train_loss: 0.0801  avg_val_loss: 0.0908  time: 103s\n",
            "Epoch 2 - avg_train_loss: 0.0801  avg_val_loss: 0.0908  time: 103s\n",
            "Epoch 2 - avg_train_loss: 0.0801  avg_val_loss: 0.0908  time: 103s\n",
            "Epoch 2 - avg_train_loss: 0.0801  avg_val_loss: 0.0908  time: 103s\n",
            "Epoch 2 - Score: 0.09075664963900469\n",
            "Epoch 2 - Score: 0.09075664963900469\n",
            "Epoch 2 - Score: 0.09075664963900469\n",
            "Epoch 2 - Score: 0.09075664963900469\n",
            "Epoch 2 - Score: 0.09075664963900469\n",
            "Epoch 2 - Score: 0.09075664963900469\n",
            "Epoch 2 - Score: 0.09075664963900469\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EVAL: [76/77] Elapsed 0m 8s (remain 0m 0s) Loss: 0.0908 \n",
            "Log loss for 1 is 0.09075664963900469\n",
            "Epoch: [3][0/306] Elapsed 0m 0s (remain 2m 49s) Loss: 0.1378 \n",
            "Epoch: [3][100/306] Elapsed 0m 31s (remain 1m 3s) Loss: 0.0394 \n",
            "Epoch: [3][200/306] Elapsed 1m 2s (remain 0m 32s) Loss: 0.0378 \n",
            "Epoch: [3][300/306] Elapsed 1m 33s (remain 0m 1s) Loss: 0.0438 \n",
            "Epoch: [3][305/306] Elapsed 1m 34s (remain 0m 0s) Loss: 0.0437 \n",
            "EVAL: [0/77] Elapsed 0m 0s (remain 0m 26s) Loss: 0.1464 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3 - avg_train_loss: 0.0437  avg_val_loss: 0.1173  time: 103s\n",
            "Epoch 3 - avg_train_loss: 0.0437  avg_val_loss: 0.1173  time: 103s\n",
            "Epoch 3 - avg_train_loss: 0.0437  avg_val_loss: 0.1173  time: 103s\n",
            "Epoch 3 - avg_train_loss: 0.0437  avg_val_loss: 0.1173  time: 103s\n",
            "Epoch 3 - avg_train_loss: 0.0437  avg_val_loss: 0.1173  time: 103s\n",
            "Epoch 3 - avg_train_loss: 0.0437  avg_val_loss: 0.1173  time: 103s\n",
            "Epoch 3 - avg_train_loss: 0.0437  avg_val_loss: 0.1173  time: 103s\n",
            "Epoch 3 - Score: 0.11728006416244632\n",
            "Epoch 3 - Score: 0.11728006416244632\n",
            "Epoch 3 - Score: 0.11728006416244632\n",
            "Epoch 3 - Score: 0.11728006416244632\n",
            "Epoch 3 - Score: 0.11728006416244632\n",
            "Epoch 3 - Score: 0.11728006416244632\n",
            "Epoch 3 - Score: 0.11728006416244632\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EVAL: [76/77] Elapsed 0m 8s (remain 0m 0s) Loss: 0.1173 \n",
            "Log loss for 2 is 0.11728006416244632\n",
            "Epoch: [4][0/306] Elapsed 0m 0s (remain 2m 47s) Loss: 0.0326 \n",
            "Epoch: [4][100/306] Elapsed 0m 31s (remain 1m 3s) Loss: 0.0228 \n",
            "Epoch: [4][200/306] Elapsed 1m 2s (remain 0m 32s) Loss: 0.0211 \n",
            "Epoch: [4][300/306] Elapsed 1m 33s (remain 0m 1s) Loss: 0.0244 \n",
            "Epoch: [4][305/306] Elapsed 1m 34s (remain 0m 0s) Loss: 0.0242 \n",
            "EVAL: [0/77] Elapsed 0m 0s (remain 0m 26s) Loss: 0.1835 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4 - avg_train_loss: 0.0242  avg_val_loss: 0.1035  time: 103s\n",
            "Epoch 4 - avg_train_loss: 0.0242  avg_val_loss: 0.1035  time: 103s\n",
            "Epoch 4 - avg_train_loss: 0.0242  avg_val_loss: 0.1035  time: 103s\n",
            "Epoch 4 - avg_train_loss: 0.0242  avg_val_loss: 0.1035  time: 103s\n",
            "Epoch 4 - avg_train_loss: 0.0242  avg_val_loss: 0.1035  time: 103s\n",
            "Epoch 4 - avg_train_loss: 0.0242  avg_val_loss: 0.1035  time: 103s\n",
            "Epoch 4 - avg_train_loss: 0.0242  avg_val_loss: 0.1035  time: 103s\n",
            "Epoch 4 - Score: 0.10354275836402922\n",
            "Epoch 4 - Score: 0.10354275836402922\n",
            "Epoch 4 - Score: 0.10354275836402922\n",
            "Epoch 4 - Score: 0.10354275836402922\n",
            "Epoch 4 - Score: 0.10354275836402922\n",
            "Epoch 4 - Score: 0.10354275836402922\n",
            "Epoch 4 - Score: 0.10354275836402922\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EVAL: [76/77] Elapsed 0m 8s (remain 0m 0s) Loss: 0.1035 \n",
            "Log loss for 3 is 0.10354275836402922\n",
            "Epoch: [5][0/306] Elapsed 0m 0s (remain 2m 48s) Loss: 0.0036 \n",
            "Epoch: [5][100/306] Elapsed 0m 31s (remain 1m 3s) Loss: 0.0160 \n",
            "Epoch: [5][200/306] Elapsed 1m 2s (remain 0m 32s) Loss: 0.0120 \n",
            "Epoch: [5][300/306] Elapsed 1m 33s (remain 0m 1s) Loss: 0.0131 \n",
            "Epoch: [5][305/306] Elapsed 1m 34s (remain 0m 0s) Loss: 0.0133 \n",
            "EVAL: [0/77] Elapsed 0m 0s (remain 0m 26s) Loss: 0.2217 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5 - avg_train_loss: 0.0133  avg_val_loss: 0.1189  time: 103s\n",
            "Epoch 5 - avg_train_loss: 0.0133  avg_val_loss: 0.1189  time: 103s\n",
            "Epoch 5 - avg_train_loss: 0.0133  avg_val_loss: 0.1189  time: 103s\n",
            "Epoch 5 - avg_train_loss: 0.0133  avg_val_loss: 0.1189  time: 103s\n",
            "Epoch 5 - avg_train_loss: 0.0133  avg_val_loss: 0.1189  time: 103s\n",
            "Epoch 5 - avg_train_loss: 0.0133  avg_val_loss: 0.1189  time: 103s\n",
            "Epoch 5 - avg_train_loss: 0.0133  avg_val_loss: 0.1189  time: 103s\n",
            "Epoch 5 - Score: 0.11891114403784646\n",
            "Epoch 5 - Score: 0.11891114403784646\n",
            "Epoch 5 - Score: 0.11891114403784646\n",
            "Epoch 5 - Score: 0.11891114403784646\n",
            "Epoch 5 - Score: 0.11891114403784646\n",
            "Epoch 5 - Score: 0.11891114403784646\n",
            "Epoch 5 - Score: 0.11891114403784646\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EVAL: [76/77] Elapsed 0m 8s (remain 0m 0s) Loss: 0.1189 \n",
            "Log loss for 4 is 0.11891114403784646\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "========== fold: 0 result ==========\n",
            "========== fold: 0 result ==========\n",
            "========== fold: 0 result ==========\n",
            "========== fold: 0 result ==========\n",
            "========== fold: 0 result ==========\n",
            "========== fold: 0 result ==========\n",
            "========== fold: 0 result ==========\n",
            "Score: 0.08298\n",
            "Score: 0.08298\n",
            "Score: 0.08298\n",
            "Score: 0.08298\n",
            "Score: 0.08298\n",
            "Score: 0.08298\n",
            "Score: 0.08298\n",
            "========== fold: 1 training ==========\n",
            "========== fold: 1 training ==========\n",
            "========== fold: 1 training ==========\n",
            "========== fold: 1 training ==========\n",
            "========== fold: 1 training ==========\n",
            "========== fold: 1 training ==========\n",
            "========== fold: 1 training ==========\n",
            "Didn't find file /content/activebus_bert_final/added_tokens.json. We won't load it.\n",
            "loading file /content/activebus_bert_final/vocab.txt\n",
            "loading file None\n",
            "loading file /content/activebus_bert_final/special_tokens_map.json\n",
            "loading file /content/activebus_bert_final/tokenizer_config.json\n",
            "loading file /content/activebus_bert_final/tokenizer.json\n",
            "Didn't find file /content/activebus_bert_final/added_tokens.json. We won't load it.\n",
            "loading file /content/activebus_bert_final/vocab.txt\n",
            "loading file None\n",
            "loading file /content/activebus_bert_final/special_tokens_map.json\n",
            "loading file /content/activebus_bert_final/tokenizer_config.json\n",
            "loading file /content/activebus_bert_final/tokenizer.json\n",
            "loading configuration file /content/activebus_bert_final/config.json\n",
            "Model config BertConfig {\n",
            "  \"_name_or_path\": \"activebus/BERT-XD_Review\",\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_ids\": 0,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.15.0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "loading weights file /content/activebus_bert_final/pytorch_model.bin\n",
            "Some weights of the model checkpoint at /content/activebus_bert_final were not used when initializing BertForSequenceClassification: ['cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /content/activebus_bert_final and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: [1][0/306] Elapsed 0m 0s (remain 2m 55s) Loss: 0.6020 \n",
            "Epoch: [1][100/306] Elapsed 0m 31s (remain 1m 3s) Loss: 0.1916 \n",
            "Epoch: [1][200/306] Elapsed 1m 2s (remain 0m 32s) Loss: 0.1661 \n",
            "Epoch: [1][300/306] Elapsed 1m 33s (remain 0m 1s) Loss: 0.1450 \n",
            "Epoch: [1][305/306] Elapsed 1m 34s (remain 0m 0s) Loss: 0.1443 \n",
            "EVAL: [0/77] Elapsed 0m 0s (remain 0m 27s) Loss: 0.0300 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1 - avg_train_loss: 0.1443  avg_val_loss: 0.0988  time: 103s\n",
            "Epoch 1 - avg_train_loss: 0.1443  avg_val_loss: 0.0988  time: 103s\n",
            "Epoch 1 - avg_train_loss: 0.1443  avg_val_loss: 0.0988  time: 103s\n",
            "Epoch 1 - avg_train_loss: 0.1443  avg_val_loss: 0.0988  time: 103s\n",
            "Epoch 1 - avg_train_loss: 0.1443  avg_val_loss: 0.0988  time: 103s\n",
            "Epoch 1 - avg_train_loss: 0.1443  avg_val_loss: 0.0988  time: 103s\n",
            "Epoch 1 - avg_train_loss: 0.1443  avg_val_loss: 0.0988  time: 103s\n",
            "Epoch 1 - Score: 0.09884846353536804\n",
            "Epoch 1 - Score: 0.09884846353536804\n",
            "Epoch 1 - Score: 0.09884846353536804\n",
            "Epoch 1 - Score: 0.09884846353536804\n",
            "Epoch 1 - Score: 0.09884846353536804\n",
            "Epoch 1 - Score: 0.09884846353536804\n",
            "Epoch 1 - Score: 0.09884846353536804\n",
            "Epoch 1 - Save Best Score: 0.0988 Model\n",
            "Epoch 1 - Save Best Score: 0.0988 Model\n",
            "Epoch 1 - Save Best Score: 0.0988 Model\n",
            "Epoch 1 - Save Best Score: 0.0988 Model\n",
            "Epoch 1 - Save Best Score: 0.0988 Model\n",
            "Epoch 1 - Save Best Score: 0.0988 Model\n",
            "Epoch 1 - Save Best Score: 0.0988 Model\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EVAL: [76/77] Elapsed 0m 8s (remain 0m 0s) Loss: 0.0988 \n",
            "Log loss for 0 is 0.09884846353536804\n",
            "Epoch: [2][0/306] Elapsed 0m 0s (remain 2m 48s) Loss: 0.0583 \n",
            "Epoch: [2][100/306] Elapsed 0m 31s (remain 1m 3s) Loss: 0.0607 \n",
            "Epoch: [2][200/306] Elapsed 1m 2s (remain 0m 32s) Loss: 0.0672 \n",
            "Epoch: [2][300/306] Elapsed 1m 33s (remain 0m 1s) Loss: 0.0666 \n",
            "Epoch: [2][305/306] Elapsed 1m 34s (remain 0m 0s) Loss: 0.0668 \n",
            "EVAL: [0/77] Elapsed 0m 0s (remain 0m 25s) Loss: 0.0064 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2 - avg_train_loss: 0.0668  avg_val_loss: 0.1208  time: 103s\n",
            "Epoch 2 - avg_train_loss: 0.0668  avg_val_loss: 0.1208  time: 103s\n",
            "Epoch 2 - avg_train_loss: 0.0668  avg_val_loss: 0.1208  time: 103s\n",
            "Epoch 2 - avg_train_loss: 0.0668  avg_val_loss: 0.1208  time: 103s\n",
            "Epoch 2 - avg_train_loss: 0.0668  avg_val_loss: 0.1208  time: 103s\n",
            "Epoch 2 - avg_train_loss: 0.0668  avg_val_loss: 0.1208  time: 103s\n",
            "Epoch 2 - avg_train_loss: 0.0668  avg_val_loss: 0.1208  time: 103s\n",
            "Epoch 2 - Score: 0.12080953223886956\n",
            "Epoch 2 - Score: 0.12080953223886956\n",
            "Epoch 2 - Score: 0.12080953223886956\n",
            "Epoch 2 - Score: 0.12080953223886956\n",
            "Epoch 2 - Score: 0.12080953223886956\n",
            "Epoch 2 - Score: 0.12080953223886956\n",
            "Epoch 2 - Score: 0.12080953223886956\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EVAL: [76/77] Elapsed 0m 8s (remain 0m 0s) Loss: 0.1208 \n",
            "Log loss for 1 is 0.12080953223886956\n",
            "Epoch: [3][0/306] Elapsed 0m 0s (remain 2m 53s) Loss: 0.0169 \n",
            "Epoch: [3][100/306] Elapsed 0m 31s (remain 1m 3s) Loss: 0.0459 \n",
            "Epoch: [3][200/306] Elapsed 1m 2s (remain 0m 32s) Loss: 0.0371 \n",
            "Epoch: [3][300/306] Elapsed 1m 33s (remain 0m 1s) Loss: 0.0355 \n",
            "Epoch: [3][305/306] Elapsed 1m 34s (remain 0m 0s) Loss: 0.0355 \n",
            "EVAL: [0/77] Elapsed 0m 0s (remain 0m 26s) Loss: 0.0323 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3 - avg_train_loss: 0.0355  avg_val_loss: 0.1314  time: 103s\n",
            "Epoch 3 - avg_train_loss: 0.0355  avg_val_loss: 0.1314  time: 103s\n",
            "Epoch 3 - avg_train_loss: 0.0355  avg_val_loss: 0.1314  time: 103s\n",
            "Epoch 3 - avg_train_loss: 0.0355  avg_val_loss: 0.1314  time: 103s\n",
            "Epoch 3 - avg_train_loss: 0.0355  avg_val_loss: 0.1314  time: 103s\n",
            "Epoch 3 - avg_train_loss: 0.0355  avg_val_loss: 0.1314  time: 103s\n",
            "Epoch 3 - avg_train_loss: 0.0355  avg_val_loss: 0.1314  time: 103s\n",
            "Epoch 3 - Score: 0.13141348626103688\n",
            "Epoch 3 - Score: 0.13141348626103688\n",
            "Epoch 3 - Score: 0.13141348626103688\n",
            "Epoch 3 - Score: 0.13141348626103688\n",
            "Epoch 3 - Score: 0.13141348626103688\n",
            "Epoch 3 - Score: 0.13141348626103688\n",
            "Epoch 3 - Score: 0.13141348626103688\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EVAL: [76/77] Elapsed 0m 8s (remain 0m 0s) Loss: 0.1314 \n",
            "Log loss for 2 is 0.13141348626103688\n",
            "Epoch: [4][0/306] Elapsed 0m 0s (remain 2m 55s) Loss: 0.0060 \n",
            "Epoch: [4][100/306] Elapsed 0m 31s (remain 1m 3s) Loss: 0.0253 \n",
            "Epoch: [4][200/306] Elapsed 1m 2s (remain 0m 32s) Loss: 0.0276 \n",
            "Epoch: [4][300/306] Elapsed 1m 33s (remain 0m 1s) Loss: 0.0294 \n",
            "Epoch: [4][305/306] Elapsed 1m 34s (remain 0m 0s) Loss: 0.0291 \n",
            "EVAL: [0/77] Elapsed 0m 0s (remain 0m 25s) Loss: 0.0136 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4 - avg_train_loss: 0.0291  avg_val_loss: 0.1271  time: 103s\n",
            "Epoch 4 - avg_train_loss: 0.0291  avg_val_loss: 0.1271  time: 103s\n",
            "Epoch 4 - avg_train_loss: 0.0291  avg_val_loss: 0.1271  time: 103s\n",
            "Epoch 4 - avg_train_loss: 0.0291  avg_val_loss: 0.1271  time: 103s\n",
            "Epoch 4 - avg_train_loss: 0.0291  avg_val_loss: 0.1271  time: 103s\n",
            "Epoch 4 - avg_train_loss: 0.0291  avg_val_loss: 0.1271  time: 103s\n",
            "Epoch 4 - avg_train_loss: 0.0291  avg_val_loss: 0.1271  time: 103s\n",
            "Epoch 4 - Score: 0.12710952358446542\n",
            "Epoch 4 - Score: 0.12710952358446542\n",
            "Epoch 4 - Score: 0.12710952358446542\n",
            "Epoch 4 - Score: 0.12710952358446542\n",
            "Epoch 4 - Score: 0.12710952358446542\n",
            "Epoch 4 - Score: 0.12710952358446542\n",
            "Epoch 4 - Score: 0.12710952358446542\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EVAL: [76/77] Elapsed 0m 8s (remain 0m 0s) Loss: 0.1271 \n",
            "Log loss for 3 is 0.12710952358446542\n",
            "Epoch: [5][0/306] Elapsed 0m 0s (remain 2m 50s) Loss: 0.0063 \n",
            "Epoch: [5][100/306] Elapsed 0m 31s (remain 1m 3s) Loss: 0.0086 \n",
            "Epoch: [5][200/306] Elapsed 1m 2s (remain 0m 32s) Loss: 0.0082 \n",
            "Epoch: [5][300/306] Elapsed 1m 33s (remain 0m 1s) Loss: 0.0115 \n",
            "Epoch: [5][305/306] Elapsed 1m 34s (remain 0m 0s) Loss: 0.0116 \n",
            "EVAL: [0/77] Elapsed 0m 0s (remain 0m 27s) Loss: 0.0007 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5 - avg_train_loss: 0.0116  avg_val_loss: 0.1354  time: 103s\n",
            "Epoch 5 - avg_train_loss: 0.0116  avg_val_loss: 0.1354  time: 103s\n",
            "Epoch 5 - avg_train_loss: 0.0116  avg_val_loss: 0.1354  time: 103s\n",
            "Epoch 5 - avg_train_loss: 0.0116  avg_val_loss: 0.1354  time: 103s\n",
            "Epoch 5 - avg_train_loss: 0.0116  avg_val_loss: 0.1354  time: 103s\n",
            "Epoch 5 - avg_train_loss: 0.0116  avg_val_loss: 0.1354  time: 103s\n",
            "Epoch 5 - avg_train_loss: 0.0116  avg_val_loss: 0.1354  time: 103s\n",
            "Epoch 5 - Score: 0.13538565808801742\n",
            "Epoch 5 - Score: 0.13538565808801742\n",
            "Epoch 5 - Score: 0.13538565808801742\n",
            "Epoch 5 - Score: 0.13538565808801742\n",
            "Epoch 5 - Score: 0.13538565808801742\n",
            "Epoch 5 - Score: 0.13538565808801742\n",
            "Epoch 5 - Score: 0.13538565808801742\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EVAL: [76/77] Elapsed 0m 8s (remain 0m 0s) Loss: 0.1354 \n",
            "Log loss for 4 is 0.13538565808801742\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "========== fold: 1 result ==========\n",
            "========== fold: 1 result ==========\n",
            "========== fold: 1 result ==========\n",
            "========== fold: 1 result ==========\n",
            "========== fold: 1 result ==========\n",
            "========== fold: 1 result ==========\n",
            "========== fold: 1 result ==========\n",
            "Score: 0.09885\n",
            "Score: 0.09885\n",
            "Score: 0.09885\n",
            "Score: 0.09885\n",
            "Score: 0.09885\n",
            "Score: 0.09885\n",
            "Score: 0.09885\n",
            "========== fold: 2 training ==========\n",
            "========== fold: 2 training ==========\n",
            "========== fold: 2 training ==========\n",
            "========== fold: 2 training ==========\n",
            "========== fold: 2 training ==========\n",
            "========== fold: 2 training ==========\n",
            "========== fold: 2 training ==========\n",
            "Didn't find file /content/activebus_bert_final/added_tokens.json. We won't load it.\n",
            "loading file /content/activebus_bert_final/vocab.txt\n",
            "loading file None\n",
            "loading file /content/activebus_bert_final/special_tokens_map.json\n",
            "loading file /content/activebus_bert_final/tokenizer_config.json\n",
            "loading file /content/activebus_bert_final/tokenizer.json\n",
            "Didn't find file /content/activebus_bert_final/added_tokens.json. We won't load it.\n",
            "loading file /content/activebus_bert_final/vocab.txt\n",
            "loading file None\n",
            "loading file /content/activebus_bert_final/special_tokens_map.json\n",
            "loading file /content/activebus_bert_final/tokenizer_config.json\n",
            "loading file /content/activebus_bert_final/tokenizer.json\n",
            "loading configuration file /content/activebus_bert_final/config.json\n",
            "Model config BertConfig {\n",
            "  \"_name_or_path\": \"activebus/BERT-XD_Review\",\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_ids\": 0,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.15.0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "loading weights file /content/activebus_bert_final/pytorch_model.bin\n",
            "Some weights of the model checkpoint at /content/activebus_bert_final were not used when initializing BertForSequenceClassification: ['cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /content/activebus_bert_final and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: [1][0/306] Elapsed 0m 0s (remain 2m 57s) Loss: 0.6987 \n",
            "Epoch: [1][100/306] Elapsed 0m 31s (remain 1m 3s) Loss: 0.1857 \n",
            "Epoch: [1][200/306] Elapsed 1m 2s (remain 0m 32s) Loss: 0.1693 \n",
            "Epoch: [1][300/306] Elapsed 1m 33s (remain 0m 1s) Loss: 0.1513 \n",
            "Epoch: [1][305/306] Elapsed 1m 34s (remain 0m 0s) Loss: 0.1512 \n",
            "EVAL: [0/77] Elapsed 0m 0s (remain 0m 26s) Loss: 0.0049 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1 - avg_train_loss: 0.1512  avg_val_loss: 0.1415  time: 103s\n",
            "Epoch 1 - avg_train_loss: 0.1512  avg_val_loss: 0.1415  time: 103s\n",
            "Epoch 1 - avg_train_loss: 0.1512  avg_val_loss: 0.1415  time: 103s\n",
            "Epoch 1 - avg_train_loss: 0.1512  avg_val_loss: 0.1415  time: 103s\n",
            "Epoch 1 - avg_train_loss: 0.1512  avg_val_loss: 0.1415  time: 103s\n",
            "Epoch 1 - avg_train_loss: 0.1512  avg_val_loss: 0.1415  time: 103s\n",
            "Epoch 1 - avg_train_loss: 0.1512  avg_val_loss: 0.1415  time: 103s\n",
            "Epoch 1 - Score: 0.14150642329343452\n",
            "Epoch 1 - Score: 0.14150642329343452\n",
            "Epoch 1 - Score: 0.14150642329343452\n",
            "Epoch 1 - Score: 0.14150642329343452\n",
            "Epoch 1 - Score: 0.14150642329343452\n",
            "Epoch 1 - Score: 0.14150642329343452\n",
            "Epoch 1 - Score: 0.14150642329343452\n",
            "Epoch 1 - Save Best Score: 0.1415 Model\n",
            "Epoch 1 - Save Best Score: 0.1415 Model\n",
            "Epoch 1 - Save Best Score: 0.1415 Model\n",
            "Epoch 1 - Save Best Score: 0.1415 Model\n",
            "Epoch 1 - Save Best Score: 0.1415 Model\n",
            "Epoch 1 - Save Best Score: 0.1415 Model\n",
            "Epoch 1 - Save Best Score: 0.1415 Model\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EVAL: [76/77] Elapsed 0m 8s (remain 0m 0s) Loss: 0.1415 \n",
            "Log loss for 0 is 0.14150642329343452\n",
            "Epoch: [2][0/306] Elapsed 0m 0s (remain 2m 55s) Loss: 0.0686 \n",
            "Epoch: [2][100/306] Elapsed 0m 31s (remain 1m 3s) Loss: 0.0780 \n",
            "Epoch: [2][200/306] Elapsed 1m 2s (remain 0m 32s) Loss: 0.0765 \n",
            "Epoch: [2][300/306] Elapsed 1m 33s (remain 0m 1s) Loss: 0.0783 \n",
            "Epoch: [2][305/306] Elapsed 1m 34s (remain 0m 0s) Loss: 0.0785 \n",
            "EVAL: [0/77] Elapsed 0m 0s (remain 0m 26s) Loss: 0.0066 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2 - avg_train_loss: 0.0785  avg_val_loss: 0.1070  time: 103s\n",
            "Epoch 2 - avg_train_loss: 0.0785  avg_val_loss: 0.1070  time: 103s\n",
            "Epoch 2 - avg_train_loss: 0.0785  avg_val_loss: 0.1070  time: 103s\n",
            "Epoch 2 - avg_train_loss: 0.0785  avg_val_loss: 0.1070  time: 103s\n",
            "Epoch 2 - avg_train_loss: 0.0785  avg_val_loss: 0.1070  time: 103s\n",
            "Epoch 2 - avg_train_loss: 0.0785  avg_val_loss: 0.1070  time: 103s\n",
            "Epoch 2 - avg_train_loss: 0.0785  avg_val_loss: 0.1070  time: 103s\n",
            "Epoch 2 - Score: 0.10701376512354831\n",
            "Epoch 2 - Score: 0.10701376512354831\n",
            "Epoch 2 - Score: 0.10701376512354831\n",
            "Epoch 2 - Score: 0.10701376512354831\n",
            "Epoch 2 - Score: 0.10701376512354831\n",
            "Epoch 2 - Score: 0.10701376512354831\n",
            "Epoch 2 - Score: 0.10701376512354831\n",
            "Epoch 2 - Save Best Score: 0.1070 Model\n",
            "Epoch 2 - Save Best Score: 0.1070 Model\n",
            "Epoch 2 - Save Best Score: 0.1070 Model\n",
            "Epoch 2 - Save Best Score: 0.1070 Model\n",
            "Epoch 2 - Save Best Score: 0.1070 Model\n",
            "Epoch 2 - Save Best Score: 0.1070 Model\n",
            "Epoch 2 - Save Best Score: 0.1070 Model\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EVAL: [76/77] Elapsed 0m 8s (remain 0m 0s) Loss: 0.1070 \n",
            "Log loss for 1 is 0.10701376512354831\n",
            "Epoch: [3][0/306] Elapsed 0m 0s (remain 2m 49s) Loss: 0.0078 \n",
            "Epoch: [3][100/306] Elapsed 0m 31s (remain 1m 3s) Loss: 0.0267 \n",
            "Epoch: [3][200/306] Elapsed 1m 2s (remain 0m 32s) Loss: 0.0273 \n",
            "Epoch: [3][300/306] Elapsed 1m 33s (remain 0m 1s) Loss: 0.0305 \n",
            "Epoch: [3][305/306] Elapsed 1m 34s (remain 0m 0s) Loss: 0.0313 \n",
            "EVAL: [0/77] Elapsed 0m 0s (remain 0m 28s) Loss: 0.0023 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3 - avg_train_loss: 0.0313  avg_val_loss: 0.1341  time: 103s\n",
            "Epoch 3 - avg_train_loss: 0.0313  avg_val_loss: 0.1341  time: 103s\n",
            "Epoch 3 - avg_train_loss: 0.0313  avg_val_loss: 0.1341  time: 103s\n",
            "Epoch 3 - avg_train_loss: 0.0313  avg_val_loss: 0.1341  time: 103s\n",
            "Epoch 3 - avg_train_loss: 0.0313  avg_val_loss: 0.1341  time: 103s\n",
            "Epoch 3 - avg_train_loss: 0.0313  avg_val_loss: 0.1341  time: 103s\n",
            "Epoch 3 - avg_train_loss: 0.0313  avg_val_loss: 0.1341  time: 103s\n",
            "Epoch 3 - Score: 0.13414044468568756\n",
            "Epoch 3 - Score: 0.13414044468568756\n",
            "Epoch 3 - Score: 0.13414044468568756\n",
            "Epoch 3 - Score: 0.13414044468568756\n",
            "Epoch 3 - Score: 0.13414044468568756\n",
            "Epoch 3 - Score: 0.13414044468568756\n",
            "Epoch 3 - Score: 0.13414044468568756\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EVAL: [76/77] Elapsed 0m 8s (remain 0m 0s) Loss: 0.1341 \n",
            "Log loss for 2 is 0.13414044468568756\n",
            "Epoch: [4][0/306] Elapsed 0m 0s (remain 2m 52s) Loss: 0.0086 \n",
            "Epoch: [4][100/306] Elapsed 0m 31s (remain 1m 3s) Loss: 0.0094 \n",
            "Epoch: [4][200/306] Elapsed 1m 2s (remain 0m 32s) Loss: 0.0136 \n",
            "Epoch: [4][300/306] Elapsed 1m 33s (remain 0m 1s) Loss: 0.0205 \n",
            "Epoch: [4][305/306] Elapsed 1m 34s (remain 0m 0s) Loss: 0.0203 \n",
            "EVAL: [0/77] Elapsed 0m 0s (remain 0m 26s) Loss: 0.0018 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4 - avg_train_loss: 0.0203  avg_val_loss: 0.1619  time: 103s\n",
            "Epoch 4 - avg_train_loss: 0.0203  avg_val_loss: 0.1619  time: 103s\n",
            "Epoch 4 - avg_train_loss: 0.0203  avg_val_loss: 0.1619  time: 103s\n",
            "Epoch 4 - avg_train_loss: 0.0203  avg_val_loss: 0.1619  time: 103s\n",
            "Epoch 4 - avg_train_loss: 0.0203  avg_val_loss: 0.1619  time: 103s\n",
            "Epoch 4 - avg_train_loss: 0.0203  avg_val_loss: 0.1619  time: 103s\n",
            "Epoch 4 - avg_train_loss: 0.0203  avg_val_loss: 0.1619  time: 103s\n",
            "Epoch 4 - Score: 0.16185510962053593\n",
            "Epoch 4 - Score: 0.16185510962053593\n",
            "Epoch 4 - Score: 0.16185510962053593\n",
            "Epoch 4 - Score: 0.16185510962053593\n",
            "Epoch 4 - Score: 0.16185510962053593\n",
            "Epoch 4 - Score: 0.16185510962053593\n",
            "Epoch 4 - Score: 0.16185510962053593\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EVAL: [76/77] Elapsed 0m 8s (remain 0m 0s) Loss: 0.1619 \n",
            "Log loss for 3 is 0.16185510962053593\n",
            "Epoch: [5][0/306] Elapsed 0m 0s (remain 2m 56s) Loss: 0.0215 \n",
            "Epoch: [5][100/306] Elapsed 0m 31s (remain 1m 3s) Loss: 0.0159 \n",
            "Epoch: [5][200/306] Elapsed 1m 2s (remain 0m 32s) Loss: 0.0112 \n",
            "Epoch: [5][300/306] Elapsed 1m 33s (remain 0m 1s) Loss: 0.0122 \n",
            "Epoch: [5][305/306] Elapsed 1m 34s (remain 0m 0s) Loss: 0.0121 \n",
            "EVAL: [0/77] Elapsed 0m 0s (remain 0m 26s) Loss: 0.0004 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5 - avg_train_loss: 0.0121  avg_val_loss: 0.1778  time: 103s\n",
            "Epoch 5 - avg_train_loss: 0.0121  avg_val_loss: 0.1778  time: 103s\n",
            "Epoch 5 - avg_train_loss: 0.0121  avg_val_loss: 0.1778  time: 103s\n",
            "Epoch 5 - avg_train_loss: 0.0121  avg_val_loss: 0.1778  time: 103s\n",
            "Epoch 5 - avg_train_loss: 0.0121  avg_val_loss: 0.1778  time: 103s\n",
            "Epoch 5 - avg_train_loss: 0.0121  avg_val_loss: 0.1778  time: 103s\n",
            "Epoch 5 - avg_train_loss: 0.0121  avg_val_loss: 0.1778  time: 103s\n",
            "Epoch 5 - Score: 0.17780881098229834\n",
            "Epoch 5 - Score: 0.17780881098229834\n",
            "Epoch 5 - Score: 0.17780881098229834\n",
            "Epoch 5 - Score: 0.17780881098229834\n",
            "Epoch 5 - Score: 0.17780881098229834\n",
            "Epoch 5 - Score: 0.17780881098229834\n",
            "Epoch 5 - Score: 0.17780881098229834\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EVAL: [76/77] Elapsed 0m 8s (remain 0m 0s) Loss: 0.1778 \n",
            "Log loss for 4 is 0.17780881098229834\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "========== fold: 2 result ==========\n",
            "========== fold: 2 result ==========\n",
            "========== fold: 2 result ==========\n",
            "========== fold: 2 result ==========\n",
            "========== fold: 2 result ==========\n",
            "========== fold: 2 result ==========\n",
            "========== fold: 2 result ==========\n",
            "Score: 0.10701\n",
            "Score: 0.10701\n",
            "Score: 0.10701\n",
            "Score: 0.10701\n",
            "Score: 0.10701\n",
            "Score: 0.10701\n",
            "Score: 0.10701\n",
            "========== fold: 3 training ==========\n",
            "========== fold: 3 training ==========\n",
            "========== fold: 3 training ==========\n",
            "========== fold: 3 training ==========\n",
            "========== fold: 3 training ==========\n",
            "========== fold: 3 training ==========\n",
            "========== fold: 3 training ==========\n",
            "Didn't find file /content/activebus_bert_final/added_tokens.json. We won't load it.\n",
            "loading file /content/activebus_bert_final/vocab.txt\n",
            "loading file None\n",
            "loading file /content/activebus_bert_final/special_tokens_map.json\n",
            "loading file /content/activebus_bert_final/tokenizer_config.json\n",
            "loading file /content/activebus_bert_final/tokenizer.json\n",
            "Didn't find file /content/activebus_bert_final/added_tokens.json. We won't load it.\n",
            "loading file /content/activebus_bert_final/vocab.txt\n",
            "loading file None\n",
            "loading file /content/activebus_bert_final/special_tokens_map.json\n",
            "loading file /content/activebus_bert_final/tokenizer_config.json\n",
            "loading file /content/activebus_bert_final/tokenizer.json\n",
            "loading configuration file /content/activebus_bert_final/config.json\n",
            "Model config BertConfig {\n",
            "  \"_name_or_path\": \"activebus/BERT-XD_Review\",\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_ids\": 0,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.15.0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "loading weights file /content/activebus_bert_final/pytorch_model.bin\n",
            "Some weights of the model checkpoint at /content/activebus_bert_final were not used when initializing BertForSequenceClassification: ['cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /content/activebus_bert_final and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: [1][0/306] Elapsed 0m 0s (remain 3m 0s) Loss: 0.7304 \n",
            "Epoch: [1][100/306] Elapsed 0m 31s (remain 1m 3s) Loss: 0.1759 \n",
            "Epoch: [1][200/306] Elapsed 1m 2s (remain 0m 32s) Loss: 0.1613 \n",
            "Epoch: [1][300/306] Elapsed 1m 33s (remain 0m 1s) Loss: 0.1548 \n",
            "Epoch: [1][305/306] Elapsed 1m 34s (remain 0m 0s) Loss: 0.1541 \n",
            "EVAL: [0/77] Elapsed 0m 0s (remain 0m 26s) Loss: 0.1517 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1 - avg_train_loss: 0.1541  avg_val_loss: 0.0973  time: 103s\n",
            "Epoch 1 - avg_train_loss: 0.1541  avg_val_loss: 0.0973  time: 103s\n",
            "Epoch 1 - avg_train_loss: 0.1541  avg_val_loss: 0.0973  time: 103s\n",
            "Epoch 1 - avg_train_loss: 0.1541  avg_val_loss: 0.0973  time: 103s\n",
            "Epoch 1 - avg_train_loss: 0.1541  avg_val_loss: 0.0973  time: 103s\n",
            "Epoch 1 - avg_train_loss: 0.1541  avg_val_loss: 0.0973  time: 103s\n",
            "Epoch 1 - avg_train_loss: 0.1541  avg_val_loss: 0.0973  time: 103s\n",
            "Epoch 1 - Score: 0.09732915023865502\n",
            "Epoch 1 - Score: 0.09732915023865502\n",
            "Epoch 1 - Score: 0.09732915023865502\n",
            "Epoch 1 - Score: 0.09732915023865502\n",
            "Epoch 1 - Score: 0.09732915023865502\n",
            "Epoch 1 - Score: 0.09732915023865502\n",
            "Epoch 1 - Score: 0.09732915023865502\n",
            "Epoch 1 - Save Best Score: 0.0973 Model\n",
            "Epoch 1 - Save Best Score: 0.0973 Model\n",
            "Epoch 1 - Save Best Score: 0.0973 Model\n",
            "Epoch 1 - Save Best Score: 0.0973 Model\n",
            "Epoch 1 - Save Best Score: 0.0973 Model\n",
            "Epoch 1 - Save Best Score: 0.0973 Model\n",
            "Epoch 1 - Save Best Score: 0.0973 Model\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EVAL: [76/77] Elapsed 0m 8s (remain 0m 0s) Loss: 0.0973 \n",
            "Log loss for 0 is 0.09732915023865502\n",
            "Epoch: [2][0/306] Elapsed 0m 0s (remain 2m 57s) Loss: 0.0948 \n",
            "Epoch: [2][100/306] Elapsed 0m 31s (remain 1m 3s) Loss: 0.0838 \n",
            "Epoch: [2][200/306] Elapsed 1m 2s (remain 0m 32s) Loss: 0.0803 \n",
            "Epoch: [2][300/306] Elapsed 1m 33s (remain 0m 1s) Loss: 0.0736 \n",
            "Epoch: [2][305/306] Elapsed 1m 34s (remain 0m 0s) Loss: 0.0733 \n",
            "EVAL: [0/77] Elapsed 0m 0s (remain 0m 28s) Loss: 0.4115 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2 - avg_train_loss: 0.0733  avg_val_loss: 0.1130  time: 103s\n",
            "Epoch 2 - avg_train_loss: 0.0733  avg_val_loss: 0.1130  time: 103s\n",
            "Epoch 2 - avg_train_loss: 0.0733  avg_val_loss: 0.1130  time: 103s\n",
            "Epoch 2 - avg_train_loss: 0.0733  avg_val_loss: 0.1130  time: 103s\n",
            "Epoch 2 - avg_train_loss: 0.0733  avg_val_loss: 0.1130  time: 103s\n",
            "Epoch 2 - avg_train_loss: 0.0733  avg_val_loss: 0.1130  time: 103s\n",
            "Epoch 2 - avg_train_loss: 0.0733  avg_val_loss: 0.1130  time: 103s\n",
            "Epoch 2 - Score: 0.11304264886554455\n",
            "Epoch 2 - Score: 0.11304264886554455\n",
            "Epoch 2 - Score: 0.11304264886554455\n",
            "Epoch 2 - Score: 0.11304264886554455\n",
            "Epoch 2 - Score: 0.11304264886554455\n",
            "Epoch 2 - Score: 0.11304264886554455\n",
            "Epoch 2 - Score: 0.11304264886554455\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EVAL: [76/77] Elapsed 0m 8s (remain 0m 0s) Loss: 0.1130 \n",
            "Log loss for 1 is 0.11304264886554455\n",
            "Epoch: [3][0/306] Elapsed 0m 0s (remain 2m 55s) Loss: 0.0076 \n",
            "Epoch: [3][100/306] Elapsed 0m 31s (remain 1m 3s) Loss: 0.0285 \n",
            "Epoch: [3][200/306] Elapsed 1m 2s (remain 0m 32s) Loss: 0.0315 \n",
            "Epoch: [3][300/306] Elapsed 1m 33s (remain 0m 1s) Loss: 0.0322 \n",
            "Epoch: [3][305/306] Elapsed 1m 34s (remain 0m 0s) Loss: 0.0322 \n",
            "EVAL: [0/77] Elapsed 0m 0s (remain 0m 28s) Loss: 0.2525 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3 - avg_train_loss: 0.0322  avg_val_loss: 0.1402  time: 103s\n",
            "Epoch 3 - avg_train_loss: 0.0322  avg_val_loss: 0.1402  time: 103s\n",
            "Epoch 3 - avg_train_loss: 0.0322  avg_val_loss: 0.1402  time: 103s\n",
            "Epoch 3 - avg_train_loss: 0.0322  avg_val_loss: 0.1402  time: 103s\n",
            "Epoch 3 - avg_train_loss: 0.0322  avg_val_loss: 0.1402  time: 103s\n",
            "Epoch 3 - avg_train_loss: 0.0322  avg_val_loss: 0.1402  time: 103s\n",
            "Epoch 3 - avg_train_loss: 0.0322  avg_val_loss: 0.1402  time: 103s\n",
            "Epoch 3 - Score: 0.1401613262588156\n",
            "Epoch 3 - Score: 0.1401613262588156\n",
            "Epoch 3 - Score: 0.1401613262588156\n",
            "Epoch 3 - Score: 0.1401613262588156\n",
            "Epoch 3 - Score: 0.1401613262588156\n",
            "Epoch 3 - Score: 0.1401613262588156\n",
            "Epoch 3 - Score: 0.1401613262588156\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EVAL: [76/77] Elapsed 0m 8s (remain 0m 0s) Loss: 0.1402 \n",
            "Log loss for 2 is 0.1401613262588156\n",
            "Epoch: [4][0/306] Elapsed 0m 0s (remain 2m 59s) Loss: 0.0013 \n",
            "Epoch: [4][100/306] Elapsed 0m 31s (remain 1m 3s) Loss: 0.0072 \n",
            "Epoch: [4][200/306] Elapsed 1m 2s (remain 0m 32s) Loss: 0.0100 \n",
            "Epoch: [4][300/306] Elapsed 1m 33s (remain 0m 1s) Loss: 0.0133 \n",
            "Epoch: [4][305/306] Elapsed 1m 34s (remain 0m 0s) Loss: 0.0131 \n",
            "EVAL: [0/77] Elapsed 0m 0s (remain 0m 27s) Loss: 0.0034 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4 - avg_train_loss: 0.0131  avg_val_loss: 0.1792  time: 103s\n",
            "Epoch 4 - avg_train_loss: 0.0131  avg_val_loss: 0.1792  time: 103s\n",
            "Epoch 4 - avg_train_loss: 0.0131  avg_val_loss: 0.1792  time: 103s\n",
            "Epoch 4 - avg_train_loss: 0.0131  avg_val_loss: 0.1792  time: 103s\n",
            "Epoch 4 - avg_train_loss: 0.0131  avg_val_loss: 0.1792  time: 103s\n",
            "Epoch 4 - avg_train_loss: 0.0131  avg_val_loss: 0.1792  time: 103s\n",
            "Epoch 4 - avg_train_loss: 0.0131  avg_val_loss: 0.1792  time: 103s\n",
            "Epoch 4 - Score: 0.17917930761266507\n",
            "Epoch 4 - Score: 0.17917930761266507\n",
            "Epoch 4 - Score: 0.17917930761266507\n",
            "Epoch 4 - Score: 0.17917930761266507\n",
            "Epoch 4 - Score: 0.17917930761266507\n",
            "Epoch 4 - Score: 0.17917930761266507\n",
            "Epoch 4 - Score: 0.17917930761266507\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EVAL: [76/77] Elapsed 0m 8s (remain 0m 0s) Loss: 0.1792 \n",
            "Log loss for 3 is 0.17917930761266507\n",
            "Epoch: [5][0/306] Elapsed 0m 0s (remain 2m 54s) Loss: 0.0014 \n",
            "Epoch: [5][100/306] Elapsed 0m 31s (remain 1m 3s) Loss: 0.0114 \n",
            "Epoch: [5][200/306] Elapsed 1m 2s (remain 0m 32s) Loss: 0.0162 \n",
            "Epoch: [5][300/306] Elapsed 1m 33s (remain 0m 1s) Loss: 0.0169 \n",
            "Epoch: [5][305/306] Elapsed 1m 34s (remain 0m 0s) Loss: 0.0170 \n",
            "EVAL: [0/77] Elapsed 0m 0s (remain 0m 26s) Loss: 0.1011 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5 - avg_train_loss: 0.0170  avg_val_loss: 0.1478  time: 103s\n",
            "Epoch 5 - avg_train_loss: 0.0170  avg_val_loss: 0.1478  time: 103s\n",
            "Epoch 5 - avg_train_loss: 0.0170  avg_val_loss: 0.1478  time: 103s\n",
            "Epoch 5 - avg_train_loss: 0.0170  avg_val_loss: 0.1478  time: 103s\n",
            "Epoch 5 - avg_train_loss: 0.0170  avg_val_loss: 0.1478  time: 103s\n",
            "Epoch 5 - avg_train_loss: 0.0170  avg_val_loss: 0.1478  time: 103s\n",
            "Epoch 5 - avg_train_loss: 0.0170  avg_val_loss: 0.1478  time: 103s\n",
            "Epoch 5 - Score: 0.14782654254399535\n",
            "Epoch 5 - Score: 0.14782654254399535\n",
            "Epoch 5 - Score: 0.14782654254399535\n",
            "Epoch 5 - Score: 0.14782654254399535\n",
            "Epoch 5 - Score: 0.14782654254399535\n",
            "Epoch 5 - Score: 0.14782654254399535\n",
            "Epoch 5 - Score: 0.14782654254399535\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EVAL: [76/77] Elapsed 0m 8s (remain 0m 0s) Loss: 0.1478 \n",
            "Log loss for 4 is 0.14782654254399535\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "========== fold: 3 result ==========\n",
            "========== fold: 3 result ==========\n",
            "========== fold: 3 result ==========\n",
            "========== fold: 3 result ==========\n",
            "========== fold: 3 result ==========\n",
            "========== fold: 3 result ==========\n",
            "========== fold: 3 result ==========\n",
            "Score: 0.09733\n",
            "Score: 0.09733\n",
            "Score: 0.09733\n",
            "Score: 0.09733\n",
            "Score: 0.09733\n",
            "Score: 0.09733\n",
            "Score: 0.09733\n",
            "========== fold: 4 training ==========\n",
            "========== fold: 4 training ==========\n",
            "========== fold: 4 training ==========\n",
            "========== fold: 4 training ==========\n",
            "========== fold: 4 training ==========\n",
            "========== fold: 4 training ==========\n",
            "========== fold: 4 training ==========\n",
            "Didn't find file /content/activebus_bert_final/added_tokens.json. We won't load it.\n",
            "loading file /content/activebus_bert_final/vocab.txt\n",
            "loading file None\n",
            "loading file /content/activebus_bert_final/special_tokens_map.json\n",
            "loading file /content/activebus_bert_final/tokenizer_config.json\n",
            "loading file /content/activebus_bert_final/tokenizer.json\n",
            "Didn't find file /content/activebus_bert_final/added_tokens.json. We won't load it.\n",
            "loading file /content/activebus_bert_final/vocab.txt\n",
            "loading file None\n",
            "loading file /content/activebus_bert_final/special_tokens_map.json\n",
            "loading file /content/activebus_bert_final/tokenizer_config.json\n",
            "loading file /content/activebus_bert_final/tokenizer.json\n",
            "loading configuration file /content/activebus_bert_final/config.json\n",
            "Model config BertConfig {\n",
            "  \"_name_or_path\": \"activebus/BERT-XD_Review\",\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_ids\": 0,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.15.0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "loading weights file /content/activebus_bert_final/pytorch_model.bin\n",
            "Some weights of the model checkpoint at /content/activebus_bert_final were not used when initializing BertForSequenceClassification: ['cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /content/activebus_bert_final and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: [1][0/306] Elapsed 0m 0s (remain 3m 0s) Loss: 0.6485 \n",
            "Epoch: [1][100/306] Elapsed 0m 31s (remain 1m 3s) Loss: 0.2023 \n",
            "Epoch: [1][200/306] Elapsed 1m 2s (remain 0m 32s) Loss: 0.1748 \n",
            "Epoch: [1][300/306] Elapsed 1m 33s (remain 0m 1s) Loss: 0.1503 \n",
            "Epoch: [1][305/306] Elapsed 1m 34s (remain 0m 0s) Loss: 0.1502 \n",
            "EVAL: [0/77] Elapsed 0m 0s (remain 0m 27s) Loss: 0.1042 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1 - avg_train_loss: 0.1502  avg_val_loss: 0.1047  time: 103s\n",
            "Epoch 1 - avg_train_loss: 0.1502  avg_val_loss: 0.1047  time: 103s\n",
            "Epoch 1 - avg_train_loss: 0.1502  avg_val_loss: 0.1047  time: 103s\n",
            "Epoch 1 - avg_train_loss: 0.1502  avg_val_loss: 0.1047  time: 103s\n",
            "Epoch 1 - avg_train_loss: 0.1502  avg_val_loss: 0.1047  time: 103s\n",
            "Epoch 1 - avg_train_loss: 0.1502  avg_val_loss: 0.1047  time: 103s\n",
            "Epoch 1 - avg_train_loss: 0.1502  avg_val_loss: 0.1047  time: 103s\n",
            "Epoch 1 - Score: 0.10469001353723653\n",
            "Epoch 1 - Score: 0.10469001353723653\n",
            "Epoch 1 - Score: 0.10469001353723653\n",
            "Epoch 1 - Score: 0.10469001353723653\n",
            "Epoch 1 - Score: 0.10469001353723653\n",
            "Epoch 1 - Score: 0.10469001353723653\n",
            "Epoch 1 - Score: 0.10469001353723653\n",
            "Epoch 1 - Save Best Score: 0.1047 Model\n",
            "Epoch 1 - Save Best Score: 0.1047 Model\n",
            "Epoch 1 - Save Best Score: 0.1047 Model\n",
            "Epoch 1 - Save Best Score: 0.1047 Model\n",
            "Epoch 1 - Save Best Score: 0.1047 Model\n",
            "Epoch 1 - Save Best Score: 0.1047 Model\n",
            "Epoch 1 - Save Best Score: 0.1047 Model\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EVAL: [76/77] Elapsed 0m 8s (remain 0m 0s) Loss: 0.1047 \n",
            "Log loss for 0 is 0.10469001353723653\n",
            "Epoch: [2][0/306] Elapsed 0m 0s (remain 2m 51s) Loss: 0.1244 \n",
            "Epoch: [2][100/306] Elapsed 0m 31s (remain 1m 3s) Loss: 0.0736 \n",
            "Epoch: [2][200/306] Elapsed 1m 2s (remain 0m 32s) Loss: 0.0776 \n",
            "Epoch: [2][300/306] Elapsed 1m 33s (remain 0m 1s) Loss: 0.0791 \n",
            "Epoch: [2][305/306] Elapsed 1m 34s (remain 0m 0s) Loss: 0.0795 \n",
            "EVAL: [0/77] Elapsed 0m 0s (remain 0m 28s) Loss: 0.0273 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2 - avg_train_loss: 0.0795  avg_val_loss: 0.0842  time: 103s\n",
            "Epoch 2 - avg_train_loss: 0.0795  avg_val_loss: 0.0842  time: 103s\n",
            "Epoch 2 - avg_train_loss: 0.0795  avg_val_loss: 0.0842  time: 103s\n",
            "Epoch 2 - avg_train_loss: 0.0795  avg_val_loss: 0.0842  time: 103s\n",
            "Epoch 2 - avg_train_loss: 0.0795  avg_val_loss: 0.0842  time: 103s\n",
            "Epoch 2 - avg_train_loss: 0.0795  avg_val_loss: 0.0842  time: 103s\n",
            "Epoch 2 - avg_train_loss: 0.0795  avg_val_loss: 0.0842  time: 103s\n",
            "Epoch 2 - Score: 0.08417380702355079\n",
            "Epoch 2 - Score: 0.08417380702355079\n",
            "Epoch 2 - Score: 0.08417380702355079\n",
            "Epoch 2 - Score: 0.08417380702355079\n",
            "Epoch 2 - Score: 0.08417380702355079\n",
            "Epoch 2 - Score: 0.08417380702355079\n",
            "Epoch 2 - Score: 0.08417380702355079\n",
            "Epoch 2 - Save Best Score: 0.0842 Model\n",
            "Epoch 2 - Save Best Score: 0.0842 Model\n",
            "Epoch 2 - Save Best Score: 0.0842 Model\n",
            "Epoch 2 - Save Best Score: 0.0842 Model\n",
            "Epoch 2 - Save Best Score: 0.0842 Model\n",
            "Epoch 2 - Save Best Score: 0.0842 Model\n",
            "Epoch 2 - Save Best Score: 0.0842 Model\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EVAL: [76/77] Elapsed 0m 8s (remain 0m 0s) Loss: 0.0842 \n",
            "Log loss for 1 is 0.08417380702355079\n",
            "Epoch: [3][0/306] Elapsed 0m 0s (remain 2m 48s) Loss: 0.0208 \n",
            "Epoch: [3][100/306] Elapsed 0m 31s (remain 1m 3s) Loss: 0.0462 \n",
            "Epoch: [3][200/306] Elapsed 1m 2s (remain 0m 32s) Loss: 0.0456 \n",
            "Epoch: [3][300/306] Elapsed 1m 33s (remain 0m 1s) Loss: 0.0410 \n",
            "Epoch: [3][305/306] Elapsed 1m 34s (remain 0m 0s) Loss: 0.0428 \n",
            "EVAL: [0/77] Elapsed 0m 0s (remain 0m 28s) Loss: 0.0011 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3 - avg_train_loss: 0.0428  avg_val_loss: 0.1099  time: 103s\n",
            "Epoch 3 - avg_train_loss: 0.0428  avg_val_loss: 0.1099  time: 103s\n",
            "Epoch 3 - avg_train_loss: 0.0428  avg_val_loss: 0.1099  time: 103s\n",
            "Epoch 3 - avg_train_loss: 0.0428  avg_val_loss: 0.1099  time: 103s\n",
            "Epoch 3 - avg_train_loss: 0.0428  avg_val_loss: 0.1099  time: 103s\n",
            "Epoch 3 - avg_train_loss: 0.0428  avg_val_loss: 0.1099  time: 103s\n",
            "Epoch 3 - avg_train_loss: 0.0428  avg_val_loss: 0.1099  time: 103s\n",
            "Epoch 3 - Score: 0.10985500352416748\n",
            "Epoch 3 - Score: 0.10985500352416748\n",
            "Epoch 3 - Score: 0.10985500352416748\n",
            "Epoch 3 - Score: 0.10985500352416748\n",
            "Epoch 3 - Score: 0.10985500352416748\n",
            "Epoch 3 - Score: 0.10985500352416748\n",
            "Epoch 3 - Score: 0.10985500352416748\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EVAL: [76/77] Elapsed 0m 8s (remain 0m 0s) Loss: 0.1099 \n",
            "Log loss for 2 is 0.10985500352416748\n",
            "Epoch: [4][0/306] Elapsed 0m 0s (remain 2m 50s) Loss: 0.0021 \n",
            "Epoch: [4][100/306] Elapsed 0m 31s (remain 1m 3s) Loss: 0.0228 \n",
            "Epoch: [4][200/306] Elapsed 1m 2s (remain 0m 32s) Loss: 0.0224 \n",
            "Epoch: [4][300/306] Elapsed 1m 33s (remain 0m 1s) Loss: 0.0245 \n",
            "Epoch: [4][305/306] Elapsed 1m 34s (remain 0m 0s) Loss: 0.0241 \n",
            "EVAL: [0/77] Elapsed 0m 0s (remain 0m 27s) Loss: 0.0010 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4 - avg_train_loss: 0.0241  avg_val_loss: 0.0995  time: 103s\n",
            "Epoch 4 - avg_train_loss: 0.0241  avg_val_loss: 0.0995  time: 103s\n",
            "Epoch 4 - avg_train_loss: 0.0241  avg_val_loss: 0.0995  time: 103s\n",
            "Epoch 4 - avg_train_loss: 0.0241  avg_val_loss: 0.0995  time: 103s\n",
            "Epoch 4 - avg_train_loss: 0.0241  avg_val_loss: 0.0995  time: 103s\n",
            "Epoch 4 - avg_train_loss: 0.0241  avg_val_loss: 0.0995  time: 103s\n",
            "Epoch 4 - avg_train_loss: 0.0241  avg_val_loss: 0.0995  time: 103s\n",
            "Epoch 4 - Score: 0.09945197343264046\n",
            "Epoch 4 - Score: 0.09945197343264046\n",
            "Epoch 4 - Score: 0.09945197343264046\n",
            "Epoch 4 - Score: 0.09945197343264046\n",
            "Epoch 4 - Score: 0.09945197343264046\n",
            "Epoch 4 - Score: 0.09945197343264046\n",
            "Epoch 4 - Score: 0.09945197343264046\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EVAL: [76/77] Elapsed 0m 8s (remain 0m 0s) Loss: 0.0995 \n",
            "Log loss for 3 is 0.09945197343264046\n",
            "Epoch: [5][0/306] Elapsed 0m 0s (remain 2m 54s) Loss: 0.0013 \n",
            "Epoch: [5][100/306] Elapsed 0m 31s (remain 1m 3s) Loss: 0.0108 \n",
            "Epoch: [5][200/306] Elapsed 1m 2s (remain 0m 32s) Loss: 0.0129 \n",
            "Epoch: [5][300/306] Elapsed 1m 33s (remain 0m 1s) Loss: 0.0153 \n",
            "Epoch: [5][305/306] Elapsed 1m 34s (remain 0m 0s) Loss: 0.0151 \n",
            "EVAL: [0/77] Elapsed 0m 0s (remain 0m 26s) Loss: 0.0007 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5 - avg_train_loss: 0.0151  avg_val_loss: 0.1116  time: 103s\n",
            "Epoch 5 - avg_train_loss: 0.0151  avg_val_loss: 0.1116  time: 103s\n",
            "Epoch 5 - avg_train_loss: 0.0151  avg_val_loss: 0.1116  time: 103s\n",
            "Epoch 5 - avg_train_loss: 0.0151  avg_val_loss: 0.1116  time: 103s\n",
            "Epoch 5 - avg_train_loss: 0.0151  avg_val_loss: 0.1116  time: 103s\n",
            "Epoch 5 - avg_train_loss: 0.0151  avg_val_loss: 0.1116  time: 103s\n",
            "Epoch 5 - avg_train_loss: 0.0151  avg_val_loss: 0.1116  time: 103s\n",
            "Epoch 5 - Score: 0.11161317696901416\n",
            "Epoch 5 - Score: 0.11161317696901416\n",
            "Epoch 5 - Score: 0.11161317696901416\n",
            "Epoch 5 - Score: 0.11161317696901416\n",
            "Epoch 5 - Score: 0.11161317696901416\n",
            "Epoch 5 - Score: 0.11161317696901416\n",
            "Epoch 5 - Score: 0.11161317696901416\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EVAL: [76/77] Elapsed 0m 8s (remain 0m 0s) Loss: 0.1116 \n",
            "Log loss for 4 is 0.11161317696901416\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "========== fold: 4 result ==========\n",
            "========== fold: 4 result ==========\n",
            "========== fold: 4 result ==========\n",
            "========== fold: 4 result ==========\n",
            "========== fold: 4 result ==========\n",
            "========== fold: 4 result ==========\n",
            "========== fold: 4 result ==========\n",
            "Score: 0.08417\n",
            "Score: 0.08417\n",
            "Score: 0.08417\n",
            "Score: 0.08417\n",
            "Score: 0.08417\n",
            "Score: 0.08417\n",
            "Score: 0.08417\n",
            "========== CV ==========\n",
            "========== CV ==========\n",
            "========== CV ==========\n",
            "========== CV ==========\n",
            "========== CV ==========\n",
            "========== CV ==========\n",
            "========== CV ==========\n",
            "Score: 0.09407\n",
            "Score: 0.09407\n",
            "Score: 0.09407\n",
            "Score: 0.09407\n",
            "Score: 0.09407\n",
            "Score: 0.09407\n",
            "Score: 0.09407\n",
            "Didn't find file /content/activebus_bert_final/added_tokens.json. We won't load it.\n",
            "loading file /content/activebus_bert_final/vocab.txt\n",
            "loading file None\n",
            "loading file /content/activebus_bert_final/special_tokens_map.json\n",
            "loading file /content/activebus_bert_final/tokenizer_config.json\n",
            "loading file /content/activebus_bert_final/tokenizer.json\n",
            "========== model: bert-base-uncased fold: 0 inference ==========\n",
            "========== model: bert-base-uncased fold: 0 inference ==========\n",
            "========== model: bert-base-uncased fold: 0 inference ==========\n",
            "========== model: bert-base-uncased fold: 0 inference ==========\n",
            "========== model: bert-base-uncased fold: 0 inference ==========\n",
            "========== model: bert-base-uncased fold: 0 inference ==========\n",
            "========== model: bert-base-uncased fold: 0 inference ==========\n",
            "loading configuration file /content/activebus_bert_final/config.json\n",
            "Model config BertConfig {\n",
            "  \"_name_or_path\": \"activebus/BERT-XD_Review\",\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_ids\": 0,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.15.0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "loading weights file /content/activebus_bert_final/pytorch_model.bin\n",
            "Some weights of the model checkpoint at /content/activebus_bert_final were not used when initializing BertForSequenceClassification: ['cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /content/activebus_bert_final and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "\n",
            "  0%|          | 0/165 [00:00<?, ?it/s]\u001b[A\n",
            "  1%|          | 1/165 [00:00<00:19,  8.33it/s]\u001b[A\n",
            "  1%|          | 2/165 [00:00<00:18,  8.75it/s]\u001b[A\n",
            "  2%|▏         | 3/165 [00:00<00:17,  9.03it/s]\u001b[A\n",
            "  2%|▏         | 4/165 [00:00<00:17,  9.18it/s]\u001b[A\n",
            "  3%|▎         | 5/165 [00:00<00:17,  9.25it/s]\u001b[A\n",
            "  4%|▎         | 6/165 [00:00<00:17,  9.28it/s]\u001b[A\n",
            "  4%|▍         | 7/165 [00:00<00:16,  9.33it/s]\u001b[A\n",
            "  5%|▍         | 8/165 [00:00<00:16,  9.34it/s]\u001b[A\n",
            "  5%|▌         | 9/165 [00:00<00:16,  9.38it/s]\u001b[A\n",
            "  6%|▌         | 10/165 [00:01<00:16,  9.39it/s]\u001b[A\n",
            "  7%|▋         | 11/165 [00:01<00:16,  9.40it/s]\u001b[A\n",
            "  7%|▋         | 12/165 [00:01<00:16,  9.40it/s]\u001b[A\n",
            "  8%|▊         | 13/165 [00:01<00:16,  9.41it/s]\u001b[A\n",
            "  8%|▊         | 14/165 [00:01<00:16,  9.41it/s]\u001b[A\n",
            "  9%|▉         | 15/165 [00:01<00:15,  9.41it/s]\u001b[A\n",
            " 10%|▉         | 16/165 [00:01<00:15,  9.42it/s]\u001b[A\n",
            " 10%|█         | 17/165 [00:01<00:15,  9.42it/s]\u001b[A\n",
            " 11%|█         | 18/165 [00:01<00:15,  9.40it/s]\u001b[A\n",
            " 12%|█▏        | 19/165 [00:02<00:15,  9.39it/s]\u001b[A\n",
            " 12%|█▏        | 20/165 [00:02<00:15,  9.41it/s]\u001b[A\n",
            " 13%|█▎        | 21/165 [00:02<00:15,  9.40it/s]\u001b[A\n",
            " 13%|█▎        | 22/165 [00:02<00:15,  9.40it/s]\u001b[A\n",
            " 14%|█▍        | 23/165 [00:02<00:15,  9.39it/s]\u001b[A\n",
            " 15%|█▍        | 24/165 [00:02<00:15,  9.40it/s]\u001b[A\n",
            " 15%|█▌        | 25/165 [00:02<00:14,  9.41it/s]\u001b[A\n",
            " 16%|█▌        | 26/165 [00:02<00:14,  9.40it/s]\u001b[A\n",
            " 16%|█▋        | 27/165 [00:02<00:14,  9.38it/s]\u001b[A\n",
            " 17%|█▋        | 28/165 [00:02<00:14,  9.39it/s]\u001b[A\n",
            " 18%|█▊        | 29/165 [00:03<00:14,  9.35it/s]\u001b[A\n",
            " 18%|█▊        | 30/165 [00:03<00:14,  9.35it/s]\u001b[A\n",
            " 19%|█▉        | 31/165 [00:03<00:14,  9.35it/s]\u001b[A\n",
            " 19%|█▉        | 32/165 [00:03<00:14,  9.37it/s]\u001b[A\n",
            " 20%|██        | 33/165 [00:03<00:16,  8.03it/s]\u001b[A\n",
            " 21%|██        | 34/165 [00:03<00:15,  8.40it/s]\u001b[A\n",
            " 21%|██        | 35/165 [00:03<00:15,  8.66it/s]\u001b[A\n",
            " 22%|██▏       | 36/165 [00:03<00:14,  8.87it/s]\u001b[A\n",
            " 22%|██▏       | 37/165 [00:04<00:14,  9.03it/s]\u001b[A\n",
            " 23%|██▎       | 38/165 [00:04<00:13,  9.13it/s]\u001b[A\n",
            " 24%|██▎       | 39/165 [00:04<00:13,  9.21it/s]\u001b[A\n",
            " 24%|██▍       | 40/165 [00:04<00:13,  9.25it/s]\u001b[A\n",
            " 25%|██▍       | 41/165 [00:04<00:13,  9.30it/s]\u001b[A\n",
            " 25%|██▌       | 42/165 [00:04<00:13,  9.34it/s]\u001b[A\n",
            " 26%|██▌       | 43/165 [00:04<00:13,  9.34it/s]\u001b[A\n",
            " 27%|██▋       | 44/165 [00:04<00:12,  9.37it/s]\u001b[A\n",
            " 27%|██▋       | 45/165 [00:04<00:12,  9.38it/s]\u001b[A\n",
            " 28%|██▊       | 46/165 [00:04<00:12,  9.39it/s]\u001b[A\n",
            " 28%|██▊       | 47/165 [00:05<00:12,  9.40it/s]\u001b[A\n",
            " 29%|██▉       | 48/165 [00:05<00:12,  9.37it/s]\u001b[A\n",
            " 30%|██▉       | 49/165 [00:05<00:12,  9.34it/s]\u001b[A\n",
            " 30%|███       | 50/165 [00:05<00:12,  9.36it/s]\u001b[A\n",
            " 31%|███       | 51/165 [00:05<00:12,  9.38it/s]\u001b[A\n",
            " 32%|███▏      | 52/165 [00:05<00:12,  9.38it/s]\u001b[A\n",
            " 32%|███▏      | 53/165 [00:05<00:11,  9.38it/s]\u001b[A\n",
            " 33%|███▎      | 54/165 [00:05<00:11,  9.38it/s]\u001b[A\n",
            " 33%|███▎      | 55/165 [00:05<00:11,  9.39it/s]\u001b[A\n",
            " 34%|███▍      | 56/165 [00:06<00:11,  9.40it/s]\u001b[A\n",
            " 35%|███▍      | 57/165 [00:06<00:11,  9.39it/s]\u001b[A\n",
            " 35%|███▌      | 58/165 [00:06<00:11,  9.39it/s]\u001b[A\n",
            " 36%|███▌      | 59/165 [00:06<00:11,  9.39it/s]\u001b[A\n",
            " 36%|███▋      | 60/165 [00:06<00:11,  9.38it/s]\u001b[A\n",
            " 37%|███▋      | 61/165 [00:06<00:11,  9.37it/s]\u001b[A\n",
            " 38%|███▊      | 62/165 [00:06<00:10,  9.39it/s]\u001b[A\n",
            " 38%|███▊      | 63/165 [00:06<00:10,  9.40it/s]\u001b[A\n",
            " 39%|███▉      | 64/165 [00:06<00:10,  9.39it/s]\u001b[A\n",
            " 39%|███▉      | 65/165 [00:06<00:10,  9.39it/s]\u001b[A\n",
            " 40%|████      | 66/165 [00:07<00:10,  9.36it/s]\u001b[A\n",
            " 41%|████      | 67/165 [00:07<00:10,  9.33it/s]\u001b[A\n",
            " 41%|████      | 68/165 [00:07<00:10,  9.35it/s]\u001b[A\n",
            " 42%|████▏     | 69/165 [00:07<00:10,  9.36it/s]\u001b[A\n",
            " 42%|████▏     | 70/165 [00:07<00:10,  9.34it/s]\u001b[A\n",
            " 43%|████▎     | 71/165 [00:07<00:10,  9.35it/s]\u001b[A\n",
            " 44%|████▎     | 72/165 [00:07<00:09,  9.36it/s]\u001b[A\n",
            " 44%|████▍     | 73/165 [00:07<00:09,  9.38it/s]\u001b[A\n",
            " 45%|████▍     | 74/165 [00:07<00:09,  9.39it/s]\u001b[A\n",
            " 45%|████▌     | 75/165 [00:08<00:09,  9.41it/s]\u001b[A\n",
            " 46%|████▌     | 76/165 [00:08<00:09,  9.41it/s]\u001b[A\n",
            " 47%|████▋     | 77/165 [00:08<00:09,  9.42it/s]\u001b[A\n",
            " 47%|████▋     | 78/165 [00:08<00:09,  9.38it/s]\u001b[A\n",
            " 48%|████▊     | 79/165 [00:08<00:09,  9.38it/s]\u001b[A\n",
            " 48%|████▊     | 80/165 [00:08<00:09,  9.37it/s]\u001b[A\n",
            " 49%|████▉     | 81/165 [00:08<00:08,  9.38it/s]\u001b[A\n",
            " 50%|████▉     | 82/165 [00:08<00:08,  9.39it/s]\u001b[A\n",
            " 50%|█████     | 83/165 [00:08<00:08,  9.40it/s]\u001b[A\n",
            " 51%|█████     | 84/165 [00:09<00:08,  9.40it/s]\u001b[A\n",
            " 52%|█████▏    | 85/165 [00:09<00:08,  9.36it/s]\u001b[A\n",
            " 52%|█████▏    | 86/165 [00:09<00:08,  9.37it/s]\u001b[A\n",
            " 53%|█████▎    | 87/165 [00:09<00:08,  9.38it/s]\u001b[A\n",
            " 53%|█████▎    | 88/165 [00:09<00:08,  9.39it/s]\u001b[A\n",
            " 54%|█████▍    | 89/165 [00:09<00:08,  9.40it/s]\u001b[A\n",
            " 55%|█████▍    | 90/165 [00:09<00:07,  9.40it/s]\u001b[A\n",
            " 55%|█████▌    | 91/165 [00:09<00:07,  9.40it/s]\u001b[A\n",
            " 56%|█████▌    | 92/165 [00:09<00:07,  9.41it/s]\u001b[A\n",
            " 56%|█████▋    | 93/165 [00:09<00:07,  9.41it/s]\u001b[A\n",
            " 57%|█████▋    | 94/165 [00:10<00:07,  9.41it/s]\u001b[A\n",
            " 58%|█████▊    | 95/165 [00:10<00:07,  9.42it/s]\u001b[A\n",
            " 58%|█████▊    | 96/165 [00:10<00:07,  9.42it/s]\u001b[A\n",
            " 59%|█████▉    | 97/165 [00:10<00:07,  9.42it/s]\u001b[A\n",
            " 59%|█████▉    | 98/165 [00:10<00:07,  9.40it/s]\u001b[A\n",
            " 60%|██████    | 99/165 [00:10<00:07,  9.40it/s]\u001b[A\n",
            " 61%|██████    | 100/165 [00:10<00:06,  9.40it/s]\u001b[A\n",
            " 61%|██████    | 101/165 [00:10<00:06,  9.41it/s]\u001b[A\n",
            " 62%|██████▏   | 102/165 [00:10<00:06,  9.40it/s]\u001b[A\n",
            " 62%|██████▏   | 103/165 [00:11<00:06,  9.41it/s]\u001b[A\n",
            " 63%|██████▎   | 104/165 [00:11<00:06,  9.41it/s]\u001b[A\n",
            " 64%|██████▎   | 105/165 [00:11<00:06,  9.40it/s]\u001b[A\n",
            " 64%|██████▍   | 106/165 [00:11<00:06,  9.42it/s]\u001b[A\n",
            " 65%|██████▍   | 107/165 [00:11<00:06,  9.42it/s]\u001b[A\n",
            " 65%|██████▌   | 108/165 [00:11<00:06,  9.42it/s]\u001b[A\n",
            " 66%|██████▌   | 109/165 [00:11<00:05,  9.42it/s]\u001b[A\n",
            " 67%|██████▋   | 110/165 [00:11<00:05,  9.43it/s]\u001b[A\n",
            " 67%|██████▋   | 111/165 [00:11<00:05,  9.43it/s]\u001b[A\n",
            " 68%|██████▊   | 112/165 [00:12<00:05,  9.43it/s]\u001b[A\n",
            " 68%|██████▊   | 113/165 [00:12<00:05,  9.43it/s]\u001b[A\n",
            " 69%|██████▉   | 114/165 [00:12<00:05,  9.44it/s]\u001b[A\n",
            " 70%|██████▉   | 115/165 [00:12<00:05,  9.30it/s]\u001b[A\n",
            " 70%|███████   | 116/165 [00:12<00:05,  9.33it/s]\u001b[A\n",
            " 71%|███████   | 117/165 [00:12<00:05,  9.33it/s]\u001b[A\n",
            " 72%|███████▏  | 118/165 [00:12<00:05,  9.35it/s]\u001b[A\n",
            " 72%|███████▏  | 119/165 [00:12<00:04,  9.35it/s]\u001b[A\n",
            " 73%|███████▎  | 120/165 [00:12<00:04,  9.36it/s]\u001b[A\n",
            " 73%|███████▎  | 121/165 [00:12<00:04,  9.37it/s]\u001b[A\n",
            " 74%|███████▍  | 122/165 [00:13<00:04,  9.38it/s]\u001b[A\n",
            " 75%|███████▍  | 123/165 [00:13<00:04,  9.39it/s]\u001b[A\n",
            " 75%|███████▌  | 124/165 [00:13<00:04,  9.37it/s]\u001b[A\n",
            " 76%|███████▌  | 125/165 [00:13<00:04,  9.39it/s]\u001b[A\n",
            " 76%|███████▋  | 126/165 [00:13<00:04,  9.40it/s]\u001b[A\n",
            " 77%|███████▋  | 127/165 [00:13<00:04,  9.40it/s]\u001b[A\n",
            " 78%|███████▊  | 128/165 [00:13<00:03,  9.41it/s]\u001b[A\n",
            " 78%|███████▊  | 129/165 [00:13<00:03,  9.40it/s]\u001b[A\n",
            " 79%|███████▉  | 130/165 [00:13<00:03,  9.41it/s]\u001b[A\n",
            " 79%|███████▉  | 131/165 [00:14<00:03,  9.39it/s]\u001b[A\n",
            " 80%|████████  | 132/165 [00:14<00:03,  9.38it/s]\u001b[A\n",
            " 81%|████████  | 133/165 [00:14<00:03,  9.38it/s]\u001b[A\n",
            " 81%|████████  | 134/165 [00:14<00:03,  9.39it/s]\u001b[A\n",
            " 82%|████████▏ | 135/165 [00:14<00:03,  9.39it/s]\u001b[A\n",
            " 82%|████████▏ | 136/165 [00:14<00:03,  9.37it/s]\u001b[A\n",
            " 83%|████████▎ | 137/165 [00:14<00:02,  9.37it/s]\u001b[A\n",
            " 84%|████████▎ | 138/165 [00:14<00:02,  9.40it/s]\u001b[A\n",
            " 84%|████████▍ | 139/165 [00:14<00:02,  9.40it/s]\u001b[A\n",
            " 85%|████████▍ | 140/165 [00:14<00:02,  9.39it/s]\u001b[A\n",
            " 85%|████████▌ | 141/165 [00:15<00:02,  9.39it/s]\u001b[A\n",
            " 86%|████████▌ | 142/165 [00:15<00:02,  9.40it/s]\u001b[A\n",
            " 87%|████████▋ | 143/165 [00:15<00:02,  9.41it/s]\u001b[A\n",
            " 87%|████████▋ | 144/165 [00:15<00:02,  9.41it/s]\u001b[A\n",
            " 88%|████████▊ | 145/165 [00:15<00:02,  9.40it/s]\u001b[A\n",
            " 88%|████████▊ | 146/165 [00:15<00:02,  9.40it/s]\u001b[A\n",
            " 89%|████████▉ | 147/165 [00:15<00:01,  9.40it/s]\u001b[A\n",
            " 90%|████████▉ | 148/165 [00:15<00:01,  9.41it/s]\u001b[A\n",
            " 90%|█████████ | 149/165 [00:15<00:01,  9.41it/s]\u001b[A\n",
            " 91%|█████████ | 150/165 [00:16<00:01,  9.42it/s]\u001b[A\n",
            " 92%|█████████▏| 151/165 [00:16<00:01,  9.42it/s]\u001b[A\n",
            " 92%|█████████▏| 152/165 [00:16<00:01,  9.41it/s]\u001b[A\n",
            " 93%|█████████▎| 153/165 [00:16<00:01,  9.41it/s]\u001b[A\n",
            " 93%|█████████▎| 154/165 [00:16<00:01,  9.43it/s]\u001b[A\n",
            " 94%|█████████▍| 155/165 [00:16<00:01,  9.44it/s]\u001b[A\n",
            " 95%|█████████▍| 156/165 [00:16<00:00,  9.43it/s]\u001b[A\n",
            " 95%|█████████▌| 157/165 [00:16<00:00,  9.40it/s]\u001b[A\n",
            " 96%|█████████▌| 158/165 [00:16<00:00,  9.42it/s]\u001b[A\n",
            " 96%|█████████▋| 159/165 [00:17<00:00,  9.43it/s]\u001b[A\n",
            " 97%|█████████▋| 160/165 [00:17<00:00,  9.45it/s]\u001b[A\n",
            " 98%|█████████▊| 161/165 [00:17<00:00,  9.47it/s]\u001b[A\n",
            " 98%|█████████▊| 162/165 [00:17<00:00,  9.47it/s]\u001b[A\n",
            " 99%|█████████▉| 163/165 [00:17<00:00,  9.47it/s]\u001b[A\n",
            "100%|██████████| 165/165 [00:17<00:00,  9.34it/s]\n",
            "========== model: bert-base-uncased fold: 1 inference ==========\n",
            "========== model: bert-base-uncased fold: 1 inference ==========\n",
            "========== model: bert-base-uncased fold: 1 inference ==========\n",
            "========== model: bert-base-uncased fold: 1 inference ==========\n",
            "========== model: bert-base-uncased fold: 1 inference ==========\n",
            "========== model: bert-base-uncased fold: 1 inference ==========\n",
            "========== model: bert-base-uncased fold: 1 inference ==========\n",
            "loading configuration file /content/activebus_bert_final/config.json\n",
            "Model config BertConfig {\n",
            "  \"_name_or_path\": \"activebus/BERT-XD_Review\",\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_ids\": 0,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.15.0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "loading weights file /content/activebus_bert_final/pytorch_model.bin\n",
            "Some weights of the model checkpoint at /content/activebus_bert_final were not used when initializing BertForSequenceClassification: ['cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /content/activebus_bert_final and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "\n",
            "  0%|          | 0/165 [00:00<?, ?it/s]\u001b[A\n",
            "  1%|          | 1/165 [00:00<00:18,  8.75it/s]\u001b[A\n",
            "  1%|          | 2/165 [00:00<00:17,  9.14it/s]\u001b[A\n",
            "  2%|▏         | 3/165 [00:00<00:17,  9.25it/s]\u001b[A\n",
            "  2%|▏         | 4/165 [00:00<00:17,  9.32it/s]\u001b[A\n",
            "  3%|▎         | 5/165 [00:00<00:17,  9.31it/s]\u001b[A\n",
            "  4%|▎         | 6/165 [00:00<00:17,  9.34it/s]\u001b[A\n",
            "  4%|▍         | 7/165 [00:00<00:16,  9.35it/s]\u001b[A\n",
            "  5%|▍         | 8/165 [00:00<00:16,  9.37it/s]\u001b[A\n",
            "  5%|▌         | 9/165 [00:00<00:16,  9.37it/s]\u001b[A\n",
            "  6%|▌         | 10/165 [00:01<00:16,  9.37it/s]\u001b[A\n",
            "  7%|▋         | 11/165 [00:01<00:16,  9.34it/s]\u001b[A\n",
            "  7%|▋         | 12/165 [00:01<00:16,  9.34it/s]\u001b[A\n",
            "  8%|▊         | 13/165 [00:01<00:16,  9.36it/s]\u001b[A\n",
            "  8%|▊         | 14/165 [00:01<00:16,  9.35it/s]\u001b[A\n",
            "  9%|▉         | 15/165 [00:01<00:16,  9.36it/s]\u001b[A\n",
            " 10%|▉         | 16/165 [00:01<00:15,  9.36it/s]\u001b[A\n",
            " 10%|█         | 17/165 [00:01<00:15,  9.37it/s]\u001b[A\n",
            " 11%|█         | 18/165 [00:01<00:15,  9.36it/s]\u001b[A\n",
            " 12%|█▏        | 19/165 [00:02<00:15,  9.37it/s]\u001b[A\n",
            " 12%|█▏        | 20/165 [00:02<00:15,  9.39it/s]\u001b[A\n",
            " 13%|█▎        | 21/165 [00:02<00:15,  9.39it/s]\u001b[A\n",
            " 13%|█▎        | 22/165 [00:02<00:15,  9.37it/s]\u001b[A\n",
            " 14%|█▍        | 23/165 [00:02<00:15,  9.37it/s]\u001b[A\n",
            " 15%|█▍        | 24/165 [00:02<00:15,  9.37it/s]\u001b[A\n",
            " 15%|█▌        | 25/165 [00:02<00:14,  9.35it/s]\u001b[A\n",
            " 16%|█▌        | 26/165 [00:02<00:14,  9.35it/s]\u001b[A\n",
            " 16%|█▋        | 27/165 [00:02<00:14,  9.36it/s]\u001b[A\n",
            " 17%|█▋        | 28/165 [00:02<00:14,  9.36it/s]\u001b[A\n",
            " 18%|█▊        | 29/165 [00:03<00:14,  9.37it/s]\u001b[A\n",
            " 18%|█▊        | 30/165 [00:03<00:14,  9.36it/s]\u001b[A\n",
            " 19%|█▉        | 31/165 [00:03<00:14,  9.37it/s]\u001b[A\n",
            " 19%|█▉        | 32/165 [00:03<00:14,  9.37it/s]\u001b[A\n",
            " 20%|██        | 33/165 [00:03<00:14,  9.34it/s]\u001b[A\n",
            " 21%|██        | 34/165 [00:03<00:13,  9.36it/s]\u001b[A\n",
            " 21%|██        | 35/165 [00:03<00:13,  9.37it/s]\u001b[A\n",
            " 22%|██▏       | 36/165 [00:03<00:13,  9.37it/s]\u001b[A\n",
            " 22%|██▏       | 37/165 [00:03<00:13,  9.37it/s]\u001b[A\n",
            " 23%|██▎       | 38/165 [00:04<00:13,  9.28it/s]\u001b[A\n",
            " 24%|██▎       | 39/165 [00:04<00:13,  9.32it/s]\u001b[A\n",
            " 24%|██▍       | 40/165 [00:04<00:13,  9.36it/s]\u001b[A\n",
            " 25%|██▍       | 41/165 [00:04<00:13,  9.36it/s]\u001b[A\n",
            " 25%|██▌       | 42/165 [00:04<00:13,  9.36it/s]\u001b[A\n",
            " 26%|██▌       | 43/165 [00:04<00:13,  9.38it/s]\u001b[A\n",
            " 27%|██▋       | 44/165 [00:04<00:12,  9.38it/s]\u001b[A\n",
            " 27%|██▋       | 45/165 [00:04<00:12,  9.39it/s]\u001b[A\n",
            " 28%|██▊       | 46/165 [00:04<00:12,  9.38it/s]\u001b[A\n",
            " 28%|██▊       | 47/165 [00:05<00:12,  9.38it/s]\u001b[A\n",
            " 29%|██▉       | 48/165 [00:05<00:12,  9.38it/s]\u001b[A\n",
            " 30%|██▉       | 49/165 [00:05<00:12,  9.37it/s]\u001b[A\n",
            " 30%|███       | 50/165 [00:05<00:12,  9.37it/s]\u001b[A\n",
            " 31%|███       | 51/165 [00:05<00:12,  9.34it/s]\u001b[A\n",
            " 32%|███▏      | 52/165 [00:05<00:12,  9.35it/s]\u001b[A\n",
            " 32%|███▏      | 53/165 [00:05<00:11,  9.36it/s]\u001b[A\n",
            " 33%|███▎      | 54/165 [00:05<00:11,  9.38it/s]\u001b[A\n",
            " 33%|███▎      | 55/165 [00:05<00:11,  9.37it/s]\u001b[A\n",
            " 34%|███▍      | 56/165 [00:05<00:11,  9.39it/s]\u001b[A\n",
            " 35%|███▍      | 57/165 [00:06<00:11,  9.37it/s]\u001b[A\n",
            " 35%|███▌      | 58/165 [00:06<00:11,  9.37it/s]\u001b[A\n",
            " 36%|███▌      | 59/165 [00:06<00:11,  9.37it/s]\u001b[A\n",
            " 36%|███▋      | 60/165 [00:06<00:11,  9.39it/s]\u001b[A\n",
            " 37%|███▋      | 61/165 [00:06<00:11,  9.39it/s]\u001b[A\n",
            " 38%|███▊      | 62/165 [00:06<00:11,  9.36it/s]\u001b[A\n",
            " 38%|███▊      | 63/165 [00:06<00:10,  9.40it/s]\u001b[A\n",
            " 39%|███▉      | 64/165 [00:06<00:10,  9.40it/s]\u001b[A\n",
            " 39%|███▉      | 65/165 [00:06<00:10,  9.39it/s]\u001b[A\n",
            " 40%|████      | 66/165 [00:07<00:10,  9.39it/s]\u001b[A\n",
            " 41%|████      | 67/165 [00:07<00:10,  9.40it/s]\u001b[A\n",
            " 41%|████      | 68/165 [00:07<00:10,  9.42it/s]\u001b[A\n",
            " 42%|████▏     | 69/165 [00:07<00:10,  9.42it/s]\u001b[A\n",
            " 42%|████▏     | 70/165 [00:07<00:10,  9.42it/s]\u001b[A\n",
            " 43%|████▎     | 71/165 [00:07<00:09,  9.42it/s]\u001b[A\n",
            " 44%|████▎     | 72/165 [00:07<00:09,  9.42it/s]\u001b[A\n",
            " 44%|████▍     | 73/165 [00:07<00:09,  9.42it/s]\u001b[A\n",
            " 45%|████▍     | 74/165 [00:07<00:09,  9.43it/s]\u001b[A\n",
            " 45%|████▌     | 75/165 [00:08<00:09,  9.42it/s]\u001b[A\n",
            " 46%|████▌     | 76/165 [00:08<00:09,  9.41it/s]\u001b[A\n",
            " 47%|████▋     | 77/165 [00:08<00:09,  9.41it/s]\u001b[A\n",
            " 47%|████▋     | 78/165 [00:08<00:09,  9.42it/s]\u001b[A\n",
            " 48%|████▊     | 79/165 [00:08<00:09,  9.41it/s]\u001b[A\n",
            " 48%|████▊     | 80/165 [00:08<00:09,  9.40it/s]\u001b[A\n",
            " 49%|████▉     | 81/165 [00:08<00:08,  9.40it/s]\u001b[A\n",
            " 50%|████▉     | 82/165 [00:08<00:08,  9.41it/s]\u001b[A\n",
            " 50%|█████     | 83/165 [00:08<00:08,  9.40it/s]\u001b[A\n",
            " 51%|█████     | 84/165 [00:08<00:08,  9.41it/s]\u001b[A\n",
            " 52%|█████▏    | 85/165 [00:09<00:08,  9.40it/s]\u001b[A\n",
            " 52%|█████▏    | 86/165 [00:09<00:08,  9.41it/s]\u001b[A\n",
            " 53%|█████▎    | 87/165 [00:09<00:08,  9.40it/s]\u001b[A\n",
            " 53%|█████▎    | 88/165 [00:09<00:08,  9.39it/s]\u001b[A\n",
            " 54%|█████▍    | 89/165 [00:09<00:08,  9.41it/s]\u001b[A\n",
            " 55%|█████▍    | 90/165 [00:09<00:07,  9.41it/s]\u001b[A\n",
            " 55%|█████▌    | 91/165 [00:09<00:07,  9.41it/s]\u001b[A\n",
            " 56%|█████▌    | 92/165 [00:09<00:07,  9.41it/s]\u001b[A\n",
            " 56%|█████▋    | 93/165 [00:09<00:07,  9.41it/s]\u001b[A\n",
            " 57%|█████▋    | 94/165 [00:10<00:07,  9.40it/s]\u001b[A\n",
            " 58%|█████▊    | 95/165 [00:10<00:07,  9.40it/s]\u001b[A\n",
            " 58%|█████▊    | 96/165 [00:10<00:07,  9.42it/s]\u001b[A\n",
            " 59%|█████▉    | 97/165 [00:10<00:07,  9.40it/s]\u001b[A\n",
            " 59%|█████▉    | 98/165 [00:10<00:07,  9.39it/s]\u001b[A\n",
            " 60%|██████    | 99/165 [00:10<00:07,  9.37it/s]\u001b[A\n",
            " 61%|██████    | 100/165 [00:10<00:06,  9.37it/s]\u001b[A\n",
            " 61%|██████    | 101/165 [00:10<00:06,  9.38it/s]\u001b[A\n",
            " 62%|██████▏   | 102/165 [00:10<00:06,  9.39it/s]\u001b[A\n",
            " 62%|██████▏   | 103/165 [00:10<00:06,  9.39it/s]\u001b[A\n",
            " 63%|██████▎   | 104/165 [00:11<00:06,  9.40it/s]\u001b[A\n",
            " 64%|██████▎   | 105/165 [00:11<00:06,  9.42it/s]\u001b[A\n",
            " 64%|██████▍   | 106/165 [00:11<00:06,  9.42it/s]\u001b[A\n",
            " 65%|██████▍   | 107/165 [00:11<00:06,  9.42it/s]\u001b[A\n",
            " 65%|██████▌   | 108/165 [00:11<00:06,  9.42it/s]\u001b[A\n",
            " 66%|██████▌   | 109/165 [00:11<00:05,  9.41it/s]\u001b[A\n",
            " 67%|██████▋   | 110/165 [00:11<00:05,  9.42it/s]\u001b[A\n",
            " 67%|██████▋   | 111/165 [00:11<00:05,  9.43it/s]\u001b[A\n",
            " 68%|██████▊   | 112/165 [00:11<00:05,  9.43it/s]\u001b[A\n",
            " 68%|██████▊   | 113/165 [00:12<00:05,  9.43it/s]\u001b[A\n",
            " 69%|██████▉   | 114/165 [00:12<00:05,  9.43it/s]\u001b[A\n",
            " 70%|██████▉   | 115/165 [00:12<00:05,  9.43it/s]\u001b[A\n",
            " 70%|███████   | 116/165 [00:12<00:05,  9.43it/s]\u001b[A\n",
            " 71%|███████   | 117/165 [00:12<00:05,  9.42it/s]\u001b[A\n",
            " 72%|███████▏  | 118/165 [00:12<00:04,  9.40it/s]\u001b[A\n",
            " 72%|███████▏  | 119/165 [00:12<00:04,  9.37it/s]\u001b[A\n",
            " 73%|███████▎  | 120/165 [00:12<00:04,  9.36it/s]\u001b[A\n",
            " 73%|███████▎  | 121/165 [00:12<00:04,  9.36it/s]\u001b[A\n",
            " 74%|███████▍  | 122/165 [00:13<00:04,  9.37it/s]\u001b[A\n",
            " 75%|███████▍  | 123/165 [00:13<00:04,  9.37it/s]\u001b[A\n",
            " 75%|███████▌  | 124/165 [00:13<00:04,  9.36it/s]\u001b[A\n",
            " 76%|███████▌  | 125/165 [00:13<00:04,  9.35it/s]\u001b[A\n",
            " 76%|███████▋  | 126/165 [00:13<00:04,  9.38it/s]\u001b[A\n",
            " 77%|███████▋  | 127/165 [00:13<00:04,  9.33it/s]\u001b[A\n",
            " 78%|███████▊  | 128/165 [00:13<00:03,  9.32it/s]\u001b[A\n",
            " 78%|███████▊  | 129/165 [00:13<00:03,  9.36it/s]\u001b[A\n",
            " 79%|███████▉  | 130/165 [00:13<00:03,  9.39it/s]\u001b[A\n",
            " 79%|███████▉  | 131/165 [00:13<00:03,  9.38it/s]\u001b[A\n",
            " 80%|████████  | 132/165 [00:14<00:03,  9.40it/s]\u001b[A\n",
            " 81%|████████  | 133/165 [00:14<00:03,  9.40it/s]\u001b[A\n",
            " 81%|████████  | 134/165 [00:14<00:03,  9.38it/s]\u001b[A\n",
            " 82%|████████▏ | 135/165 [00:14<00:03,  9.40it/s]\u001b[A\n",
            " 82%|████████▏ | 136/165 [00:14<00:03,  9.36it/s]\u001b[A\n",
            " 83%|████████▎ | 137/165 [00:14<00:02,  9.37it/s]\u001b[A\n",
            " 84%|████████▎ | 138/165 [00:14<00:02,  9.39it/s]\u001b[A\n",
            " 84%|████████▍ | 139/165 [00:14<00:02,  9.38it/s]\u001b[A\n",
            " 85%|████████▍ | 140/165 [00:14<00:02,  9.37it/s]\u001b[A\n",
            " 85%|████████▌ | 141/165 [00:15<00:02,  9.37it/s]\u001b[A\n",
            " 86%|████████▌ | 142/165 [00:15<00:02,  9.37it/s]\u001b[A\n",
            " 87%|████████▋ | 143/165 [00:15<00:02,  9.38it/s]\u001b[A\n",
            " 87%|████████▋ | 144/165 [00:15<00:02,  9.39it/s]\u001b[A\n",
            " 88%|████████▊ | 145/165 [00:15<00:02,  9.38it/s]\u001b[A\n",
            " 88%|████████▊ | 146/165 [00:15<00:02,  9.40it/s]\u001b[A\n",
            " 89%|████████▉ | 147/165 [00:15<00:01,  9.38it/s]\u001b[A\n",
            " 90%|████████▉ | 148/165 [00:15<00:01,  9.40it/s]\u001b[A\n",
            " 90%|█████████ | 149/165 [00:15<00:01,  9.36it/s]\u001b[A\n",
            " 91%|█████████ | 150/165 [00:15<00:01,  9.35it/s]\u001b[A\n",
            " 92%|█████████▏| 151/165 [00:16<00:01,  9.31it/s]\u001b[A\n",
            " 92%|█████████▏| 152/165 [00:16<00:01,  9.34it/s]\u001b[A\n",
            " 93%|█████████▎| 153/165 [00:16<00:01,  9.34it/s]\u001b[A\n",
            " 93%|█████████▎| 154/165 [00:16<00:01,  9.35it/s]\u001b[A\n",
            " 94%|█████████▍| 155/165 [00:16<00:01,  9.33it/s]\u001b[A\n",
            " 95%|█████████▍| 156/165 [00:16<00:00,  9.33it/s]\u001b[A\n",
            " 95%|█████████▌| 157/165 [00:16<00:00,  9.33it/s]\u001b[A\n",
            " 96%|█████████▌| 158/165 [00:16<00:00,  9.36it/s]\u001b[A\n",
            " 96%|█████████▋| 159/165 [00:16<00:00,  9.39it/s]\u001b[A\n",
            " 97%|█████████▋| 160/165 [00:17<00:00,  9.41it/s]\u001b[A\n",
            " 98%|█████████▊| 161/165 [00:17<00:00,  9.42it/s]\u001b[A\n",
            " 98%|█████████▊| 162/165 [00:17<00:00,  9.43it/s]\u001b[A\n",
            " 99%|█████████▉| 163/165 [00:17<00:00,  9.42it/s]\u001b[A\n",
            "100%|██████████| 165/165 [00:17<00:00,  9.37it/s]\n",
            "========== model: bert-base-uncased fold: 2 inference ==========\n",
            "========== model: bert-base-uncased fold: 2 inference ==========\n",
            "========== model: bert-base-uncased fold: 2 inference ==========\n",
            "========== model: bert-base-uncased fold: 2 inference ==========\n",
            "========== model: bert-base-uncased fold: 2 inference ==========\n",
            "========== model: bert-base-uncased fold: 2 inference ==========\n",
            "========== model: bert-base-uncased fold: 2 inference ==========\n",
            "loading configuration file /content/activebus_bert_final/config.json\n",
            "Model config BertConfig {\n",
            "  \"_name_or_path\": \"activebus/BERT-XD_Review\",\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_ids\": 0,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.15.0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "loading weights file /content/activebus_bert_final/pytorch_model.bin\n",
            "Some weights of the model checkpoint at /content/activebus_bert_final were not used when initializing BertForSequenceClassification: ['cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /content/activebus_bert_final and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "\n",
            "  0%|          | 0/165 [00:00<?, ?it/s]\u001b[A\n",
            "  1%|          | 1/165 [00:00<00:19,  8.33it/s]\u001b[A\n",
            "  1%|          | 2/165 [00:00<00:18,  8.93it/s]\u001b[A\n",
            "  2%|▏         | 3/165 [00:00<00:17,  9.14it/s]\u001b[A\n",
            "  2%|▏         | 4/165 [00:00<00:17,  9.24it/s]\u001b[A\n",
            "  3%|▎         | 5/165 [00:00<00:17,  9.25it/s]\u001b[A\n",
            "  4%|▎         | 6/165 [00:00<00:17,  9.29it/s]\u001b[A\n",
            "  4%|▍         | 7/165 [00:00<00:16,  9.29it/s]\u001b[A\n",
            "  5%|▍         | 8/165 [00:00<00:16,  9.31it/s]\u001b[A\n",
            "  5%|▌         | 9/165 [00:00<00:16,  9.30it/s]\u001b[A\n",
            "  6%|▌         | 10/165 [00:01<00:16,  9.30it/s]\u001b[A\n",
            "  7%|▋         | 11/165 [00:01<00:16,  9.32it/s]\u001b[A\n",
            "  7%|▋         | 12/165 [00:01<00:16,  9.34it/s]\u001b[A\n",
            "  8%|▊         | 13/165 [00:01<00:16,  9.35it/s]\u001b[A\n",
            "  8%|▊         | 14/165 [00:01<00:16,  9.35it/s]\u001b[A\n",
            "  9%|▉         | 15/165 [00:01<00:16,  9.37it/s]\u001b[A\n",
            " 10%|▉         | 16/165 [00:01<00:15,  9.36it/s]\u001b[A\n",
            " 10%|█         | 17/165 [00:01<00:15,  9.37it/s]\u001b[A\n",
            " 11%|█         | 18/165 [00:01<00:15,  9.36it/s]\u001b[A\n",
            " 12%|█▏        | 19/165 [00:02<00:15,  9.37it/s]\u001b[A\n",
            " 12%|█▏        | 20/165 [00:02<00:15,  9.33it/s]\u001b[A\n",
            " 13%|█▎        | 21/165 [00:02<00:15,  9.35it/s]\u001b[A\n",
            " 13%|█▎        | 22/165 [00:02<00:15,  9.38it/s]\u001b[A\n",
            " 14%|█▍        | 23/165 [00:02<00:15,  9.37it/s]\u001b[A\n",
            " 15%|█▍        | 24/165 [00:02<00:15,  9.35it/s]\u001b[A\n",
            " 15%|█▌        | 25/165 [00:02<00:14,  9.37it/s]\u001b[A\n",
            " 16%|█▌        | 26/165 [00:02<00:14,  9.37it/s]\u001b[A\n",
            " 16%|█▋        | 27/165 [00:02<00:14,  9.38it/s]\u001b[A\n",
            " 17%|█▋        | 28/165 [00:03<00:14,  9.39it/s]\u001b[A\n",
            " 18%|█▊        | 29/165 [00:03<00:14,  9.39it/s]\u001b[A\n",
            " 18%|█▊        | 30/165 [00:03<00:14,  9.41it/s]\u001b[A\n",
            " 19%|█▉        | 31/165 [00:03<00:14,  9.40it/s]\u001b[A\n",
            " 19%|█▉        | 32/165 [00:03<00:14,  9.40it/s]\u001b[A\n",
            " 20%|██        | 33/165 [00:03<00:14,  9.39it/s]\u001b[A\n",
            " 21%|██        | 34/165 [00:03<00:13,  9.40it/s]\u001b[A\n",
            " 21%|██        | 35/165 [00:03<00:13,  9.38it/s]\u001b[A\n",
            " 22%|██▏       | 36/165 [00:03<00:13,  9.36it/s]\u001b[A\n",
            " 22%|██▏       | 37/165 [00:03<00:13,  9.37it/s]\u001b[A\n",
            " 23%|██▎       | 38/165 [00:04<00:13,  9.37it/s]\u001b[A\n",
            " 24%|██▎       | 39/165 [00:04<00:13,  9.38it/s]\u001b[A\n",
            " 24%|██▍       | 40/165 [00:04<00:13,  9.38it/s]\u001b[A\n",
            " 25%|██▍       | 41/165 [00:04<00:13,  9.39it/s]\u001b[A\n",
            " 25%|██▌       | 42/165 [00:04<00:13,  9.41it/s]\u001b[A\n",
            " 26%|██▌       | 43/165 [00:04<00:12,  9.39it/s]\u001b[A\n",
            " 27%|██▋       | 44/165 [00:04<00:12,  9.38it/s]\u001b[A\n",
            " 27%|██▋       | 45/165 [00:04<00:12,  9.37it/s]\u001b[A\n",
            " 28%|██▊       | 46/165 [00:04<00:12,  9.39it/s]\u001b[A\n",
            " 28%|██▊       | 47/165 [00:05<00:12,  9.42it/s]\u001b[A\n",
            " 29%|██▉       | 48/165 [00:05<00:12,  9.41it/s]\u001b[A\n",
            " 30%|██▉       | 49/165 [00:05<00:12,  9.39it/s]\u001b[A\n",
            " 30%|███       | 50/165 [00:05<00:12,  9.40it/s]\u001b[A\n",
            " 31%|███       | 51/165 [00:05<00:12,  9.40it/s]\u001b[A\n",
            " 32%|███▏      | 52/165 [00:05<00:12,  9.41it/s]\u001b[A\n",
            " 32%|███▏      | 53/165 [00:05<00:11,  9.41it/s]\u001b[A\n",
            " 33%|███▎      | 54/165 [00:05<00:11,  9.38it/s]\u001b[A\n",
            " 33%|███▎      | 55/165 [00:05<00:11,  9.39it/s]\u001b[A\n",
            " 34%|███▍      | 56/165 [00:05<00:11,  9.36it/s]\u001b[A\n",
            " 35%|███▍      | 57/165 [00:06<00:11,  9.36it/s]\u001b[A\n",
            " 35%|███▌      | 58/165 [00:06<00:11,  9.38it/s]\u001b[A\n",
            " 36%|███▌      | 59/165 [00:06<00:11,  9.38it/s]\u001b[A\n",
            " 36%|███▋      | 60/165 [00:06<00:11,  9.35it/s]\u001b[A\n",
            " 37%|███▋      | 61/165 [00:06<00:11,  9.37it/s]\u001b[A\n",
            " 38%|███▊      | 62/165 [00:06<00:10,  9.38it/s]\u001b[A\n",
            " 38%|███▊      | 63/165 [00:06<00:10,  9.36it/s]\u001b[A\n",
            " 39%|███▉      | 64/165 [00:06<00:10,  9.33it/s]\u001b[A\n",
            " 39%|███▉      | 65/165 [00:06<00:10,  9.33it/s]\u001b[A\n",
            " 40%|████      | 66/165 [00:07<00:10,  9.35it/s]\u001b[A\n",
            " 41%|████      | 67/165 [00:07<00:10,  9.33it/s]\u001b[A\n",
            " 41%|████      | 68/165 [00:07<00:10,  9.35it/s]\u001b[A\n",
            " 42%|████▏     | 69/165 [00:07<00:10,  9.36it/s]\u001b[A\n",
            " 42%|████▏     | 70/165 [00:07<00:10,  9.33it/s]\u001b[A\n",
            " 43%|████▎     | 71/165 [00:07<00:10,  9.33it/s]\u001b[A\n",
            " 44%|████▎     | 72/165 [00:07<00:09,  9.34it/s]\u001b[A\n",
            " 44%|████▍     | 73/165 [00:07<00:09,  9.35it/s]\u001b[A\n",
            " 45%|████▍     | 74/165 [00:07<00:09,  9.35it/s]\u001b[A\n",
            " 45%|████▌     | 75/165 [00:08<00:09,  9.34it/s]\u001b[A\n",
            " 46%|████▌     | 76/165 [00:08<00:09,  9.34it/s]\u001b[A\n",
            " 47%|████▋     | 77/165 [00:08<00:09,  9.36it/s]\u001b[A\n",
            " 47%|████▋     | 78/165 [00:08<00:09,  9.35it/s]\u001b[A\n",
            " 48%|████▊     | 79/165 [00:08<00:09,  9.34it/s]\u001b[A\n",
            " 48%|████▊     | 80/165 [00:08<00:09,  9.34it/s]\u001b[A\n",
            " 49%|████▉     | 81/165 [00:08<00:09,  9.33it/s]\u001b[A\n",
            " 50%|████▉     | 82/165 [00:08<00:08,  9.35it/s]\u001b[A\n",
            " 50%|█████     | 83/165 [00:08<00:08,  9.36it/s]\u001b[A\n",
            " 51%|█████     | 84/165 [00:08<00:08,  9.32it/s]\u001b[A\n",
            " 52%|█████▏    | 85/165 [00:09<00:08,  9.34it/s]\u001b[A\n",
            " 52%|█████▏    | 86/165 [00:09<00:08,  9.35it/s]\u001b[A\n",
            " 53%|█████▎    | 87/165 [00:09<00:08,  9.33it/s]\u001b[A\n",
            " 53%|█████▎    | 88/165 [00:09<00:08,  9.36it/s]\u001b[A\n",
            " 54%|█████▍    | 89/165 [00:09<00:08,  9.38it/s]\u001b[A\n",
            " 55%|█████▍    | 90/165 [00:09<00:07,  9.40it/s]\u001b[A\n",
            " 55%|█████▌    | 91/165 [00:09<00:07,  9.41it/s]\u001b[A\n",
            " 56%|█████▌    | 92/165 [00:09<00:07,  9.41it/s]\u001b[A\n",
            " 56%|█████▋    | 93/165 [00:09<00:07,  9.40it/s]\u001b[A\n",
            " 57%|█████▋    | 94/165 [00:10<00:07,  9.40it/s]\u001b[A\n",
            " 58%|█████▊    | 95/165 [00:10<00:07,  9.40it/s]\u001b[A\n",
            " 58%|█████▊    | 96/165 [00:10<00:07,  9.40it/s]\u001b[A\n",
            " 59%|█████▉    | 97/165 [00:10<00:07,  9.40it/s]\u001b[A\n",
            " 59%|█████▉    | 98/165 [00:10<00:07,  9.39it/s]\u001b[A\n",
            " 60%|██████    | 99/165 [00:10<00:07,  9.37it/s]\u001b[A\n",
            " 61%|██████    | 100/165 [00:10<00:06,  9.38it/s]\u001b[A\n",
            " 61%|██████    | 101/165 [00:10<00:06,  9.39it/s]\u001b[A\n",
            " 62%|██████▏   | 102/165 [00:10<00:06,  9.40it/s]\u001b[A\n",
            " 62%|██████▏   | 103/165 [00:11<00:06,  9.40it/s]\u001b[A\n",
            " 63%|██████▎   | 104/165 [00:11<00:06,  9.37it/s]\u001b[A\n",
            " 64%|██████▎   | 105/165 [00:11<00:06,  9.39it/s]\u001b[A\n",
            " 64%|██████▍   | 106/165 [00:11<00:06,  9.39it/s]\u001b[A\n",
            " 65%|██████▍   | 107/165 [00:11<00:06,  9.39it/s]\u001b[A\n",
            " 65%|██████▌   | 108/165 [00:11<00:06,  9.41it/s]\u001b[A\n",
            " 66%|██████▌   | 109/165 [00:11<00:05,  9.40it/s]\u001b[A\n",
            " 67%|██████▋   | 110/165 [00:11<00:05,  9.39it/s]\u001b[A\n",
            " 67%|██████▋   | 111/165 [00:11<00:05,  9.37it/s]\u001b[A\n",
            " 68%|██████▊   | 112/165 [00:11<00:05,  9.38it/s]\u001b[A\n",
            " 68%|██████▊   | 113/165 [00:12<00:05,  9.37it/s]\u001b[A\n",
            " 69%|██████▉   | 114/165 [00:12<00:05,  9.39it/s]\u001b[A\n",
            " 70%|██████▉   | 115/165 [00:12<00:05,  9.37it/s]\u001b[A\n",
            " 70%|███████   | 116/165 [00:12<00:05,  9.33it/s]\u001b[A\n",
            " 71%|███████   | 117/165 [00:12<00:05,  9.28it/s]\u001b[A\n",
            " 72%|███████▏  | 118/165 [00:12<00:05,  9.28it/s]\u001b[A\n",
            " 72%|███████▏  | 119/165 [00:12<00:04,  9.32it/s]\u001b[A\n",
            " 73%|███████▎  | 120/165 [00:12<00:04,  9.35it/s]\u001b[A\n",
            " 73%|███████▎  | 121/165 [00:12<00:04,  9.37it/s]\u001b[A\n",
            " 74%|███████▍  | 122/165 [00:13<00:04,  9.37it/s]\u001b[A\n",
            " 75%|███████▍  | 123/165 [00:13<00:04,  9.37it/s]\u001b[A\n",
            " 75%|███████▌  | 124/165 [00:13<00:04,  9.37it/s]\u001b[A\n",
            " 76%|███████▌  | 125/165 [00:13<00:04,  9.38it/s]\u001b[A\n",
            " 76%|███████▋  | 126/165 [00:13<00:04,  9.38it/s]\u001b[A\n",
            " 77%|███████▋  | 127/165 [00:13<00:04,  9.37it/s]\u001b[A\n",
            " 78%|███████▊  | 128/165 [00:13<00:03,  9.36it/s]\u001b[A\n",
            " 78%|███████▊  | 129/165 [00:13<00:03,  9.39it/s]\u001b[A\n",
            " 79%|███████▉  | 130/165 [00:13<00:03,  9.39it/s]\u001b[A\n",
            " 79%|███████▉  | 131/165 [00:13<00:03,  9.39it/s]\u001b[A\n",
            " 80%|████████  | 132/165 [00:14<00:03,  9.40it/s]\u001b[A\n",
            " 81%|████████  | 133/165 [00:14<00:03,  9.40it/s]\u001b[A\n",
            " 81%|████████  | 134/165 [00:14<00:03,  9.41it/s]\u001b[A\n",
            " 82%|████████▏ | 135/165 [00:14<00:03,  9.40it/s]\u001b[A\n",
            " 82%|████████▏ | 136/165 [00:14<00:03,  9.41it/s]\u001b[A\n",
            " 83%|████████▎ | 137/165 [00:14<00:02,  9.41it/s]\u001b[A\n",
            " 84%|████████▎ | 138/165 [00:14<00:02,  9.41it/s]\u001b[A\n",
            " 84%|████████▍ | 139/165 [00:14<00:02,  9.41it/s]\u001b[A\n",
            " 85%|████████▍ | 140/165 [00:14<00:02,  9.41it/s]\u001b[A\n",
            " 85%|████████▌ | 141/165 [00:15<00:02,  9.41it/s]\u001b[A\n",
            " 86%|████████▌ | 142/165 [00:15<00:02,  9.40it/s]\u001b[A\n",
            " 87%|████████▋ | 143/165 [00:15<00:02,  9.41it/s]\u001b[A\n",
            " 87%|████████▋ | 144/165 [00:15<00:02,  9.42it/s]\u001b[A\n",
            " 88%|████████▊ | 145/165 [00:15<00:02,  9.43it/s]\u001b[A\n",
            " 88%|████████▊ | 146/165 [00:15<00:02,  9.41it/s]\u001b[A\n",
            " 89%|████████▉ | 147/165 [00:15<00:01,  9.41it/s]\u001b[A\n",
            " 90%|████████▉ | 148/165 [00:15<00:01,  9.41it/s]\u001b[A\n",
            " 90%|█████████ | 149/165 [00:15<00:01,  9.41it/s]\u001b[A\n",
            " 91%|█████████ | 150/165 [00:16<00:01,  9.38it/s]\u001b[A\n",
            " 92%|█████████▏| 151/165 [00:16<00:01,  9.38it/s]\u001b[A\n",
            " 92%|█████████▏| 152/165 [00:16<00:01,  9.38it/s]\u001b[A\n",
            " 93%|█████████▎| 153/165 [00:16<00:01,  9.39it/s]\u001b[A\n",
            " 93%|█████████▎| 154/165 [00:16<00:01,  9.40it/s]\u001b[A\n",
            " 94%|█████████▍| 155/165 [00:16<00:01,  9.40it/s]\u001b[A\n",
            " 95%|█████████▍| 156/165 [00:16<00:00,  9.41it/s]\u001b[A\n",
            " 95%|█████████▌| 157/165 [00:16<00:00,  9.28it/s]\u001b[A\n",
            " 96%|█████████▌| 158/165 [00:16<00:00,  9.33it/s]\u001b[A\n",
            " 96%|█████████▋| 159/165 [00:16<00:00,  9.36it/s]\u001b[A\n",
            " 97%|█████████▋| 160/165 [00:17<00:00,  9.39it/s]\u001b[A\n",
            " 98%|█████████▊| 161/165 [00:17<00:00,  9.41it/s]\u001b[A\n",
            " 98%|█████████▊| 162/165 [00:17<00:00,  9.44it/s]\u001b[A\n",
            " 99%|█████████▉| 163/165 [00:17<00:00,  9.45it/s]\u001b[A\n",
            "100%|██████████| 165/165 [00:17<00:00,  9.36it/s]\n",
            "========== model: bert-base-uncased fold: 3 inference ==========\n",
            "========== model: bert-base-uncased fold: 3 inference ==========\n",
            "========== model: bert-base-uncased fold: 3 inference ==========\n",
            "========== model: bert-base-uncased fold: 3 inference ==========\n",
            "========== model: bert-base-uncased fold: 3 inference ==========\n",
            "========== model: bert-base-uncased fold: 3 inference ==========\n",
            "========== model: bert-base-uncased fold: 3 inference ==========\n",
            "loading configuration file /content/activebus_bert_final/config.json\n",
            "Model config BertConfig {\n",
            "  \"_name_or_path\": \"activebus/BERT-XD_Review\",\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_ids\": 0,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.15.0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "loading weights file /content/activebus_bert_final/pytorch_model.bin\n",
            "Some weights of the model checkpoint at /content/activebus_bert_final were not used when initializing BertForSequenceClassification: ['cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /content/activebus_bert_final and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "\n",
            "  0%|          | 0/165 [00:00<?, ?it/s]\u001b[A\n",
            "  1%|          | 1/165 [00:00<00:19,  8.36it/s]\u001b[A\n",
            "  1%|          | 2/165 [00:00<00:18,  8.93it/s]\u001b[A\n",
            "  2%|▏         | 3/165 [00:00<00:17,  9.12it/s]\u001b[A\n",
            "  2%|▏         | 4/165 [00:00<00:17,  9.22it/s]\u001b[A\n",
            "  3%|▎         | 5/165 [00:00<00:17,  9.31it/s]\u001b[A\n",
            "  4%|▎         | 6/165 [00:00<00:16,  9.35it/s]\u001b[A\n",
            "  4%|▍         | 7/165 [00:00<00:16,  9.37it/s]\u001b[A\n",
            "  5%|▍         | 8/165 [00:00<00:16,  9.36it/s]\u001b[A\n",
            "  5%|▌         | 9/165 [00:00<00:16,  9.38it/s]\u001b[A\n",
            "  6%|▌         | 10/165 [00:01<00:16,  9.40it/s]\u001b[A\n",
            "  7%|▋         | 11/165 [00:01<00:16,  9.41it/s]\u001b[A\n",
            "  7%|▋         | 12/165 [00:01<00:16,  9.40it/s]\u001b[A\n",
            "  8%|▊         | 13/165 [00:01<00:16,  9.41it/s]\u001b[A\n",
            "  8%|▊         | 14/165 [00:01<00:16,  9.42it/s]\u001b[A\n",
            "  9%|▉         | 15/165 [00:01<00:15,  9.43it/s]\u001b[A\n",
            " 10%|▉         | 16/165 [00:01<00:15,  9.31it/s]\u001b[A\n",
            " 10%|█         | 17/165 [00:01<00:15,  9.34it/s]\u001b[A\n",
            " 11%|█         | 18/165 [00:01<00:15,  9.36it/s]\u001b[A\n",
            " 12%|█▏        | 19/165 [00:02<00:15,  9.38it/s]\u001b[A\n",
            " 12%|█▏        | 20/165 [00:02<00:15,  9.41it/s]\u001b[A\n",
            " 13%|█▎        | 21/165 [00:02<00:15,  9.38it/s]\u001b[A\n",
            " 13%|█▎        | 22/165 [00:02<00:15,  9.39it/s]\u001b[A\n",
            " 14%|█▍        | 23/165 [00:02<00:15,  9.40it/s]\u001b[A\n",
            " 15%|█▍        | 24/165 [00:02<00:15,  9.39it/s]\u001b[A\n",
            " 15%|█▌        | 25/165 [00:02<00:14,  9.41it/s]\u001b[A\n",
            " 16%|█▌        | 26/165 [00:02<00:14,  9.41it/s]\u001b[A\n",
            " 16%|█▋        | 27/165 [00:02<00:14,  9.42it/s]\u001b[A\n",
            " 17%|█▋        | 28/165 [00:02<00:14,  9.35it/s]\u001b[A\n",
            " 18%|█▊        | 29/165 [00:03<00:14,  9.35it/s]\u001b[A\n",
            " 18%|█▊        | 30/165 [00:03<00:14,  9.36it/s]\u001b[A\n",
            " 19%|█▉        | 31/165 [00:03<00:14,  9.36it/s]\u001b[A\n",
            " 19%|█▉        | 32/165 [00:03<00:14,  9.35it/s]\u001b[A\n",
            " 20%|██        | 33/165 [00:03<00:14,  9.33it/s]\u001b[A\n",
            " 21%|██        | 34/165 [00:03<00:14,  9.34it/s]\u001b[A\n",
            " 21%|██        | 35/165 [00:03<00:13,  9.35it/s]\u001b[A\n",
            " 22%|██▏       | 36/165 [00:03<00:13,  9.36it/s]\u001b[A\n",
            " 22%|██▏       | 37/165 [00:03<00:13,  9.37it/s]\u001b[A\n",
            " 23%|██▎       | 38/165 [00:04<00:13,  9.38it/s]\u001b[A\n",
            " 24%|██▎       | 39/165 [00:04<00:13,  9.38it/s]\u001b[A\n",
            " 24%|██▍       | 40/165 [00:04<00:13,  9.38it/s]\u001b[A\n",
            " 25%|██▍       | 41/165 [00:04<00:13,  9.36it/s]\u001b[A\n",
            " 25%|██▌       | 42/165 [00:04<00:13,  9.39it/s]\u001b[A\n",
            " 26%|██▌       | 43/165 [00:04<00:13,  9.38it/s]\u001b[A\n",
            " 27%|██▋       | 44/165 [00:04<00:12,  9.36it/s]\u001b[A\n",
            " 27%|██▋       | 45/165 [00:04<00:12,  9.38it/s]\u001b[A\n",
            " 28%|██▊       | 46/165 [00:04<00:12,  9.37it/s]\u001b[A\n",
            " 28%|██▊       | 47/165 [00:05<00:12,  9.39it/s]\u001b[A\n",
            " 29%|██▉       | 48/165 [00:05<00:12,  9.35it/s]\u001b[A\n",
            " 30%|██▉       | 49/165 [00:05<00:12,  9.33it/s]\u001b[A\n",
            " 30%|███       | 50/165 [00:05<00:12,  9.34it/s]\u001b[A\n",
            " 31%|███       | 51/165 [00:05<00:12,  9.34it/s]\u001b[A\n",
            " 32%|███▏      | 52/165 [00:05<00:12,  9.35it/s]\u001b[A\n",
            " 32%|███▏      | 53/165 [00:05<00:11,  9.36it/s]\u001b[A\n",
            " 33%|███▎      | 54/165 [00:05<00:11,  9.37it/s]\u001b[A\n",
            " 33%|███▎      | 55/165 [00:05<00:11,  9.38it/s]\u001b[A\n",
            " 34%|███▍      | 56/165 [00:05<00:11,  9.37it/s]\u001b[A\n",
            " 35%|███▍      | 57/165 [00:06<00:11,  9.38it/s]\u001b[A\n",
            " 35%|███▌      | 58/165 [00:06<00:11,  9.33it/s]\u001b[A\n",
            " 36%|███▌      | 59/165 [00:06<00:11,  9.35it/s]\u001b[A\n",
            " 36%|███▋      | 60/165 [00:06<00:11,  9.37it/s]\u001b[A\n",
            " 37%|███▋      | 61/165 [00:06<00:11,  9.38it/s]\u001b[A\n",
            " 38%|███▊      | 62/165 [00:06<00:10,  9.40it/s]\u001b[A\n",
            " 38%|███▊      | 63/165 [00:06<00:10,  9.40it/s]\u001b[A\n",
            " 39%|███▉      | 64/165 [00:06<00:10,  9.40it/s]\u001b[A\n",
            " 39%|███▉      | 65/165 [00:06<00:10,  9.40it/s]\u001b[A\n",
            " 40%|████      | 66/165 [00:07<00:10,  9.41it/s]\u001b[A\n",
            " 41%|████      | 67/165 [00:07<00:10,  9.42it/s]\u001b[A\n",
            " 41%|████      | 68/165 [00:07<00:10,  9.40it/s]\u001b[A\n",
            " 42%|████▏     | 69/165 [00:07<00:10,  9.42it/s]\u001b[A\n",
            " 42%|████▏     | 70/165 [00:07<00:10,  9.43it/s]\u001b[A\n",
            " 43%|████▎     | 71/165 [00:07<00:10,  9.39it/s]\u001b[A\n",
            " 44%|████▎     | 72/165 [00:07<00:09,  9.38it/s]\u001b[A\n",
            " 44%|████▍     | 73/165 [00:07<00:09,  9.39it/s]\u001b[A\n",
            " 45%|████▍     | 74/165 [00:07<00:09,  9.39it/s]\u001b[A\n",
            " 45%|████▌     | 75/165 [00:08<00:09,  9.38it/s]\u001b[A\n",
            " 46%|████▌     | 76/165 [00:08<00:09,  9.38it/s]\u001b[A\n",
            " 47%|████▋     | 77/165 [00:08<00:09,  9.38it/s]\u001b[A\n",
            " 47%|████▋     | 78/165 [00:08<00:09,  9.36it/s]\u001b[A\n",
            " 48%|████▊     | 79/165 [00:08<00:09,  9.39it/s]\u001b[A\n",
            " 48%|████▊     | 80/165 [00:08<00:09,  9.40it/s]\u001b[A\n",
            " 49%|████▉     | 81/165 [00:08<00:08,  9.41it/s]\u001b[A\n",
            " 50%|████▉     | 82/165 [00:08<00:08,  9.41it/s]\u001b[A\n",
            " 50%|█████     | 83/165 [00:08<00:08,  9.37it/s]\u001b[A\n",
            " 51%|█████     | 84/165 [00:08<00:08,  9.37it/s]\u001b[A\n",
            " 52%|█████▏    | 85/165 [00:09<00:08,  9.38it/s]\u001b[A\n",
            " 52%|█████▏    | 86/165 [00:09<00:08,  9.39it/s]\u001b[A\n",
            " 53%|█████▎    | 87/165 [00:09<00:08,  9.41it/s]\u001b[A\n",
            " 53%|█████▎    | 88/165 [00:09<00:08,  9.40it/s]\u001b[A\n",
            " 54%|█████▍    | 89/165 [00:09<00:08,  9.38it/s]\u001b[A\n",
            " 55%|█████▍    | 90/165 [00:09<00:07,  9.39it/s]\u001b[A\n",
            " 55%|█████▌    | 91/165 [00:09<00:07,  9.37it/s]\u001b[A\n",
            " 56%|█████▌    | 92/165 [00:09<00:07,  9.34it/s]\u001b[A\n",
            " 56%|█████▋    | 93/165 [00:09<00:07,  9.37it/s]\u001b[A\n",
            " 57%|█████▋    | 94/165 [00:10<00:07,  9.39it/s]\u001b[A\n",
            " 58%|█████▊    | 95/165 [00:10<00:07,  9.40it/s]\u001b[A\n",
            " 58%|█████▊    | 96/165 [00:10<00:07,  9.39it/s]\u001b[A\n",
            " 59%|█████▉    | 97/165 [00:10<00:07,  9.36it/s]\u001b[A\n",
            " 59%|█████▉    | 98/165 [00:10<00:07,  9.35it/s]\u001b[A\n",
            " 60%|██████    | 99/165 [00:10<00:07,  9.36it/s]\u001b[A\n",
            " 61%|██████    | 100/165 [00:10<00:06,  9.33it/s]\u001b[A\n",
            " 61%|██████    | 101/165 [00:10<00:06,  9.33it/s]\u001b[A\n",
            " 62%|██████▏   | 102/165 [00:10<00:06,  9.32it/s]\u001b[A\n",
            " 62%|██████▏   | 103/165 [00:10<00:06,  9.34it/s]\u001b[A\n",
            " 63%|██████▎   | 104/165 [00:11<00:06,  9.37it/s]\u001b[A\n",
            " 64%|██████▎   | 105/165 [00:11<00:06,  9.39it/s]\u001b[A\n",
            " 64%|██████▍   | 106/165 [00:11<00:06,  9.40it/s]\u001b[A\n",
            " 65%|██████▍   | 107/165 [00:11<00:06,  9.40it/s]\u001b[A\n",
            " 65%|██████▌   | 108/165 [00:11<00:06,  9.41it/s]\u001b[A\n",
            " 66%|██████▌   | 109/165 [00:11<00:05,  9.43it/s]\u001b[A\n",
            " 67%|██████▋   | 110/165 [00:11<00:05,  9.41it/s]\u001b[A\n",
            " 67%|██████▋   | 111/165 [00:11<00:05,  9.40it/s]\u001b[A\n",
            " 68%|██████▊   | 112/165 [00:11<00:05,  9.40it/s]\u001b[A\n",
            " 68%|██████▊   | 113/165 [00:12<00:05,  9.37it/s]\u001b[A\n",
            " 69%|██████▉   | 114/165 [00:12<00:05,  9.37it/s]\u001b[A\n",
            " 70%|██████▉   | 115/165 [00:12<00:05,  9.39it/s]\u001b[A\n",
            " 70%|███████   | 116/165 [00:12<00:05,  9.41it/s]\u001b[A\n",
            " 71%|███████   | 117/165 [00:12<00:05,  9.40it/s]\u001b[A\n",
            " 72%|███████▏  | 118/165 [00:12<00:05,  9.40it/s]\u001b[A\n",
            " 72%|███████▏  | 119/165 [00:12<00:04,  9.37it/s]\u001b[A\n",
            " 73%|███████▎  | 120/165 [00:12<00:04,  9.37it/s]\u001b[A\n",
            " 73%|███████▎  | 121/165 [00:12<00:04,  9.38it/s]\u001b[A\n",
            " 74%|███████▍  | 122/165 [00:13<00:04,  9.39it/s]\u001b[A\n",
            " 75%|███████▍  | 123/165 [00:13<00:04,  9.39it/s]\u001b[A\n",
            " 75%|███████▌  | 124/165 [00:13<00:04,  9.40it/s]\u001b[A\n",
            " 76%|███████▌  | 125/165 [00:13<00:04,  9.42it/s]\u001b[A\n",
            " 76%|███████▋  | 126/165 [00:13<00:04,  9.43it/s]\u001b[A\n",
            " 77%|███████▋  | 127/165 [00:13<00:04,  9.42it/s]\u001b[A\n",
            " 78%|███████▊  | 128/165 [00:13<00:03,  9.43it/s]\u001b[A\n",
            " 78%|███████▊  | 129/165 [00:13<00:03,  9.43it/s]\u001b[A\n",
            " 79%|███████▉  | 130/165 [00:13<00:03,  9.42it/s]\u001b[A\n",
            " 79%|███████▉  | 131/165 [00:13<00:03,  9.41it/s]\u001b[A\n",
            " 80%|████████  | 132/165 [00:14<00:03,  9.39it/s]\u001b[A\n",
            " 81%|████████  | 133/165 [00:14<00:03,  9.39it/s]\u001b[A\n",
            " 81%|████████  | 134/165 [00:14<00:03,  9.40it/s]\u001b[A\n",
            " 82%|████████▏ | 135/165 [00:14<00:03,  9.40it/s]\u001b[A\n",
            " 82%|████████▏ | 136/165 [00:14<00:03,  9.42it/s]\u001b[A\n",
            " 83%|████████▎ | 137/165 [00:14<00:02,  9.42it/s]\u001b[A\n",
            " 84%|████████▎ | 138/165 [00:14<00:02,  9.41it/s]\u001b[A\n",
            " 84%|████████▍ | 139/165 [00:14<00:02,  9.40it/s]\u001b[A\n",
            " 85%|████████▍ | 140/165 [00:14<00:02,  9.41it/s]\u001b[A\n",
            " 85%|████████▌ | 141/165 [00:15<00:02,  9.43it/s]\u001b[A\n",
            " 86%|████████▌ | 142/165 [00:15<00:02,  9.43it/s]\u001b[A\n",
            " 87%|████████▋ | 143/165 [00:15<00:02,  9.41it/s]\u001b[A\n",
            " 87%|████████▋ | 144/165 [00:15<00:02,  9.40it/s]\u001b[A\n",
            " 88%|████████▊ | 145/165 [00:15<00:02,  9.33it/s]\u001b[A\n",
            " 88%|████████▊ | 146/165 [00:15<00:02,  9.36it/s]\u001b[A\n",
            " 89%|████████▉ | 147/165 [00:15<00:01,  9.38it/s]\u001b[A\n",
            " 90%|████████▉ | 148/165 [00:15<00:01,  9.39it/s]\u001b[A\n",
            " 90%|█████████ | 149/165 [00:15<00:01,  9.39it/s]\u001b[A\n",
            " 91%|█████████ | 150/165 [00:15<00:01,  9.41it/s]\u001b[A\n",
            " 92%|█████████▏| 151/165 [00:16<00:01,  9.40it/s]\u001b[A\n",
            " 92%|█████████▏| 152/165 [00:16<00:01,  9.41it/s]\u001b[A\n",
            " 93%|█████████▎| 153/165 [00:16<00:01,  9.42it/s]\u001b[A\n",
            " 93%|█████████▎| 154/165 [00:16<00:01,  9.42it/s]\u001b[A\n",
            " 94%|█████████▍| 155/165 [00:16<00:01,  9.42it/s]\u001b[A\n",
            " 95%|█████████▍| 156/165 [00:16<00:00,  9.42it/s]\u001b[A\n",
            " 95%|█████████▌| 157/165 [00:16<00:00,  9.42it/s]\u001b[A\n",
            " 96%|█████████▌| 158/165 [00:16<00:00,  9.44it/s]\u001b[A\n",
            " 96%|█████████▋| 159/165 [00:16<00:00,  9.45it/s]\u001b[A\n",
            " 97%|█████████▋| 160/165 [00:17<00:00,  9.44it/s]\u001b[A\n",
            " 98%|█████████▊| 161/165 [00:17<00:00,  9.44it/s]\u001b[A\n",
            " 98%|█████████▊| 162/165 [00:17<00:00,  9.45it/s]\u001b[A\n",
            " 99%|█████████▉| 163/165 [00:17<00:00,  9.46it/s]\u001b[A\n",
            "100%|██████████| 165/165 [00:17<00:00,  9.37it/s]\n",
            "========== model: bert-base-uncased fold: 4 inference ==========\n",
            "========== model: bert-base-uncased fold: 4 inference ==========\n",
            "========== model: bert-base-uncased fold: 4 inference ==========\n",
            "========== model: bert-base-uncased fold: 4 inference ==========\n",
            "========== model: bert-base-uncased fold: 4 inference ==========\n",
            "========== model: bert-base-uncased fold: 4 inference ==========\n",
            "========== model: bert-base-uncased fold: 4 inference ==========\n",
            "loading configuration file /content/activebus_bert_final/config.json\n",
            "Model config BertConfig {\n",
            "  \"_name_or_path\": \"activebus/BERT-XD_Review\",\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_ids\": 0,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.15.0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "loading weights file /content/activebus_bert_final/pytorch_model.bin\n",
            "Some weights of the model checkpoint at /content/activebus_bert_final were not used when initializing BertForSequenceClassification: ['cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /content/activebus_bert_final and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "\n",
            "  0%|          | 0/165 [00:00<?, ?it/s]\u001b[A\n",
            "  1%|          | 1/165 [00:00<00:18,  8.97it/s]\u001b[A\n",
            "  1%|          | 2/165 [00:00<00:17,  9.15it/s]\u001b[A\n",
            "  2%|▏         | 3/165 [00:00<00:17,  9.24it/s]\u001b[A\n",
            "  2%|▏         | 4/165 [00:00<00:17,  9.30it/s]\u001b[A\n",
            "  3%|▎         | 5/165 [00:00<00:17,  9.32it/s]\u001b[A\n",
            "  4%|▎         | 6/165 [00:00<00:16,  9.36it/s]\u001b[A\n",
            "  4%|▍         | 7/165 [00:00<00:16,  9.36it/s]\u001b[A\n",
            "  5%|▍         | 8/165 [00:00<00:16,  9.37it/s]\u001b[A\n",
            "  5%|▌         | 9/165 [00:00<00:16,  9.37it/s]\u001b[A\n",
            "  6%|▌         | 10/165 [00:01<00:16,  9.40it/s]\u001b[A\n",
            "  7%|▋         | 11/165 [00:01<00:16,  9.41it/s]\u001b[A\n",
            "  7%|▋         | 12/165 [00:01<00:16,  9.42it/s]\u001b[A\n",
            "  8%|▊         | 13/165 [00:01<00:16,  9.39it/s]\u001b[A\n",
            "  8%|▊         | 14/165 [00:01<00:16,  9.39it/s]\u001b[A\n",
            "  9%|▉         | 15/165 [00:01<00:15,  9.39it/s]\u001b[A\n",
            " 10%|▉         | 16/165 [00:01<00:15,  9.40it/s]\u001b[A\n",
            " 10%|█         | 17/165 [00:01<00:15,  9.41it/s]\u001b[A\n",
            " 11%|█         | 18/165 [00:01<00:15,  9.41it/s]\u001b[A\n",
            " 12%|█▏        | 19/165 [00:02<00:15,  9.41it/s]\u001b[A\n",
            " 12%|█▏        | 20/165 [00:02<00:15,  9.43it/s]\u001b[A\n",
            " 13%|█▎        | 21/165 [00:02<00:15,  9.42it/s]\u001b[A\n",
            " 13%|█▎        | 22/165 [00:02<00:15,  9.42it/s]\u001b[A\n",
            " 14%|█▍        | 23/165 [00:02<00:15,  9.41it/s]\u001b[A\n",
            " 15%|█▍        | 24/165 [00:02<00:14,  9.41it/s]\u001b[A\n",
            " 15%|█▌        | 25/165 [00:02<00:14,  9.42it/s]\u001b[A\n",
            " 16%|█▌        | 26/165 [00:02<00:14,  9.41it/s]\u001b[A\n",
            " 16%|█▋        | 27/165 [00:02<00:14,  9.41it/s]\u001b[A\n",
            " 17%|█▋        | 28/165 [00:02<00:14,  9.41it/s]\u001b[A\n",
            " 18%|█▊        | 29/165 [00:03<00:14,  9.40it/s]\u001b[A\n",
            " 18%|█▊        | 30/165 [00:03<00:14,  9.39it/s]\u001b[A\n",
            " 19%|█▉        | 31/165 [00:03<00:14,  9.41it/s]\u001b[A\n",
            " 19%|█▉        | 32/165 [00:03<00:14,  9.41it/s]\u001b[A\n",
            " 20%|██        | 33/165 [00:03<00:14,  9.42it/s]\u001b[A\n",
            " 21%|██        | 34/165 [00:03<00:13,  9.41it/s]\u001b[A\n",
            " 21%|██        | 35/165 [00:03<00:13,  9.41it/s]\u001b[A\n",
            " 22%|██▏       | 36/165 [00:03<00:13,  9.39it/s]\u001b[A\n",
            " 22%|██▏       | 37/165 [00:03<00:13,  9.38it/s]\u001b[A\n",
            " 23%|██▎       | 38/165 [00:04<00:13,  9.39it/s]\u001b[A\n",
            " 24%|██▎       | 39/165 [00:04<00:13,  9.40it/s]\u001b[A\n",
            " 24%|██▍       | 40/165 [00:04<00:13,  9.39it/s]\u001b[A\n",
            " 25%|██▍       | 41/165 [00:04<00:13,  9.37it/s]\u001b[A\n",
            " 25%|██▌       | 42/165 [00:04<00:13,  9.39it/s]\u001b[A\n",
            " 26%|██▌       | 43/165 [00:04<00:12,  9.40it/s]\u001b[A\n",
            " 27%|██▋       | 44/165 [00:04<00:12,  9.38it/s]\u001b[A\n",
            " 27%|██▋       | 45/165 [00:04<00:12,  9.38it/s]\u001b[A\n",
            " 28%|██▊       | 46/165 [00:04<00:12,  9.38it/s]\u001b[A\n",
            " 28%|██▊       | 47/165 [00:05<00:12,  9.38it/s]\u001b[A\n",
            " 29%|██▉       | 48/165 [00:05<00:12,  9.39it/s]\u001b[A\n",
            " 30%|██▉       | 49/165 [00:05<00:12,  9.39it/s]\u001b[A\n",
            " 30%|███       | 50/165 [00:05<00:12,  9.39it/s]\u001b[A\n",
            " 31%|███       | 51/165 [00:05<00:12,  9.39it/s]\u001b[A\n",
            " 32%|███▏      | 52/165 [00:05<00:12,  9.38it/s]\u001b[A\n",
            " 32%|███▏      | 53/165 [00:05<00:11,  9.36it/s]\u001b[A\n",
            " 33%|███▎      | 54/165 [00:05<00:11,  9.37it/s]\u001b[A\n",
            " 33%|███▎      | 55/165 [00:05<00:11,  9.37it/s]\u001b[A\n",
            " 34%|███▍      | 56/165 [00:05<00:11,  9.38it/s]\u001b[A\n",
            " 35%|███▍      | 57/165 [00:06<00:11,  9.37it/s]\u001b[A\n",
            " 35%|███▌      | 58/165 [00:06<00:11,  9.35it/s]\u001b[A\n",
            " 36%|███▌      | 59/165 [00:06<00:11,  9.36it/s]\u001b[A\n",
            " 36%|███▋      | 60/165 [00:06<00:11,  9.36it/s]\u001b[A\n",
            " 37%|███▋      | 61/165 [00:06<00:11,  9.37it/s]\u001b[A\n",
            " 38%|███▊      | 62/165 [00:06<00:10,  9.38it/s]\u001b[A\n",
            " 38%|███▊      | 63/165 [00:06<00:10,  9.38it/s]\u001b[A\n",
            " 39%|███▉      | 64/165 [00:06<00:10,  9.38it/s]\u001b[A\n",
            " 39%|███▉      | 65/165 [00:06<00:10,  9.39it/s]\u001b[A\n",
            " 40%|████      | 66/165 [00:07<00:10,  9.40it/s]\u001b[A\n",
            " 41%|████      | 67/165 [00:07<00:10,  9.41it/s]\u001b[A\n",
            " 41%|████      | 68/165 [00:07<00:10,  9.42it/s]\u001b[A\n",
            " 42%|████▏     | 69/165 [00:07<00:10,  9.39it/s]\u001b[A\n",
            " 42%|████▏     | 70/165 [00:07<00:10,  9.38it/s]\u001b[A\n",
            " 43%|████▎     | 71/165 [00:07<00:10,  9.40it/s]\u001b[A\n",
            " 44%|████▎     | 72/165 [00:07<00:09,  9.39it/s]\u001b[A\n",
            " 44%|████▍     | 73/165 [00:07<00:09,  9.40it/s]\u001b[A\n",
            " 45%|████▍     | 74/165 [00:07<00:09,  9.40it/s]\u001b[A\n",
            " 45%|████▌     | 75/165 [00:07<00:09,  9.39it/s]\u001b[A\n",
            " 46%|████▌     | 76/165 [00:08<00:09,  9.41it/s]\u001b[A\n",
            " 47%|████▋     | 77/165 [00:08<00:09,  9.40it/s]\u001b[A\n",
            " 47%|████▋     | 78/165 [00:08<00:09,  9.40it/s]\u001b[A\n",
            " 48%|████▊     | 79/165 [00:08<00:09,  9.40it/s]\u001b[A\n",
            " 48%|████▊     | 80/165 [00:08<00:09,  9.40it/s]\u001b[A\n",
            " 49%|████▉     | 81/165 [00:08<00:08,  9.42it/s]\u001b[A\n",
            " 50%|████▉     | 82/165 [00:08<00:08,  9.42it/s]\u001b[A\n",
            " 50%|█████     | 83/165 [00:08<00:08,  9.43it/s]\u001b[A\n",
            " 51%|█████     | 84/165 [00:08<00:08,  9.43it/s]\u001b[A\n",
            " 52%|█████▏    | 85/165 [00:09<00:08,  9.43it/s]\u001b[A\n",
            " 52%|█████▏    | 86/165 [00:09<00:08,  9.43it/s]\u001b[A\n",
            " 53%|█████▎    | 87/165 [00:09<00:08,  9.43it/s]\u001b[A\n",
            " 53%|█████▎    | 88/165 [00:09<00:08,  9.41it/s]\u001b[A\n",
            " 54%|█████▍    | 89/165 [00:09<00:08,  9.41it/s]\u001b[A\n",
            " 55%|█████▍    | 90/165 [00:09<00:07,  9.39it/s]\u001b[A\n",
            " 55%|█████▌    | 91/165 [00:09<00:07,  9.39it/s]\u001b[A\n",
            " 56%|█████▌    | 92/165 [00:09<00:07,  9.38it/s]\u001b[A\n",
            " 56%|█████▋    | 93/165 [00:09<00:07,  9.40it/s]\u001b[A\n",
            " 57%|█████▋    | 94/165 [00:10<00:07,  9.40it/s]\u001b[A\n",
            " 58%|█████▊    | 95/165 [00:10<00:07,  9.40it/s]\u001b[A\n",
            " 58%|█████▊    | 96/165 [00:10<00:07,  9.41it/s]\u001b[A\n",
            " 59%|█████▉    | 97/165 [00:10<00:07,  9.39it/s]\u001b[A\n",
            " 59%|█████▉    | 98/165 [00:10<00:07,  9.40it/s]\u001b[A\n",
            " 60%|██████    | 99/165 [00:10<00:07,  9.40it/s]\u001b[A\n",
            " 61%|██████    | 100/165 [00:10<00:06,  9.40it/s]\u001b[A\n",
            " 61%|██████    | 101/165 [00:10<00:06,  9.40it/s]\u001b[A\n",
            " 62%|██████▏   | 102/165 [00:10<00:06,  9.40it/s]\u001b[A\n",
            " 62%|██████▏   | 103/165 [00:10<00:06,  9.39it/s]\u001b[A\n",
            " 63%|██████▎   | 104/165 [00:11<00:06,  9.39it/s]\u001b[A\n",
            " 64%|██████▎   | 105/165 [00:11<00:06,  9.39it/s]\u001b[A\n",
            " 64%|██████▍   | 106/165 [00:11<00:06,  9.40it/s]\u001b[A\n",
            " 65%|██████▍   | 107/165 [00:11<00:06,  9.41it/s]\u001b[A\n",
            " 65%|██████▌   | 108/165 [00:11<00:06,  9.42it/s]\u001b[A\n",
            " 66%|██████▌   | 109/165 [00:11<00:05,  9.41it/s]\u001b[A\n",
            " 67%|██████▋   | 110/165 [00:11<00:05,  9.38it/s]\u001b[A\n",
            " 67%|██████▋   | 111/165 [00:11<00:05,  9.40it/s]\u001b[A\n",
            " 68%|██████▊   | 112/165 [00:11<00:05,  9.40it/s]\u001b[A\n",
            " 68%|██████▊   | 113/165 [00:12<00:05,  9.40it/s]\u001b[A\n",
            " 69%|██████▉   | 114/165 [00:12<00:05,  9.41it/s]\u001b[A\n",
            " 70%|██████▉   | 115/165 [00:12<00:05,  9.41it/s]\u001b[A\n",
            " 70%|███████   | 116/165 [00:12<00:05,  9.42it/s]\u001b[A\n",
            " 71%|███████   | 117/165 [00:12<00:05,  9.42it/s]\u001b[A\n",
            " 72%|███████▏  | 118/165 [00:12<00:05,  9.40it/s]\u001b[A\n",
            " 72%|███████▏  | 119/165 [00:12<00:04,  9.38it/s]\u001b[A\n",
            " 73%|███████▎  | 120/165 [00:12<00:04,  9.39it/s]\u001b[A\n",
            " 73%|███████▎  | 121/165 [00:12<00:04,  9.38it/s]\u001b[A\n",
            " 74%|███████▍  | 122/165 [00:12<00:04,  9.40it/s]\u001b[A\n",
            " 75%|███████▍  | 123/165 [00:13<00:04,  9.41it/s]\u001b[A\n",
            " 75%|███████▌  | 124/165 [00:13<00:04,  9.43it/s]\u001b[A\n",
            " 76%|███████▌  | 125/165 [00:13<00:04,  9.41it/s]\u001b[A\n",
            " 76%|███████▋  | 126/165 [00:13<00:04,  9.40it/s]\u001b[A\n",
            " 77%|███████▋  | 127/165 [00:13<00:04,  9.40it/s]\u001b[A\n",
            " 78%|███████▊  | 128/165 [00:13<00:03,  9.38it/s]\u001b[A\n",
            " 78%|███████▊  | 129/165 [00:13<00:03,  9.39it/s]\u001b[A\n",
            " 79%|███████▉  | 130/165 [00:13<00:03,  9.38it/s]\u001b[A\n",
            " 79%|███████▉  | 131/165 [00:13<00:03,  9.38it/s]\u001b[A\n",
            " 80%|████████  | 132/165 [00:14<00:03,  9.34it/s]\u001b[A\n",
            " 81%|████████  | 133/165 [00:14<00:03,  9.33it/s]\u001b[A\n",
            " 81%|████████  | 134/165 [00:14<00:03,  9.33it/s]\u001b[A\n",
            " 82%|████████▏ | 135/165 [00:14<00:03,  9.35it/s]\u001b[A\n",
            " 82%|████████▏ | 136/165 [00:14<00:03,  9.36it/s]\u001b[A\n",
            " 83%|████████▎ | 137/165 [00:14<00:02,  9.36it/s]\u001b[A\n",
            " 84%|████████▎ | 138/165 [00:14<00:02,  9.37it/s]\u001b[A\n",
            " 84%|████████▍ | 139/165 [00:14<00:02,  9.40it/s]\u001b[A\n",
            " 85%|████████▍ | 140/165 [00:14<00:02,  9.41it/s]\u001b[A\n",
            " 85%|████████▌ | 141/165 [00:15<00:02,  9.41it/s]\u001b[A\n",
            " 86%|████████▌ | 142/165 [00:15<00:02,  9.41it/s]\u001b[A\n",
            " 87%|████████▋ | 143/165 [00:15<00:02,  9.40it/s]\u001b[A\n",
            " 87%|████████▋ | 144/165 [00:15<00:02,  9.38it/s]\u001b[A\n",
            " 88%|████████▊ | 145/165 [00:15<00:02,  9.39it/s]\u001b[A\n",
            " 88%|████████▊ | 146/165 [00:15<00:02,  9.39it/s]\u001b[A\n",
            " 89%|████████▉ | 147/165 [00:15<00:01,  9.39it/s]\u001b[A\n",
            " 90%|████████▉ | 148/165 [00:15<00:01,  9.39it/s]\u001b[A\n",
            " 90%|█████████ | 149/165 [00:15<00:01,  9.40it/s]\u001b[A\n",
            " 91%|█████████ | 150/165 [00:15<00:01,  9.41it/s]\u001b[A\n",
            " 92%|█████████▏| 151/165 [00:16<00:01,  9.41it/s]\u001b[A\n",
            " 92%|█████████▏| 152/165 [00:16<00:01,  9.41it/s]\u001b[A\n",
            " 93%|█████████▎| 153/165 [00:16<00:01,  9.40it/s]\u001b[A\n",
            " 93%|█████████▎| 154/165 [00:16<00:01,  9.40it/s]\u001b[A\n",
            " 94%|█████████▍| 155/165 [00:16<00:01,  9.40it/s]\u001b[A\n",
            " 95%|█████████▍| 156/165 [00:16<00:00,  9.38it/s]\u001b[A\n",
            " 95%|█████████▌| 157/165 [00:16<00:00,  9.38it/s]\u001b[A\n",
            " 96%|█████████▌| 158/165 [00:16<00:00,  9.39it/s]\u001b[A\n",
            " 96%|█████████▋| 159/165 [00:16<00:00,  9.42it/s]\u001b[A\n",
            " 97%|█████████▋| 160/165 [00:17<00:00,  9.44it/s]\u001b[A\n",
            " 98%|█████████▊| 161/165 [00:17<00:00,  9.45it/s]\u001b[A\n",
            " 98%|█████████▊| 162/165 [00:17<00:00,  9.46it/s]\u001b[A\n",
            " 99%|█████████▉| 163/165 [00:17<00:00,  9.45it/s]\u001b[A\n",
            "100%|██████████| 165/165 [00:17<00:00,  9.38it/s]\n",
            " 33%|███▎      | 1/3 [45:24<1:30:48, 2724.34s/it]Didn't find file /content/activebus_bert_final/added_tokens.json. We won't load it.\n",
            "loading file /content/activebus_bert_final/vocab.txt\n",
            "loading file /content/activebus_bert_final/tokenizer.json\n",
            "loading file None\n",
            "loading file /content/activebus_bert_final/special_tokens_map.json\n",
            "loading file /content/activebus_bert_final/tokenizer_config.json\n",
            "========== fold: 0 training ==========\n",
            "========== fold: 0 training ==========\n",
            "========== fold: 0 training ==========\n",
            "========== fold: 0 training ==========\n",
            "========== fold: 0 training ==========\n",
            "========== fold: 0 training ==========\n",
            "========== fold: 0 training ==========\n",
            "========== fold: 0 training ==========\n",
            "Didn't find file /content/activebus_bert_final/added_tokens.json. We won't load it.\n",
            "loading file /content/activebus_bert_final/vocab.txt\n",
            "loading file None\n",
            "loading file /content/activebus_bert_final/special_tokens_map.json\n",
            "loading file /content/activebus_bert_final/tokenizer_config.json\n",
            "loading file /content/activebus_bert_final/tokenizer.json\n",
            "Didn't find file /content/activebus_bert_final/added_tokens.json. We won't load it.\n",
            "loading file /content/activebus_bert_final/vocab.txt\n",
            "loading file None\n",
            "loading file /content/activebus_bert_final/special_tokens_map.json\n",
            "loading file /content/activebus_bert_final/tokenizer_config.json\n",
            "loading file /content/activebus_bert_final/tokenizer.json\n",
            "loading configuration file /content/activebus_bert_final/config.json\n",
            "Model config BertConfig {\n",
            "  \"_name_or_path\": \"activebus/BERT-XD_Review\",\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_ids\": 0,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.15.0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "loading weights file /content/activebus_bert_final/pytorch_model.bin\n",
            "Some weights of the model checkpoint at /content/activebus_bert_final were not used when initializing BertForSequenceClassification: ['cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /content/activebus_bert_final and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: [1][0/306] Elapsed 0m 0s (remain 3m 5s) Loss: 0.7139 \n",
            "Epoch: [1][100/306] Elapsed 0m 31s (remain 1m 3s) Loss: 0.2754 \n",
            "Epoch: [1][200/306] Elapsed 1m 2s (remain 0m 32s) Loss: 0.2286 \n",
            "Epoch: [1][300/306] Elapsed 1m 33s (remain 0m 1s) Loss: 0.2133 \n",
            "Epoch: [1][305/306] Elapsed 1m 34s (remain 0m 0s) Loss: 0.2122 \n",
            "EVAL: [0/77] Elapsed 0m 0s (remain 0m 26s) Loss: 0.1076 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1 - avg_train_loss: 0.2122  avg_val_loss: 0.1368  time: 103s\n",
            "Epoch 1 - avg_train_loss: 0.2122  avg_val_loss: 0.1368  time: 103s\n",
            "Epoch 1 - avg_train_loss: 0.2122  avg_val_loss: 0.1368  time: 103s\n",
            "Epoch 1 - avg_train_loss: 0.2122  avg_val_loss: 0.1368  time: 103s\n",
            "Epoch 1 - avg_train_loss: 0.2122  avg_val_loss: 0.1368  time: 103s\n",
            "Epoch 1 - avg_train_loss: 0.2122  avg_val_loss: 0.1368  time: 103s\n",
            "Epoch 1 - avg_train_loss: 0.2122  avg_val_loss: 0.1368  time: 103s\n",
            "Epoch 1 - avg_train_loss: 0.2122  avg_val_loss: 0.1368  time: 103s\n",
            "Epoch 1 - Score: 0.13683615430259732\n",
            "Epoch 1 - Score: 0.13683615430259732\n",
            "Epoch 1 - Score: 0.13683615430259732\n",
            "Epoch 1 - Score: 0.13683615430259732\n",
            "Epoch 1 - Score: 0.13683615430259732\n",
            "Epoch 1 - Score: 0.13683615430259732\n",
            "Epoch 1 - Score: 0.13683615430259732\n",
            "Epoch 1 - Score: 0.13683615430259732\n",
            "Epoch 1 - Save Best Score: 0.1368 Model\n",
            "Epoch 1 - Save Best Score: 0.1368 Model\n",
            "Epoch 1 - Save Best Score: 0.1368 Model\n",
            "Epoch 1 - Save Best Score: 0.1368 Model\n",
            "Epoch 1 - Save Best Score: 0.1368 Model\n",
            "Epoch 1 - Save Best Score: 0.1368 Model\n",
            "Epoch 1 - Save Best Score: 0.1368 Model\n",
            "Epoch 1 - Save Best Score: 0.1368 Model\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EVAL: [76/77] Elapsed 0m 8s (remain 0m 0s) Loss: 0.1368 \n",
            "Log loss for 0 is 0.13683615430259732\n",
            "Epoch: [2][0/306] Elapsed 0m 0s (remain 2m 49s) Loss: 0.0179 \n",
            "Epoch: [2][100/306] Elapsed 0m 31s (remain 1m 3s) Loss: 0.1162 \n",
            "Epoch: [2][200/306] Elapsed 1m 2s (remain 0m 32s) Loss: 0.1128 \n",
            "Epoch: [2][300/306] Elapsed 1m 33s (remain 0m 1s) Loss: 0.1129 \n",
            "Epoch: [2][305/306] Elapsed 1m 34s (remain 0m 0s) Loss: 0.1119 \n",
            "EVAL: [0/77] Elapsed 0m 0s (remain 0m 25s) Loss: 0.1777 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2 - avg_train_loss: 0.1119  avg_val_loss: 0.1526  time: 103s\n",
            "Epoch 2 - avg_train_loss: 0.1119  avg_val_loss: 0.1526  time: 103s\n",
            "Epoch 2 - avg_train_loss: 0.1119  avg_val_loss: 0.1526  time: 103s\n",
            "Epoch 2 - avg_train_loss: 0.1119  avg_val_loss: 0.1526  time: 103s\n",
            "Epoch 2 - avg_train_loss: 0.1119  avg_val_loss: 0.1526  time: 103s\n",
            "Epoch 2 - avg_train_loss: 0.1119  avg_val_loss: 0.1526  time: 103s\n",
            "Epoch 2 - avg_train_loss: 0.1119  avg_val_loss: 0.1526  time: 103s\n",
            "Epoch 2 - avg_train_loss: 0.1119  avg_val_loss: 0.1526  time: 103s\n",
            "Epoch 2 - Score: 0.15259661114924808\n",
            "Epoch 2 - Score: 0.15259661114924808\n",
            "Epoch 2 - Score: 0.15259661114924808\n",
            "Epoch 2 - Score: 0.15259661114924808\n",
            "Epoch 2 - Score: 0.15259661114924808\n",
            "Epoch 2 - Score: 0.15259661114924808\n",
            "Epoch 2 - Score: 0.15259661114924808\n",
            "Epoch 2 - Score: 0.15259661114924808\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EVAL: [76/77] Elapsed 0m 8s (remain 0m 0s) Loss: 0.1526 \n",
            "Log loss for 1 is 0.15259661114924808\n",
            "Epoch: [3][0/306] Elapsed 0m 0s (remain 2m 50s) Loss: 0.0056 \n",
            "Epoch: [3][100/306] Elapsed 0m 31s (remain 1m 3s) Loss: 0.0544 \n",
            "Epoch: [3][200/306] Elapsed 1m 2s (remain 0m 32s) Loss: 0.0603 \n",
            "Epoch: [3][300/306] Elapsed 1m 33s (remain 0m 1s) Loss: 0.0614 \n",
            "Epoch: [3][305/306] Elapsed 1m 34s (remain 0m 0s) Loss: 0.0606 \n",
            "EVAL: [0/77] Elapsed 0m 0s (remain 0m 26s) Loss: 0.1023 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3 - avg_train_loss: 0.0606  avg_val_loss: 0.1806  time: 103s\n",
            "Epoch 3 - avg_train_loss: 0.0606  avg_val_loss: 0.1806  time: 103s\n",
            "Epoch 3 - avg_train_loss: 0.0606  avg_val_loss: 0.1806  time: 103s\n",
            "Epoch 3 - avg_train_loss: 0.0606  avg_val_loss: 0.1806  time: 103s\n",
            "Epoch 3 - avg_train_loss: 0.0606  avg_val_loss: 0.1806  time: 103s\n",
            "Epoch 3 - avg_train_loss: 0.0606  avg_val_loss: 0.1806  time: 103s\n",
            "Epoch 3 - avg_train_loss: 0.0606  avg_val_loss: 0.1806  time: 103s\n",
            "Epoch 3 - avg_train_loss: 0.0606  avg_val_loss: 0.1806  time: 103s\n",
            "Epoch 3 - Score: 0.18057931515629744\n",
            "Epoch 3 - Score: 0.18057931515629744\n",
            "Epoch 3 - Score: 0.18057931515629744\n",
            "Epoch 3 - Score: 0.18057931515629744\n",
            "Epoch 3 - Score: 0.18057931515629744\n",
            "Epoch 3 - Score: 0.18057931515629744\n",
            "Epoch 3 - Score: 0.18057931515629744\n",
            "Epoch 3 - Score: 0.18057931515629744\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EVAL: [76/77] Elapsed 0m 8s (remain 0m 0s) Loss: 0.1806 \n",
            "Log loss for 2 is 0.18057931515629744\n",
            "Epoch: [4][0/306] Elapsed 0m 0s (remain 2m 57s) Loss: 0.0221 \n",
            "Epoch: [4][100/306] Elapsed 0m 31s (remain 1m 3s) Loss: 0.0399 \n",
            "Epoch: [4][200/306] Elapsed 1m 2s (remain 0m 32s) Loss: 0.0401 \n",
            "Epoch: [4][300/306] Elapsed 1m 33s (remain 0m 1s) Loss: 0.0363 \n",
            "Epoch: [4][305/306] Elapsed 1m 34s (remain 0m 0s) Loss: 0.0368 \n",
            "EVAL: [0/77] Elapsed 0m 0s (remain 0m 25s) Loss: 0.0400 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4 - avg_train_loss: 0.0368  avg_val_loss: 0.2049  time: 103s\n",
            "Epoch 4 - avg_train_loss: 0.0368  avg_val_loss: 0.2049  time: 103s\n",
            "Epoch 4 - avg_train_loss: 0.0368  avg_val_loss: 0.2049  time: 103s\n",
            "Epoch 4 - avg_train_loss: 0.0368  avg_val_loss: 0.2049  time: 103s\n",
            "Epoch 4 - avg_train_loss: 0.0368  avg_val_loss: 0.2049  time: 103s\n",
            "Epoch 4 - avg_train_loss: 0.0368  avg_val_loss: 0.2049  time: 103s\n",
            "Epoch 4 - avg_train_loss: 0.0368  avg_val_loss: 0.2049  time: 103s\n",
            "Epoch 4 - avg_train_loss: 0.0368  avg_val_loss: 0.2049  time: 103s\n",
            "Epoch 4 - Score: 0.20490947726904263\n",
            "Epoch 4 - Score: 0.20490947726904263\n",
            "Epoch 4 - Score: 0.20490947726904263\n",
            "Epoch 4 - Score: 0.20490947726904263\n",
            "Epoch 4 - Score: 0.20490947726904263\n",
            "Epoch 4 - Score: 0.20490947726904263\n",
            "Epoch 4 - Score: 0.20490947726904263\n",
            "Epoch 4 - Score: 0.20490947726904263\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EVAL: [76/77] Elapsed 0m 8s (remain 0m 0s) Loss: 0.2049 \n",
            "Log loss for 3 is 0.20490947726904263\n",
            "Epoch: [5][0/306] Elapsed 0m 0s (remain 2m 56s) Loss: 0.0025 \n",
            "Epoch: [5][100/306] Elapsed 0m 31s (remain 1m 3s) Loss: 0.0328 \n",
            "Epoch: [5][200/306] Elapsed 1m 2s (remain 0m 32s) Loss: 0.0258 \n",
            "Epoch: [5][300/306] Elapsed 1m 33s (remain 0m 1s) Loss: 0.0232 \n",
            "Epoch: [5][305/306] Elapsed 1m 34s (remain 0m 0s) Loss: 0.0229 \n",
            "EVAL: [0/77] Elapsed 0m 0s (remain 0m 26s) Loss: 0.0042 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5 - avg_train_loss: 0.0229  avg_val_loss: 0.2456  time: 103s\n",
            "Epoch 5 - avg_train_loss: 0.0229  avg_val_loss: 0.2456  time: 103s\n",
            "Epoch 5 - avg_train_loss: 0.0229  avg_val_loss: 0.2456  time: 103s\n",
            "Epoch 5 - avg_train_loss: 0.0229  avg_val_loss: 0.2456  time: 103s\n",
            "Epoch 5 - avg_train_loss: 0.0229  avg_val_loss: 0.2456  time: 103s\n",
            "Epoch 5 - avg_train_loss: 0.0229  avg_val_loss: 0.2456  time: 103s\n",
            "Epoch 5 - avg_train_loss: 0.0229  avg_val_loss: 0.2456  time: 103s\n",
            "Epoch 5 - avg_train_loss: 0.0229  avg_val_loss: 0.2456  time: 103s\n",
            "Epoch 5 - Score: 0.24557268272642446\n",
            "Epoch 5 - Score: 0.24557268272642446\n",
            "Epoch 5 - Score: 0.24557268272642446\n",
            "Epoch 5 - Score: 0.24557268272642446\n",
            "Epoch 5 - Score: 0.24557268272642446\n",
            "Epoch 5 - Score: 0.24557268272642446\n",
            "Epoch 5 - Score: 0.24557268272642446\n",
            "Epoch 5 - Score: 0.24557268272642446\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EVAL: [76/77] Elapsed 0m 8s (remain 0m 0s) Loss: 0.2456 \n",
            "Log loss for 4 is 0.24557268272642446\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "========== fold: 0 result ==========\n",
            "========== fold: 0 result ==========\n",
            "========== fold: 0 result ==========\n",
            "========== fold: 0 result ==========\n",
            "========== fold: 0 result ==========\n",
            "========== fold: 0 result ==========\n",
            "========== fold: 0 result ==========\n",
            "========== fold: 0 result ==========\n",
            "Score: 0.13684\n",
            "Score: 0.13684\n",
            "Score: 0.13684\n",
            "Score: 0.13684\n",
            "Score: 0.13684\n",
            "Score: 0.13684\n",
            "Score: 0.13684\n",
            "Score: 0.13684\n",
            "========== fold: 1 training ==========\n",
            "========== fold: 1 training ==========\n",
            "========== fold: 1 training ==========\n",
            "========== fold: 1 training ==========\n",
            "========== fold: 1 training ==========\n",
            "========== fold: 1 training ==========\n",
            "========== fold: 1 training ==========\n",
            "========== fold: 1 training ==========\n",
            "Didn't find file /content/activebus_bert_final/added_tokens.json. We won't load it.\n",
            "loading file /content/activebus_bert_final/vocab.txt\n",
            "loading file None\n",
            "loading file /content/activebus_bert_final/special_tokens_map.json\n",
            "loading file /content/activebus_bert_final/tokenizer_config.json\n",
            "loading file /content/activebus_bert_final/tokenizer.json\n",
            "Didn't find file /content/activebus_bert_final/added_tokens.json. We won't load it.\n",
            "loading file /content/activebus_bert_final/vocab.txt\n",
            "loading file None\n",
            "loading file /content/activebus_bert_final/special_tokens_map.json\n",
            "loading file /content/activebus_bert_final/tokenizer_config.json\n",
            "loading file /content/activebus_bert_final/tokenizer.json\n",
            "loading configuration file /content/activebus_bert_final/config.json\n",
            "Model config BertConfig {\n",
            "  \"_name_or_path\": \"activebus/BERT-XD_Review\",\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_ids\": 0,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.15.0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "loading weights file /content/activebus_bert_final/pytorch_model.bin\n",
            "Some weights of the model checkpoint at /content/activebus_bert_final were not used when initializing BertForSequenceClassification: ['cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /content/activebus_bert_final and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: [1][0/306] Elapsed 0m 0s (remain 2m 48s) Loss: 0.6099 \n",
            "Epoch: [1][100/306] Elapsed 0m 31s (remain 1m 3s) Loss: 0.2919 \n",
            "Epoch: [1][200/306] Elapsed 1m 2s (remain 0m 32s) Loss: 0.2399 \n",
            "Epoch: [1][300/306] Elapsed 1m 33s (remain 0m 1s) Loss: 0.2126 \n",
            "Epoch: [1][305/306] Elapsed 1m 34s (remain 0m 0s) Loss: 0.2129 \n",
            "EVAL: [0/77] Elapsed 0m 0s (remain 0m 26s) Loss: 0.2656 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1 - avg_train_loss: 0.2129  avg_val_loss: 0.1427  time: 103s\n",
            "Epoch 1 - avg_train_loss: 0.2129  avg_val_loss: 0.1427  time: 103s\n",
            "Epoch 1 - avg_train_loss: 0.2129  avg_val_loss: 0.1427  time: 103s\n",
            "Epoch 1 - avg_train_loss: 0.2129  avg_val_loss: 0.1427  time: 103s\n",
            "Epoch 1 - avg_train_loss: 0.2129  avg_val_loss: 0.1427  time: 103s\n",
            "Epoch 1 - avg_train_loss: 0.2129  avg_val_loss: 0.1427  time: 103s\n",
            "Epoch 1 - avg_train_loss: 0.2129  avg_val_loss: 0.1427  time: 103s\n",
            "Epoch 1 - avg_train_loss: 0.2129  avg_val_loss: 0.1427  time: 103s\n",
            "Epoch 1 - Score: 0.1427089203860286\n",
            "Epoch 1 - Score: 0.1427089203860286\n",
            "Epoch 1 - Score: 0.1427089203860286\n",
            "Epoch 1 - Score: 0.1427089203860286\n",
            "Epoch 1 - Score: 0.1427089203860286\n",
            "Epoch 1 - Score: 0.1427089203860286\n",
            "Epoch 1 - Score: 0.1427089203860286\n",
            "Epoch 1 - Score: 0.1427089203860286\n",
            "Epoch 1 - Save Best Score: 0.1427 Model\n",
            "Epoch 1 - Save Best Score: 0.1427 Model\n",
            "Epoch 1 - Save Best Score: 0.1427 Model\n",
            "Epoch 1 - Save Best Score: 0.1427 Model\n",
            "Epoch 1 - Save Best Score: 0.1427 Model\n",
            "Epoch 1 - Save Best Score: 0.1427 Model\n",
            "Epoch 1 - Save Best Score: 0.1427 Model\n",
            "Epoch 1 - Save Best Score: 0.1427 Model\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EVAL: [76/77] Elapsed 0m 8s (remain 0m 0s) Loss: 0.1427 \n",
            "Log loss for 0 is 0.1427089203860286\n",
            "Epoch: [2][0/306] Elapsed 0m 0s (remain 2m 52s) Loss: 0.3478 \n",
            "Epoch: [2][100/306] Elapsed 0m 31s (remain 1m 3s) Loss: 0.1445 \n",
            "Epoch: [2][200/306] Elapsed 1m 2s (remain 0m 32s) Loss: 0.1309 \n",
            "Epoch: [2][300/306] Elapsed 1m 33s (remain 0m 1s) Loss: 0.1178 \n",
            "Epoch: [2][305/306] Elapsed 1m 34s (remain 0m 0s) Loss: 0.1188 \n",
            "EVAL: [0/77] Elapsed 0m 0s (remain 0m 28s) Loss: 0.6553 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2 - avg_train_loss: 0.1188  avg_val_loss: 0.1150  time: 103s\n",
            "Epoch 2 - avg_train_loss: 0.1188  avg_val_loss: 0.1150  time: 103s\n",
            "Epoch 2 - avg_train_loss: 0.1188  avg_val_loss: 0.1150  time: 103s\n",
            "Epoch 2 - avg_train_loss: 0.1188  avg_val_loss: 0.1150  time: 103s\n",
            "Epoch 2 - avg_train_loss: 0.1188  avg_val_loss: 0.1150  time: 103s\n",
            "Epoch 2 - avg_train_loss: 0.1188  avg_val_loss: 0.1150  time: 103s\n",
            "Epoch 2 - avg_train_loss: 0.1188  avg_val_loss: 0.1150  time: 103s\n",
            "Epoch 2 - avg_train_loss: 0.1188  avg_val_loss: 0.1150  time: 103s\n",
            "Epoch 2 - Score: 0.11504713524669596\n",
            "Epoch 2 - Score: 0.11504713524669596\n",
            "Epoch 2 - Score: 0.11504713524669596\n",
            "Epoch 2 - Score: 0.11504713524669596\n",
            "Epoch 2 - Score: 0.11504713524669596\n",
            "Epoch 2 - Score: 0.11504713524669596\n",
            "Epoch 2 - Score: 0.11504713524669596\n",
            "Epoch 2 - Score: 0.11504713524669596\n",
            "Epoch 2 - Save Best Score: 0.1150 Model\n",
            "Epoch 2 - Save Best Score: 0.1150 Model\n",
            "Epoch 2 - Save Best Score: 0.1150 Model\n",
            "Epoch 2 - Save Best Score: 0.1150 Model\n",
            "Epoch 2 - Save Best Score: 0.1150 Model\n",
            "Epoch 2 - Save Best Score: 0.1150 Model\n",
            "Epoch 2 - Save Best Score: 0.1150 Model\n",
            "Epoch 2 - Save Best Score: 0.1150 Model\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EVAL: [76/77] Elapsed 0m 8s (remain 0m 0s) Loss: 0.1150 \n",
            "Log loss for 1 is 0.11504713524669596\n",
            "Epoch: [3][0/306] Elapsed 0m 0s (remain 2m 50s) Loss: 0.0075 \n",
            "Epoch: [3][100/306] Elapsed 0m 31s (remain 1m 3s) Loss: 0.0686 \n",
            "Epoch: [3][200/306] Elapsed 1m 2s (remain 0m 32s) Loss: 0.0691 \n",
            "Epoch: [3][300/306] Elapsed 1m 33s (remain 0m 1s) Loss: 0.0661 \n",
            "Epoch: [3][305/306] Elapsed 1m 34s (remain 0m 0s) Loss: 0.0671 \n",
            "EVAL: [0/77] Elapsed 0m 0s (remain 0m 25s) Loss: 0.4850 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3 - avg_train_loss: 0.0671  avg_val_loss: 0.1719  time: 103s\n",
            "Epoch 3 - avg_train_loss: 0.0671  avg_val_loss: 0.1719  time: 103s\n",
            "Epoch 3 - avg_train_loss: 0.0671  avg_val_loss: 0.1719  time: 103s\n",
            "Epoch 3 - avg_train_loss: 0.0671  avg_val_loss: 0.1719  time: 103s\n",
            "Epoch 3 - avg_train_loss: 0.0671  avg_val_loss: 0.1719  time: 103s\n",
            "Epoch 3 - avg_train_loss: 0.0671  avg_val_loss: 0.1719  time: 103s\n",
            "Epoch 3 - avg_train_loss: 0.0671  avg_val_loss: 0.1719  time: 103s\n",
            "Epoch 3 - avg_train_loss: 0.0671  avg_val_loss: 0.1719  time: 103s\n",
            "Epoch 3 - Score: 0.17187556224482736\n",
            "Epoch 3 - Score: 0.17187556224482736\n",
            "Epoch 3 - Score: 0.17187556224482736\n",
            "Epoch 3 - Score: 0.17187556224482736\n",
            "Epoch 3 - Score: 0.17187556224482736\n",
            "Epoch 3 - Score: 0.17187556224482736\n",
            "Epoch 3 - Score: 0.17187556224482736\n",
            "Epoch 3 - Score: 0.17187556224482736\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EVAL: [76/77] Elapsed 0m 8s (remain 0m 0s) Loss: 0.1719 \n",
            "Log loss for 2 is 0.17187556224482736\n",
            "Epoch: [4][0/306] Elapsed 0m 0s (remain 2m 48s) Loss: 0.0142 \n",
            "Epoch: [4][100/306] Elapsed 0m 31s (remain 1m 3s) Loss: 0.0617 \n",
            "Epoch: [4][200/306] Elapsed 1m 2s (remain 0m 32s) Loss: 0.0501 \n",
            "Epoch: [4][300/306] Elapsed 1m 33s (remain 0m 1s) Loss: 0.0439 \n",
            "Epoch: [4][305/306] Elapsed 1m 34s (remain 0m 0s) Loss: 0.0439 \n",
            "EVAL: [0/77] Elapsed 0m 0s (remain 0m 27s) Loss: 1.1269 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4 - avg_train_loss: 0.0439  avg_val_loss: 0.1736  time: 103s\n",
            "Epoch 4 - avg_train_loss: 0.0439  avg_val_loss: 0.1736  time: 103s\n",
            "Epoch 4 - avg_train_loss: 0.0439  avg_val_loss: 0.1736  time: 103s\n",
            "Epoch 4 - avg_train_loss: 0.0439  avg_val_loss: 0.1736  time: 103s\n",
            "Epoch 4 - avg_train_loss: 0.0439  avg_val_loss: 0.1736  time: 103s\n",
            "Epoch 4 - avg_train_loss: 0.0439  avg_val_loss: 0.1736  time: 103s\n",
            "Epoch 4 - avg_train_loss: 0.0439  avg_val_loss: 0.1736  time: 103s\n",
            "Epoch 4 - avg_train_loss: 0.0439  avg_val_loss: 0.1736  time: 103s\n",
            "Epoch 4 - Score: 0.1735645689650119\n",
            "Epoch 4 - Score: 0.1735645689650119\n",
            "Epoch 4 - Score: 0.1735645689650119\n",
            "Epoch 4 - Score: 0.1735645689650119\n",
            "Epoch 4 - Score: 0.1735645689650119\n",
            "Epoch 4 - Score: 0.1735645689650119\n",
            "Epoch 4 - Score: 0.1735645689650119\n",
            "Epoch 4 - Score: 0.1735645689650119\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EVAL: [76/77] Elapsed 0m 8s (remain 0m 0s) Loss: 0.1736 \n",
            "Log loss for 3 is 0.1735645689650119\n",
            "Epoch: [5][0/306] Elapsed 0m 0s (remain 2m 50s) Loss: 0.0024 \n",
            "Epoch: [5][100/306] Elapsed 0m 31s (remain 1m 3s) Loss: 0.0230 \n",
            "Epoch: [5][200/306] Elapsed 1m 2s (remain 0m 32s) Loss: 0.0175 \n",
            "Epoch: [5][300/306] Elapsed 1m 33s (remain 0m 1s) Loss: 0.0205 \n",
            "Epoch: [5][305/306] Elapsed 1m 34s (remain 0m 0s) Loss: 0.0206 \n",
            "EVAL: [0/77] Elapsed 0m 0s (remain 0m 26s) Loss: 1.0777 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5 - avg_train_loss: 0.0206  avg_val_loss: 0.1680  time: 103s\n",
            "Epoch 5 - avg_train_loss: 0.0206  avg_val_loss: 0.1680  time: 103s\n",
            "Epoch 5 - avg_train_loss: 0.0206  avg_val_loss: 0.1680  time: 103s\n",
            "Epoch 5 - avg_train_loss: 0.0206  avg_val_loss: 0.1680  time: 103s\n",
            "Epoch 5 - avg_train_loss: 0.0206  avg_val_loss: 0.1680  time: 103s\n",
            "Epoch 5 - avg_train_loss: 0.0206  avg_val_loss: 0.1680  time: 103s\n",
            "Epoch 5 - avg_train_loss: 0.0206  avg_val_loss: 0.1680  time: 103s\n",
            "Epoch 5 - avg_train_loss: 0.0206  avg_val_loss: 0.1680  time: 103s\n",
            "Epoch 5 - Score: 0.16798613186344616\n",
            "Epoch 5 - Score: 0.16798613186344616\n",
            "Epoch 5 - Score: 0.16798613186344616\n",
            "Epoch 5 - Score: 0.16798613186344616\n",
            "Epoch 5 - Score: 0.16798613186344616\n",
            "Epoch 5 - Score: 0.16798613186344616\n",
            "Epoch 5 - Score: 0.16798613186344616\n",
            "Epoch 5 - Score: 0.16798613186344616\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EVAL: [76/77] Elapsed 0m 8s (remain 0m 0s) Loss: 0.1680 \n",
            "Log loss for 4 is 0.16798613186344616\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "========== fold: 1 result ==========\n",
            "========== fold: 1 result ==========\n",
            "========== fold: 1 result ==========\n",
            "========== fold: 1 result ==========\n",
            "========== fold: 1 result ==========\n",
            "========== fold: 1 result ==========\n",
            "========== fold: 1 result ==========\n",
            "========== fold: 1 result ==========\n",
            "Score: 0.11505\n",
            "Score: 0.11505\n",
            "Score: 0.11505\n",
            "Score: 0.11505\n",
            "Score: 0.11505\n",
            "Score: 0.11505\n",
            "Score: 0.11505\n",
            "Score: 0.11505\n",
            "========== fold: 2 training ==========\n",
            "========== fold: 2 training ==========\n",
            "========== fold: 2 training ==========\n",
            "========== fold: 2 training ==========\n",
            "========== fold: 2 training ==========\n",
            "========== fold: 2 training ==========\n",
            "========== fold: 2 training ==========\n",
            "========== fold: 2 training ==========\n",
            "Didn't find file /content/activebus_bert_final/added_tokens.json. We won't load it.\n",
            "loading file /content/activebus_bert_final/vocab.txt\n",
            "loading file None\n",
            "loading file /content/activebus_bert_final/special_tokens_map.json\n",
            "loading file /content/activebus_bert_final/tokenizer_config.json\n",
            "loading file /content/activebus_bert_final/tokenizer.json\n",
            "Didn't find file /content/activebus_bert_final/added_tokens.json. We won't load it.\n",
            "loading file /content/activebus_bert_final/vocab.txt\n",
            "loading file None\n",
            "loading file /content/activebus_bert_final/special_tokens_map.json\n",
            "loading file /content/activebus_bert_final/tokenizer_config.json\n",
            "loading file /content/activebus_bert_final/tokenizer.json\n",
            "loading configuration file /content/activebus_bert_final/config.json\n",
            "Model config BertConfig {\n",
            "  \"_name_or_path\": \"activebus/BERT-XD_Review\",\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_ids\": 0,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.15.0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "loading weights file /content/activebus_bert_final/pytorch_model.bin\n",
            "Some weights of the model checkpoint at /content/activebus_bert_final were not used when initializing BertForSequenceClassification: ['cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /content/activebus_bert_final and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: [1][0/306] Elapsed 0m 0s (remain 2m 59s) Loss: 0.7253 \n",
            "Epoch: [1][100/306] Elapsed 0m 31s (remain 1m 3s) Loss: 0.2897 \n",
            "Epoch: [1][200/306] Elapsed 1m 2s (remain 0m 32s) Loss: 0.2283 \n",
            "Epoch: [1][300/306] Elapsed 1m 33s (remain 0m 1s) Loss: 0.2085 \n",
            "Epoch: [1][305/306] Elapsed 1m 34s (remain 0m 0s) Loss: 0.2070 \n",
            "EVAL: [0/77] Elapsed 0m 0s (remain 0m 26s) Loss: 0.3249 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1 - avg_train_loss: 0.2070  avg_val_loss: 0.1738  time: 103s\n",
            "Epoch 1 - avg_train_loss: 0.2070  avg_val_loss: 0.1738  time: 103s\n",
            "Epoch 1 - avg_train_loss: 0.2070  avg_val_loss: 0.1738  time: 103s\n",
            "Epoch 1 - avg_train_loss: 0.2070  avg_val_loss: 0.1738  time: 103s\n",
            "Epoch 1 - avg_train_loss: 0.2070  avg_val_loss: 0.1738  time: 103s\n",
            "Epoch 1 - avg_train_loss: 0.2070  avg_val_loss: 0.1738  time: 103s\n",
            "Epoch 1 - avg_train_loss: 0.2070  avg_val_loss: 0.1738  time: 103s\n",
            "Epoch 1 - avg_train_loss: 0.2070  avg_val_loss: 0.1738  time: 103s\n",
            "Epoch 1 - Score: 0.1737867138863138\n",
            "Epoch 1 - Score: 0.1737867138863138\n",
            "Epoch 1 - Score: 0.1737867138863138\n",
            "Epoch 1 - Score: 0.1737867138863138\n",
            "Epoch 1 - Score: 0.1737867138863138\n",
            "Epoch 1 - Score: 0.1737867138863138\n",
            "Epoch 1 - Score: 0.1737867138863138\n",
            "Epoch 1 - Score: 0.1737867138863138\n",
            "Epoch 1 - Save Best Score: 0.1738 Model\n",
            "Epoch 1 - Save Best Score: 0.1738 Model\n",
            "Epoch 1 - Save Best Score: 0.1738 Model\n",
            "Epoch 1 - Save Best Score: 0.1738 Model\n",
            "Epoch 1 - Save Best Score: 0.1738 Model\n",
            "Epoch 1 - Save Best Score: 0.1738 Model\n",
            "Epoch 1 - Save Best Score: 0.1738 Model\n",
            "Epoch 1 - Save Best Score: 0.1738 Model\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EVAL: [76/77] Elapsed 0m 8s (remain 0m 0s) Loss: 0.1738 \n",
            "Log loss for 0 is 0.1737867138863138\n",
            "Epoch: [2][0/306] Elapsed 0m 0s (remain 2m 51s) Loss: 0.0359 \n",
            "Epoch: [2][100/306] Elapsed 0m 31s (remain 1m 3s) Loss: 0.1165 \n",
            "Epoch: [2][200/306] Elapsed 1m 2s (remain 0m 32s) Loss: 0.1092 \n",
            "Epoch: [2][300/306] Elapsed 1m 33s (remain 0m 1s) Loss: 0.1162 \n",
            "Epoch: [2][305/306] Elapsed 1m 34s (remain 0m 0s) Loss: 0.1181 \n",
            "EVAL: [0/77] Elapsed 0m 0s (remain 0m 25s) Loss: 0.3270 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2 - avg_train_loss: 0.1181  avg_val_loss: 0.1637  time: 103s\n",
            "Epoch 2 - avg_train_loss: 0.1181  avg_val_loss: 0.1637  time: 103s\n",
            "Epoch 2 - avg_train_loss: 0.1181  avg_val_loss: 0.1637  time: 103s\n",
            "Epoch 2 - avg_train_loss: 0.1181  avg_val_loss: 0.1637  time: 103s\n",
            "Epoch 2 - avg_train_loss: 0.1181  avg_val_loss: 0.1637  time: 103s\n",
            "Epoch 2 - avg_train_loss: 0.1181  avg_val_loss: 0.1637  time: 103s\n",
            "Epoch 2 - avg_train_loss: 0.1181  avg_val_loss: 0.1637  time: 103s\n",
            "Epoch 2 - avg_train_loss: 0.1181  avg_val_loss: 0.1637  time: 103s\n",
            "Epoch 2 - Score: 0.16366581245535755\n",
            "Epoch 2 - Score: 0.16366581245535755\n",
            "Epoch 2 - Score: 0.16366581245535755\n",
            "Epoch 2 - Score: 0.16366581245535755\n",
            "Epoch 2 - Score: 0.16366581245535755\n",
            "Epoch 2 - Score: 0.16366581245535755\n",
            "Epoch 2 - Score: 0.16366581245535755\n",
            "Epoch 2 - Score: 0.16366581245535755\n",
            "Epoch 2 - Save Best Score: 0.1637 Model\n",
            "Epoch 2 - Save Best Score: 0.1637 Model\n",
            "Epoch 2 - Save Best Score: 0.1637 Model\n",
            "Epoch 2 - Save Best Score: 0.1637 Model\n",
            "Epoch 2 - Save Best Score: 0.1637 Model\n",
            "Epoch 2 - Save Best Score: 0.1637 Model\n",
            "Epoch 2 - Save Best Score: 0.1637 Model\n",
            "Epoch 2 - Save Best Score: 0.1637 Model\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EVAL: [76/77] Elapsed 0m 8s (remain 0m 0s) Loss: 0.1637 \n",
            "Log loss for 1 is 0.16366581245535755\n",
            "Epoch: [3][0/306] Elapsed 0m 0s (remain 2m 52s) Loss: 0.1058 \n",
            "Epoch: [3][100/306] Elapsed 0m 31s (remain 1m 3s) Loss: 0.0850 \n",
            "Epoch: [3][200/306] Elapsed 1m 2s (remain 0m 32s) Loss: 0.0762 \n",
            "Epoch: [3][300/306] Elapsed 1m 33s (remain 0m 1s) Loss: 0.0704 \n",
            "Epoch: [3][305/306] Elapsed 1m 34s (remain 0m 0s) Loss: 0.0725 \n",
            "EVAL: [0/77] Elapsed 0m 0s (remain 0m 27s) Loss: 0.3879 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3 - avg_train_loss: 0.0725  avg_val_loss: 0.2322  time: 103s\n",
            "Epoch 3 - avg_train_loss: 0.0725  avg_val_loss: 0.2322  time: 103s\n",
            "Epoch 3 - avg_train_loss: 0.0725  avg_val_loss: 0.2322  time: 103s\n",
            "Epoch 3 - avg_train_loss: 0.0725  avg_val_loss: 0.2322  time: 103s\n",
            "Epoch 3 - avg_train_loss: 0.0725  avg_val_loss: 0.2322  time: 103s\n",
            "Epoch 3 - avg_train_loss: 0.0725  avg_val_loss: 0.2322  time: 103s\n",
            "Epoch 3 - avg_train_loss: 0.0725  avg_val_loss: 0.2322  time: 103s\n",
            "Epoch 3 - avg_train_loss: 0.0725  avg_val_loss: 0.2322  time: 103s\n",
            "Epoch 3 - Score: 0.23224587813249356\n",
            "Epoch 3 - Score: 0.23224587813249356\n",
            "Epoch 3 - Score: 0.23224587813249356\n",
            "Epoch 3 - Score: 0.23224587813249356\n",
            "Epoch 3 - Score: 0.23224587813249356\n",
            "Epoch 3 - Score: 0.23224587813249356\n",
            "Epoch 3 - Score: 0.23224587813249356\n",
            "Epoch 3 - Score: 0.23224587813249356\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EVAL: [76/77] Elapsed 0m 8s (remain 0m 0s) Loss: 0.2322 \n",
            "Log loss for 2 is 0.23224587813249356\n",
            "Epoch: [4][0/306] Elapsed 0m 0s (remain 2m 51s) Loss: 0.0069 \n",
            "Epoch: [4][100/306] Elapsed 0m 31s (remain 1m 3s) Loss: 0.0416 \n",
            "Epoch: [4][200/306] Elapsed 1m 2s (remain 0m 32s) Loss: 0.0427 \n",
            "Epoch: [4][300/306] Elapsed 1m 33s (remain 0m 1s) Loss: 0.0429 \n",
            "Epoch: [4][305/306] Elapsed 1m 34s (remain 0m 0s) Loss: 0.0423 \n",
            "EVAL: [0/77] Elapsed 0m 0s (remain 0m 27s) Loss: 0.4089 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4 - avg_train_loss: 0.0423  avg_val_loss: 0.2319  time: 103s\n",
            "Epoch 4 - avg_train_loss: 0.0423  avg_val_loss: 0.2319  time: 103s\n",
            "Epoch 4 - avg_train_loss: 0.0423  avg_val_loss: 0.2319  time: 103s\n",
            "Epoch 4 - avg_train_loss: 0.0423  avg_val_loss: 0.2319  time: 103s\n",
            "Epoch 4 - avg_train_loss: 0.0423  avg_val_loss: 0.2319  time: 103s\n",
            "Epoch 4 - avg_train_loss: 0.0423  avg_val_loss: 0.2319  time: 103s\n",
            "Epoch 4 - avg_train_loss: 0.0423  avg_val_loss: 0.2319  time: 103s\n",
            "Epoch 4 - avg_train_loss: 0.0423  avg_val_loss: 0.2319  time: 103s\n",
            "Epoch 4 - Score: 0.23188002155820467\n",
            "Epoch 4 - Score: 0.23188002155820467\n",
            "Epoch 4 - Score: 0.23188002155820467\n",
            "Epoch 4 - Score: 0.23188002155820467\n",
            "Epoch 4 - Score: 0.23188002155820467\n",
            "Epoch 4 - Score: 0.23188002155820467\n",
            "Epoch 4 - Score: 0.23188002155820467\n",
            "Epoch 4 - Score: 0.23188002155820467\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EVAL: [76/77] Elapsed 0m 8s (remain 0m 0s) Loss: 0.2319 \n",
            "Log loss for 3 is 0.23188002155820467\n",
            "Epoch: [5][0/306] Elapsed 0m 0s (remain 2m 55s) Loss: 0.0021 \n",
            "Epoch: [5][100/306] Elapsed 0m 31s (remain 1m 3s) Loss: 0.0202 \n",
            "Epoch: [5][200/306] Elapsed 1m 2s (remain 0m 32s) Loss: 0.0213 \n",
            "Epoch: [5][300/306] Elapsed 1m 33s (remain 0m 1s) Loss: 0.0218 \n",
            "Epoch: [5][305/306] Elapsed 1m 34s (remain 0m 0s) Loss: 0.0219 \n",
            "EVAL: [0/77] Elapsed 0m 0s (remain 0m 27s) Loss: 0.4872 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5 - avg_train_loss: 0.0219  avg_val_loss: 0.2509  time: 103s\n",
            "Epoch 5 - avg_train_loss: 0.0219  avg_val_loss: 0.2509  time: 103s\n",
            "Epoch 5 - avg_train_loss: 0.0219  avg_val_loss: 0.2509  time: 103s\n",
            "Epoch 5 - avg_train_loss: 0.0219  avg_val_loss: 0.2509  time: 103s\n",
            "Epoch 5 - avg_train_loss: 0.0219  avg_val_loss: 0.2509  time: 103s\n",
            "Epoch 5 - avg_train_loss: 0.0219  avg_val_loss: 0.2509  time: 103s\n",
            "Epoch 5 - avg_train_loss: 0.0219  avg_val_loss: 0.2509  time: 103s\n",
            "Epoch 5 - avg_train_loss: 0.0219  avg_val_loss: 0.2509  time: 103s\n",
            "Epoch 5 - Score: 0.2509089370232583\n",
            "Epoch 5 - Score: 0.2509089370232583\n",
            "Epoch 5 - Score: 0.2509089370232583\n",
            "Epoch 5 - Score: 0.2509089370232583\n",
            "Epoch 5 - Score: 0.2509089370232583\n",
            "Epoch 5 - Score: 0.2509089370232583\n",
            "Epoch 5 - Score: 0.2509089370232583\n",
            "Epoch 5 - Score: 0.2509089370232583\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EVAL: [76/77] Elapsed 0m 8s (remain 0m 0s) Loss: 0.2509 \n",
            "Log loss for 4 is 0.2509089370232583\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "========== fold: 2 result ==========\n",
            "========== fold: 2 result ==========\n",
            "========== fold: 2 result ==========\n",
            "========== fold: 2 result ==========\n",
            "========== fold: 2 result ==========\n",
            "========== fold: 2 result ==========\n",
            "========== fold: 2 result ==========\n",
            "========== fold: 2 result ==========\n",
            "Score: 0.16367\n",
            "Score: 0.16367\n",
            "Score: 0.16367\n",
            "Score: 0.16367\n",
            "Score: 0.16367\n",
            "Score: 0.16367\n",
            "Score: 0.16367\n",
            "Score: 0.16367\n",
            "========== fold: 3 training ==========\n",
            "========== fold: 3 training ==========\n",
            "========== fold: 3 training ==========\n",
            "========== fold: 3 training ==========\n",
            "========== fold: 3 training ==========\n",
            "========== fold: 3 training ==========\n",
            "========== fold: 3 training ==========\n",
            "========== fold: 3 training ==========\n",
            "Didn't find file /content/activebus_bert_final/added_tokens.json. We won't load it.\n",
            "loading file /content/activebus_bert_final/vocab.txt\n",
            "loading file None\n",
            "loading file /content/activebus_bert_final/special_tokens_map.json\n",
            "loading file /content/activebus_bert_final/tokenizer_config.json\n",
            "loading file /content/activebus_bert_final/tokenizer.json\n",
            "Didn't find file /content/activebus_bert_final/added_tokens.json. We won't load it.\n",
            "loading file /content/activebus_bert_final/vocab.txt\n",
            "loading file None\n",
            "loading file /content/activebus_bert_final/special_tokens_map.json\n",
            "loading file /content/activebus_bert_final/tokenizer_config.json\n",
            "loading file /content/activebus_bert_final/tokenizer.json\n",
            "loading configuration file /content/activebus_bert_final/config.json\n",
            "Model config BertConfig {\n",
            "  \"_name_or_path\": \"activebus/BERT-XD_Review\",\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_ids\": 0,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.15.0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "loading weights file /content/activebus_bert_final/pytorch_model.bin\n",
            "Some weights of the model checkpoint at /content/activebus_bert_final were not used when initializing BertForSequenceClassification: ['cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /content/activebus_bert_final and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: [1][0/306] Elapsed 0m 0s (remain 2m 56s) Loss: 0.7235 \n",
            "Epoch: [1][100/306] Elapsed 0m 31s (remain 1m 3s) Loss: 0.2897 \n",
            "Epoch: [1][200/306] Elapsed 1m 2s (remain 0m 32s) Loss: 0.2320 \n",
            "Epoch: [1][300/306] Elapsed 1m 33s (remain 0m 1s) Loss: 0.2146 \n",
            "Epoch: [1][305/306] Elapsed 1m 34s (remain 0m 0s) Loss: 0.2137 \n",
            "EVAL: [0/77] Elapsed 0m 0s (remain 0m 27s) Loss: 0.2488 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1 - avg_train_loss: 0.2137  avg_val_loss: 0.1301  time: 103s\n",
            "Epoch 1 - avg_train_loss: 0.2137  avg_val_loss: 0.1301  time: 103s\n",
            "Epoch 1 - avg_train_loss: 0.2137  avg_val_loss: 0.1301  time: 103s\n",
            "Epoch 1 - avg_train_loss: 0.2137  avg_val_loss: 0.1301  time: 103s\n",
            "Epoch 1 - avg_train_loss: 0.2137  avg_val_loss: 0.1301  time: 103s\n",
            "Epoch 1 - avg_train_loss: 0.2137  avg_val_loss: 0.1301  time: 103s\n",
            "Epoch 1 - avg_train_loss: 0.2137  avg_val_loss: 0.1301  time: 103s\n",
            "Epoch 1 - avg_train_loss: 0.2137  avg_val_loss: 0.1301  time: 103s\n",
            "Epoch 1 - Score: 0.13008509848962105\n",
            "Epoch 1 - Score: 0.13008509848962105\n",
            "Epoch 1 - Score: 0.13008509848962105\n",
            "Epoch 1 - Score: 0.13008509848962105\n",
            "Epoch 1 - Score: 0.13008509848962105\n",
            "Epoch 1 - Score: 0.13008509848962105\n",
            "Epoch 1 - Score: 0.13008509848962105\n",
            "Epoch 1 - Score: 0.13008509848962105\n",
            "Epoch 1 - Save Best Score: 0.1301 Model\n",
            "Epoch 1 - Save Best Score: 0.1301 Model\n",
            "Epoch 1 - Save Best Score: 0.1301 Model\n",
            "Epoch 1 - Save Best Score: 0.1301 Model\n",
            "Epoch 1 - Save Best Score: 0.1301 Model\n",
            "Epoch 1 - Save Best Score: 0.1301 Model\n",
            "Epoch 1 - Save Best Score: 0.1301 Model\n",
            "Epoch 1 - Save Best Score: 0.1301 Model\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EVAL: [76/77] Elapsed 0m 8s (remain 0m 0s) Loss: 0.1301 \n",
            "Log loss for 0 is 0.13008509848962105\n",
            "Epoch: [2][0/306] Elapsed 0m 0s (remain 2m 52s) Loss: 0.0463 \n",
            "Epoch: [2][100/306] Elapsed 0m 31s (remain 1m 3s) Loss: 0.1109 \n",
            "Epoch: [2][200/306] Elapsed 1m 2s (remain 0m 32s) Loss: 0.1178 \n",
            "Epoch: [2][300/306] Elapsed 1m 33s (remain 0m 1s) Loss: 0.1197 \n",
            "Epoch: [2][305/306] Elapsed 1m 34s (remain 0m 0s) Loss: 0.1192 \n",
            "EVAL: [0/77] Elapsed 0m 0s (remain 0m 26s) Loss: 0.2170 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2 - avg_train_loss: 0.1192  avg_val_loss: 0.1586  time: 103s\n",
            "Epoch 2 - avg_train_loss: 0.1192  avg_val_loss: 0.1586  time: 103s\n",
            "Epoch 2 - avg_train_loss: 0.1192  avg_val_loss: 0.1586  time: 103s\n",
            "Epoch 2 - avg_train_loss: 0.1192  avg_val_loss: 0.1586  time: 103s\n",
            "Epoch 2 - avg_train_loss: 0.1192  avg_val_loss: 0.1586  time: 103s\n",
            "Epoch 2 - avg_train_loss: 0.1192  avg_val_loss: 0.1586  time: 103s\n",
            "Epoch 2 - avg_train_loss: 0.1192  avg_val_loss: 0.1586  time: 103s\n",
            "Epoch 2 - avg_train_loss: 0.1192  avg_val_loss: 0.1586  time: 103s\n",
            "Epoch 2 - Score: 0.15861009905143614\n",
            "Epoch 2 - Score: 0.15861009905143614\n",
            "Epoch 2 - Score: 0.15861009905143614\n",
            "Epoch 2 - Score: 0.15861009905143614\n",
            "Epoch 2 - Score: 0.15861009905143614\n",
            "Epoch 2 - Score: 0.15861009905143614\n",
            "Epoch 2 - Score: 0.15861009905143614\n",
            "Epoch 2 - Score: 0.15861009905143614\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EVAL: [76/77] Elapsed 0m 8s (remain 0m 0s) Loss: 0.1586 \n",
            "Log loss for 1 is 0.15861009905143614\n",
            "Epoch: [3][0/306] Elapsed 0m 0s (remain 2m 55s) Loss: 0.0817 \n",
            "Epoch: [3][100/306] Elapsed 0m 31s (remain 1m 3s) Loss: 0.0635 \n",
            "Epoch: [3][200/306] Elapsed 1m 2s (remain 0m 32s) Loss: 0.0720 \n",
            "Epoch: [3][300/306] Elapsed 1m 33s (remain 0m 1s) Loss: 0.0729 \n",
            "Epoch: [3][305/306] Elapsed 1m 34s (remain 0m 0s) Loss: 0.0734 \n",
            "EVAL: [0/77] Elapsed 0m 0s (remain 0m 25s) Loss: 0.3748 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3 - avg_train_loss: 0.0734  avg_val_loss: 0.1265  time: 103s\n",
            "Epoch 3 - avg_train_loss: 0.0734  avg_val_loss: 0.1265  time: 103s\n",
            "Epoch 3 - avg_train_loss: 0.0734  avg_val_loss: 0.1265  time: 103s\n",
            "Epoch 3 - avg_train_loss: 0.0734  avg_val_loss: 0.1265  time: 103s\n",
            "Epoch 3 - avg_train_loss: 0.0734  avg_val_loss: 0.1265  time: 103s\n",
            "Epoch 3 - avg_train_loss: 0.0734  avg_val_loss: 0.1265  time: 103s\n",
            "Epoch 3 - avg_train_loss: 0.0734  avg_val_loss: 0.1265  time: 103s\n",
            "Epoch 3 - avg_train_loss: 0.0734  avg_val_loss: 0.1265  time: 103s\n",
            "Epoch 3 - Score: 0.12652794546066284\n",
            "Epoch 3 - Score: 0.12652794546066284\n",
            "Epoch 3 - Score: 0.12652794546066284\n",
            "Epoch 3 - Score: 0.12652794546066284\n",
            "Epoch 3 - Score: 0.12652794546066284\n",
            "Epoch 3 - Score: 0.12652794546066284\n",
            "Epoch 3 - Score: 0.12652794546066284\n",
            "Epoch 3 - Score: 0.12652794546066284\n",
            "Epoch 3 - Save Best Score: 0.1265 Model\n",
            "Epoch 3 - Save Best Score: 0.1265 Model\n",
            "Epoch 3 - Save Best Score: 0.1265 Model\n",
            "Epoch 3 - Save Best Score: 0.1265 Model\n",
            "Epoch 3 - Save Best Score: 0.1265 Model\n",
            "Epoch 3 - Save Best Score: 0.1265 Model\n",
            "Epoch 3 - Save Best Score: 0.1265 Model\n",
            "Epoch 3 - Save Best Score: 0.1265 Model\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EVAL: [76/77] Elapsed 0m 8s (remain 0m 0s) Loss: 0.1265 \n",
            "Log loss for 2 is 0.12652794546066284\n",
            "Epoch: [4][0/306] Elapsed 0m 0s (remain 2m 48s) Loss: 0.0078 \n",
            "Epoch: [4][100/306] Elapsed 0m 31s (remain 1m 3s) Loss: 0.0386 \n",
            "Epoch: [4][200/306] Elapsed 1m 2s (remain 0m 32s) Loss: 0.0356 \n",
            "Epoch: [4][300/306] Elapsed 1m 33s (remain 0m 1s) Loss: 0.0345 \n",
            "Epoch: [4][305/306] Elapsed 1m 34s (remain 0m 0s) Loss: 0.0342 \n",
            "EVAL: [0/77] Elapsed 0m 0s (remain 0m 26s) Loss: 0.4282 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4 - avg_train_loss: 0.0342  avg_val_loss: 0.1666  time: 103s\n",
            "Epoch 4 - avg_train_loss: 0.0342  avg_val_loss: 0.1666  time: 103s\n",
            "Epoch 4 - avg_train_loss: 0.0342  avg_val_loss: 0.1666  time: 103s\n",
            "Epoch 4 - avg_train_loss: 0.0342  avg_val_loss: 0.1666  time: 103s\n",
            "Epoch 4 - avg_train_loss: 0.0342  avg_val_loss: 0.1666  time: 103s\n",
            "Epoch 4 - avg_train_loss: 0.0342  avg_val_loss: 0.1666  time: 103s\n",
            "Epoch 4 - avg_train_loss: 0.0342  avg_val_loss: 0.1666  time: 103s\n",
            "Epoch 4 - avg_train_loss: 0.0342  avg_val_loss: 0.1666  time: 103s\n",
            "Epoch 4 - Score: 0.16659811894909746\n",
            "Epoch 4 - Score: 0.16659811894909746\n",
            "Epoch 4 - Score: 0.16659811894909746\n",
            "Epoch 4 - Score: 0.16659811894909746\n",
            "Epoch 4 - Score: 0.16659811894909746\n",
            "Epoch 4 - Score: 0.16659811894909746\n",
            "Epoch 4 - Score: 0.16659811894909746\n",
            "Epoch 4 - Score: 0.16659811894909746\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EVAL: [76/77] Elapsed 0m 8s (remain 0m 0s) Loss: 0.1666 \n",
            "Log loss for 3 is 0.16659811894909746\n",
            "Epoch: [5][0/306] Elapsed 0m 0s (remain 2m 58s) Loss: 0.0032 \n",
            "Epoch: [5][100/306] Elapsed 0m 31s (remain 1m 3s) Loss: 0.0219 \n",
            "Epoch: [5][200/306] Elapsed 1m 2s (remain 0m 32s) Loss: 0.0210 \n",
            "Epoch: [5][300/306] Elapsed 1m 33s (remain 0m 1s) Loss: 0.0236 \n",
            "Epoch: [5][305/306] Elapsed 1m 34s (remain 0m 0s) Loss: 0.0233 \n",
            "EVAL: [0/77] Elapsed 0m 0s (remain 0m 28s) Loss: 0.4262 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5 - avg_train_loss: 0.0233  avg_val_loss: 0.1689  time: 103s\n",
            "Epoch 5 - avg_train_loss: 0.0233  avg_val_loss: 0.1689  time: 103s\n",
            "Epoch 5 - avg_train_loss: 0.0233  avg_val_loss: 0.1689  time: 103s\n",
            "Epoch 5 - avg_train_loss: 0.0233  avg_val_loss: 0.1689  time: 103s\n",
            "Epoch 5 - avg_train_loss: 0.0233  avg_val_loss: 0.1689  time: 103s\n",
            "Epoch 5 - avg_train_loss: 0.0233  avg_val_loss: 0.1689  time: 103s\n",
            "Epoch 5 - avg_train_loss: 0.0233  avg_val_loss: 0.1689  time: 103s\n",
            "Epoch 5 - avg_train_loss: 0.0233  avg_val_loss: 0.1689  time: 103s\n",
            "Epoch 5 - Score: 0.16886617612275284\n",
            "Epoch 5 - Score: 0.16886617612275284\n",
            "Epoch 5 - Score: 0.16886617612275284\n",
            "Epoch 5 - Score: 0.16886617612275284\n",
            "Epoch 5 - Score: 0.16886617612275284\n",
            "Epoch 5 - Score: 0.16886617612275284\n",
            "Epoch 5 - Score: 0.16886617612275284\n",
            "Epoch 5 - Score: 0.16886617612275284\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EVAL: [76/77] Elapsed 0m 8s (remain 0m 0s) Loss: 0.1689 \n",
            "Log loss for 4 is 0.16886617612275284\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "========== fold: 3 result ==========\n",
            "========== fold: 3 result ==========\n",
            "========== fold: 3 result ==========\n",
            "========== fold: 3 result ==========\n",
            "========== fold: 3 result ==========\n",
            "========== fold: 3 result ==========\n",
            "========== fold: 3 result ==========\n",
            "========== fold: 3 result ==========\n",
            "Score: 0.12653\n",
            "Score: 0.12653\n",
            "Score: 0.12653\n",
            "Score: 0.12653\n",
            "Score: 0.12653\n",
            "Score: 0.12653\n",
            "Score: 0.12653\n",
            "Score: 0.12653\n",
            "========== fold: 4 training ==========\n",
            "========== fold: 4 training ==========\n",
            "========== fold: 4 training ==========\n",
            "========== fold: 4 training ==========\n",
            "========== fold: 4 training ==========\n",
            "========== fold: 4 training ==========\n",
            "========== fold: 4 training ==========\n",
            "========== fold: 4 training ==========\n",
            "Didn't find file /content/activebus_bert_final/added_tokens.json. We won't load it.\n",
            "loading file /content/activebus_bert_final/vocab.txt\n",
            "loading file None\n",
            "loading file /content/activebus_bert_final/special_tokens_map.json\n",
            "loading file /content/activebus_bert_final/tokenizer_config.json\n",
            "loading file /content/activebus_bert_final/tokenizer.json\n",
            "Didn't find file /content/activebus_bert_final/added_tokens.json. We won't load it.\n",
            "loading file /content/activebus_bert_final/vocab.txt\n",
            "loading file None\n",
            "loading file /content/activebus_bert_final/special_tokens_map.json\n",
            "loading file /content/activebus_bert_final/tokenizer_config.json\n",
            "loading file /content/activebus_bert_final/tokenizer.json\n",
            "loading configuration file /content/activebus_bert_final/config.json\n",
            "Model config BertConfig {\n",
            "  \"_name_or_path\": \"activebus/BERT-XD_Review\",\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_ids\": 0,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.15.0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "loading weights file /content/activebus_bert_final/pytorch_model.bin\n",
            "Some weights of the model checkpoint at /content/activebus_bert_final were not used when initializing BertForSequenceClassification: ['cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /content/activebus_bert_final and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: [1][0/306] Elapsed 0m 0s (remain 2m 54s) Loss: 0.6566 \n",
            "Epoch: [1][100/306] Elapsed 0m 31s (remain 1m 3s) Loss: 0.2668 \n",
            "Epoch: [1][200/306] Elapsed 1m 2s (remain 0m 32s) Loss: 0.2134 \n",
            "Epoch: [1][300/306] Elapsed 1m 33s (remain 0m 1s) Loss: 0.1957 \n",
            "Epoch: [1][305/306] Elapsed 1m 34s (remain 0m 0s) Loss: 0.1943 \n",
            "EVAL: [0/77] Elapsed 0m 0s (remain 0m 27s) Loss: 0.1697 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1 - avg_train_loss: 0.1943  avg_val_loss: 0.1625  time: 103s\n",
            "Epoch 1 - avg_train_loss: 0.1943  avg_val_loss: 0.1625  time: 103s\n",
            "Epoch 1 - avg_train_loss: 0.1943  avg_val_loss: 0.1625  time: 103s\n",
            "Epoch 1 - avg_train_loss: 0.1943  avg_val_loss: 0.1625  time: 103s\n",
            "Epoch 1 - avg_train_loss: 0.1943  avg_val_loss: 0.1625  time: 103s\n",
            "Epoch 1 - avg_train_loss: 0.1943  avg_val_loss: 0.1625  time: 103s\n",
            "Epoch 1 - avg_train_loss: 0.1943  avg_val_loss: 0.1625  time: 103s\n",
            "Epoch 1 - avg_train_loss: 0.1943  avg_val_loss: 0.1625  time: 103s\n",
            "Epoch 1 - Score: 0.16253766525967903\n",
            "Epoch 1 - Score: 0.16253766525967903\n",
            "Epoch 1 - Score: 0.16253766525967903\n",
            "Epoch 1 - Score: 0.16253766525967903\n",
            "Epoch 1 - Score: 0.16253766525967903\n",
            "Epoch 1 - Score: 0.16253766525967903\n",
            "Epoch 1 - Score: 0.16253766525967903\n",
            "Epoch 1 - Score: 0.16253766525967903\n",
            "Epoch 1 - Save Best Score: 0.1625 Model\n",
            "Epoch 1 - Save Best Score: 0.1625 Model\n",
            "Epoch 1 - Save Best Score: 0.1625 Model\n",
            "Epoch 1 - Save Best Score: 0.1625 Model\n",
            "Epoch 1 - Save Best Score: 0.1625 Model\n",
            "Epoch 1 - Save Best Score: 0.1625 Model\n",
            "Epoch 1 - Save Best Score: 0.1625 Model\n",
            "Epoch 1 - Save Best Score: 0.1625 Model\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EVAL: [76/77] Elapsed 0m 8s (remain 0m 0s) Loss: 0.1625 \n",
            "Log loss for 0 is 0.16253766525967903\n",
            "Epoch: [2][0/306] Elapsed 0m 0s (remain 2m 46s) Loss: 0.2797 \n",
            "Epoch: [2][100/306] Elapsed 0m 31s (remain 1m 3s) Loss: 0.1233 \n",
            "Epoch: [2][200/306] Elapsed 1m 2s (remain 0m 32s) Loss: 0.1125 \n",
            "Epoch: [2][300/306] Elapsed 1m 33s (remain 0m 1s) Loss: 0.1115 \n",
            "Epoch: [2][305/306] Elapsed 1m 34s (remain 0m 0s) Loss: 0.1123 \n",
            "EVAL: [0/77] Elapsed 0m 0s (remain 0m 26s) Loss: 0.1382 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2 - avg_train_loss: 0.1123  avg_val_loss: 0.1606  time: 103s\n",
            "Epoch 2 - avg_train_loss: 0.1123  avg_val_loss: 0.1606  time: 103s\n",
            "Epoch 2 - avg_train_loss: 0.1123  avg_val_loss: 0.1606  time: 103s\n",
            "Epoch 2 - avg_train_loss: 0.1123  avg_val_loss: 0.1606  time: 103s\n",
            "Epoch 2 - avg_train_loss: 0.1123  avg_val_loss: 0.1606  time: 103s\n",
            "Epoch 2 - avg_train_loss: 0.1123  avg_val_loss: 0.1606  time: 103s\n",
            "Epoch 2 - avg_train_loss: 0.1123  avg_val_loss: 0.1606  time: 103s\n",
            "Epoch 2 - avg_train_loss: 0.1123  avg_val_loss: 0.1606  time: 103s\n",
            "Epoch 2 - Score: 0.16056339827877755\n",
            "Epoch 2 - Score: 0.16056339827877755\n",
            "Epoch 2 - Score: 0.16056339827877755\n",
            "Epoch 2 - Score: 0.16056339827877755\n",
            "Epoch 2 - Score: 0.16056339827877755\n",
            "Epoch 2 - Score: 0.16056339827877755\n",
            "Epoch 2 - Score: 0.16056339827877755\n",
            "Epoch 2 - Score: 0.16056339827877755\n",
            "Epoch 2 - Save Best Score: 0.1606 Model\n",
            "Epoch 2 - Save Best Score: 0.1606 Model\n",
            "Epoch 2 - Save Best Score: 0.1606 Model\n",
            "Epoch 2 - Save Best Score: 0.1606 Model\n",
            "Epoch 2 - Save Best Score: 0.1606 Model\n",
            "Epoch 2 - Save Best Score: 0.1606 Model\n",
            "Epoch 2 - Save Best Score: 0.1606 Model\n",
            "Epoch 2 - Save Best Score: 0.1606 Model\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EVAL: [76/77] Elapsed 0m 8s (remain 0m 0s) Loss: 0.1606 \n",
            "Log loss for 1 is 0.16056339827877755\n",
            "Epoch: [3][0/306] Elapsed 0m 0s (remain 2m 54s) Loss: 0.1588 \n",
            "Epoch: [3][100/306] Elapsed 0m 31s (remain 1m 3s) Loss: 0.0706 \n",
            "Epoch: [3][200/306] Elapsed 1m 2s (remain 0m 32s) Loss: 0.0680 \n",
            "Epoch: [3][300/306] Elapsed 1m 33s (remain 0m 1s) Loss: 0.0675 \n",
            "Epoch: [3][305/306] Elapsed 1m 34s (remain 0m 0s) Loss: 0.0675 \n",
            "EVAL: [0/77] Elapsed 0m 0s (remain 0m 26s) Loss: 0.4961 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3 - avg_train_loss: 0.0675  avg_val_loss: 0.2401  time: 103s\n",
            "Epoch 3 - avg_train_loss: 0.0675  avg_val_loss: 0.2401  time: 103s\n",
            "Epoch 3 - avg_train_loss: 0.0675  avg_val_loss: 0.2401  time: 103s\n",
            "Epoch 3 - avg_train_loss: 0.0675  avg_val_loss: 0.2401  time: 103s\n",
            "Epoch 3 - avg_train_loss: 0.0675  avg_val_loss: 0.2401  time: 103s\n",
            "Epoch 3 - avg_train_loss: 0.0675  avg_val_loss: 0.2401  time: 103s\n",
            "Epoch 3 - avg_train_loss: 0.0675  avg_val_loss: 0.2401  time: 103s\n",
            "Epoch 3 - avg_train_loss: 0.0675  avg_val_loss: 0.2401  time: 103s\n",
            "Epoch 3 - Score: 0.24012682628863397\n",
            "Epoch 3 - Score: 0.24012682628863397\n",
            "Epoch 3 - Score: 0.24012682628863397\n",
            "Epoch 3 - Score: 0.24012682628863397\n",
            "Epoch 3 - Score: 0.24012682628863397\n",
            "Epoch 3 - Score: 0.24012682628863397\n",
            "Epoch 3 - Score: 0.24012682628863397\n",
            "Epoch 3 - Score: 0.24012682628863397\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EVAL: [76/77] Elapsed 0m 8s (remain 0m 0s) Loss: 0.2401 \n",
            "Log loss for 2 is 0.24012682628863397\n",
            "Epoch: [4][0/306] Elapsed 0m 0s (remain 2m 53s) Loss: 0.0176 \n",
            "Epoch: [4][100/306] Elapsed 0m 31s (remain 1m 3s) Loss: 0.0374 \n",
            "Epoch: [4][200/306] Elapsed 1m 2s (remain 0m 32s) Loss: 0.0293 \n",
            "Epoch: [4][300/306] Elapsed 1m 33s (remain 0m 1s) Loss: 0.0308 \n",
            "Epoch: [4][305/306] Elapsed 1m 34s (remain 0m 0s) Loss: 0.0305 \n",
            "EVAL: [0/77] Elapsed 0m 0s (remain 0m 26s) Loss: 0.2881 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4 - avg_train_loss: 0.0305  avg_val_loss: 0.2273  time: 103s\n",
            "Epoch 4 - avg_train_loss: 0.0305  avg_val_loss: 0.2273  time: 103s\n",
            "Epoch 4 - avg_train_loss: 0.0305  avg_val_loss: 0.2273  time: 103s\n",
            "Epoch 4 - avg_train_loss: 0.0305  avg_val_loss: 0.2273  time: 103s\n",
            "Epoch 4 - avg_train_loss: 0.0305  avg_val_loss: 0.2273  time: 103s\n",
            "Epoch 4 - avg_train_loss: 0.0305  avg_val_loss: 0.2273  time: 103s\n",
            "Epoch 4 - avg_train_loss: 0.0305  avg_val_loss: 0.2273  time: 103s\n",
            "Epoch 4 - avg_train_loss: 0.0305  avg_val_loss: 0.2273  time: 103s\n",
            "Epoch 4 - Score: 0.22731768861023052\n",
            "Epoch 4 - Score: 0.22731768861023052\n",
            "Epoch 4 - Score: 0.22731768861023052\n",
            "Epoch 4 - Score: 0.22731768861023052\n",
            "Epoch 4 - Score: 0.22731768861023052\n",
            "Epoch 4 - Score: 0.22731768861023052\n",
            "Epoch 4 - Score: 0.22731768861023052\n",
            "Epoch 4 - Score: 0.22731768861023052\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EVAL: [76/77] Elapsed 0m 8s (remain 0m 0s) Loss: 0.2273 \n",
            "Log loss for 3 is 0.22731768861023052\n",
            "Epoch: [5][0/306] Elapsed 0m 0s (remain 2m 54s) Loss: 0.0036 \n",
            "Epoch: [5][100/306] Elapsed 0m 31s (remain 1m 3s) Loss: 0.0172 \n",
            "Epoch: [5][200/306] Elapsed 1m 2s (remain 0m 32s) Loss: 0.0248 \n",
            "Epoch: [5][300/306] Elapsed 1m 33s (remain 0m 1s) Loss: 0.0218 \n",
            "Epoch: [5][305/306] Elapsed 1m 34s (remain 0m 0s) Loss: 0.0215 \n",
            "EVAL: [0/77] Elapsed 0m 0s (remain 0m 26s) Loss: 0.1411 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5 - avg_train_loss: 0.0215  avg_val_loss: 0.2949  time: 103s\n",
            "Epoch 5 - avg_train_loss: 0.0215  avg_val_loss: 0.2949  time: 103s\n",
            "Epoch 5 - avg_train_loss: 0.0215  avg_val_loss: 0.2949  time: 103s\n",
            "Epoch 5 - avg_train_loss: 0.0215  avg_val_loss: 0.2949  time: 103s\n",
            "Epoch 5 - avg_train_loss: 0.0215  avg_val_loss: 0.2949  time: 103s\n",
            "Epoch 5 - avg_train_loss: 0.0215  avg_val_loss: 0.2949  time: 103s\n",
            "Epoch 5 - avg_train_loss: 0.0215  avg_val_loss: 0.2949  time: 103s\n",
            "Epoch 5 - avg_train_loss: 0.0215  avg_val_loss: 0.2949  time: 103s\n",
            "Epoch 5 - Score: 0.2948624035680819\n",
            "Epoch 5 - Score: 0.2948624035680819\n",
            "Epoch 5 - Score: 0.2948624035680819\n",
            "Epoch 5 - Score: 0.2948624035680819\n",
            "Epoch 5 - Score: 0.2948624035680819\n",
            "Epoch 5 - Score: 0.2948624035680819\n",
            "Epoch 5 - Score: 0.2948624035680819\n",
            "Epoch 5 - Score: 0.2948624035680819\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EVAL: [76/77] Elapsed 0m 8s (remain 0m 0s) Loss: 0.2949 \n",
            "Log loss for 4 is 0.2948624035680819\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "========== fold: 4 result ==========\n",
            "========== fold: 4 result ==========\n",
            "========== fold: 4 result ==========\n",
            "========== fold: 4 result ==========\n",
            "========== fold: 4 result ==========\n",
            "========== fold: 4 result ==========\n",
            "========== fold: 4 result ==========\n",
            "========== fold: 4 result ==========\n",
            "Score: 0.16056\n",
            "Score: 0.16056\n",
            "Score: 0.16056\n",
            "Score: 0.16056\n",
            "Score: 0.16056\n",
            "Score: 0.16056\n",
            "Score: 0.16056\n",
            "Score: 0.16056\n",
            "========== CV ==========\n",
            "========== CV ==========\n",
            "========== CV ==========\n",
            "========== CV ==========\n",
            "========== CV ==========\n",
            "========== CV ==========\n",
            "========== CV ==========\n",
            "========== CV ==========\n",
            "Score: 0.14053\n",
            "Score: 0.14053\n",
            "Score: 0.14053\n",
            "Score: 0.14053\n",
            "Score: 0.14053\n",
            "Score: 0.14053\n",
            "Score: 0.14053\n",
            "Score: 0.14053\n",
            "Didn't find file /content/activebus_bert_final/added_tokens.json. We won't load it.\n",
            "loading file /content/activebus_bert_final/vocab.txt\n",
            "loading file None\n",
            "loading file /content/activebus_bert_final/special_tokens_map.json\n",
            "loading file /content/activebus_bert_final/tokenizer_config.json\n",
            "loading file /content/activebus_bert_final/tokenizer.json\n",
            "========== model: bert-base-uncased fold: 0 inference ==========\n",
            "========== model: bert-base-uncased fold: 0 inference ==========\n",
            "========== model: bert-base-uncased fold: 0 inference ==========\n",
            "========== model: bert-base-uncased fold: 0 inference ==========\n",
            "========== model: bert-base-uncased fold: 0 inference ==========\n",
            "========== model: bert-base-uncased fold: 0 inference ==========\n",
            "========== model: bert-base-uncased fold: 0 inference ==========\n",
            "========== model: bert-base-uncased fold: 0 inference ==========\n",
            "loading configuration file /content/activebus_bert_final/config.json\n",
            "Model config BertConfig {\n",
            "  \"_name_or_path\": \"activebus/BERT-XD_Review\",\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_ids\": 0,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.15.0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "loading weights file /content/activebus_bert_final/pytorch_model.bin\n",
            "Some weights of the model checkpoint at /content/activebus_bert_final were not used when initializing BertForSequenceClassification: ['cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /content/activebus_bert_final and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "\n",
            "  0%|          | 0/165 [00:00<?, ?it/s]\u001b[A\n",
            "  1%|          | 1/165 [00:00<00:21,  7.81it/s]\u001b[A\n",
            "  1%|          | 2/165 [00:00<00:19,  8.24it/s]\u001b[A\n",
            "  2%|▏         | 3/165 [00:00<00:18,  8.64it/s]\u001b[A\n",
            "  2%|▏         | 4/165 [00:00<00:18,  8.92it/s]\u001b[A\n",
            "  3%|▎         | 5/165 [00:00<00:17,  9.09it/s]\u001b[A\n",
            "  4%|▎         | 6/165 [00:00<00:17,  9.19it/s]\u001b[A\n",
            "  4%|▍         | 7/165 [00:00<00:17,  9.27it/s]\u001b[A\n",
            "  5%|▍         | 8/165 [00:00<00:16,  9.32it/s]\u001b[A\n",
            "  5%|▌         | 9/165 [00:00<00:16,  9.35it/s]\u001b[A\n",
            "  6%|▌         | 10/165 [00:01<00:16,  9.38it/s]\u001b[A\n",
            "  7%|▋         | 11/165 [00:01<00:16,  9.37it/s]\u001b[A\n",
            "  7%|▋         | 12/165 [00:01<00:16,  9.39it/s]\u001b[A\n",
            "  8%|▊         | 13/165 [00:01<00:16,  9.39it/s]\u001b[A\n",
            "  8%|▊         | 14/165 [00:01<00:16,  9.41it/s]\u001b[A\n",
            "  9%|▉         | 15/165 [00:01<00:15,  9.38it/s]\u001b[A\n",
            " 10%|▉         | 16/165 [00:01<00:15,  9.38it/s]\u001b[A\n",
            " 10%|█         | 17/165 [00:01<00:15,  9.39it/s]\u001b[A\n",
            " 11%|█         | 18/165 [00:01<00:15,  9.41it/s]\u001b[A\n",
            " 12%|█▏        | 19/165 [00:02<00:15,  9.39it/s]\u001b[A\n",
            " 12%|█▏        | 20/165 [00:02<00:15,  9.36it/s]\u001b[A\n",
            " 13%|█▎        | 21/165 [00:02<00:15,  9.38it/s]\u001b[A\n",
            " 13%|█▎        | 22/165 [00:02<00:15,  9.38it/s]\u001b[A\n",
            " 14%|█▍        | 23/165 [00:02<00:15,  9.38it/s]\u001b[A\n",
            " 15%|█▍        | 24/165 [00:02<00:15,  9.38it/s]\u001b[A\n",
            " 15%|█▌        | 25/165 [00:02<00:14,  9.40it/s]\u001b[A\n",
            " 16%|█▌        | 26/165 [00:02<00:14,  9.40it/s]\u001b[A\n",
            " 16%|█▋        | 27/165 [00:02<00:14,  9.40it/s]\u001b[A\n",
            " 17%|█▋        | 28/165 [00:03<00:14,  9.40it/s]\u001b[A\n",
            " 18%|█▊        | 29/165 [00:03<00:14,  9.41it/s]\u001b[A\n",
            " 18%|█▊        | 30/165 [00:03<00:14,  9.40it/s]\u001b[A\n",
            " 19%|█▉        | 31/165 [00:03<00:14,  9.39it/s]\u001b[A\n",
            " 19%|█▉        | 32/165 [00:03<00:14,  9.38it/s]\u001b[A\n",
            " 20%|██        | 33/165 [00:03<00:14,  9.37it/s]\u001b[A\n",
            " 21%|██        | 34/165 [00:03<00:13,  9.40it/s]\u001b[A\n",
            " 21%|██        | 35/165 [00:03<00:13,  9.39it/s]\u001b[A\n",
            " 22%|██▏       | 36/165 [00:03<00:13,  9.37it/s]\u001b[A\n",
            " 22%|██▏       | 37/165 [00:03<00:13,  9.36it/s]\u001b[A\n",
            " 23%|██▎       | 38/165 [00:04<00:13,  9.38it/s]\u001b[A\n",
            " 24%|██▎       | 39/165 [00:04<00:13,  9.37it/s]\u001b[A\n",
            " 24%|██▍       | 40/165 [00:04<00:13,  9.36it/s]\u001b[A\n",
            " 25%|██▍       | 41/165 [00:04<00:13,  9.39it/s]\u001b[A\n",
            " 25%|██▌       | 42/165 [00:04<00:13,  9.40it/s]\u001b[A\n",
            " 26%|██▌       | 43/165 [00:04<00:12,  9.41it/s]\u001b[A\n",
            " 27%|██▋       | 44/165 [00:04<00:12,  9.42it/s]\u001b[A\n",
            " 27%|██▋       | 45/165 [00:04<00:12,  9.43it/s]\u001b[A\n",
            " 28%|██▊       | 46/165 [00:04<00:12,  9.42it/s]\u001b[A\n",
            " 28%|██▊       | 47/165 [00:05<00:12,  9.42it/s]\u001b[A\n",
            " 29%|██▉       | 48/165 [00:05<00:12,  9.40it/s]\u001b[A\n",
            " 30%|██▉       | 49/165 [00:05<00:12,  9.41it/s]\u001b[A\n",
            " 30%|███       | 50/165 [00:05<00:12,  9.39it/s]\u001b[A\n",
            " 31%|███       | 51/165 [00:05<00:12,  9.40it/s]\u001b[A\n",
            " 32%|███▏      | 52/165 [00:05<00:12,  9.39it/s]\u001b[A\n",
            " 32%|███▏      | 53/165 [00:05<00:11,  9.35it/s]\u001b[A\n",
            " 33%|███▎      | 54/165 [00:05<00:11,  9.36it/s]\u001b[A\n",
            " 33%|███▎      | 55/165 [00:05<00:11,  9.36it/s]\u001b[A\n",
            " 34%|███▍      | 56/165 [00:05<00:11,  9.38it/s]\u001b[A\n",
            " 35%|███▍      | 57/165 [00:06<00:11,  9.38it/s]\u001b[A\n",
            " 35%|███▌      | 58/165 [00:06<00:11,  9.38it/s]\u001b[A\n",
            " 36%|███▌      | 59/165 [00:06<00:11,  9.40it/s]\u001b[A\n",
            " 36%|███▋      | 60/165 [00:06<00:11,  9.40it/s]\u001b[A\n",
            " 37%|███▋      | 61/165 [00:06<00:11,  9.40it/s]\u001b[A\n",
            " 38%|███▊      | 62/165 [00:06<00:10,  9.40it/s]\u001b[A\n",
            " 38%|███▊      | 63/165 [00:06<00:10,  9.40it/s]\u001b[A\n",
            " 39%|███▉      | 64/165 [00:06<00:10,  9.39it/s]\u001b[A\n",
            " 39%|███▉      | 65/165 [00:06<00:10,  9.39it/s]\u001b[A\n",
            " 40%|████      | 66/165 [00:07<00:10,  9.41it/s]\u001b[A\n",
            " 41%|████      | 67/165 [00:07<00:10,  9.40it/s]\u001b[A\n",
            " 41%|████      | 68/165 [00:07<00:10,  9.38it/s]\u001b[A\n",
            " 42%|████▏     | 69/165 [00:07<00:10,  9.40it/s]\u001b[A\n",
            " 42%|████▏     | 70/165 [00:07<00:10,  9.39it/s]\u001b[A\n",
            " 43%|████▎     | 71/165 [00:07<00:09,  9.41it/s]\u001b[A\n",
            " 44%|████▎     | 72/165 [00:07<00:09,  9.40it/s]\u001b[A\n",
            " 44%|████▍     | 73/165 [00:07<00:09,  9.40it/s]\u001b[A\n",
            " 45%|████▍     | 74/165 [00:07<00:09,  9.40it/s]\u001b[A\n",
            " 45%|████▌     | 75/165 [00:08<00:09,  9.40it/s]\u001b[A\n",
            " 46%|████▌     | 76/165 [00:08<00:09,  9.35it/s]\u001b[A\n",
            " 47%|████▋     | 77/165 [00:08<00:09,  9.35it/s]\u001b[A\n",
            " 47%|████▋     | 78/165 [00:08<00:09,  9.36it/s]\u001b[A\n",
            " 48%|████▊     | 79/165 [00:08<00:09,  9.36it/s]\u001b[A\n",
            " 48%|████▊     | 80/165 [00:08<00:09,  9.37it/s]\u001b[A\n",
            " 49%|████▉     | 81/165 [00:08<00:08,  9.37it/s]\u001b[A\n",
            " 50%|████▉     | 82/165 [00:08<00:08,  9.37it/s]\u001b[A\n",
            " 50%|█████     | 83/165 [00:08<00:08,  9.38it/s]\u001b[A\n",
            " 51%|█████     | 84/165 [00:08<00:08,  9.38it/s]\u001b[A\n",
            " 52%|█████▏    | 85/165 [00:09<00:08,  9.40it/s]\u001b[A\n",
            " 52%|█████▏    | 86/165 [00:09<00:08,  9.42it/s]\u001b[A\n",
            " 53%|█████▎    | 87/165 [00:09<00:08,  9.40it/s]\u001b[A\n",
            " 53%|█████▎    | 88/165 [00:09<00:08,  9.38it/s]\u001b[A\n",
            " 54%|█████▍    | 89/165 [00:09<00:08,  9.37it/s]\u001b[A\n",
            " 55%|█████▍    | 90/165 [00:09<00:07,  9.40it/s]\u001b[A\n",
            " 55%|█████▌    | 91/165 [00:09<00:07,  9.38it/s]\u001b[A\n",
            " 56%|█████▌    | 92/165 [00:09<00:07,  9.39it/s]\u001b[A\n",
            " 56%|█████▋    | 93/165 [00:09<00:07,  9.39it/s]\u001b[A\n",
            " 57%|█████▋    | 94/165 [00:10<00:07,  9.38it/s]\u001b[A\n",
            " 58%|█████▊    | 95/165 [00:10<00:07,  9.36it/s]\u001b[A\n",
            " 58%|█████▊    | 96/165 [00:10<00:07,  9.39it/s]\u001b[A\n",
            " 59%|█████▉    | 97/165 [00:10<00:07,  9.39it/s]\u001b[A\n",
            " 59%|█████▉    | 98/165 [00:10<00:07,  9.40it/s]\u001b[A\n",
            " 60%|██████    | 99/165 [00:10<00:07,  9.39it/s]\u001b[A\n",
            " 61%|██████    | 100/165 [00:10<00:06,  9.40it/s]\u001b[A\n",
            " 61%|██████    | 101/165 [00:10<00:06,  9.40it/s]\u001b[A\n",
            " 62%|██████▏   | 102/165 [00:10<00:06,  9.39it/s]\u001b[A\n",
            " 62%|██████▏   | 103/165 [00:11<00:06,  9.38it/s]\u001b[A\n",
            " 63%|██████▎   | 104/165 [00:11<00:06,  9.37it/s]\u001b[A\n",
            " 64%|██████▎   | 105/165 [00:11<00:06,  9.39it/s]\u001b[A\n",
            " 64%|██████▍   | 106/165 [00:11<00:06,  9.38it/s]\u001b[A\n",
            " 65%|██████▍   | 107/165 [00:11<00:06,  9.36it/s]\u001b[A\n",
            " 65%|██████▌   | 108/165 [00:11<00:06,  9.37it/s]\u001b[A\n",
            " 66%|██████▌   | 109/165 [00:11<00:05,  9.38it/s]\u001b[A\n",
            " 67%|██████▋   | 110/165 [00:11<00:05,  9.39it/s]\u001b[A\n",
            " 67%|██████▋   | 111/165 [00:11<00:05,  9.40it/s]\u001b[A\n",
            " 68%|██████▊   | 112/165 [00:11<00:05,  9.39it/s]\u001b[A\n",
            " 68%|██████▊   | 113/165 [00:12<00:05,  9.38it/s]\u001b[A\n",
            " 69%|██████▉   | 114/165 [00:12<00:05,  9.38it/s]\u001b[A\n",
            " 70%|██████▉   | 115/165 [00:12<00:05,  9.39it/s]\u001b[A\n",
            " 70%|███████   | 116/165 [00:12<00:05,  9.34it/s]\u001b[A\n",
            " 71%|███████   | 117/165 [00:12<00:05,  9.35it/s]\u001b[A\n",
            " 72%|███████▏  | 118/165 [00:12<00:05,  9.34it/s]\u001b[A\n",
            " 72%|███████▏  | 119/165 [00:12<00:04,  9.36it/s]\u001b[A\n",
            " 73%|███████▎  | 120/165 [00:12<00:04,  9.38it/s]\u001b[A\n",
            " 73%|███████▎  | 121/165 [00:12<00:04,  9.39it/s]\u001b[A\n",
            " 74%|███████▍  | 122/165 [00:13<00:04,  9.38it/s]\u001b[A\n",
            " 75%|███████▍  | 123/165 [00:13<00:04,  9.37it/s]\u001b[A\n",
            " 75%|███████▌  | 124/165 [00:13<00:04,  9.37it/s]\u001b[A\n",
            " 76%|███████▌  | 125/165 [00:13<00:04,  9.35it/s]\u001b[A\n",
            " 76%|███████▋  | 126/165 [00:13<00:04,  9.35it/s]\u001b[A\n",
            " 77%|███████▋  | 127/165 [00:13<00:04,  9.34it/s]\u001b[A\n",
            " 78%|███████▊  | 128/165 [00:13<00:03,  9.35it/s]\u001b[A\n",
            " 78%|███████▊  | 129/165 [00:13<00:03,  9.36it/s]\u001b[A\n",
            " 79%|███████▉  | 130/165 [00:13<00:03,  9.38it/s]\u001b[A\n",
            " 79%|███████▉  | 131/165 [00:13<00:03,  9.39it/s]\u001b[A\n",
            " 80%|████████  | 132/165 [00:14<00:03,  9.39it/s]\u001b[A\n",
            " 81%|████████  | 133/165 [00:14<00:03,  9.36it/s]\u001b[A\n",
            " 81%|████████  | 134/165 [00:14<00:03,  9.37it/s]\u001b[A\n",
            " 82%|████████▏ | 135/165 [00:14<00:03,  9.38it/s]\u001b[A\n",
            " 82%|████████▏ | 136/165 [00:14<00:03,  9.40it/s]\u001b[A\n",
            " 83%|████████▎ | 137/165 [00:14<00:02,  9.40it/s]\u001b[A\n",
            " 84%|████████▎ | 138/165 [00:14<00:02,  9.39it/s]\u001b[A\n",
            " 84%|████████▍ | 139/165 [00:14<00:02,  9.40it/s]\u001b[A\n",
            " 85%|████████▍ | 140/165 [00:14<00:02,  9.39it/s]\u001b[A\n",
            " 85%|████████▌ | 141/165 [00:15<00:02,  9.39it/s]\u001b[A\n",
            " 86%|████████▌ | 142/165 [00:15<00:02,  9.38it/s]\u001b[A\n",
            " 87%|████████▋ | 143/165 [00:15<00:02,  9.35it/s]\u001b[A\n",
            " 87%|████████▋ | 144/165 [00:15<00:02,  9.33it/s]\u001b[A\n",
            " 88%|████████▊ | 145/165 [00:15<00:02,  9.37it/s]\u001b[A\n",
            " 88%|████████▊ | 146/165 [00:15<00:02,  9.37it/s]\u001b[A\n",
            " 89%|████████▉ | 147/165 [00:15<00:01,  9.37it/s]\u001b[A\n",
            " 90%|████████▉ | 148/165 [00:15<00:01,  9.36it/s]\u001b[A\n",
            " 90%|█████████ | 149/165 [00:15<00:01,  9.31it/s]\u001b[A\n",
            " 91%|█████████ | 150/165 [00:16<00:01,  9.34it/s]\u001b[A\n",
            " 92%|█████████▏| 151/165 [00:16<00:01,  9.37it/s]\u001b[A\n",
            " 92%|█████████▏| 152/165 [00:16<00:01,  9.38it/s]\u001b[A\n",
            " 93%|█████████▎| 153/165 [00:16<00:01,  9.39it/s]\u001b[A\n",
            " 93%|█████████▎| 154/165 [00:16<00:01,  9.41it/s]\u001b[A\n",
            " 94%|█████████▍| 155/165 [00:16<00:01,  9.41it/s]\u001b[A\n",
            " 95%|█████████▍| 156/165 [00:16<00:00,  9.41it/s]\u001b[A\n",
            " 95%|█████████▌| 157/165 [00:16<00:00,  9.39it/s]\u001b[A\n",
            " 96%|█████████▌| 158/165 [00:16<00:00,  9.40it/s]\u001b[A\n",
            " 96%|█████████▋| 159/165 [00:16<00:00,  9.41it/s]\u001b[A\n",
            " 97%|█████████▋| 160/165 [00:17<00:00,  9.42it/s]\u001b[A\n",
            " 98%|█████████▊| 161/165 [00:17<00:00,  9.42it/s]\u001b[A\n",
            " 98%|█████████▊| 162/165 [00:17<00:00,  9.42it/s]\u001b[A\n",
            " 99%|█████████▉| 163/165 [00:17<00:00,  9.43it/s]\u001b[A\n",
            "100%|██████████| 165/165 [00:17<00:00,  9.35it/s]\n",
            "========== model: bert-base-uncased fold: 1 inference ==========\n",
            "========== model: bert-base-uncased fold: 1 inference ==========\n",
            "========== model: bert-base-uncased fold: 1 inference ==========\n",
            "========== model: bert-base-uncased fold: 1 inference ==========\n",
            "========== model: bert-base-uncased fold: 1 inference ==========\n",
            "========== model: bert-base-uncased fold: 1 inference ==========\n",
            "========== model: bert-base-uncased fold: 1 inference ==========\n",
            "========== model: bert-base-uncased fold: 1 inference ==========\n",
            "loading configuration file /content/activebus_bert_final/config.json\n",
            "Model config BertConfig {\n",
            "  \"_name_or_path\": \"activebus/BERT-XD_Review\",\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_ids\": 0,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.15.0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "loading weights file /content/activebus_bert_final/pytorch_model.bin\n",
            "Some weights of the model checkpoint at /content/activebus_bert_final were not used when initializing BertForSequenceClassification: ['cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /content/activebus_bert_final and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "\n",
            "  0%|          | 0/165 [00:00<?, ?it/s]\u001b[A\n",
            "  1%|          | 1/165 [00:00<00:18,  9.01it/s]\u001b[A\n",
            "  1%|          | 2/165 [00:00<00:17,  9.20it/s]\u001b[A\n",
            "  2%|▏         | 3/165 [00:00<00:17,  9.28it/s]\u001b[A\n",
            "  2%|▏         | 4/165 [00:00<00:17,  9.34it/s]\u001b[A\n",
            "  3%|▎         | 5/165 [00:00<00:17,  9.34it/s]\u001b[A\n",
            "  4%|▎         | 6/165 [00:00<00:16,  9.36it/s]\u001b[A\n",
            "  4%|▍         | 7/165 [00:00<00:16,  9.38it/s]\u001b[A\n",
            "  5%|▍         | 8/165 [00:00<00:16,  9.39it/s]\u001b[A\n",
            "  5%|▌         | 9/165 [00:00<00:16,  9.41it/s]\u001b[A\n",
            "  6%|▌         | 10/165 [00:01<00:16,  9.41it/s]\u001b[A\n",
            "  7%|▋         | 11/165 [00:01<00:16,  9.41it/s]\u001b[A\n",
            "  7%|▋         | 12/165 [00:01<00:16,  9.42it/s]\u001b[A\n",
            "  8%|▊         | 13/165 [00:01<00:16,  9.41it/s]\u001b[A\n",
            "  8%|▊         | 14/165 [00:01<00:16,  9.40it/s]\u001b[A\n",
            "  9%|▉         | 15/165 [00:01<00:16,  9.37it/s]\u001b[A\n",
            " 10%|▉         | 16/165 [00:01<00:15,  9.36it/s]\u001b[A\n",
            " 10%|█         | 17/165 [00:01<00:15,  9.38it/s]\u001b[A\n",
            " 11%|█         | 18/165 [00:01<00:15,  9.40it/s]\u001b[A\n",
            " 12%|█▏        | 19/165 [00:02<00:15,  9.38it/s]\u001b[A\n",
            " 12%|█▏        | 20/165 [00:02<00:15,  9.40it/s]\u001b[A\n",
            " 13%|█▎        | 21/165 [00:02<00:15,  9.40it/s]\u001b[A\n",
            " 13%|█▎        | 22/165 [00:02<00:15,  9.41it/s]\u001b[A\n",
            " 14%|█▍        | 23/165 [00:02<00:15,  9.43it/s]\u001b[A\n",
            " 15%|█▍        | 24/165 [00:02<00:14,  9.42it/s]\u001b[A\n",
            " 15%|█▌        | 25/165 [00:02<00:14,  9.39it/s]\u001b[A\n",
            " 16%|█▌        | 26/165 [00:02<00:14,  9.40it/s]\u001b[A\n",
            " 16%|█▋        | 27/165 [00:02<00:14,  9.40it/s]\u001b[A\n",
            " 17%|█▋        | 28/165 [00:02<00:14,  9.39it/s]\u001b[A\n",
            " 18%|█▊        | 29/165 [00:03<00:14,  9.39it/s]\u001b[A\n",
            " 18%|█▊        | 30/165 [00:03<00:14,  9.39it/s]\u001b[A\n",
            " 19%|█▉        | 31/165 [00:03<00:14,  9.40it/s]\u001b[A\n",
            " 19%|█▉        | 32/165 [00:03<00:14,  9.41it/s]\u001b[A\n",
            " 20%|██        | 33/165 [00:03<00:14,  9.39it/s]\u001b[A\n",
            " 21%|██        | 34/165 [00:03<00:13,  9.39it/s]\u001b[A\n",
            " 21%|██        | 35/165 [00:03<00:13,  9.37it/s]\u001b[A\n",
            " 22%|██▏       | 36/165 [00:03<00:13,  9.38it/s]\u001b[A\n",
            " 22%|██▏       | 37/165 [00:03<00:13,  9.40it/s]\u001b[A\n",
            " 23%|██▎       | 38/165 [00:04<00:13,  9.41it/s]\u001b[A\n",
            " 24%|██▎       | 39/165 [00:04<00:13,  9.42it/s]\u001b[A\n",
            " 24%|██▍       | 40/165 [00:04<00:13,  9.41it/s]\u001b[A\n",
            " 25%|██▍       | 41/165 [00:04<00:13,  9.39it/s]\u001b[A\n",
            " 25%|██▌       | 42/165 [00:04<00:13,  9.39it/s]\u001b[A\n",
            " 26%|██▌       | 43/165 [00:04<00:12,  9.40it/s]\u001b[A\n",
            " 27%|██▋       | 44/165 [00:04<00:12,  9.40it/s]\u001b[A\n",
            " 27%|██▋       | 45/165 [00:04<00:12,  9.40it/s]\u001b[A\n",
            " 28%|██▊       | 46/165 [00:04<00:12,  9.39it/s]\u001b[A\n",
            " 28%|██▊       | 47/165 [00:05<00:12,  9.38it/s]\u001b[A\n",
            " 29%|██▉       | 48/165 [00:05<00:12,  9.39it/s]\u001b[A\n",
            " 30%|██▉       | 49/165 [00:05<00:12,  9.40it/s]\u001b[A\n",
            " 30%|███       | 50/165 [00:05<00:12,  9.41it/s]\u001b[A\n",
            " 31%|███       | 51/165 [00:05<00:12,  9.40it/s]\u001b[A\n",
            " 32%|███▏      | 52/165 [00:05<00:12,  9.40it/s]\u001b[A\n",
            " 32%|███▏      | 53/165 [00:05<00:11,  9.39it/s]\u001b[A\n",
            " 33%|███▎      | 54/165 [00:05<00:11,  9.40it/s]\u001b[A\n",
            " 33%|███▎      | 55/165 [00:05<00:11,  9.39it/s]\u001b[A\n",
            " 34%|███▍      | 56/165 [00:05<00:11,  9.40it/s]\u001b[A\n",
            " 35%|███▍      | 57/165 [00:06<00:11,  9.40it/s]\u001b[A\n",
            " 35%|███▌      | 58/165 [00:06<00:11,  9.41it/s]\u001b[A\n",
            " 36%|███▌      | 59/165 [00:06<00:11,  9.42it/s]\u001b[A\n",
            " 36%|███▋      | 60/165 [00:06<00:11,  9.40it/s]\u001b[A\n",
            " 37%|███▋      | 61/165 [00:06<00:11,  9.41it/s]\u001b[A\n",
            " 38%|███▊      | 62/165 [00:06<00:10,  9.39it/s]\u001b[A\n",
            " 38%|███▊      | 63/165 [00:06<00:10,  9.35it/s]\u001b[A\n",
            " 39%|███▉      | 64/165 [00:06<00:10,  9.38it/s]\u001b[A\n",
            " 39%|███▉      | 65/165 [00:06<00:10,  9.38it/s]\u001b[A\n",
            " 40%|████      | 66/165 [00:07<00:10,  9.38it/s]\u001b[A\n",
            " 41%|████      | 67/165 [00:07<00:10,  9.39it/s]\u001b[A\n",
            " 41%|████      | 68/165 [00:07<00:10,  9.37it/s]\u001b[A\n",
            " 42%|████▏     | 69/165 [00:07<00:10,  9.36it/s]\u001b[A\n",
            " 42%|████▏     | 70/165 [00:07<00:10,  9.35it/s]\u001b[A\n",
            " 43%|████▎     | 71/165 [00:07<00:10,  9.36it/s]\u001b[A\n",
            " 44%|████▎     | 72/165 [00:07<00:09,  9.37it/s]\u001b[A\n",
            " 44%|████▍     | 73/165 [00:07<00:09,  9.40it/s]\u001b[A\n",
            " 45%|████▍     | 74/165 [00:07<00:09,  9.41it/s]\u001b[A\n",
            " 45%|████▌     | 75/165 [00:07<00:09,  9.40it/s]\u001b[A\n",
            " 46%|████▌     | 76/165 [00:08<00:09,  9.42it/s]\u001b[A\n",
            " 47%|████▋     | 77/165 [00:08<00:09,  9.42it/s]\u001b[A\n",
            " 47%|████▋     | 78/165 [00:08<00:09,  9.42it/s]\u001b[A\n",
            " 48%|████▊     | 79/165 [00:08<00:09,  9.42it/s]\u001b[A\n",
            " 48%|████▊     | 80/165 [00:08<00:09,  9.43it/s]\u001b[A\n",
            " 49%|████▉     | 81/165 [00:08<00:08,  9.43it/s]\u001b[A\n",
            " 50%|████▉     | 82/165 [00:08<00:08,  9.43it/s]\u001b[A\n",
            " 50%|█████     | 83/165 [00:08<00:08,  9.42it/s]\u001b[A\n",
            " 51%|█████     | 84/165 [00:08<00:08,  9.38it/s]\u001b[A\n",
            " 52%|█████▏    | 85/165 [00:09<00:08,  9.36it/s]\u001b[A\n",
            " 52%|█████▏    | 86/165 [00:09<00:08,  9.35it/s]\u001b[A\n",
            " 53%|█████▎    | 87/165 [00:09<00:08,  9.37it/s]\u001b[A\n",
            " 53%|█████▎    | 88/165 [00:09<00:08,  9.37it/s]\u001b[A\n",
            " 54%|█████▍    | 89/165 [00:09<00:08,  9.38it/s]\u001b[A\n",
            " 55%|█████▍    | 90/165 [00:09<00:08,  9.37it/s]\u001b[A\n",
            " 55%|█████▌    | 91/165 [00:09<00:07,  9.38it/s]\u001b[A\n",
            " 56%|█████▌    | 92/165 [00:09<00:07,  9.39it/s]\u001b[A\n",
            " 56%|█████▋    | 93/165 [00:09<00:07,  9.43it/s]\u001b[A\n",
            " 57%|█████▋    | 94/165 [00:10<00:07,  9.42it/s]\u001b[A\n",
            " 58%|█████▊    | 95/165 [00:10<00:07,  9.42it/s]\u001b[A\n",
            " 58%|█████▊    | 96/165 [00:10<00:07,  9.42it/s]\u001b[A\n",
            " 59%|█████▉    | 97/165 [00:10<00:07,  9.43it/s]\u001b[A\n",
            " 59%|█████▉    | 98/165 [00:10<00:07,  9.42it/s]\u001b[A\n",
            " 60%|██████    | 99/165 [00:10<00:07,  9.43it/s]\u001b[A\n",
            " 61%|██████    | 100/165 [00:10<00:06,  9.42it/s]\u001b[A\n",
            " 61%|██████    | 101/165 [00:10<00:06,  9.41it/s]\u001b[A\n",
            " 62%|██████▏   | 102/165 [00:10<00:06,  9.41it/s]\u001b[A\n",
            " 62%|██████▏   | 103/165 [00:10<00:06,  9.40it/s]\u001b[A\n",
            " 63%|██████▎   | 104/165 [00:11<00:06,  9.41it/s]\u001b[A\n",
            " 64%|██████▎   | 105/165 [00:11<00:06,  9.40it/s]\u001b[A\n",
            " 64%|██████▍   | 106/165 [00:11<00:06,  9.41it/s]\u001b[A\n",
            " 65%|██████▍   | 107/165 [00:11<00:06,  9.39it/s]\u001b[A\n",
            " 65%|██████▌   | 108/165 [00:11<00:06,  9.35it/s]\u001b[A\n",
            " 66%|██████▌   | 109/165 [00:11<00:05,  9.38it/s]\u001b[A\n",
            " 67%|██████▋   | 110/165 [00:11<00:05,  9.38it/s]\u001b[A\n",
            " 67%|██████▋   | 111/165 [00:11<00:05,  9.39it/s]\u001b[A\n",
            " 68%|██████▊   | 112/165 [00:11<00:05,  9.38it/s]\u001b[A\n",
            " 68%|██████▊   | 113/165 [00:12<00:05,  9.40it/s]\u001b[A\n",
            " 69%|██████▉   | 114/165 [00:12<00:05,  9.41it/s]\u001b[A\n",
            " 70%|██████▉   | 115/165 [00:12<00:05,  9.41it/s]\u001b[A\n",
            " 70%|███████   | 116/165 [00:12<00:05,  9.40it/s]\u001b[A\n",
            " 71%|███████   | 117/165 [00:12<00:05,  9.38it/s]\u001b[A\n",
            " 72%|███████▏  | 118/165 [00:12<00:05,  9.36it/s]\u001b[A\n",
            " 72%|███████▏  | 119/165 [00:12<00:04,  9.37it/s]\u001b[A\n",
            " 73%|███████▎  | 120/165 [00:12<00:04,  9.37it/s]\u001b[A\n",
            " 73%|███████▎  | 121/165 [00:12<00:04,  9.37it/s]\u001b[A\n",
            " 74%|███████▍  | 122/165 [00:12<00:04,  9.36it/s]\u001b[A\n",
            " 75%|███████▍  | 123/165 [00:13<00:04,  9.35it/s]\u001b[A\n",
            " 75%|███████▌  | 124/165 [00:13<00:04,  9.36it/s]\u001b[A\n",
            " 76%|███████▌  | 125/165 [00:13<00:04,  9.36it/s]\u001b[A\n",
            " 76%|███████▋  | 126/165 [00:13<00:04,  9.37it/s]\u001b[A\n",
            " 77%|███████▋  | 127/165 [00:13<00:04,  9.37it/s]\u001b[A\n",
            " 78%|███████▊  | 128/165 [00:13<00:03,  9.34it/s]\u001b[A\n",
            " 78%|███████▊  | 129/165 [00:13<00:03,  9.31it/s]\u001b[A\n",
            " 79%|███████▉  | 130/165 [00:13<00:03,  9.33it/s]\u001b[A\n",
            " 79%|███████▉  | 131/165 [00:13<00:03,  9.33it/s]\u001b[A\n",
            " 80%|████████  | 132/165 [00:14<00:03,  9.34it/s]\u001b[A\n",
            " 81%|████████  | 133/165 [00:14<00:03,  9.36it/s]\u001b[A\n",
            " 81%|████████  | 134/165 [00:14<00:03,  9.38it/s]\u001b[A\n",
            " 82%|████████▏ | 135/165 [00:14<00:03,  9.38it/s]\u001b[A\n",
            " 82%|████████▏ | 136/165 [00:14<00:03,  9.40it/s]\u001b[A\n",
            " 83%|████████▎ | 137/165 [00:14<00:02,  9.40it/s]\u001b[A\n",
            " 84%|████████▎ | 138/165 [00:14<00:02,  9.40it/s]\u001b[A\n",
            " 84%|████████▍ | 139/165 [00:14<00:02,  9.40it/s]\u001b[A\n",
            " 85%|████████▍ | 140/165 [00:14<00:02,  9.41it/s]\u001b[A\n",
            " 85%|████████▌ | 141/165 [00:15<00:02,  9.40it/s]\u001b[A\n",
            " 86%|████████▌ | 142/165 [00:15<00:03,  6.80it/s]\u001b[A\n",
            " 87%|████████▋ | 143/165 [00:15<00:02,  7.40it/s]\u001b[A\n",
            " 87%|████████▋ | 144/165 [00:15<00:02,  7.91it/s]\u001b[A\n",
            " 88%|████████▊ | 145/165 [00:15<00:02,  8.30it/s]\u001b[A\n",
            " 88%|████████▊ | 146/165 [00:15<00:02,  8.59it/s]\u001b[A\n",
            " 89%|████████▉ | 147/165 [00:15<00:02,  8.82it/s]\u001b[A\n",
            " 90%|████████▉ | 148/165 [00:15<00:01,  8.98it/s]\u001b[A\n",
            " 90%|█████████ | 149/165 [00:16<00:01,  9.11it/s]\u001b[A\n",
            " 91%|█████████ | 150/165 [00:16<00:01,  9.20it/s]\u001b[A\n",
            " 92%|█████████▏| 151/165 [00:16<00:01,  9.25it/s]\u001b[A\n",
            " 92%|█████████▏| 152/165 [00:16<00:01,  9.30it/s]\u001b[A\n",
            " 93%|█████████▎| 153/165 [00:16<00:01,  9.35it/s]\u001b[A\n",
            " 93%|█████████▎| 154/165 [00:16<00:01,  9.37it/s]\u001b[A\n",
            " 94%|█████████▍| 155/165 [00:16<00:01,  9.35it/s]\u001b[A\n",
            " 95%|█████████▍| 156/165 [00:16<00:00,  9.33it/s]\u001b[A\n",
            " 95%|█████████▌| 157/165 [00:16<00:00,  9.33it/s]\u001b[A\n",
            " 96%|█████████▌| 158/165 [00:16<00:00,  9.36it/s]\u001b[A\n",
            " 96%|█████████▋| 159/165 [00:17<00:00,  9.39it/s]\u001b[A\n",
            " 97%|█████████▋| 160/165 [00:17<00:00,  9.41it/s]\u001b[A\n",
            " 98%|█████████▊| 161/165 [00:17<00:00,  9.41it/s]\u001b[A\n",
            " 98%|█████████▊| 162/165 [00:17<00:00,  9.43it/s]\u001b[A\n",
            " 99%|█████████▉| 163/165 [00:17<00:00,  9.44it/s]\u001b[A\n",
            "100%|██████████| 165/165 [00:17<00:00,  9.30it/s]\n",
            "========== model: bert-base-uncased fold: 2 inference ==========\n",
            "========== model: bert-base-uncased fold: 2 inference ==========\n",
            "========== model: bert-base-uncased fold: 2 inference ==========\n",
            "========== model: bert-base-uncased fold: 2 inference ==========\n",
            "========== model: bert-base-uncased fold: 2 inference ==========\n",
            "========== model: bert-base-uncased fold: 2 inference ==========\n",
            "========== model: bert-base-uncased fold: 2 inference ==========\n",
            "========== model: bert-base-uncased fold: 2 inference ==========\n",
            "loading configuration file /content/activebus_bert_final/config.json\n",
            "Model config BertConfig {\n",
            "  \"_name_or_path\": \"activebus/BERT-XD_Review\",\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_ids\": 0,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.15.0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "loading weights file /content/activebus_bert_final/pytorch_model.bin\n",
            "Some weights of the model checkpoint at /content/activebus_bert_final were not used when initializing BertForSequenceClassification: ['cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /content/activebus_bert_final and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "\n",
            "  0%|          | 0/165 [00:00<?, ?it/s]\u001b[A\n",
            "  1%|          | 1/165 [00:00<00:19,  8.58it/s]\u001b[A\n",
            "  1%|          | 2/165 [00:00<00:18,  9.02it/s]\u001b[A\n",
            "  2%|▏         | 3/165 [00:00<00:17,  9.17it/s]\u001b[A\n",
            "  2%|▏         | 4/165 [00:00<00:17,  9.25it/s]\u001b[A\n",
            "  3%|▎         | 5/165 [00:00<00:17,  9.29it/s]\u001b[A\n",
            "  4%|▎         | 6/165 [00:00<00:17,  9.33it/s]\u001b[A\n",
            "  4%|▍         | 7/165 [00:00<00:16,  9.36it/s]\u001b[A\n",
            "  5%|▍         | 8/165 [00:00<00:16,  9.37it/s]\u001b[A\n",
            "  5%|▌         | 9/165 [00:00<00:16,  9.39it/s]\u001b[A\n",
            "  6%|▌         | 10/165 [00:01<00:16,  9.39it/s]\u001b[A\n",
            "  7%|▋         | 11/165 [00:01<00:16,  9.39it/s]\u001b[A\n",
            "  7%|▋         | 12/165 [00:01<00:16,  9.39it/s]\u001b[A\n",
            "  8%|▊         | 13/165 [00:01<00:16,  9.37it/s]\u001b[A\n",
            "  8%|▊         | 14/165 [00:01<00:16,  9.29it/s]\u001b[A\n",
            "  9%|▉         | 15/165 [00:01<00:16,  9.31it/s]\u001b[A\n",
            " 10%|▉         | 16/165 [00:01<00:15,  9.33it/s]\u001b[A\n",
            " 10%|█         | 17/165 [00:01<00:15,  9.35it/s]\u001b[A\n",
            " 11%|█         | 18/165 [00:01<00:15,  9.34it/s]\u001b[A\n",
            " 12%|█▏        | 19/165 [00:02<00:15,  9.36it/s]\u001b[A\n",
            " 12%|█▏        | 20/165 [00:02<00:15,  9.34it/s]\u001b[A\n",
            " 13%|█▎        | 21/165 [00:02<00:15,  9.33it/s]\u001b[A\n",
            " 13%|█▎        | 22/165 [00:02<00:15,  9.35it/s]\u001b[A\n",
            " 14%|█▍        | 23/165 [00:02<00:15,  9.38it/s]\u001b[A\n",
            " 15%|█▍        | 24/165 [00:02<00:15,  9.39it/s]\u001b[A\n",
            " 15%|█▌        | 25/165 [00:02<00:14,  9.39it/s]\u001b[A\n",
            " 16%|█▌        | 26/165 [00:02<00:14,  9.41it/s]\u001b[A\n",
            " 16%|█▋        | 27/165 [00:02<00:14,  9.42it/s]\u001b[A\n",
            " 17%|█▋        | 28/165 [00:02<00:14,  9.42it/s]\u001b[A\n",
            " 18%|█▊        | 29/165 [00:03<00:14,  9.41it/s]\u001b[A\n",
            " 18%|█▊        | 30/165 [00:03<00:14,  9.41it/s]\u001b[A\n",
            " 19%|█▉        | 31/165 [00:03<00:14,  9.41it/s]\u001b[A\n",
            " 19%|█▉        | 32/165 [00:03<00:14,  9.39it/s]\u001b[A\n",
            " 20%|██        | 33/165 [00:03<00:14,  9.40it/s]\u001b[A\n",
            " 21%|██        | 34/165 [00:03<00:13,  9.39it/s]\u001b[A\n",
            " 21%|██        | 35/165 [00:03<00:13,  9.36it/s]\u001b[A\n",
            " 22%|██▏       | 36/165 [00:03<00:13,  9.37it/s]\u001b[A\n",
            " 22%|██▏       | 37/165 [00:03<00:13,  9.39it/s]\u001b[A\n",
            " 23%|██▎       | 38/165 [00:04<00:13,  9.39it/s]\u001b[A\n",
            " 24%|██▎       | 39/165 [00:04<00:13,  9.39it/s]\u001b[A\n",
            " 24%|██▍       | 40/165 [00:04<00:13,  9.39it/s]\u001b[A\n",
            " 25%|██▍       | 41/165 [00:04<00:13,  9.38it/s]\u001b[A\n",
            " 25%|██▌       | 42/165 [00:04<00:13,  9.35it/s]\u001b[A\n",
            " 26%|██▌       | 43/165 [00:04<00:13,  9.38it/s]\u001b[A\n",
            " 27%|██▋       | 44/165 [00:04<00:12,  9.39it/s]\u001b[A\n",
            " 27%|██▋       | 45/165 [00:04<00:12,  9.38it/s]\u001b[A\n",
            " 28%|██▊       | 46/165 [00:04<00:12,  9.38it/s]\u001b[A\n",
            " 28%|██▊       | 47/165 [00:05<00:12,  9.40it/s]\u001b[A\n",
            " 29%|██▉       | 48/165 [00:05<00:12,  9.41it/s]\u001b[A\n",
            " 30%|██▉       | 49/165 [00:05<00:12,  9.44it/s]\u001b[A\n",
            " 30%|███       | 50/165 [00:05<00:12,  9.43it/s]\u001b[A\n",
            " 31%|███       | 51/165 [00:05<00:12,  9.43it/s]\u001b[A\n",
            " 32%|███▏      | 52/165 [00:05<00:11,  9.43it/s]\u001b[A\n",
            " 32%|███▏      | 53/165 [00:05<00:11,  9.40it/s]\u001b[A\n",
            " 33%|███▎      | 54/165 [00:05<00:11,  9.40it/s]\u001b[A\n",
            " 33%|███▎      | 55/165 [00:05<00:11,  9.41it/s]\u001b[A\n",
            " 34%|███▍      | 56/165 [00:05<00:11,  9.40it/s]\u001b[A\n",
            " 35%|███▍      | 57/165 [00:06<00:11,  9.41it/s]\u001b[A\n",
            " 35%|███▌      | 58/165 [00:06<00:11,  9.41it/s]\u001b[A\n",
            " 36%|███▌      | 59/165 [00:06<00:11,  9.42it/s]\u001b[A\n",
            " 36%|███▋      | 60/165 [00:06<00:11,  9.42it/s]\u001b[A\n",
            " 37%|███▋      | 61/165 [00:06<00:11,  9.43it/s]\u001b[A\n",
            " 38%|███▊      | 62/165 [00:06<00:10,  9.42it/s]\u001b[A\n",
            " 38%|███▊      | 63/165 [00:06<00:10,  9.42it/s]\u001b[A\n",
            " 39%|███▉      | 64/165 [00:06<00:10,  9.43it/s]\u001b[A\n",
            " 39%|███▉      | 65/165 [00:06<00:10,  9.43it/s]\u001b[A\n",
            " 40%|████      | 66/165 [00:07<00:10,  9.43it/s]\u001b[A\n",
            " 41%|████      | 67/165 [00:07<00:10,  9.42it/s]\u001b[A\n",
            " 41%|████      | 68/165 [00:07<00:10,  9.43it/s]\u001b[A\n",
            " 42%|████▏     | 69/165 [00:07<00:10,  9.42it/s]\u001b[A\n",
            " 42%|████▏     | 70/165 [00:07<00:10,  9.42it/s]\u001b[A\n",
            " 43%|████▎     | 71/165 [00:07<00:09,  9.41it/s]\u001b[A\n",
            " 44%|████▎     | 72/165 [00:07<00:09,  9.40it/s]\u001b[A\n",
            " 44%|████▍     | 73/165 [00:07<00:09,  9.40it/s]\u001b[A\n",
            " 45%|████▍     | 74/165 [00:07<00:09,  9.40it/s]\u001b[A\n",
            " 45%|████▌     | 75/165 [00:07<00:09,  9.37it/s]\u001b[A\n",
            " 46%|████▌     | 76/165 [00:08<00:09,  9.35it/s]\u001b[A\n",
            " 47%|████▋     | 77/165 [00:08<00:09,  9.36it/s]\u001b[A\n",
            " 47%|████▋     | 78/165 [00:08<00:09,  9.35it/s]\u001b[A\n",
            " 48%|████▊     | 79/165 [00:08<00:09,  9.34it/s]\u001b[A\n",
            " 48%|████▊     | 80/165 [00:08<00:09,  9.35it/s]\u001b[A\n",
            " 49%|████▉     | 81/165 [00:08<00:08,  9.35it/s]\u001b[A\n",
            " 50%|████▉     | 82/165 [00:08<00:08,  9.35it/s]\u001b[A\n",
            " 50%|█████     | 83/165 [00:08<00:08,  9.35it/s]\u001b[A\n",
            " 51%|█████     | 84/165 [00:08<00:08,  9.36it/s]\u001b[A\n",
            " 52%|█████▏    | 85/165 [00:09<00:08,  9.37it/s]\u001b[A\n",
            " 52%|█████▏    | 86/165 [00:09<00:08,  9.38it/s]\u001b[A\n",
            " 53%|█████▎    | 87/165 [00:09<00:08,  9.40it/s]\u001b[A\n",
            " 53%|█████▎    | 88/165 [00:09<00:08,  9.38it/s]\u001b[A\n",
            " 54%|█████▍    | 89/165 [00:09<00:08,  9.37it/s]\u001b[A\n",
            " 55%|█████▍    | 90/165 [00:09<00:07,  9.38it/s]\u001b[A\n",
            " 55%|█████▌    | 91/165 [00:09<00:07,  9.39it/s]\u001b[A\n",
            " 56%|█████▌    | 92/165 [00:09<00:07,  9.41it/s]\u001b[A\n",
            " 56%|█████▋    | 93/165 [00:09<00:07,  9.41it/s]\u001b[A\n",
            " 57%|█████▋    | 94/165 [00:10<00:07,  9.42it/s]\u001b[A\n",
            " 58%|█████▊    | 95/165 [00:10<00:07,  9.41it/s]\u001b[A\n",
            " 58%|█████▊    | 96/165 [00:10<00:07,  9.39it/s]\u001b[A\n",
            " 59%|█████▉    | 97/165 [00:10<00:07,  9.40it/s]\u001b[A\n",
            " 59%|█████▉    | 98/165 [00:10<00:07,  9.38it/s]\u001b[A\n",
            " 60%|██████    | 99/165 [00:10<00:07,  9.39it/s]\u001b[A\n",
            " 61%|██████    | 100/165 [00:10<00:06,  9.39it/s]\u001b[A\n",
            " 61%|██████    | 101/165 [00:10<00:06,  9.39it/s]\u001b[A\n",
            " 62%|██████▏   | 102/165 [00:10<00:06,  9.39it/s]\u001b[A\n",
            " 62%|██████▏   | 103/165 [00:10<00:06,  9.40it/s]\u001b[A\n",
            " 63%|██████▎   | 104/165 [00:11<00:06,  9.40it/s]\u001b[A\n",
            " 64%|██████▎   | 105/165 [00:11<00:06,  9.31it/s]\u001b[A\n",
            " 64%|██████▍   | 106/165 [00:11<00:06,  9.32it/s]\u001b[A\n",
            " 65%|██████▍   | 107/165 [00:11<00:06,  9.32it/s]\u001b[A\n",
            " 65%|██████▌   | 108/165 [00:11<00:06,  9.34it/s]\u001b[A\n",
            " 66%|██████▌   | 109/165 [00:11<00:05,  9.35it/s]\u001b[A\n",
            " 67%|██████▋   | 110/165 [00:11<00:05,  9.33it/s]\u001b[A\n",
            " 67%|██████▋   | 111/165 [00:11<00:05,  9.34it/s]\u001b[A\n",
            " 68%|██████▊   | 112/165 [00:11<00:05,  9.32it/s]\u001b[A\n",
            " 68%|██████▊   | 113/165 [00:12<00:05,  9.31it/s]\u001b[A\n",
            " 69%|██████▉   | 114/165 [00:12<00:05,  9.33it/s]\u001b[A\n",
            " 70%|██████▉   | 115/165 [00:12<00:05,  9.36it/s]\u001b[A\n",
            " 70%|███████   | 116/165 [00:12<00:05,  9.36it/s]\u001b[A\n",
            " 71%|███████   | 117/165 [00:12<00:05,  9.37it/s]\u001b[A\n",
            " 72%|███████▏  | 118/165 [00:12<00:05,  9.36it/s]\u001b[A\n",
            " 72%|███████▏  | 119/165 [00:12<00:04,  9.36it/s]\u001b[A\n",
            " 73%|███████▎  | 120/165 [00:12<00:04,  9.36it/s]\u001b[A\n",
            " 73%|███████▎  | 121/165 [00:12<00:04,  9.37it/s]\u001b[A\n",
            " 74%|███████▍  | 122/165 [00:13<00:04,  9.37it/s]\u001b[A\n",
            " 75%|███████▍  | 123/165 [00:13<00:04,  9.37it/s]\u001b[A\n",
            " 75%|███████▌  | 124/165 [00:13<00:04,  9.36it/s]\u001b[A\n",
            " 76%|███████▌  | 125/165 [00:13<00:04,  9.34it/s]\u001b[A\n",
            " 76%|███████▋  | 126/165 [00:13<00:04,  9.35it/s]\u001b[A\n",
            " 77%|███████▋  | 127/165 [00:13<00:04,  9.37it/s]\u001b[A\n",
            " 78%|███████▊  | 128/165 [00:13<00:03,  9.38it/s]\u001b[A\n",
            " 78%|███████▊  | 129/165 [00:13<00:03,  9.38it/s]\u001b[A\n",
            " 79%|███████▉  | 130/165 [00:13<00:03,  9.39it/s]\u001b[A\n",
            " 79%|███████▉  | 131/165 [00:13<00:03,  9.38it/s]\u001b[A\n",
            " 80%|████████  | 132/165 [00:14<00:03,  9.41it/s]\u001b[A\n",
            " 81%|████████  | 133/165 [00:14<00:03,  9.41it/s]\u001b[A\n",
            " 81%|████████  | 134/165 [00:14<00:03,  9.43it/s]\u001b[A\n",
            " 82%|████████▏ | 135/165 [00:14<00:03,  9.41it/s]\u001b[A\n",
            " 82%|████████▏ | 136/165 [00:14<00:03,  9.41it/s]\u001b[A\n",
            " 83%|████████▎ | 137/165 [00:14<00:02,  9.42it/s]\u001b[A\n",
            " 84%|████████▎ | 138/165 [00:14<00:02,  9.41it/s]\u001b[A\n",
            " 84%|████████▍ | 139/165 [00:14<00:02,  9.42it/s]\u001b[A\n",
            " 85%|████████▍ | 140/165 [00:14<00:02,  9.41it/s]\u001b[A\n",
            " 85%|████████▌ | 141/165 [00:15<00:02,  9.37it/s]\u001b[A\n",
            " 86%|████████▌ | 142/165 [00:15<00:02,  9.36it/s]\u001b[A\n",
            " 87%|████████▋ | 143/165 [00:15<00:02,  9.39it/s]\u001b[A\n",
            " 87%|████████▋ | 144/165 [00:15<00:02,  9.39it/s]\u001b[A\n",
            " 88%|████████▊ | 145/165 [00:15<00:02,  9.42it/s]\u001b[A\n",
            " 88%|████████▊ | 146/165 [00:15<00:02,  9.42it/s]\u001b[A\n",
            " 89%|████████▉ | 147/165 [00:15<00:01,  9.42it/s]\u001b[A\n",
            " 90%|████████▉ | 148/165 [00:15<00:01,  9.40it/s]\u001b[A\n",
            " 90%|█████████ | 149/165 [00:15<00:01,  9.40it/s]\u001b[A\n",
            " 91%|█████████ | 150/165 [00:15<00:01,  9.40it/s]\u001b[A\n",
            " 92%|█████████▏| 151/165 [00:16<00:01,  9.37it/s]\u001b[A\n",
            " 92%|█████████▏| 152/165 [00:16<00:01,  9.33it/s]\u001b[A\n",
            " 93%|█████████▎| 153/165 [00:16<00:01,  9.36it/s]\u001b[A\n",
            " 93%|█████████▎| 154/165 [00:16<00:01,  9.35it/s]\u001b[A\n",
            " 94%|█████████▍| 155/165 [00:16<00:01,  9.36it/s]\u001b[A\n",
            " 95%|█████████▍| 156/165 [00:16<00:00,  9.36it/s]\u001b[A\n",
            " 95%|█████████▌| 157/165 [00:16<00:00,  9.37it/s]\u001b[A\n",
            " 96%|█████████▌| 158/165 [00:16<00:00,  9.40it/s]\u001b[A\n",
            " 96%|█████████▋| 159/165 [00:16<00:00,  9.40it/s]\u001b[A\n",
            " 97%|█████████▋| 160/165 [00:17<00:00,  9.42it/s]\u001b[A\n",
            " 98%|█████████▊| 161/165 [00:17<00:00,  9.43it/s]\u001b[A\n",
            " 98%|█████████▊| 162/165 [00:17<00:00,  9.42it/s]\u001b[A\n",
            " 99%|█████████▉| 163/165 [00:17<00:00,  9.43it/s]\u001b[A\n",
            "100%|██████████| 165/165 [00:17<00:00,  9.37it/s]\n",
            "========== model: bert-base-uncased fold: 3 inference ==========\n",
            "========== model: bert-base-uncased fold: 3 inference ==========\n",
            "========== model: bert-base-uncased fold: 3 inference ==========\n",
            "========== model: bert-base-uncased fold: 3 inference ==========\n",
            "========== model: bert-base-uncased fold: 3 inference ==========\n",
            "========== model: bert-base-uncased fold: 3 inference ==========\n",
            "========== model: bert-base-uncased fold: 3 inference ==========\n",
            "========== model: bert-base-uncased fold: 3 inference ==========\n",
            "loading configuration file /content/activebus_bert_final/config.json\n",
            "Model config BertConfig {\n",
            "  \"_name_or_path\": \"activebus/BERT-XD_Review\",\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_ids\": 0,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.15.0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "loading weights file /content/activebus_bert_final/pytorch_model.bin\n",
            "Some weights of the model checkpoint at /content/activebus_bert_final were not used when initializing BertForSequenceClassification: ['cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /content/activebus_bert_final and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "\n",
            "  0%|          | 0/165 [00:00<?, ?it/s]\u001b[A\n",
            "  1%|          | 1/165 [00:00<00:18,  8.90it/s]\u001b[A\n",
            "  1%|          | 2/165 [00:00<00:17,  9.15it/s]\u001b[A\n",
            "  2%|▏         | 3/165 [00:00<00:17,  9.24it/s]\u001b[A\n",
            "  2%|▏         | 4/165 [00:00<00:17,  9.28it/s]\u001b[A\n",
            "  3%|▎         | 5/165 [00:00<00:17,  9.33it/s]\u001b[A\n",
            "  4%|▎         | 6/165 [00:00<00:16,  9.36it/s]\u001b[A\n",
            "  4%|▍         | 7/165 [00:00<00:16,  9.37it/s]\u001b[A\n",
            "  5%|▍         | 8/165 [00:00<00:16,  9.38it/s]\u001b[A\n",
            "  5%|▌         | 9/165 [00:00<00:16,  9.39it/s]\u001b[A\n",
            "  6%|▌         | 10/165 [00:01<00:16,  9.40it/s]\u001b[A\n",
            "  7%|▋         | 11/165 [00:01<00:16,  9.40it/s]\u001b[A\n",
            "  7%|▋         | 12/165 [00:01<00:16,  9.40it/s]\u001b[A\n",
            "  8%|▊         | 13/165 [00:01<00:16,  9.41it/s]\u001b[A\n",
            "  8%|▊         | 14/165 [00:01<00:16,  9.41it/s]\u001b[A\n",
            "  9%|▉         | 15/165 [00:01<00:15,  9.42it/s]\u001b[A\n",
            " 10%|▉         | 16/165 [00:01<00:15,  9.41it/s]\u001b[A\n",
            " 10%|█         | 17/165 [00:01<00:15,  9.39it/s]\u001b[A\n",
            " 11%|█         | 18/165 [00:01<00:15,  9.40it/s]\u001b[A\n",
            " 12%|█▏        | 19/165 [00:02<00:15,  9.38it/s]\u001b[A\n",
            " 12%|█▏        | 20/165 [00:02<00:15,  9.31it/s]\u001b[A\n",
            " 13%|█▎        | 21/165 [00:02<00:15,  9.33it/s]\u001b[A\n",
            " 13%|█▎        | 22/165 [00:02<00:15,  9.35it/s]\u001b[A\n",
            " 14%|█▍        | 23/165 [00:02<00:15,  9.38it/s]\u001b[A\n",
            " 15%|█▍        | 24/165 [00:02<00:15,  9.40it/s]\u001b[A\n",
            " 15%|█▌        | 25/165 [00:02<00:14,  9.39it/s]\u001b[A\n",
            " 16%|█▌        | 26/165 [00:02<00:14,  9.40it/s]\u001b[A\n",
            " 16%|█▋        | 27/165 [00:02<00:14,  9.41it/s]\u001b[A\n",
            " 17%|█▋        | 28/165 [00:02<00:14,  9.42it/s]\u001b[A\n",
            " 18%|█▊        | 29/165 [00:03<00:14,  9.42it/s]\u001b[A\n",
            " 18%|█▊        | 30/165 [00:03<00:14,  9.41it/s]\u001b[A\n",
            " 19%|█▉        | 31/165 [00:03<00:14,  9.40it/s]\u001b[A\n",
            " 19%|█▉        | 32/165 [00:03<00:14,  9.41it/s]\u001b[A\n",
            " 20%|██        | 33/165 [00:03<00:14,  9.42it/s]\u001b[A\n",
            " 21%|██        | 34/165 [00:03<00:13,  9.41it/s]\u001b[A\n",
            " 21%|██        | 35/165 [00:03<00:13,  9.41it/s]\u001b[A\n",
            " 22%|██▏       | 36/165 [00:03<00:13,  9.38it/s]\u001b[A\n",
            " 22%|██▏       | 37/165 [00:03<00:13,  9.38it/s]\u001b[A\n",
            " 23%|██▎       | 38/165 [00:04<00:13,  9.38it/s]\u001b[A\n",
            " 24%|██▎       | 39/165 [00:04<00:13,  9.37it/s]\u001b[A\n",
            " 24%|██▍       | 40/165 [00:04<00:13,  9.38it/s]\u001b[A\n",
            " 25%|██▍       | 41/165 [00:04<00:13,  9.38it/s]\u001b[A\n",
            " 25%|██▌       | 42/165 [00:04<00:13,  9.38it/s]\u001b[A\n",
            " 26%|██▌       | 43/165 [00:04<00:13,  9.37it/s]\u001b[A\n",
            " 27%|██▋       | 44/165 [00:04<00:12,  9.38it/s]\u001b[A\n",
            " 27%|██▋       | 45/165 [00:04<00:12,  9.34it/s]\u001b[A\n",
            " 28%|██▊       | 46/165 [00:04<00:12,  9.36it/s]\u001b[A\n",
            " 28%|██▊       | 47/165 [00:05<00:12,  9.37it/s]\u001b[A\n",
            " 29%|██▉       | 48/165 [00:05<00:12,  9.37it/s]\u001b[A\n",
            " 30%|██▉       | 49/165 [00:05<00:12,  9.35it/s]\u001b[A\n",
            " 30%|███       | 50/165 [00:05<00:12,  9.37it/s]\u001b[A\n",
            " 31%|███       | 51/165 [00:05<00:12,  9.38it/s]\u001b[A\n",
            " 32%|███▏      | 52/165 [00:05<00:12,  9.39it/s]\u001b[A\n",
            " 32%|███▏      | 53/165 [00:05<00:11,  9.37it/s]\u001b[A\n",
            " 33%|███▎      | 54/165 [00:05<00:11,  9.38it/s]\u001b[A\n",
            " 33%|███▎      | 55/165 [00:05<00:11,  9.41it/s]\u001b[A\n",
            " 34%|███▍      | 56/165 [00:05<00:11,  9.41it/s]\u001b[A\n",
            " 35%|███▍      | 57/165 [00:06<00:11,  9.40it/s]\u001b[A\n",
            " 35%|███▌      | 58/165 [00:06<00:11,  9.10it/s]\u001b[A\n",
            " 36%|███▌      | 59/165 [00:06<00:11,  9.19it/s]\u001b[A\n",
            " 36%|███▋      | 60/165 [00:06<00:11,  9.26it/s]\u001b[A\n",
            " 37%|███▋      | 61/165 [00:06<00:11,  9.31it/s]\u001b[A\n",
            " 38%|███▊      | 62/165 [00:06<00:11,  9.34it/s]\u001b[A\n",
            " 38%|███▊      | 63/165 [00:06<00:10,  9.37it/s]\u001b[A\n",
            " 39%|███▉      | 64/165 [00:06<00:10,  9.38it/s]\u001b[A\n",
            " 39%|███▉      | 65/165 [00:06<00:10,  9.37it/s]\u001b[A\n",
            " 40%|████      | 66/165 [00:07<00:10,  9.37it/s]\u001b[A\n",
            " 41%|████      | 67/165 [00:07<00:10,  9.38it/s]\u001b[A\n",
            " 41%|████      | 68/165 [00:07<00:10,  9.40it/s]\u001b[A\n",
            " 42%|████▏     | 69/165 [00:07<00:10,  9.39it/s]\u001b[A\n",
            " 42%|████▏     | 70/165 [00:07<00:10,  9.39it/s]\u001b[A\n",
            " 43%|████▎     | 71/165 [00:07<00:10,  9.38it/s]\u001b[A\n",
            " 44%|████▎     | 72/165 [00:07<00:09,  9.37it/s]\u001b[A\n",
            " 44%|████▍     | 73/165 [00:07<00:09,  9.37it/s]\u001b[A\n",
            " 45%|████▍     | 74/165 [00:07<00:09,  9.39it/s]\u001b[A\n",
            " 45%|████▌     | 75/165 [00:08<00:09,  9.39it/s]\u001b[A\n",
            " 46%|████▌     | 76/165 [00:08<00:09,  9.32it/s]\u001b[A\n",
            " 47%|████▋     | 77/165 [00:08<00:09,  9.35it/s]\u001b[A\n",
            " 47%|████▋     | 78/165 [00:08<00:09,  9.36it/s]\u001b[A\n",
            " 48%|████▊     | 79/165 [00:08<00:09,  9.39it/s]\u001b[A\n",
            " 48%|████▊     | 80/165 [00:08<00:09,  9.38it/s]\u001b[A\n",
            " 49%|████▉     | 81/165 [00:08<00:08,  9.39it/s]\u001b[A\n",
            " 50%|████▉     | 82/165 [00:08<00:08,  9.41it/s]\u001b[A\n",
            " 50%|█████     | 83/165 [00:08<00:08,  9.41it/s]\u001b[A\n",
            " 51%|█████     | 84/165 [00:08<00:08,  9.38it/s]\u001b[A\n",
            " 52%|█████▏    | 85/165 [00:09<00:08,  9.36it/s]\u001b[A\n",
            " 52%|█████▏    | 86/165 [00:09<00:08,  9.39it/s]\u001b[A\n",
            " 53%|█████▎    | 87/165 [00:09<00:08,  9.40it/s]\u001b[A\n",
            " 53%|█████▎    | 88/165 [00:09<00:08,  9.40it/s]\u001b[A\n",
            " 54%|█████▍    | 89/165 [00:09<00:08,  9.37it/s]\u001b[A\n",
            " 55%|█████▍    | 90/165 [00:09<00:08,  9.36it/s]\u001b[A\n",
            " 55%|█████▌    | 91/165 [00:09<00:07,  9.36it/s]\u001b[A\n",
            " 56%|█████▌    | 92/165 [00:09<00:07,  9.37it/s]\u001b[A\n",
            " 56%|█████▋    | 93/165 [00:09<00:07,  9.35it/s]\u001b[A\n",
            " 57%|█████▋    | 94/165 [00:10<00:07,  9.36it/s]\u001b[A\n",
            " 58%|█████▊    | 95/165 [00:10<00:07,  9.31it/s]\u001b[A\n",
            " 58%|█████▊    | 96/165 [00:10<00:07,  9.33it/s]\u001b[A\n",
            " 59%|█████▉    | 97/165 [00:10<00:07,  9.33it/s]\u001b[A\n",
            " 59%|█████▉    | 98/165 [00:10<00:07,  9.36it/s]\u001b[A\n",
            " 60%|██████    | 99/165 [00:10<00:07,  9.38it/s]\u001b[A\n",
            " 61%|██████    | 100/165 [00:10<00:06,  9.39it/s]\u001b[A\n",
            " 61%|██████    | 101/165 [00:10<00:06,  9.39it/s]\u001b[A\n",
            " 62%|██████▏   | 102/165 [00:10<00:06,  9.40it/s]\u001b[A\n",
            " 62%|██████▏   | 103/165 [00:10<00:06,  9.39it/s]\u001b[A\n",
            " 63%|██████▎   | 104/165 [00:11<00:06,  9.40it/s]\u001b[A\n",
            " 64%|██████▎   | 105/165 [00:11<00:06,  9.40it/s]\u001b[A\n",
            " 64%|██████▍   | 106/165 [00:11<00:06,  9.38it/s]\u001b[A\n",
            " 65%|██████▍   | 107/165 [00:11<00:06,  9.38it/s]\u001b[A\n",
            " 65%|██████▌   | 108/165 [00:11<00:06,  9.39it/s]\u001b[A\n",
            " 66%|██████▌   | 109/165 [00:11<00:05,  9.40it/s]\u001b[A\n",
            " 67%|██████▋   | 110/165 [00:11<00:05,  9.41it/s]\u001b[A\n",
            " 67%|██████▋   | 111/165 [00:11<00:05,  9.39it/s]\u001b[A\n",
            " 68%|██████▊   | 112/165 [00:11<00:05,  9.39it/s]\u001b[A\n",
            " 68%|██████▊   | 113/165 [00:12<00:05,  9.39it/s]\u001b[A\n",
            " 69%|██████▉   | 114/165 [00:12<00:05,  9.39it/s]\u001b[A\n",
            " 70%|██████▉   | 115/165 [00:12<00:05,  9.41it/s]\u001b[A\n",
            " 70%|███████   | 116/165 [00:12<00:05,  9.41it/s]\u001b[A\n",
            " 71%|███████   | 117/165 [00:12<00:05,  9.41it/s]\u001b[A\n",
            " 72%|███████▏  | 118/165 [00:12<00:04,  9.41it/s]\u001b[A\n",
            " 72%|███████▏  | 119/165 [00:12<00:04,  9.39it/s]\u001b[A\n",
            " 73%|███████▎  | 120/165 [00:12<00:04,  9.37it/s]\u001b[A\n",
            " 73%|███████▎  | 121/165 [00:12<00:04,  9.39it/s]\u001b[A\n",
            " 74%|███████▍  | 122/165 [00:13<00:04,  9.40it/s]\u001b[A\n",
            " 75%|███████▍  | 123/165 [00:13<00:04,  9.38it/s]\u001b[A\n",
            " 75%|███████▌  | 124/165 [00:13<00:04,  9.38it/s]\u001b[A\n",
            " 76%|███████▌  | 125/165 [00:13<00:04,  9.40it/s]\u001b[A\n",
            " 76%|███████▋  | 126/165 [00:13<00:04,  9.41it/s]\u001b[A\n",
            " 77%|███████▋  | 127/165 [00:13<00:04,  9.41it/s]\u001b[A\n",
            " 78%|███████▊  | 128/165 [00:13<00:03,  9.39it/s]\u001b[A\n",
            " 78%|███████▊  | 129/165 [00:13<00:03,  9.38it/s]\u001b[A\n",
            " 79%|███████▉  | 130/165 [00:13<00:03,  9.39it/s]\u001b[A\n",
            " 79%|███████▉  | 131/165 [00:13<00:03,  9.38it/s]\u001b[A\n",
            " 80%|████████  | 132/165 [00:14<00:03,  9.39it/s]\u001b[A\n",
            " 81%|████████  | 133/165 [00:14<00:03,  9.39it/s]\u001b[A\n",
            " 81%|████████  | 134/165 [00:14<00:03,  9.40it/s]\u001b[A\n",
            " 82%|████████▏ | 135/165 [00:14<00:03,  9.39it/s]\u001b[A\n",
            " 82%|████████▏ | 136/165 [00:14<00:03,  9.39it/s]\u001b[A\n",
            " 83%|████████▎ | 137/165 [00:14<00:02,  9.40it/s]\u001b[A\n",
            " 84%|████████▎ | 138/165 [00:14<00:02,  9.41it/s]\u001b[A\n",
            " 84%|████████▍ | 139/165 [00:14<00:02,  9.41it/s]\u001b[A\n",
            " 85%|████████▍ | 140/165 [00:14<00:02,  9.38it/s]\u001b[A\n",
            " 85%|████████▌ | 141/165 [00:15<00:02,  9.38it/s]\u001b[A\n",
            " 86%|████████▌ | 142/165 [00:15<00:02,  9.38it/s]\u001b[A\n",
            " 87%|████████▋ | 143/165 [00:15<00:02,  9.40it/s]\u001b[A\n",
            " 87%|████████▋ | 144/165 [00:15<00:02,  9.40it/s]\u001b[A\n",
            " 88%|████████▊ | 145/165 [00:15<00:02,  9.40it/s]\u001b[A\n",
            " 88%|████████▊ | 146/165 [00:15<00:02,  9.41it/s]\u001b[A\n",
            " 89%|████████▉ | 147/165 [00:15<00:01,  9.39it/s]\u001b[A\n",
            " 90%|████████▉ | 148/165 [00:15<00:01,  9.40it/s]\u001b[A\n",
            " 90%|█████████ | 149/165 [00:15<00:01,  9.40it/s]\u001b[A\n",
            " 91%|█████████ | 150/165 [00:15<00:01,  9.41it/s]\u001b[A\n",
            " 92%|█████████▏| 151/165 [00:16<00:01,  9.41it/s]\u001b[A\n",
            " 92%|█████████▏| 152/165 [00:16<00:01,  9.41it/s]\u001b[A\n",
            " 93%|█████████▎| 153/165 [00:16<00:01,  9.38it/s]\u001b[A\n",
            " 93%|█████████▎| 154/165 [00:16<00:01,  9.38it/s]\u001b[A\n",
            " 94%|█████████▍| 155/165 [00:16<00:01,  9.37it/s]\u001b[A\n",
            " 95%|█████████▍| 156/165 [00:16<00:00,  9.37it/s]\u001b[A\n",
            " 95%|█████████▌| 157/165 [00:16<00:00,  9.37it/s]\u001b[A\n",
            " 96%|█████████▌| 158/165 [00:16<00:00,  9.40it/s]\u001b[A\n",
            " 96%|█████████▋| 159/165 [00:16<00:00,  9.42it/s]\u001b[A\n",
            " 97%|█████████▋| 160/165 [00:17<00:00,  9.42it/s]\u001b[A\n",
            " 98%|█████████▊| 161/165 [00:17<00:00,  9.43it/s]\u001b[A\n",
            " 98%|█████████▊| 162/165 [00:17<00:00,  9.45it/s]\u001b[A\n",
            " 99%|█████████▉| 163/165 [00:17<00:00,  9.45it/s]\u001b[A\n",
            "100%|██████████| 165/165 [00:17<00:00,  9.37it/s]\n",
            "========== model: bert-base-uncased fold: 4 inference ==========\n",
            "========== model: bert-base-uncased fold: 4 inference ==========\n",
            "========== model: bert-base-uncased fold: 4 inference ==========\n",
            "========== model: bert-base-uncased fold: 4 inference ==========\n",
            "========== model: bert-base-uncased fold: 4 inference ==========\n",
            "========== model: bert-base-uncased fold: 4 inference ==========\n",
            "========== model: bert-base-uncased fold: 4 inference ==========\n",
            "========== model: bert-base-uncased fold: 4 inference ==========\n",
            "loading configuration file /content/activebus_bert_final/config.json\n",
            "Model config BertConfig {\n",
            "  \"_name_or_path\": \"activebus/BERT-XD_Review\",\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_ids\": 0,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.15.0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "loading weights file /content/activebus_bert_final/pytorch_model.bin\n",
            "Some weights of the model checkpoint at /content/activebus_bert_final were not used when initializing BertForSequenceClassification: ['cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /content/activebus_bert_final and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "\n",
            "  0%|          | 0/165 [00:00<?, ?it/s]\u001b[A\n",
            "  1%|          | 1/165 [00:00<00:20,  8.14it/s]\u001b[A\n",
            "  1%|          | 2/165 [00:00<00:18,  8.80it/s]\u001b[A\n",
            "  2%|▏         | 3/165 [00:00<00:17,  9.06it/s]\u001b[A\n",
            "  2%|▏         | 4/165 [00:00<00:17,  9.21it/s]\u001b[A\n",
            "  3%|▎         | 5/165 [00:00<00:17,  9.29it/s]\u001b[A\n",
            "  4%|▎         | 6/165 [00:00<00:17,  9.31it/s]\u001b[A\n",
            "  4%|▍         | 7/165 [00:00<00:16,  9.35it/s]\u001b[A\n",
            "  5%|▍         | 8/165 [00:00<00:16,  9.36it/s]\u001b[A\n",
            "  5%|▌         | 9/165 [00:00<00:16,  9.39it/s]\u001b[A\n",
            "  6%|▌         | 10/165 [00:01<00:16,  9.38it/s]\u001b[A\n",
            "  7%|▋         | 11/165 [00:01<00:16,  9.35it/s]\u001b[A\n",
            "  7%|▋         | 12/165 [00:01<00:16,  9.33it/s]\u001b[A\n",
            "  8%|▊         | 13/165 [00:01<00:16,  9.36it/s]\u001b[A\n",
            "  8%|▊         | 14/165 [00:01<00:16,  9.37it/s]\u001b[A\n",
            "  9%|▉         | 15/165 [00:01<00:15,  9.38it/s]\u001b[A\n",
            " 10%|▉         | 16/165 [00:01<00:15,  9.40it/s]\u001b[A\n",
            " 10%|█         | 17/165 [00:01<00:15,  9.40it/s]\u001b[A\n",
            " 11%|█         | 18/165 [00:01<00:15,  9.41it/s]\u001b[A\n",
            " 12%|█▏        | 19/165 [00:02<00:15,  9.41it/s]\u001b[A\n",
            " 12%|█▏        | 20/165 [00:02<00:15,  9.38it/s]\u001b[A\n",
            " 13%|█▎        | 21/165 [00:02<00:15,  9.40it/s]\u001b[A\n",
            " 13%|█▎        | 22/165 [00:02<00:15,  9.39it/s]\u001b[A\n",
            " 14%|█▍        | 23/165 [00:02<00:15,  9.42it/s]\u001b[A\n",
            " 15%|█▍        | 24/165 [00:02<00:14,  9.40it/s]\u001b[A\n",
            " 15%|█▌        | 25/165 [00:02<00:14,  9.39it/s]\u001b[A\n",
            " 16%|█▌        | 26/165 [00:02<00:14,  9.39it/s]\u001b[A\n",
            " 16%|█▋        | 27/165 [00:02<00:14,  9.38it/s]\u001b[A\n",
            " 17%|█▋        | 28/165 [00:02<00:14,  9.39it/s]\u001b[A\n",
            " 18%|█▊        | 29/165 [00:03<00:14,  9.38it/s]\u001b[A\n",
            " 18%|█▊        | 30/165 [00:03<00:14,  9.38it/s]\u001b[A\n",
            " 19%|█▉        | 31/165 [00:03<00:14,  9.40it/s]\u001b[A\n",
            " 19%|█▉        | 32/165 [00:03<00:14,  9.32it/s]\u001b[A\n",
            " 20%|██        | 33/165 [00:03<00:14,  9.36it/s]\u001b[A\n",
            " 21%|██        | 34/165 [00:03<00:13,  9.39it/s]\u001b[A\n",
            " 21%|██        | 35/165 [00:03<00:13,  9.38it/s]\u001b[A\n",
            " 22%|██▏       | 36/165 [00:03<00:13,  9.38it/s]\u001b[A\n",
            " 22%|██▏       | 37/165 [00:03<00:13,  9.39it/s]\u001b[A\n",
            " 23%|██▎       | 38/165 [00:04<00:13,  9.39it/s]\u001b[A\n",
            " 24%|██▎       | 39/165 [00:04<00:13,  9.39it/s]\u001b[A\n",
            " 24%|██▍       | 40/165 [00:04<00:13,  9.39it/s]\u001b[A\n",
            " 25%|██▍       | 41/165 [00:04<00:13,  9.41it/s]\u001b[A\n",
            " 25%|██▌       | 42/165 [00:04<00:13,  9.40it/s]\u001b[A\n",
            " 26%|██▌       | 43/165 [00:04<00:13,  9.37it/s]\u001b[A\n",
            " 27%|██▋       | 44/165 [00:04<00:12,  9.36it/s]\u001b[A\n",
            " 27%|██▋       | 45/165 [00:04<00:12,  9.35it/s]\u001b[A\n",
            " 28%|██▊       | 46/165 [00:04<00:12,  9.36it/s]\u001b[A\n",
            " 28%|██▊       | 47/165 [00:05<00:12,  9.39it/s]\u001b[A\n",
            " 29%|██▉       | 48/165 [00:05<00:12,  9.40it/s]\u001b[A\n",
            " 30%|██▉       | 49/165 [00:05<00:12,  9.38it/s]\u001b[A\n",
            " 30%|███       | 50/165 [00:05<00:12,  9.36it/s]\u001b[A\n",
            " 31%|███       | 51/165 [00:05<00:12,  9.36it/s]\u001b[A\n",
            " 32%|███▏      | 52/165 [00:05<00:12,  9.37it/s]\u001b[A\n",
            " 32%|███▏      | 53/165 [00:05<00:11,  9.35it/s]\u001b[A\n",
            " 33%|███▎      | 54/165 [00:05<00:11,  9.36it/s]\u001b[A\n",
            " 33%|███▎      | 55/165 [00:05<00:11,  9.38it/s]\u001b[A\n",
            " 34%|███▍      | 56/165 [00:05<00:11,  9.36it/s]\u001b[A\n",
            " 35%|███▍      | 57/165 [00:06<00:11,  9.37it/s]\u001b[A\n",
            " 35%|███▌      | 58/165 [00:06<00:11,  9.38it/s]\u001b[A\n",
            " 36%|███▌      | 59/165 [00:06<00:11,  9.37it/s]\u001b[A\n",
            " 36%|███▋      | 60/165 [00:06<00:11,  9.35it/s]\u001b[A\n",
            " 37%|███▋      | 61/165 [00:06<00:11,  9.32it/s]\u001b[A\n",
            " 38%|███▊      | 62/165 [00:06<00:11,  9.34it/s]\u001b[A\n",
            " 38%|███▊      | 63/165 [00:06<00:10,  9.35it/s]\u001b[A\n",
            " 39%|███▉      | 64/165 [00:06<00:10,  9.35it/s]\u001b[A\n",
            " 39%|███▉      | 65/165 [00:06<00:10,  9.36it/s]\u001b[A\n",
            " 40%|████      | 66/165 [00:07<00:10,  9.39it/s]\u001b[A\n",
            " 41%|████      | 67/165 [00:07<00:10,  9.40it/s]\u001b[A\n",
            " 41%|████      | 68/165 [00:07<00:10,  9.39it/s]\u001b[A\n",
            " 42%|████▏     | 69/165 [00:07<00:10,  9.38it/s]\u001b[A\n",
            " 42%|████▏     | 70/165 [00:07<00:10,  9.38it/s]\u001b[A\n",
            " 43%|████▎     | 71/165 [00:07<00:10,  9.39it/s]\u001b[A\n",
            " 44%|████▎     | 72/165 [00:07<00:09,  9.39it/s]\u001b[A\n",
            " 44%|████▍     | 73/165 [00:07<00:09,  9.40it/s]\u001b[A\n",
            " 45%|████▍     | 74/165 [00:07<00:09,  9.36it/s]\u001b[A\n",
            " 45%|████▌     | 75/165 [00:08<00:09,  9.37it/s]\u001b[A\n",
            " 46%|████▌     | 76/165 [00:08<00:09,  9.38it/s]\u001b[A\n",
            " 47%|████▋     | 77/165 [00:08<00:09,  9.39it/s]\u001b[A\n",
            " 47%|████▋     | 78/165 [00:08<00:09,  9.39it/s]\u001b[A\n",
            " 48%|████▊     | 79/165 [00:08<00:09,  9.41it/s]\u001b[A\n",
            " 48%|████▊     | 80/165 [00:08<00:09,  9.38it/s]\u001b[A\n",
            " 49%|████▉     | 81/165 [00:08<00:08,  9.38it/s]\u001b[A\n",
            " 50%|████▉     | 82/165 [00:08<00:08,  9.38it/s]\u001b[A\n",
            " 50%|█████     | 83/165 [00:08<00:08,  9.40it/s]\u001b[A\n",
            " 51%|█████     | 84/165 [00:08<00:08,  9.40it/s]\u001b[A\n",
            " 52%|█████▏    | 85/165 [00:09<00:08,  9.40it/s]\u001b[A\n",
            " 52%|█████▏    | 86/165 [00:09<00:08,  9.41it/s]\u001b[A\n",
            " 53%|█████▎    | 87/165 [00:09<00:08,  9.40it/s]\u001b[A\n",
            " 53%|█████▎    | 88/165 [00:09<00:08,  9.39it/s]\u001b[A\n",
            " 54%|█████▍    | 89/165 [00:09<00:08,  9.40it/s]\u001b[A\n",
            " 55%|█████▍    | 90/165 [00:09<00:07,  9.42it/s]\u001b[A\n",
            " 55%|█████▌    | 91/165 [00:09<00:07,  9.42it/s]\u001b[A\n",
            " 56%|█████▌    | 92/165 [00:09<00:07,  9.40it/s]\u001b[A\n",
            " 56%|█████▋    | 93/165 [00:09<00:07,  9.39it/s]\u001b[A\n",
            " 57%|█████▋    | 94/165 [00:10<00:07,  9.37it/s]\u001b[A\n",
            " 58%|█████▊    | 95/165 [00:10<00:07,  9.36it/s]\u001b[A\n",
            " 58%|█████▊    | 96/165 [00:10<00:07,  9.39it/s]\u001b[A\n",
            " 59%|█████▉    | 97/165 [00:10<00:07,  9.38it/s]\u001b[A\n",
            " 59%|█████▉    | 98/165 [00:10<00:07,  9.38it/s]\u001b[A\n",
            " 60%|██████    | 99/165 [00:10<00:07,  9.38it/s]\u001b[A\n",
            " 61%|██████    | 100/165 [00:10<00:06,  9.38it/s]\u001b[A\n",
            " 61%|██████    | 101/165 [00:10<00:06,  9.39it/s]\u001b[A\n",
            " 62%|██████▏   | 102/165 [00:10<00:06,  9.40it/s]\u001b[A\n",
            " 62%|██████▏   | 103/165 [00:10<00:06,  9.41it/s]\u001b[A\n",
            " 63%|██████▎   | 104/165 [00:11<00:06,  9.36it/s]\u001b[A\n",
            " 64%|██████▎   | 105/165 [00:11<00:06,  9.37it/s]\u001b[A\n",
            " 64%|██████▍   | 106/165 [00:11<00:06,  9.37it/s]\u001b[A\n",
            " 65%|██████▍   | 107/165 [00:11<00:06,  9.35it/s]\u001b[A\n",
            " 65%|██████▌   | 108/165 [00:11<00:06,  9.36it/s]\u001b[A\n",
            " 66%|██████▌   | 109/165 [00:11<00:05,  9.38it/s]\u001b[A\n",
            " 67%|██████▋   | 110/165 [00:11<00:05,  9.38it/s]\u001b[A\n",
            " 67%|██████▋   | 111/165 [00:11<00:05,  9.39it/s]\u001b[A\n",
            " 68%|██████▊   | 112/165 [00:11<00:05,  9.38it/s]\u001b[A\n",
            " 68%|██████▊   | 113/165 [00:12<00:05,  9.38it/s]\u001b[A\n",
            " 69%|██████▉   | 114/165 [00:12<00:05,  9.40it/s]\u001b[A\n",
            " 70%|██████▉   | 115/165 [00:12<00:05,  9.42it/s]\u001b[A\n",
            " 70%|███████   | 116/165 [00:12<00:05,  9.41it/s]\u001b[A\n",
            " 71%|███████   | 117/165 [00:12<00:05,  9.41it/s]\u001b[A\n",
            " 72%|███████▏  | 118/165 [00:12<00:05,  9.39it/s]\u001b[A\n",
            " 72%|███████▏  | 119/165 [00:12<00:04,  9.40it/s]\u001b[A\n",
            " 73%|███████▎  | 120/165 [00:12<00:04,  9.41it/s]\u001b[A\n",
            " 73%|███████▎  | 121/165 [00:12<00:04,  9.43it/s]\u001b[A\n",
            " 74%|███████▍  | 122/165 [00:13<00:04,  9.42it/s]\u001b[A\n",
            " 75%|███████▍  | 123/165 [00:13<00:04,  9.42it/s]\u001b[A\n",
            " 75%|███████▌  | 124/165 [00:13<00:04,  9.43it/s]\u001b[A\n",
            " 76%|███████▌  | 125/165 [00:13<00:04,  9.43it/s]\u001b[A\n",
            " 76%|███████▋  | 126/165 [00:13<00:04,  9.43it/s]\u001b[A\n",
            " 77%|███████▋  | 127/165 [00:13<00:04,  9.41it/s]\u001b[A\n",
            " 78%|███████▊  | 128/165 [00:13<00:03,  9.41it/s]\u001b[A\n",
            " 78%|███████▊  | 129/165 [00:13<00:03,  9.41it/s]\u001b[A\n",
            " 79%|███████▉  | 130/165 [00:13<00:03,  9.41it/s]\u001b[A\n",
            " 79%|███████▉  | 131/165 [00:13<00:03,  9.41it/s]\u001b[A\n",
            " 80%|████████  | 132/165 [00:14<00:03,  9.41it/s]\u001b[A\n",
            " 81%|████████  | 133/165 [00:14<00:03,  9.41it/s]\u001b[A\n",
            " 81%|████████  | 134/165 [00:14<00:03,  9.41it/s]\u001b[A\n",
            " 82%|████████▏ | 135/165 [00:14<00:03,  9.37it/s]\u001b[A\n",
            " 82%|████████▏ | 136/165 [00:14<00:03,  9.37it/s]\u001b[A\n",
            " 83%|████████▎ | 137/165 [00:14<00:02,  9.39it/s]\u001b[A\n",
            " 84%|████████▎ | 138/165 [00:14<00:02,  9.38it/s]\u001b[A\n",
            " 84%|████████▍ | 139/165 [00:14<00:02,  9.39it/s]\u001b[A\n",
            " 85%|████████▍ | 140/165 [00:14<00:02,  9.40it/s]\u001b[A\n",
            " 85%|████████▌ | 141/165 [00:15<00:02,  9.41it/s]\u001b[A\n",
            " 86%|████████▌ | 142/165 [00:15<00:02,  9.41it/s]\u001b[A\n",
            " 87%|████████▋ | 143/165 [00:15<00:02,  9.42it/s]\u001b[A\n",
            " 87%|████████▋ | 144/165 [00:15<00:02,  9.43it/s]\u001b[A\n",
            " 88%|████████▊ | 145/165 [00:15<00:02,  9.41it/s]\u001b[A\n",
            " 88%|████████▊ | 146/165 [00:15<00:02,  9.38it/s]\u001b[A\n",
            " 89%|████████▉ | 147/165 [00:15<00:01,  9.36it/s]\u001b[A\n",
            " 90%|████████▉ | 148/165 [00:15<00:01,  9.39it/s]\u001b[A\n",
            " 90%|█████████ | 149/165 [00:15<00:01,  9.39it/s]\u001b[A\n",
            " 91%|█████████ | 150/165 [00:15<00:01,  9.39it/s]\u001b[A\n",
            " 92%|█████████▏| 151/165 [00:16<00:01,  9.39it/s]\u001b[A\n",
            " 92%|█████████▏| 152/165 [00:16<00:01,  9.41it/s]\u001b[A\n",
            " 93%|█████████▎| 153/165 [00:16<00:01,  9.39it/s]\u001b[A\n",
            " 93%|█████████▎| 154/165 [00:16<00:01,  9.40it/s]\u001b[A\n",
            " 94%|█████████▍| 155/165 [00:16<00:01,  9.39it/s]\u001b[A\n",
            " 95%|█████████▍| 156/165 [00:16<00:00,  9.40it/s]\u001b[A\n",
            " 95%|█████████▌| 157/165 [00:16<00:00,  9.40it/s]\u001b[A\n",
            " 96%|█████████▌| 158/165 [00:16<00:00,  9.40it/s]\u001b[A\n",
            " 96%|█████████▋| 159/165 [00:16<00:00,  9.41it/s]\u001b[A\n",
            " 97%|█████████▋| 160/165 [00:17<00:00,  9.43it/s]\u001b[A\n",
            " 98%|█████████▊| 161/165 [00:17<00:00,  9.45it/s]\u001b[A\n",
            " 98%|█████████▊| 162/165 [00:17<00:00,  9.45it/s]\u001b[A\n",
            " 99%|█████████▉| 163/165 [00:17<00:00,  9.45it/s]\u001b[A\n",
            "100%|██████████| 165/165 [00:17<00:00,  9.37it/s]\n",
            " 67%|██████▋   | 2/3 [1:30:48<45:24, 2724.36s/it]Didn't find file /content/activebus_bert_final/added_tokens.json. We won't load it.\n",
            "loading file /content/activebus_bert_final/vocab.txt\n",
            "loading file /content/activebus_bert_final/tokenizer.json\n",
            "loading file None\n",
            "loading file /content/activebus_bert_final/special_tokens_map.json\n",
            "loading file /content/activebus_bert_final/tokenizer_config.json\n",
            "========== fold: 0 training ==========\n",
            "========== fold: 0 training ==========\n",
            "========== fold: 0 training ==========\n",
            "========== fold: 0 training ==========\n",
            "========== fold: 0 training ==========\n",
            "========== fold: 0 training ==========\n",
            "========== fold: 0 training ==========\n",
            "========== fold: 0 training ==========\n",
            "========== fold: 0 training ==========\n",
            "Didn't find file /content/activebus_bert_final/added_tokens.json. We won't load it.\n",
            "loading file /content/activebus_bert_final/vocab.txt\n",
            "loading file None\n",
            "loading file /content/activebus_bert_final/special_tokens_map.json\n",
            "loading file /content/activebus_bert_final/tokenizer_config.json\n",
            "loading file /content/activebus_bert_final/tokenizer.json\n",
            "Didn't find file /content/activebus_bert_final/added_tokens.json. We won't load it.\n",
            "loading file /content/activebus_bert_final/vocab.txt\n",
            "loading file None\n",
            "loading file /content/activebus_bert_final/special_tokens_map.json\n",
            "loading file /content/activebus_bert_final/tokenizer_config.json\n",
            "loading file /content/activebus_bert_final/tokenizer.json\n",
            "loading configuration file /content/activebus_bert_final/config.json\n",
            "Model config BertConfig {\n",
            "  \"_name_or_path\": \"activebus/BERT-XD_Review\",\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_ids\": 0,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.15.0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "loading weights file /content/activebus_bert_final/pytorch_model.bin\n",
            "Some weights of the model checkpoint at /content/activebus_bert_final were not used when initializing BertForSequenceClassification: ['cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /content/activebus_bert_final and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: [1][0/306] Elapsed 0m 0s (remain 2m 51s) Loss: 0.7184 \n",
            "Epoch: [1][100/306] Elapsed 0m 31s (remain 1m 3s) Loss: 0.2319 \n",
            "Epoch: [1][200/306] Elapsed 1m 2s (remain 0m 32s) Loss: 0.1641 \n",
            "Epoch: [1][300/306] Elapsed 1m 33s (remain 0m 1s) Loss: 0.1351 \n",
            "Epoch: [1][305/306] Elapsed 1m 34s (remain 0m 0s) Loss: 0.1348 \n",
            "EVAL: [0/77] Elapsed 0m 0s (remain 0m 27s) Loss: 0.0043 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1 - avg_train_loss: 0.1348  avg_val_loss: 0.0792  time: 103s\n",
            "Epoch 1 - avg_train_loss: 0.1348  avg_val_loss: 0.0792  time: 103s\n",
            "Epoch 1 - avg_train_loss: 0.1348  avg_val_loss: 0.0792  time: 103s\n",
            "Epoch 1 - avg_train_loss: 0.1348  avg_val_loss: 0.0792  time: 103s\n",
            "Epoch 1 - avg_train_loss: 0.1348  avg_val_loss: 0.0792  time: 103s\n",
            "Epoch 1 - avg_train_loss: 0.1348  avg_val_loss: 0.0792  time: 103s\n",
            "Epoch 1 - avg_train_loss: 0.1348  avg_val_loss: 0.0792  time: 103s\n",
            "Epoch 1 - avg_train_loss: 0.1348  avg_val_loss: 0.0792  time: 103s\n",
            "Epoch 1 - avg_train_loss: 0.1348  avg_val_loss: 0.0792  time: 103s\n",
            "Epoch 1 - Score: 0.07919243863042055\n",
            "Epoch 1 - Score: 0.07919243863042055\n",
            "Epoch 1 - Score: 0.07919243863042055\n",
            "Epoch 1 - Score: 0.07919243863042055\n",
            "Epoch 1 - Score: 0.07919243863042055\n",
            "Epoch 1 - Score: 0.07919243863042055\n",
            "Epoch 1 - Score: 0.07919243863042055\n",
            "Epoch 1 - Score: 0.07919243863042055\n",
            "Epoch 1 - Score: 0.07919243863042055\n",
            "Epoch 1 - Save Best Score: 0.0792 Model\n",
            "Epoch 1 - Save Best Score: 0.0792 Model\n",
            "Epoch 1 - Save Best Score: 0.0792 Model\n",
            "Epoch 1 - Save Best Score: 0.0792 Model\n",
            "Epoch 1 - Save Best Score: 0.0792 Model\n",
            "Epoch 1 - Save Best Score: 0.0792 Model\n",
            "Epoch 1 - Save Best Score: 0.0792 Model\n",
            "Epoch 1 - Save Best Score: 0.0792 Model\n",
            "Epoch 1 - Save Best Score: 0.0792 Model\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EVAL: [76/77] Elapsed 0m 8s (remain 0m 0s) Loss: 0.0792 \n",
            "Log loss for 0 is 0.07919243863042055\n",
            "Epoch: [2][0/306] Elapsed 0m 0s (remain 2m 52s) Loss: 0.2494 \n",
            "Epoch: [2][100/306] Elapsed 0m 31s (remain 1m 3s) Loss: 0.0599 \n",
            "Epoch: [2][200/306] Elapsed 1m 2s (remain 0m 32s) Loss: 0.0535 \n",
            "Epoch: [2][300/306] Elapsed 1m 33s (remain 0m 1s) Loss: 0.0595 \n",
            "Epoch: [2][305/306] Elapsed 1m 34s (remain 0m 0s) Loss: 0.0595 \n",
            "EVAL: [0/77] Elapsed 0m 0s (remain 0m 26s) Loss: 0.0051 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2 - avg_train_loss: 0.0595  avg_val_loss: 0.0876  time: 103s\n",
            "Epoch 2 - avg_train_loss: 0.0595  avg_val_loss: 0.0876  time: 103s\n",
            "Epoch 2 - avg_train_loss: 0.0595  avg_val_loss: 0.0876  time: 103s\n",
            "Epoch 2 - avg_train_loss: 0.0595  avg_val_loss: 0.0876  time: 103s\n",
            "Epoch 2 - avg_train_loss: 0.0595  avg_val_loss: 0.0876  time: 103s\n",
            "Epoch 2 - avg_train_loss: 0.0595  avg_val_loss: 0.0876  time: 103s\n",
            "Epoch 2 - avg_train_loss: 0.0595  avg_val_loss: 0.0876  time: 103s\n",
            "Epoch 2 - avg_train_loss: 0.0595  avg_val_loss: 0.0876  time: 103s\n",
            "Epoch 2 - avg_train_loss: 0.0595  avg_val_loss: 0.0876  time: 103s\n",
            "Epoch 2 - Score: 0.08755994881469156\n",
            "Epoch 2 - Score: 0.08755994881469156\n",
            "Epoch 2 - Score: 0.08755994881469156\n",
            "Epoch 2 - Score: 0.08755994881469156\n",
            "Epoch 2 - Score: 0.08755994881469156\n",
            "Epoch 2 - Score: 0.08755994881469156\n",
            "Epoch 2 - Score: 0.08755994881469156\n",
            "Epoch 2 - Score: 0.08755994881469156\n",
            "Epoch 2 - Score: 0.08755994881469156\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EVAL: [76/77] Elapsed 0m 8s (remain 0m 0s) Loss: 0.0876 \n",
            "Log loss for 1 is 0.08755994881469156\n",
            "Epoch: [3][0/306] Elapsed 0m 0s (remain 3m 0s) Loss: 0.0031 \n",
            "Epoch: [3][100/306] Elapsed 0m 31s (remain 1m 3s) Loss: 0.0418 \n",
            "Epoch: [3][200/306] Elapsed 1m 2s (remain 0m 32s) Loss: 0.0351 \n",
            "Epoch: [3][300/306] Elapsed 1m 33s (remain 0m 1s) Loss: 0.0359 \n",
            "Epoch: [3][305/306] Elapsed 1m 34s (remain 0m 0s) Loss: 0.0361 \n",
            "EVAL: [0/77] Elapsed 0m 0s (remain 0m 26s) Loss: 0.0017 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3 - avg_train_loss: 0.0361  avg_val_loss: 0.0762  time: 103s\n",
            "Epoch 3 - avg_train_loss: 0.0361  avg_val_loss: 0.0762  time: 103s\n",
            "Epoch 3 - avg_train_loss: 0.0361  avg_val_loss: 0.0762  time: 103s\n",
            "Epoch 3 - avg_train_loss: 0.0361  avg_val_loss: 0.0762  time: 103s\n",
            "Epoch 3 - avg_train_loss: 0.0361  avg_val_loss: 0.0762  time: 103s\n",
            "Epoch 3 - avg_train_loss: 0.0361  avg_val_loss: 0.0762  time: 103s\n",
            "Epoch 3 - avg_train_loss: 0.0361  avg_val_loss: 0.0762  time: 103s\n",
            "Epoch 3 - avg_train_loss: 0.0361  avg_val_loss: 0.0762  time: 103s\n",
            "Epoch 3 - avg_train_loss: 0.0361  avg_val_loss: 0.0762  time: 103s\n",
            "Epoch 3 - Score: 0.07616919284298332\n",
            "Epoch 3 - Score: 0.07616919284298332\n",
            "Epoch 3 - Score: 0.07616919284298332\n",
            "Epoch 3 - Score: 0.07616919284298332\n",
            "Epoch 3 - Score: 0.07616919284298332\n",
            "Epoch 3 - Score: 0.07616919284298332\n",
            "Epoch 3 - Score: 0.07616919284298332\n",
            "Epoch 3 - Score: 0.07616919284298332\n",
            "Epoch 3 - Score: 0.07616919284298332\n",
            "Epoch 3 - Save Best Score: 0.0762 Model\n",
            "Epoch 3 - Save Best Score: 0.0762 Model\n",
            "Epoch 3 - Save Best Score: 0.0762 Model\n",
            "Epoch 3 - Save Best Score: 0.0762 Model\n",
            "Epoch 3 - Save Best Score: 0.0762 Model\n",
            "Epoch 3 - Save Best Score: 0.0762 Model\n",
            "Epoch 3 - Save Best Score: 0.0762 Model\n",
            "Epoch 3 - Save Best Score: 0.0762 Model\n",
            "Epoch 3 - Save Best Score: 0.0762 Model\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EVAL: [76/77] Elapsed 0m 8s (remain 0m 0s) Loss: 0.0762 \n",
            "Log loss for 2 is 0.07616919284298332\n",
            "Epoch: [4][0/306] Elapsed 0m 0s (remain 2m 48s) Loss: 0.0054 \n",
            "Epoch: [4][100/306] Elapsed 0m 31s (remain 1m 3s) Loss: 0.0200 \n",
            "Epoch: [4][200/306] Elapsed 1m 2s (remain 0m 32s) Loss: 0.0197 \n",
            "Epoch: [4][300/306] Elapsed 1m 33s (remain 0m 1s) Loss: 0.0206 \n",
            "Epoch: [4][305/306] Elapsed 1m 34s (remain 0m 0s) Loss: 0.0205 \n",
            "EVAL: [0/77] Elapsed 0m 0s (remain 0m 28s) Loss: 0.0011 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4 - avg_train_loss: 0.0205  avg_val_loss: 0.0857  time: 103s\n",
            "Epoch 4 - avg_train_loss: 0.0205  avg_val_loss: 0.0857  time: 103s\n",
            "Epoch 4 - avg_train_loss: 0.0205  avg_val_loss: 0.0857  time: 103s\n",
            "Epoch 4 - avg_train_loss: 0.0205  avg_val_loss: 0.0857  time: 103s\n",
            "Epoch 4 - avg_train_loss: 0.0205  avg_val_loss: 0.0857  time: 103s\n",
            "Epoch 4 - avg_train_loss: 0.0205  avg_val_loss: 0.0857  time: 103s\n",
            "Epoch 4 - avg_train_loss: 0.0205  avg_val_loss: 0.0857  time: 103s\n",
            "Epoch 4 - avg_train_loss: 0.0205  avg_val_loss: 0.0857  time: 103s\n",
            "Epoch 4 - avg_train_loss: 0.0205  avg_val_loss: 0.0857  time: 103s\n",
            "Epoch 4 - Score: 0.0856849680687316\n",
            "Epoch 4 - Score: 0.0856849680687316\n",
            "Epoch 4 - Score: 0.0856849680687316\n",
            "Epoch 4 - Score: 0.0856849680687316\n",
            "Epoch 4 - Score: 0.0856849680687316\n",
            "Epoch 4 - Score: 0.0856849680687316\n",
            "Epoch 4 - Score: 0.0856849680687316\n",
            "Epoch 4 - Score: 0.0856849680687316\n",
            "Epoch 4 - Score: 0.0856849680687316\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EVAL: [76/77] Elapsed 0m 8s (remain 0m 0s) Loss: 0.0857 \n",
            "Log loss for 3 is 0.0856849680687316\n",
            "Epoch: [5][0/306] Elapsed 0m 0s (remain 2m 52s) Loss: 0.0018 \n",
            "Epoch: [5][100/306] Elapsed 0m 31s (remain 1m 3s) Loss: 0.0117 \n",
            "Epoch: [5][200/306] Elapsed 1m 2s (remain 0m 32s) Loss: 0.0125 \n",
            "Epoch: [5][300/306] Elapsed 1m 33s (remain 0m 1s) Loss: 0.0152 \n",
            "Epoch: [5][305/306] Elapsed 1m 34s (remain 0m 0s) Loss: 0.0167 \n",
            "EVAL: [0/77] Elapsed 0m 0s (remain 0m 28s) Loss: 0.0008 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5 - avg_train_loss: 0.0167  avg_val_loss: 0.0977  time: 103s\n",
            "Epoch 5 - avg_train_loss: 0.0167  avg_val_loss: 0.0977  time: 103s\n",
            "Epoch 5 - avg_train_loss: 0.0167  avg_val_loss: 0.0977  time: 103s\n",
            "Epoch 5 - avg_train_loss: 0.0167  avg_val_loss: 0.0977  time: 103s\n",
            "Epoch 5 - avg_train_loss: 0.0167  avg_val_loss: 0.0977  time: 103s\n",
            "Epoch 5 - avg_train_loss: 0.0167  avg_val_loss: 0.0977  time: 103s\n",
            "Epoch 5 - avg_train_loss: 0.0167  avg_val_loss: 0.0977  time: 103s\n",
            "Epoch 5 - avg_train_loss: 0.0167  avg_val_loss: 0.0977  time: 103s\n",
            "Epoch 5 - avg_train_loss: 0.0167  avg_val_loss: 0.0977  time: 103s\n",
            "Epoch 5 - Score: 0.0976558279337622\n",
            "Epoch 5 - Score: 0.0976558279337622\n",
            "Epoch 5 - Score: 0.0976558279337622\n",
            "Epoch 5 - Score: 0.0976558279337622\n",
            "Epoch 5 - Score: 0.0976558279337622\n",
            "Epoch 5 - Score: 0.0976558279337622\n",
            "Epoch 5 - Score: 0.0976558279337622\n",
            "Epoch 5 - Score: 0.0976558279337622\n",
            "Epoch 5 - Score: 0.0976558279337622\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EVAL: [76/77] Elapsed 0m 8s (remain 0m 0s) Loss: 0.0977 \n",
            "Log loss for 4 is 0.0976558279337622\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "========== fold: 0 result ==========\n",
            "========== fold: 0 result ==========\n",
            "========== fold: 0 result ==========\n",
            "========== fold: 0 result ==========\n",
            "========== fold: 0 result ==========\n",
            "========== fold: 0 result ==========\n",
            "========== fold: 0 result ==========\n",
            "========== fold: 0 result ==========\n",
            "========== fold: 0 result ==========\n",
            "Score: 0.07617\n",
            "Score: 0.07617\n",
            "Score: 0.07617\n",
            "Score: 0.07617\n",
            "Score: 0.07617\n",
            "Score: 0.07617\n",
            "Score: 0.07617\n",
            "Score: 0.07617\n",
            "Score: 0.07617\n",
            "========== fold: 1 training ==========\n",
            "========== fold: 1 training ==========\n",
            "========== fold: 1 training ==========\n",
            "========== fold: 1 training ==========\n",
            "========== fold: 1 training ==========\n",
            "========== fold: 1 training ==========\n",
            "========== fold: 1 training ==========\n",
            "========== fold: 1 training ==========\n",
            "========== fold: 1 training ==========\n",
            "Didn't find file /content/activebus_bert_final/added_tokens.json. We won't load it.\n",
            "loading file /content/activebus_bert_final/vocab.txt\n",
            "loading file None\n",
            "loading file /content/activebus_bert_final/special_tokens_map.json\n",
            "loading file /content/activebus_bert_final/tokenizer_config.json\n",
            "loading file /content/activebus_bert_final/tokenizer.json\n",
            "Didn't find file /content/activebus_bert_final/added_tokens.json. We won't load it.\n",
            "loading file /content/activebus_bert_final/vocab.txt\n",
            "loading file None\n",
            "loading file /content/activebus_bert_final/special_tokens_map.json\n",
            "loading file /content/activebus_bert_final/tokenizer_config.json\n",
            "loading file /content/activebus_bert_final/tokenizer.json\n",
            "loading configuration file /content/activebus_bert_final/config.json\n",
            "Model config BertConfig {\n",
            "  \"_name_or_path\": \"activebus/BERT-XD_Review\",\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_ids\": 0,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.15.0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "loading weights file /content/activebus_bert_final/pytorch_model.bin\n",
            "Some weights of the model checkpoint at /content/activebus_bert_final were not used when initializing BertForSequenceClassification: ['cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /content/activebus_bert_final and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: [1][0/306] Elapsed 0m 0s (remain 2m 52s) Loss: 0.6594 \n",
            "Epoch: [1][100/306] Elapsed 0m 31s (remain 1m 3s) Loss: 0.2115 \n",
            "Epoch: [1][200/306] Elapsed 1m 2s (remain 0m 32s) Loss: 0.1494 \n",
            "Epoch: [1][300/306] Elapsed 1m 33s (remain 0m 1s) Loss: 0.1273 \n",
            "Epoch: [1][305/306] Elapsed 1m 34s (remain 0m 0s) Loss: 0.1285 \n",
            "EVAL: [0/77] Elapsed 0m 0s (remain 0m 26s) Loss: 0.0893 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1 - avg_train_loss: 0.1285  avg_val_loss: 0.0615  time: 103s\n",
            "Epoch 1 - avg_train_loss: 0.1285  avg_val_loss: 0.0615  time: 103s\n",
            "Epoch 1 - avg_train_loss: 0.1285  avg_val_loss: 0.0615  time: 103s\n",
            "Epoch 1 - avg_train_loss: 0.1285  avg_val_loss: 0.0615  time: 103s\n",
            "Epoch 1 - avg_train_loss: 0.1285  avg_val_loss: 0.0615  time: 103s\n",
            "Epoch 1 - avg_train_loss: 0.1285  avg_val_loss: 0.0615  time: 103s\n",
            "Epoch 1 - avg_train_loss: 0.1285  avg_val_loss: 0.0615  time: 103s\n",
            "Epoch 1 - avg_train_loss: 0.1285  avg_val_loss: 0.0615  time: 103s\n",
            "Epoch 1 - avg_train_loss: 0.1285  avg_val_loss: 0.0615  time: 103s\n",
            "Epoch 1 - Score: 0.06152612784423523\n",
            "Epoch 1 - Score: 0.06152612784423523\n",
            "Epoch 1 - Score: 0.06152612784423523\n",
            "Epoch 1 - Score: 0.06152612784423523\n",
            "Epoch 1 - Score: 0.06152612784423523\n",
            "Epoch 1 - Score: 0.06152612784423523\n",
            "Epoch 1 - Score: 0.06152612784423523\n",
            "Epoch 1 - Score: 0.06152612784423523\n",
            "Epoch 1 - Score: 0.06152612784423523\n",
            "Epoch 1 - Save Best Score: 0.0615 Model\n",
            "Epoch 1 - Save Best Score: 0.0615 Model\n",
            "Epoch 1 - Save Best Score: 0.0615 Model\n",
            "Epoch 1 - Save Best Score: 0.0615 Model\n",
            "Epoch 1 - Save Best Score: 0.0615 Model\n",
            "Epoch 1 - Save Best Score: 0.0615 Model\n",
            "Epoch 1 - Save Best Score: 0.0615 Model\n",
            "Epoch 1 - Save Best Score: 0.0615 Model\n",
            "Epoch 1 - Save Best Score: 0.0615 Model\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EVAL: [76/77] Elapsed 0m 8s (remain 0m 0s) Loss: 0.0615 \n",
            "Log loss for 0 is 0.06152612784423523\n",
            "Epoch: [2][0/306] Elapsed 0m 0s (remain 2m 55s) Loss: 0.0083 \n",
            "Epoch: [2][100/306] Elapsed 0m 31s (remain 1m 3s) Loss: 0.0527 \n",
            "Epoch: [2][200/306] Elapsed 1m 2s (remain 0m 32s) Loss: 0.0531 \n",
            "Epoch: [2][300/306] Elapsed 1m 33s (remain 0m 1s) Loss: 0.0569 \n",
            "Epoch: [2][305/306] Elapsed 1m 34s (remain 0m 0s) Loss: 0.0569 \n",
            "EVAL: [0/77] Elapsed 0m 0s (remain 0m 26s) Loss: 0.2021 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2 - avg_train_loss: 0.0569  avg_val_loss: 0.0699  time: 103s\n",
            "Epoch 2 - avg_train_loss: 0.0569  avg_val_loss: 0.0699  time: 103s\n",
            "Epoch 2 - avg_train_loss: 0.0569  avg_val_loss: 0.0699  time: 103s\n",
            "Epoch 2 - avg_train_loss: 0.0569  avg_val_loss: 0.0699  time: 103s\n",
            "Epoch 2 - avg_train_loss: 0.0569  avg_val_loss: 0.0699  time: 103s\n",
            "Epoch 2 - avg_train_loss: 0.0569  avg_val_loss: 0.0699  time: 103s\n",
            "Epoch 2 - avg_train_loss: 0.0569  avg_val_loss: 0.0699  time: 103s\n",
            "Epoch 2 - avg_train_loss: 0.0569  avg_val_loss: 0.0699  time: 103s\n",
            "Epoch 2 - avg_train_loss: 0.0569  avg_val_loss: 0.0699  time: 103s\n",
            "Epoch 2 - Score: 0.06993157639820584\n",
            "Epoch 2 - Score: 0.06993157639820584\n",
            "Epoch 2 - Score: 0.06993157639820584\n",
            "Epoch 2 - Score: 0.06993157639820584\n",
            "Epoch 2 - Score: 0.06993157639820584\n",
            "Epoch 2 - Score: 0.06993157639820584\n",
            "Epoch 2 - Score: 0.06993157639820584\n",
            "Epoch 2 - Score: 0.06993157639820584\n",
            "Epoch 2 - Score: 0.06993157639820584\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EVAL: [76/77] Elapsed 0m 8s (remain 0m 0s) Loss: 0.0699 \n",
            "Log loss for 1 is 0.06993157639820584\n",
            "Epoch: [3][0/306] Elapsed 0m 0s (remain 3m 0s) Loss: 0.0053 \n",
            "Epoch: [3][100/306] Elapsed 0m 31s (remain 1m 3s) Loss: 0.0233 \n",
            "Epoch: [3][200/306] Elapsed 1m 2s (remain 0m 32s) Loss: 0.0270 \n",
            "Epoch: [3][300/306] Elapsed 1m 33s (remain 0m 1s) Loss: 0.0270 \n",
            "Epoch: [3][305/306] Elapsed 1m 34s (remain 0m 0s) Loss: 0.0279 \n",
            "EVAL: [0/77] Elapsed 0m 0s (remain 0m 25s) Loss: 0.3433 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3 - avg_train_loss: 0.0279  avg_val_loss: 0.1604  time: 103s\n",
            "Epoch 3 - avg_train_loss: 0.0279  avg_val_loss: 0.1604  time: 103s\n",
            "Epoch 3 - avg_train_loss: 0.0279  avg_val_loss: 0.1604  time: 103s\n",
            "Epoch 3 - avg_train_loss: 0.0279  avg_val_loss: 0.1604  time: 103s\n",
            "Epoch 3 - avg_train_loss: 0.0279  avg_val_loss: 0.1604  time: 103s\n",
            "Epoch 3 - avg_train_loss: 0.0279  avg_val_loss: 0.1604  time: 103s\n",
            "Epoch 3 - avg_train_loss: 0.0279  avg_val_loss: 0.1604  time: 103s\n",
            "Epoch 3 - avg_train_loss: 0.0279  avg_val_loss: 0.1604  time: 103s\n",
            "Epoch 3 - avg_train_loss: 0.0279  avg_val_loss: 0.1604  time: 103s\n",
            "Epoch 3 - Score: 0.16035847616319893\n",
            "Epoch 3 - Score: 0.16035847616319893\n",
            "Epoch 3 - Score: 0.16035847616319893\n",
            "Epoch 3 - Score: 0.16035847616319893\n",
            "Epoch 3 - Score: 0.16035847616319893\n",
            "Epoch 3 - Score: 0.16035847616319893\n",
            "Epoch 3 - Score: 0.16035847616319893\n",
            "Epoch 3 - Score: 0.16035847616319893\n",
            "Epoch 3 - Score: 0.16035847616319893\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EVAL: [76/77] Elapsed 0m 8s (remain 0m 0s) Loss: 0.1604 \n",
            "Log loss for 2 is 0.16035847616319893\n",
            "Epoch: [4][0/306] Elapsed 0m 0s (remain 2m 53s) Loss: 0.0042 \n",
            "Epoch: [4][100/306] Elapsed 0m 31s (remain 1m 3s) Loss: 0.0288 \n",
            "Epoch: [4][200/306] Elapsed 1m 2s (remain 0m 32s) Loss: 0.0246 \n",
            "Epoch: [4][300/306] Elapsed 1m 33s (remain 0m 1s) Loss: 0.0251 \n",
            "Epoch: [4][305/306] Elapsed 1m 34s (remain 0m 0s) Loss: 0.0253 \n",
            "EVAL: [0/77] Elapsed 0m 0s (remain 0m 26s) Loss: 0.1796 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4 - avg_train_loss: 0.0253  avg_val_loss: 0.0826  time: 103s\n",
            "Epoch 4 - avg_train_loss: 0.0253  avg_val_loss: 0.0826  time: 103s\n",
            "Epoch 4 - avg_train_loss: 0.0253  avg_val_loss: 0.0826  time: 103s\n",
            "Epoch 4 - avg_train_loss: 0.0253  avg_val_loss: 0.0826  time: 103s\n",
            "Epoch 4 - avg_train_loss: 0.0253  avg_val_loss: 0.0826  time: 103s\n",
            "Epoch 4 - avg_train_loss: 0.0253  avg_val_loss: 0.0826  time: 103s\n",
            "Epoch 4 - avg_train_loss: 0.0253  avg_val_loss: 0.0826  time: 103s\n",
            "Epoch 4 - avg_train_loss: 0.0253  avg_val_loss: 0.0826  time: 103s\n",
            "Epoch 4 - avg_train_loss: 0.0253  avg_val_loss: 0.0826  time: 103s\n",
            "Epoch 4 - Score: 0.0825671245167605\n",
            "Epoch 4 - Score: 0.0825671245167605\n",
            "Epoch 4 - Score: 0.0825671245167605\n",
            "Epoch 4 - Score: 0.0825671245167605\n",
            "Epoch 4 - Score: 0.0825671245167605\n",
            "Epoch 4 - Score: 0.0825671245167605\n",
            "Epoch 4 - Score: 0.0825671245167605\n",
            "Epoch 4 - Score: 0.0825671245167605\n",
            "Epoch 4 - Score: 0.0825671245167605\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EVAL: [76/77] Elapsed 0m 8s (remain 0m 0s) Loss: 0.0826 \n",
            "Log loss for 3 is 0.0825671245167605\n",
            "Epoch: [5][0/306] Elapsed 0m 0s (remain 3m 12s) Loss: 0.0101 \n",
            "Epoch: [5][100/306] Elapsed 0m 31s (remain 1m 3s) Loss: 0.0126 \n",
            "Epoch: [5][200/306] Elapsed 1m 2s (remain 0m 32s) Loss: 0.0130 \n",
            "Epoch: [5][300/306] Elapsed 1m 33s (remain 0m 1s) Loss: 0.0147 \n",
            "Epoch: [5][305/306] Elapsed 1m 34s (remain 0m 0s) Loss: 0.0146 \n",
            "EVAL: [0/77] Elapsed 0m 0s (remain 0m 26s) Loss: 0.0849 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5 - avg_train_loss: 0.0146  avg_val_loss: 0.0886  time: 103s\n",
            "Epoch 5 - avg_train_loss: 0.0146  avg_val_loss: 0.0886  time: 103s\n",
            "Epoch 5 - avg_train_loss: 0.0146  avg_val_loss: 0.0886  time: 103s\n",
            "Epoch 5 - avg_train_loss: 0.0146  avg_val_loss: 0.0886  time: 103s\n",
            "Epoch 5 - avg_train_loss: 0.0146  avg_val_loss: 0.0886  time: 103s\n",
            "Epoch 5 - avg_train_loss: 0.0146  avg_val_loss: 0.0886  time: 103s\n",
            "Epoch 5 - avg_train_loss: 0.0146  avg_val_loss: 0.0886  time: 103s\n",
            "Epoch 5 - avg_train_loss: 0.0146  avg_val_loss: 0.0886  time: 103s\n",
            "Epoch 5 - avg_train_loss: 0.0146  avg_val_loss: 0.0886  time: 103s\n",
            "Epoch 5 - Score: 0.08855027821827331\n",
            "Epoch 5 - Score: 0.08855027821827331\n",
            "Epoch 5 - Score: 0.08855027821827331\n",
            "Epoch 5 - Score: 0.08855027821827331\n",
            "Epoch 5 - Score: 0.08855027821827331\n",
            "Epoch 5 - Score: 0.08855027821827331\n",
            "Epoch 5 - Score: 0.08855027821827331\n",
            "Epoch 5 - Score: 0.08855027821827331\n",
            "Epoch 5 - Score: 0.08855027821827331\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EVAL: [76/77] Elapsed 0m 8s (remain 0m 0s) Loss: 0.0886 \n",
            "Log loss for 4 is 0.08855027821827331\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "========== fold: 1 result ==========\n",
            "========== fold: 1 result ==========\n",
            "========== fold: 1 result ==========\n",
            "========== fold: 1 result ==========\n",
            "========== fold: 1 result ==========\n",
            "========== fold: 1 result ==========\n",
            "========== fold: 1 result ==========\n",
            "========== fold: 1 result ==========\n",
            "========== fold: 1 result ==========\n",
            "Score: 0.06153\n",
            "Score: 0.06153\n",
            "Score: 0.06153\n",
            "Score: 0.06153\n",
            "Score: 0.06153\n",
            "Score: 0.06153\n",
            "Score: 0.06153\n",
            "Score: 0.06153\n",
            "Score: 0.06153\n",
            "========== fold: 2 training ==========\n",
            "========== fold: 2 training ==========\n",
            "========== fold: 2 training ==========\n",
            "========== fold: 2 training ==========\n",
            "========== fold: 2 training ==========\n",
            "========== fold: 2 training ==========\n",
            "========== fold: 2 training ==========\n",
            "========== fold: 2 training ==========\n",
            "========== fold: 2 training ==========\n",
            "Didn't find file /content/activebus_bert_final/added_tokens.json. We won't load it.\n",
            "loading file /content/activebus_bert_final/vocab.txt\n",
            "loading file None\n",
            "loading file /content/activebus_bert_final/special_tokens_map.json\n",
            "loading file /content/activebus_bert_final/tokenizer_config.json\n",
            "loading file /content/activebus_bert_final/tokenizer.json\n",
            "Didn't find file /content/activebus_bert_final/added_tokens.json. We won't load it.\n",
            "loading file /content/activebus_bert_final/vocab.txt\n",
            "loading file None\n",
            "loading file /content/activebus_bert_final/special_tokens_map.json\n",
            "loading file /content/activebus_bert_final/tokenizer_config.json\n",
            "loading file /content/activebus_bert_final/tokenizer.json\n",
            "loading configuration file /content/activebus_bert_final/config.json\n",
            "Model config BertConfig {\n",
            "  \"_name_or_path\": \"activebus/BERT-XD_Review\",\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_ids\": 0,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.15.0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "loading weights file /content/activebus_bert_final/pytorch_model.bin\n",
            "Some weights of the model checkpoint at /content/activebus_bert_final were not used when initializing BertForSequenceClassification: ['cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /content/activebus_bert_final and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: [1][0/306] Elapsed 0m 0s (remain 3m 2s) Loss: 0.7098 \n",
            "Epoch: [1][100/306] Elapsed 0m 31s (remain 1m 3s) Loss: 0.2197 \n",
            "Epoch: [1][200/306] Elapsed 1m 2s (remain 0m 32s) Loss: 0.1613 \n",
            "Epoch: [1][300/306] Elapsed 1m 33s (remain 0m 1s) Loss: 0.1388 \n",
            "Epoch: [1][305/306] Elapsed 1m 34s (remain 0m 0s) Loss: 0.1385 \n",
            "EVAL: [0/77] Elapsed 0m 0s (remain 0m 26s) Loss: 0.0130 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1 - avg_train_loss: 0.1385  avg_val_loss: 0.0699  time: 103s\n",
            "Epoch 1 - avg_train_loss: 0.1385  avg_val_loss: 0.0699  time: 103s\n",
            "Epoch 1 - avg_train_loss: 0.1385  avg_val_loss: 0.0699  time: 103s\n",
            "Epoch 1 - avg_train_loss: 0.1385  avg_val_loss: 0.0699  time: 103s\n",
            "Epoch 1 - avg_train_loss: 0.1385  avg_val_loss: 0.0699  time: 103s\n",
            "Epoch 1 - avg_train_loss: 0.1385  avg_val_loss: 0.0699  time: 103s\n",
            "Epoch 1 - avg_train_loss: 0.1385  avg_val_loss: 0.0699  time: 103s\n",
            "Epoch 1 - avg_train_loss: 0.1385  avg_val_loss: 0.0699  time: 103s\n",
            "Epoch 1 - avg_train_loss: 0.1385  avg_val_loss: 0.0699  time: 103s\n",
            "Epoch 1 - Score: 0.06990683017987014\n",
            "Epoch 1 - Score: 0.06990683017987014\n",
            "Epoch 1 - Score: 0.06990683017987014\n",
            "Epoch 1 - Score: 0.06990683017987014\n",
            "Epoch 1 - Score: 0.06990683017987014\n",
            "Epoch 1 - Score: 0.06990683017987014\n",
            "Epoch 1 - Score: 0.06990683017987014\n",
            "Epoch 1 - Score: 0.06990683017987014\n",
            "Epoch 1 - Score: 0.06990683017987014\n",
            "Epoch 1 - Save Best Score: 0.0699 Model\n",
            "Epoch 1 - Save Best Score: 0.0699 Model\n",
            "Epoch 1 - Save Best Score: 0.0699 Model\n",
            "Epoch 1 - Save Best Score: 0.0699 Model\n",
            "Epoch 1 - Save Best Score: 0.0699 Model\n",
            "Epoch 1 - Save Best Score: 0.0699 Model\n",
            "Epoch 1 - Save Best Score: 0.0699 Model\n",
            "Epoch 1 - Save Best Score: 0.0699 Model\n",
            "Epoch 1 - Save Best Score: 0.0699 Model\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EVAL: [76/77] Elapsed 0m 8s (remain 0m 0s) Loss: 0.0699 \n",
            "Log loss for 0 is 0.06990683017987014\n",
            "Epoch: [2][0/306] Elapsed 0m 0s (remain 2m 57s) Loss: 0.0322 \n",
            "Epoch: [2][100/306] Elapsed 0m 31s (remain 1m 3s) Loss: 0.0723 \n",
            "Epoch: [2][200/306] Elapsed 1m 2s (remain 0m 32s) Loss: 0.0616 \n",
            "Epoch: [2][300/306] Elapsed 1m 33s (remain 0m 1s) Loss: 0.0628 \n",
            "Epoch: [2][305/306] Elapsed 1m 34s (remain 0m 0s) Loss: 0.0621 \n",
            "EVAL: [0/77] Elapsed 0m 0s (remain 0m 26s) Loss: 0.0096 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2 - avg_train_loss: 0.0621  avg_val_loss: 0.0506  time: 103s\n",
            "Epoch 2 - avg_train_loss: 0.0621  avg_val_loss: 0.0506  time: 103s\n",
            "Epoch 2 - avg_train_loss: 0.0621  avg_val_loss: 0.0506  time: 103s\n",
            "Epoch 2 - avg_train_loss: 0.0621  avg_val_loss: 0.0506  time: 103s\n",
            "Epoch 2 - avg_train_loss: 0.0621  avg_val_loss: 0.0506  time: 103s\n",
            "Epoch 2 - avg_train_loss: 0.0621  avg_val_loss: 0.0506  time: 103s\n",
            "Epoch 2 - avg_train_loss: 0.0621  avg_val_loss: 0.0506  time: 103s\n",
            "Epoch 2 - avg_train_loss: 0.0621  avg_val_loss: 0.0506  time: 103s\n",
            "Epoch 2 - avg_train_loss: 0.0621  avg_val_loss: 0.0506  time: 103s\n",
            "Epoch 2 - Score: 0.05064287068469931\n",
            "Epoch 2 - Score: 0.05064287068469931\n",
            "Epoch 2 - Score: 0.05064287068469931\n",
            "Epoch 2 - Score: 0.05064287068469931\n",
            "Epoch 2 - Score: 0.05064287068469931\n",
            "Epoch 2 - Score: 0.05064287068469931\n",
            "Epoch 2 - Score: 0.05064287068469931\n",
            "Epoch 2 - Score: 0.05064287068469931\n",
            "Epoch 2 - Score: 0.05064287068469931\n",
            "Epoch 2 - Save Best Score: 0.0506 Model\n",
            "Epoch 2 - Save Best Score: 0.0506 Model\n",
            "Epoch 2 - Save Best Score: 0.0506 Model\n",
            "Epoch 2 - Save Best Score: 0.0506 Model\n",
            "Epoch 2 - Save Best Score: 0.0506 Model\n",
            "Epoch 2 - Save Best Score: 0.0506 Model\n",
            "Epoch 2 - Save Best Score: 0.0506 Model\n",
            "Epoch 2 - Save Best Score: 0.0506 Model\n",
            "Epoch 2 - Save Best Score: 0.0506 Model\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EVAL: [76/77] Elapsed 0m 8s (remain 0m 0s) Loss: 0.0506 \n",
            "Log loss for 1 is 0.05064287068469931\n",
            "Epoch: [3][0/306] Elapsed 0m 0s (remain 2m 48s) Loss: 0.0484 \n",
            "Epoch: [3][100/306] Elapsed 0m 31s (remain 1m 3s) Loss: 0.0408 \n",
            "Epoch: [3][200/306] Elapsed 1m 2s (remain 0m 32s) Loss: 0.0310 \n",
            "Epoch: [3][300/306] Elapsed 1m 33s (remain 0m 1s) Loss: 0.0340 \n",
            "Epoch: [3][305/306] Elapsed 1m 34s (remain 0m 0s) Loss: 0.0345 \n",
            "EVAL: [0/77] Elapsed 0m 0s (remain 0m 26s) Loss: 0.0076 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3 - avg_train_loss: 0.0345  avg_val_loss: 0.0663  time: 103s\n",
            "Epoch 3 - avg_train_loss: 0.0345  avg_val_loss: 0.0663  time: 103s\n",
            "Epoch 3 - avg_train_loss: 0.0345  avg_val_loss: 0.0663  time: 103s\n",
            "Epoch 3 - avg_train_loss: 0.0345  avg_val_loss: 0.0663  time: 103s\n",
            "Epoch 3 - avg_train_loss: 0.0345  avg_val_loss: 0.0663  time: 103s\n",
            "Epoch 3 - avg_train_loss: 0.0345  avg_val_loss: 0.0663  time: 103s\n",
            "Epoch 3 - avg_train_loss: 0.0345  avg_val_loss: 0.0663  time: 103s\n",
            "Epoch 3 - avg_train_loss: 0.0345  avg_val_loss: 0.0663  time: 103s\n",
            "Epoch 3 - avg_train_loss: 0.0345  avg_val_loss: 0.0663  time: 103s\n",
            "Epoch 3 - Score: 0.06625015416430019\n",
            "Epoch 3 - Score: 0.06625015416430019\n",
            "Epoch 3 - Score: 0.06625015416430019\n",
            "Epoch 3 - Score: 0.06625015416430019\n",
            "Epoch 3 - Score: 0.06625015416430019\n",
            "Epoch 3 - Score: 0.06625015416430019\n",
            "Epoch 3 - Score: 0.06625015416430019\n",
            "Epoch 3 - Score: 0.06625015416430019\n",
            "Epoch 3 - Score: 0.06625015416430019\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EVAL: [76/77] Elapsed 0m 8s (remain 0m 0s) Loss: 0.0663 \n",
            "Log loss for 2 is 0.06625015416430019\n",
            "Epoch: [4][0/306] Elapsed 0m 0s (remain 3m 2s) Loss: 0.0034 \n",
            "Epoch: [4][100/306] Elapsed 0m 31s (remain 1m 3s) Loss: 0.0279 \n",
            "Epoch: [4][200/306] Elapsed 1m 2s (remain 0m 32s) Loss: 0.0184 \n",
            "Epoch: [4][300/306] Elapsed 1m 33s (remain 0m 1s) Loss: 0.0199 \n",
            "Epoch: [4][305/306] Elapsed 1m 34s (remain 0m 0s) Loss: 0.0196 \n",
            "EVAL: [0/77] Elapsed 0m 0s (remain 0m 27s) Loss: 0.0015 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4 - avg_train_loss: 0.0196  avg_val_loss: 0.0752  time: 103s\n",
            "Epoch 4 - avg_train_loss: 0.0196  avg_val_loss: 0.0752  time: 103s\n",
            "Epoch 4 - avg_train_loss: 0.0196  avg_val_loss: 0.0752  time: 103s\n",
            "Epoch 4 - avg_train_loss: 0.0196  avg_val_loss: 0.0752  time: 103s\n",
            "Epoch 4 - avg_train_loss: 0.0196  avg_val_loss: 0.0752  time: 103s\n",
            "Epoch 4 - avg_train_loss: 0.0196  avg_val_loss: 0.0752  time: 103s\n",
            "Epoch 4 - avg_train_loss: 0.0196  avg_val_loss: 0.0752  time: 103s\n",
            "Epoch 4 - avg_train_loss: 0.0196  avg_val_loss: 0.0752  time: 103s\n",
            "Epoch 4 - avg_train_loss: 0.0196  avg_val_loss: 0.0752  time: 103s\n",
            "Epoch 4 - Score: 0.07518264821820403\n",
            "Epoch 4 - Score: 0.07518264821820403\n",
            "Epoch 4 - Score: 0.07518264821820403\n",
            "Epoch 4 - Score: 0.07518264821820403\n",
            "Epoch 4 - Score: 0.07518264821820403\n",
            "Epoch 4 - Score: 0.07518264821820403\n",
            "Epoch 4 - Score: 0.07518264821820403\n",
            "Epoch 4 - Score: 0.07518264821820403\n",
            "Epoch 4 - Score: 0.07518264821820403\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EVAL: [76/77] Elapsed 0m 8s (remain 0m 0s) Loss: 0.0752 \n",
            "Log loss for 3 is 0.07518264821820403\n",
            "Epoch: [5][0/306] Elapsed 0m 0s (remain 3m 0s) Loss: 0.0011 \n",
            "Epoch: [5][100/306] Elapsed 0m 31s (remain 1m 3s) Loss: 0.0053 \n",
            "Epoch: [5][200/306] Elapsed 1m 2s (remain 0m 32s) Loss: 0.0091 \n",
            "Epoch: [5][300/306] Elapsed 1m 33s (remain 0m 1s) Loss: 0.0142 \n",
            "Epoch: [5][305/306] Elapsed 1m 34s (remain 0m 0s) Loss: 0.0143 \n",
            "EVAL: [0/77] Elapsed 0m 0s (remain 0m 27s) Loss: 0.0059 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5 - avg_train_loss: 0.0143  avg_val_loss: 0.0578  time: 103s\n",
            "Epoch 5 - avg_train_loss: 0.0143  avg_val_loss: 0.0578  time: 103s\n",
            "Epoch 5 - avg_train_loss: 0.0143  avg_val_loss: 0.0578  time: 103s\n",
            "Epoch 5 - avg_train_loss: 0.0143  avg_val_loss: 0.0578  time: 103s\n",
            "Epoch 5 - avg_train_loss: 0.0143  avg_val_loss: 0.0578  time: 103s\n",
            "Epoch 5 - avg_train_loss: 0.0143  avg_val_loss: 0.0578  time: 103s\n",
            "Epoch 5 - avg_train_loss: 0.0143  avg_val_loss: 0.0578  time: 103s\n",
            "Epoch 5 - avg_train_loss: 0.0143  avg_val_loss: 0.0578  time: 103s\n",
            "Epoch 5 - avg_train_loss: 0.0143  avg_val_loss: 0.0578  time: 103s\n",
            "Epoch 5 - Score: 0.05784644274250595\n",
            "Epoch 5 - Score: 0.05784644274250595\n",
            "Epoch 5 - Score: 0.05784644274250595\n",
            "Epoch 5 - Score: 0.05784644274250595\n",
            "Epoch 5 - Score: 0.05784644274250595\n",
            "Epoch 5 - Score: 0.05784644274250595\n",
            "Epoch 5 - Score: 0.05784644274250595\n",
            "Epoch 5 - Score: 0.05784644274250595\n",
            "Epoch 5 - Score: 0.05784644274250595\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EVAL: [76/77] Elapsed 0m 8s (remain 0m 0s) Loss: 0.0578 \n",
            "Log loss for 4 is 0.05784644274250595\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "========== fold: 2 result ==========\n",
            "========== fold: 2 result ==========\n",
            "========== fold: 2 result ==========\n",
            "========== fold: 2 result ==========\n",
            "========== fold: 2 result ==========\n",
            "========== fold: 2 result ==========\n",
            "========== fold: 2 result ==========\n",
            "========== fold: 2 result ==========\n",
            "========== fold: 2 result ==========\n",
            "Score: 0.05064\n",
            "Score: 0.05064\n",
            "Score: 0.05064\n",
            "Score: 0.05064\n",
            "Score: 0.05064\n",
            "Score: 0.05064\n",
            "Score: 0.05064\n",
            "Score: 0.05064\n",
            "Score: 0.05064\n",
            "========== fold: 3 training ==========\n",
            "========== fold: 3 training ==========\n",
            "========== fold: 3 training ==========\n",
            "========== fold: 3 training ==========\n",
            "========== fold: 3 training ==========\n",
            "========== fold: 3 training ==========\n",
            "========== fold: 3 training ==========\n",
            "========== fold: 3 training ==========\n",
            "========== fold: 3 training ==========\n",
            "Didn't find file /content/activebus_bert_final/added_tokens.json. We won't load it.\n",
            "loading file /content/activebus_bert_final/vocab.txt\n",
            "loading file None\n",
            "loading file /content/activebus_bert_final/special_tokens_map.json\n",
            "loading file /content/activebus_bert_final/tokenizer_config.json\n",
            "loading file /content/activebus_bert_final/tokenizer.json\n",
            "Didn't find file /content/activebus_bert_final/added_tokens.json. We won't load it.\n",
            "loading file /content/activebus_bert_final/vocab.txt\n",
            "loading file None\n",
            "loading file /content/activebus_bert_final/special_tokens_map.json\n",
            "loading file /content/activebus_bert_final/tokenizer_config.json\n",
            "loading file /content/activebus_bert_final/tokenizer.json\n",
            "loading configuration file /content/activebus_bert_final/config.json\n",
            "Model config BertConfig {\n",
            "  \"_name_or_path\": \"activebus/BERT-XD_Review\",\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_ids\": 0,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.15.0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "loading weights file /content/activebus_bert_final/pytorch_model.bin\n",
            "Some weights of the model checkpoint at /content/activebus_bert_final were not used when initializing BertForSequenceClassification: ['cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /content/activebus_bert_final and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: [1][0/306] Elapsed 0m 0s (remain 2m 58s) Loss: 0.7185 \n",
            "Epoch: [1][100/306] Elapsed 0m 31s (remain 1m 3s) Loss: 0.2134 \n",
            "Epoch: [1][200/306] Elapsed 1m 2s (remain 0m 32s) Loss: 0.1535 \n",
            "Epoch: [1][300/306] Elapsed 1m 33s (remain 0m 1s) Loss: 0.1299 \n",
            "Epoch: [1][305/306] Elapsed 1m 34s (remain 0m 0s) Loss: 0.1284 \n",
            "EVAL: [0/77] Elapsed 0m 0s (remain 0m 26s) Loss: 0.0539 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1 - avg_train_loss: 0.1284  avg_val_loss: 0.0974  time: 103s\n",
            "Epoch 1 - avg_train_loss: 0.1284  avg_val_loss: 0.0974  time: 103s\n",
            "Epoch 1 - avg_train_loss: 0.1284  avg_val_loss: 0.0974  time: 103s\n",
            "Epoch 1 - avg_train_loss: 0.1284  avg_val_loss: 0.0974  time: 103s\n",
            "Epoch 1 - avg_train_loss: 0.1284  avg_val_loss: 0.0974  time: 103s\n",
            "Epoch 1 - avg_train_loss: 0.1284  avg_val_loss: 0.0974  time: 103s\n",
            "Epoch 1 - avg_train_loss: 0.1284  avg_val_loss: 0.0974  time: 103s\n",
            "Epoch 1 - avg_train_loss: 0.1284  avg_val_loss: 0.0974  time: 103s\n",
            "Epoch 1 - avg_train_loss: 0.1284  avg_val_loss: 0.0974  time: 103s\n",
            "Epoch 1 - Score: 0.09736591145531338\n",
            "Epoch 1 - Score: 0.09736591145531338\n",
            "Epoch 1 - Score: 0.09736591145531338\n",
            "Epoch 1 - Score: 0.09736591145531338\n",
            "Epoch 1 - Score: 0.09736591145531338\n",
            "Epoch 1 - Score: 0.09736591145531338\n",
            "Epoch 1 - Score: 0.09736591145531338\n",
            "Epoch 1 - Score: 0.09736591145531338\n",
            "Epoch 1 - Score: 0.09736591145531338\n",
            "Epoch 1 - Save Best Score: 0.0974 Model\n",
            "Epoch 1 - Save Best Score: 0.0974 Model\n",
            "Epoch 1 - Save Best Score: 0.0974 Model\n",
            "Epoch 1 - Save Best Score: 0.0974 Model\n",
            "Epoch 1 - Save Best Score: 0.0974 Model\n",
            "Epoch 1 - Save Best Score: 0.0974 Model\n",
            "Epoch 1 - Save Best Score: 0.0974 Model\n",
            "Epoch 1 - Save Best Score: 0.0974 Model\n",
            "Epoch 1 - Save Best Score: 0.0974 Model\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EVAL: [76/77] Elapsed 0m 8s (remain 0m 0s) Loss: 0.0974 \n",
            "Log loss for 0 is 0.09736591145531338\n",
            "Epoch: [2][0/306] Elapsed 0m 0s (remain 2m 44s) Loss: 0.0210 \n",
            "Epoch: [2][100/306] Elapsed 0m 31s (remain 1m 3s) Loss: 0.0375 \n",
            "Epoch: [2][200/306] Elapsed 1m 2s (remain 0m 32s) Loss: 0.0521 \n",
            "Epoch: [2][300/306] Elapsed 1m 33s (remain 0m 1s) Loss: 0.0520 \n",
            "Epoch: [2][305/306] Elapsed 1m 34s (remain 0m 0s) Loss: 0.0524 \n",
            "EVAL: [0/77] Elapsed 0m 0s (remain 0m 26s) Loss: 0.0324 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2 - avg_train_loss: 0.0524  avg_val_loss: 0.0749  time: 103s\n",
            "Epoch 2 - avg_train_loss: 0.0524  avg_val_loss: 0.0749  time: 103s\n",
            "Epoch 2 - avg_train_loss: 0.0524  avg_val_loss: 0.0749  time: 103s\n",
            "Epoch 2 - avg_train_loss: 0.0524  avg_val_loss: 0.0749  time: 103s\n",
            "Epoch 2 - avg_train_loss: 0.0524  avg_val_loss: 0.0749  time: 103s\n",
            "Epoch 2 - avg_train_loss: 0.0524  avg_val_loss: 0.0749  time: 103s\n",
            "Epoch 2 - avg_train_loss: 0.0524  avg_val_loss: 0.0749  time: 103s\n",
            "Epoch 2 - avg_train_loss: 0.0524  avg_val_loss: 0.0749  time: 103s\n",
            "Epoch 2 - avg_train_loss: 0.0524  avg_val_loss: 0.0749  time: 103s\n",
            "Epoch 2 - Score: 0.07485361192348855\n",
            "Epoch 2 - Score: 0.07485361192348855\n",
            "Epoch 2 - Score: 0.07485361192348855\n",
            "Epoch 2 - Score: 0.07485361192348855\n",
            "Epoch 2 - Score: 0.07485361192348855\n",
            "Epoch 2 - Score: 0.07485361192348855\n",
            "Epoch 2 - Score: 0.07485361192348855\n",
            "Epoch 2 - Score: 0.07485361192348855\n",
            "Epoch 2 - Score: 0.07485361192348855\n",
            "Epoch 2 - Save Best Score: 0.0749 Model\n",
            "Epoch 2 - Save Best Score: 0.0749 Model\n",
            "Epoch 2 - Save Best Score: 0.0749 Model\n",
            "Epoch 2 - Save Best Score: 0.0749 Model\n",
            "Epoch 2 - Save Best Score: 0.0749 Model\n",
            "Epoch 2 - Save Best Score: 0.0749 Model\n",
            "Epoch 2 - Save Best Score: 0.0749 Model\n",
            "Epoch 2 - Save Best Score: 0.0749 Model\n",
            "Epoch 2 - Save Best Score: 0.0749 Model\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EVAL: [76/77] Elapsed 0m 8s (remain 0m 0s) Loss: 0.0749 \n",
            "Log loss for 1 is 0.07485361192348855\n",
            "Epoch: [3][0/306] Elapsed 0m 0s (remain 2m 47s) Loss: 0.0134 \n",
            "Epoch: [3][100/306] Elapsed 0m 31s (remain 1m 3s) Loss: 0.0202 \n",
            "Epoch: [3][200/306] Elapsed 1m 2s (remain 0m 32s) Loss: 0.0305 \n",
            "Epoch: [3][300/306] Elapsed 1m 33s (remain 0m 1s) Loss: 0.0325 \n",
            "Epoch: [3][305/306] Elapsed 1m 34s (remain 0m 0s) Loss: 0.0336 \n",
            "EVAL: [0/77] Elapsed 0m 0s (remain 0m 26s) Loss: 0.0110 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3 - avg_train_loss: 0.0336  avg_val_loss: 0.0821  time: 103s\n",
            "Epoch 3 - avg_train_loss: 0.0336  avg_val_loss: 0.0821  time: 103s\n",
            "Epoch 3 - avg_train_loss: 0.0336  avg_val_loss: 0.0821  time: 103s\n",
            "Epoch 3 - avg_train_loss: 0.0336  avg_val_loss: 0.0821  time: 103s\n",
            "Epoch 3 - avg_train_loss: 0.0336  avg_val_loss: 0.0821  time: 103s\n",
            "Epoch 3 - avg_train_loss: 0.0336  avg_val_loss: 0.0821  time: 103s\n",
            "Epoch 3 - avg_train_loss: 0.0336  avg_val_loss: 0.0821  time: 103s\n",
            "Epoch 3 - avg_train_loss: 0.0336  avg_val_loss: 0.0821  time: 103s\n",
            "Epoch 3 - avg_train_loss: 0.0336  avg_val_loss: 0.0821  time: 103s\n",
            "Epoch 3 - Score: 0.0820863125032211\n",
            "Epoch 3 - Score: 0.0820863125032211\n",
            "Epoch 3 - Score: 0.0820863125032211\n",
            "Epoch 3 - Score: 0.0820863125032211\n",
            "Epoch 3 - Score: 0.0820863125032211\n",
            "Epoch 3 - Score: 0.0820863125032211\n",
            "Epoch 3 - Score: 0.0820863125032211\n",
            "Epoch 3 - Score: 0.0820863125032211\n",
            "Epoch 3 - Score: 0.0820863125032211\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EVAL: [76/77] Elapsed 0m 8s (remain 0m 0s) Loss: 0.0821 \n",
            "Log loss for 2 is 0.0820863125032211\n",
            "Epoch: [4][0/306] Elapsed 0m 0s (remain 3m 2s) Loss: 0.0036 \n",
            "Epoch: [4][100/306] Elapsed 0m 31s (remain 1m 3s) Loss: 0.0148 \n",
            "Epoch: [4][200/306] Elapsed 1m 2s (remain 0m 32s) Loss: 0.0145 \n",
            "Epoch: [4][300/306] Elapsed 1m 33s (remain 0m 1s) Loss: 0.0174 \n",
            "Epoch: [4][305/306] Elapsed 1m 34s (remain 0m 0s) Loss: 0.0173 \n",
            "EVAL: [0/77] Elapsed 0m 0s (remain 0m 25s) Loss: 0.0090 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4 - avg_train_loss: 0.0173  avg_val_loss: 0.0985  time: 103s\n",
            "Epoch 4 - avg_train_loss: 0.0173  avg_val_loss: 0.0985  time: 103s\n",
            "Epoch 4 - avg_train_loss: 0.0173  avg_val_loss: 0.0985  time: 103s\n",
            "Epoch 4 - avg_train_loss: 0.0173  avg_val_loss: 0.0985  time: 103s\n",
            "Epoch 4 - avg_train_loss: 0.0173  avg_val_loss: 0.0985  time: 103s\n",
            "Epoch 4 - avg_train_loss: 0.0173  avg_val_loss: 0.0985  time: 103s\n",
            "Epoch 4 - avg_train_loss: 0.0173  avg_val_loss: 0.0985  time: 103s\n",
            "Epoch 4 - avg_train_loss: 0.0173  avg_val_loss: 0.0985  time: 103s\n",
            "Epoch 4 - avg_train_loss: 0.0173  avg_val_loss: 0.0985  time: 103s\n",
            "Epoch 4 - Score: 0.0985427453777428\n",
            "Epoch 4 - Score: 0.0985427453777428\n",
            "Epoch 4 - Score: 0.0985427453777428\n",
            "Epoch 4 - Score: 0.0985427453777428\n",
            "Epoch 4 - Score: 0.0985427453777428\n",
            "Epoch 4 - Score: 0.0985427453777428\n",
            "Epoch 4 - Score: 0.0985427453777428\n",
            "Epoch 4 - Score: 0.0985427453777428\n",
            "Epoch 4 - Score: 0.0985427453777428\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EVAL: [76/77] Elapsed 0m 8s (remain 0m 0s) Loss: 0.0985 \n",
            "Log loss for 3 is 0.0985427453777428\n",
            "Epoch: [5][0/306] Elapsed 0m 0s (remain 2m 56s) Loss: 0.0092 \n",
            "Epoch: [5][100/306] Elapsed 0m 31s (remain 1m 3s) Loss: 0.0086 \n",
            "Epoch: [5][200/306] Elapsed 1m 2s (remain 0m 32s) Loss: 0.0121 \n",
            "Epoch: [5][300/306] Elapsed 1m 33s (remain 0m 1s) Loss: 0.0139 \n",
            "Epoch: [5][305/306] Elapsed 1m 34s (remain 0m 0s) Loss: 0.0137 \n",
            "EVAL: [0/77] Elapsed 0m 0s (remain 0m 26s) Loss: 0.0026 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5 - avg_train_loss: 0.0137  avg_val_loss: 0.1097  time: 103s\n",
            "Epoch 5 - avg_train_loss: 0.0137  avg_val_loss: 0.1097  time: 103s\n",
            "Epoch 5 - avg_train_loss: 0.0137  avg_val_loss: 0.1097  time: 103s\n",
            "Epoch 5 - avg_train_loss: 0.0137  avg_val_loss: 0.1097  time: 103s\n",
            "Epoch 5 - avg_train_loss: 0.0137  avg_val_loss: 0.1097  time: 103s\n",
            "Epoch 5 - avg_train_loss: 0.0137  avg_val_loss: 0.1097  time: 103s\n",
            "Epoch 5 - avg_train_loss: 0.0137  avg_val_loss: 0.1097  time: 103s\n",
            "Epoch 5 - avg_train_loss: 0.0137  avg_val_loss: 0.1097  time: 103s\n",
            "Epoch 5 - avg_train_loss: 0.0137  avg_val_loss: 0.1097  time: 103s\n",
            "Epoch 5 - Score: 0.10973015160427164\n",
            "Epoch 5 - Score: 0.10973015160427164\n",
            "Epoch 5 - Score: 0.10973015160427164\n",
            "Epoch 5 - Score: 0.10973015160427164\n",
            "Epoch 5 - Score: 0.10973015160427164\n",
            "Epoch 5 - Score: 0.10973015160427164\n",
            "Epoch 5 - Score: 0.10973015160427164\n",
            "Epoch 5 - Score: 0.10973015160427164\n",
            "Epoch 5 - Score: 0.10973015160427164\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EVAL: [76/77] Elapsed 0m 8s (remain 0m 0s) Loss: 0.1097 \n",
            "Log loss for 4 is 0.10973015160427164\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "========== fold: 3 result ==========\n",
            "========== fold: 3 result ==========\n",
            "========== fold: 3 result ==========\n",
            "========== fold: 3 result ==========\n",
            "========== fold: 3 result ==========\n",
            "========== fold: 3 result ==========\n",
            "========== fold: 3 result ==========\n",
            "========== fold: 3 result ==========\n",
            "========== fold: 3 result ==========\n",
            "Score: 0.07485\n",
            "Score: 0.07485\n",
            "Score: 0.07485\n",
            "Score: 0.07485\n",
            "Score: 0.07485\n",
            "Score: 0.07485\n",
            "Score: 0.07485\n",
            "Score: 0.07485\n",
            "Score: 0.07485\n",
            "========== fold: 4 training ==========\n",
            "========== fold: 4 training ==========\n",
            "========== fold: 4 training ==========\n",
            "========== fold: 4 training ==========\n",
            "========== fold: 4 training ==========\n",
            "========== fold: 4 training ==========\n",
            "========== fold: 4 training ==========\n",
            "========== fold: 4 training ==========\n",
            "========== fold: 4 training ==========\n",
            "Didn't find file /content/activebus_bert_final/added_tokens.json. We won't load it.\n",
            "loading file /content/activebus_bert_final/vocab.txt\n",
            "loading file None\n",
            "loading file /content/activebus_bert_final/special_tokens_map.json\n",
            "loading file /content/activebus_bert_final/tokenizer_config.json\n",
            "loading file /content/activebus_bert_final/tokenizer.json\n",
            "Didn't find file /content/activebus_bert_final/added_tokens.json. We won't load it.\n",
            "loading file /content/activebus_bert_final/vocab.txt\n",
            "loading file None\n",
            "loading file /content/activebus_bert_final/special_tokens_map.json\n",
            "loading file /content/activebus_bert_final/tokenizer_config.json\n",
            "loading file /content/activebus_bert_final/tokenizer.json\n",
            "loading configuration file /content/activebus_bert_final/config.json\n",
            "Model config BertConfig {\n",
            "  \"_name_or_path\": \"activebus/BERT-XD_Review\",\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_ids\": 0,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.15.0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "loading weights file /content/activebus_bert_final/pytorch_model.bin\n",
            "Some weights of the model checkpoint at /content/activebus_bert_final were not used when initializing BertForSequenceClassification: ['cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /content/activebus_bert_final and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: [1][0/306] Elapsed 0m 0s (remain 2m 51s) Loss: 0.6615 \n",
            "Epoch: [1][100/306] Elapsed 0m 31s (remain 1m 3s) Loss: 0.2074 \n",
            "Epoch: [1][200/306] Elapsed 1m 2s (remain 0m 32s) Loss: 0.1520 \n",
            "Epoch: [1][300/306] Elapsed 1m 33s (remain 0m 1s) Loss: 0.1318 \n",
            "Epoch: [1][305/306] Elapsed 1m 34s (remain 0m 0s) Loss: 0.1320 \n",
            "EVAL: [0/77] Elapsed 0m 0s (remain 0m 26s) Loss: 0.2751 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1 - avg_train_loss: 0.1320  avg_val_loss: 0.0914  time: 103s\n",
            "Epoch 1 - avg_train_loss: 0.1320  avg_val_loss: 0.0914  time: 103s\n",
            "Epoch 1 - avg_train_loss: 0.1320  avg_val_loss: 0.0914  time: 103s\n",
            "Epoch 1 - avg_train_loss: 0.1320  avg_val_loss: 0.0914  time: 103s\n",
            "Epoch 1 - avg_train_loss: 0.1320  avg_val_loss: 0.0914  time: 103s\n",
            "Epoch 1 - avg_train_loss: 0.1320  avg_val_loss: 0.0914  time: 103s\n",
            "Epoch 1 - avg_train_loss: 0.1320  avg_val_loss: 0.0914  time: 103s\n",
            "Epoch 1 - avg_train_loss: 0.1320  avg_val_loss: 0.0914  time: 103s\n",
            "Epoch 1 - avg_train_loss: 0.1320  avg_val_loss: 0.0914  time: 103s\n",
            "Epoch 1 - Score: 0.09144457988023527\n",
            "Epoch 1 - Score: 0.09144457988023527\n",
            "Epoch 1 - Score: 0.09144457988023527\n",
            "Epoch 1 - Score: 0.09144457988023527\n",
            "Epoch 1 - Score: 0.09144457988023527\n",
            "Epoch 1 - Score: 0.09144457988023527\n",
            "Epoch 1 - Score: 0.09144457988023527\n",
            "Epoch 1 - Score: 0.09144457988023527\n",
            "Epoch 1 - Score: 0.09144457988023527\n",
            "Epoch 1 - Save Best Score: 0.0914 Model\n",
            "Epoch 1 - Save Best Score: 0.0914 Model\n",
            "Epoch 1 - Save Best Score: 0.0914 Model\n",
            "Epoch 1 - Save Best Score: 0.0914 Model\n",
            "Epoch 1 - Save Best Score: 0.0914 Model\n",
            "Epoch 1 - Save Best Score: 0.0914 Model\n",
            "Epoch 1 - Save Best Score: 0.0914 Model\n",
            "Epoch 1 - Save Best Score: 0.0914 Model\n",
            "Epoch 1 - Save Best Score: 0.0914 Model\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EVAL: [76/77] Elapsed 0m 8s (remain 0m 0s) Loss: 0.0914 \n",
            "Log loss for 0 is 0.09144457988023527\n",
            "Epoch: [2][0/306] Elapsed 0m 0s (remain 2m 55s) Loss: 0.0142 \n",
            "Epoch: [2][100/306] Elapsed 0m 31s (remain 1m 3s) Loss: 0.0691 \n",
            "Epoch: [2][200/306] Elapsed 1m 2s (remain 0m 32s) Loss: 0.0576 \n",
            "Epoch: [2][300/306] Elapsed 1m 33s (remain 0m 1s) Loss: 0.0564 \n",
            "Epoch: [2][305/306] Elapsed 1m 34s (remain 0m 0s) Loss: 0.0561 \n",
            "EVAL: [0/77] Elapsed 0m 0s (remain 0m 28s) Loss: 0.3056 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2 - avg_train_loss: 0.0561  avg_val_loss: 0.1063  time: 103s\n",
            "Epoch 2 - avg_train_loss: 0.0561  avg_val_loss: 0.1063  time: 103s\n",
            "Epoch 2 - avg_train_loss: 0.0561  avg_val_loss: 0.1063  time: 103s\n",
            "Epoch 2 - avg_train_loss: 0.0561  avg_val_loss: 0.1063  time: 103s\n",
            "Epoch 2 - avg_train_loss: 0.0561  avg_val_loss: 0.1063  time: 103s\n",
            "Epoch 2 - avg_train_loss: 0.0561  avg_val_loss: 0.1063  time: 103s\n",
            "Epoch 2 - avg_train_loss: 0.0561  avg_val_loss: 0.1063  time: 103s\n",
            "Epoch 2 - avg_train_loss: 0.0561  avg_val_loss: 0.1063  time: 103s\n",
            "Epoch 2 - avg_train_loss: 0.0561  avg_val_loss: 0.1063  time: 103s\n",
            "Epoch 2 - Score: 0.10632240525687822\n",
            "Epoch 2 - Score: 0.10632240525687822\n",
            "Epoch 2 - Score: 0.10632240525687822\n",
            "Epoch 2 - Score: 0.10632240525687822\n",
            "Epoch 2 - Score: 0.10632240525687822\n",
            "Epoch 2 - Score: 0.10632240525687822\n",
            "Epoch 2 - Score: 0.10632240525687822\n",
            "Epoch 2 - Score: 0.10632240525687822\n",
            "Epoch 2 - Score: 0.10632240525687822\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EVAL: [76/77] Elapsed 0m 8s (remain 0m 0s) Loss: 0.1063 \n",
            "Log loss for 1 is 0.10632240525687822\n",
            "Epoch: [3][0/306] Elapsed 0m 0s (remain 2m 53s) Loss: 0.5272 \n",
            "Epoch: [3][100/306] Elapsed 0m 31s (remain 1m 3s) Loss: 0.0432 \n",
            "Epoch: [3][200/306] Elapsed 1m 2s (remain 0m 32s) Loss: 0.0360 \n",
            "Epoch: [3][300/306] Elapsed 1m 33s (remain 0m 1s) Loss: 0.0301 \n",
            "Epoch: [3][305/306] Elapsed 1m 34s (remain 0m 0s) Loss: 0.0317 \n",
            "EVAL: [0/77] Elapsed 0m 0s (remain 0m 26s) Loss: 0.0923 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3 - avg_train_loss: 0.0317  avg_val_loss: 0.1080  time: 103s\n",
            "Epoch 3 - avg_train_loss: 0.0317  avg_val_loss: 0.1080  time: 103s\n",
            "Epoch 3 - avg_train_loss: 0.0317  avg_val_loss: 0.1080  time: 103s\n",
            "Epoch 3 - avg_train_loss: 0.0317  avg_val_loss: 0.1080  time: 103s\n",
            "Epoch 3 - avg_train_loss: 0.0317  avg_val_loss: 0.1080  time: 103s\n",
            "Epoch 3 - avg_train_loss: 0.0317  avg_val_loss: 0.1080  time: 103s\n",
            "Epoch 3 - avg_train_loss: 0.0317  avg_val_loss: 0.1080  time: 103s\n",
            "Epoch 3 - avg_train_loss: 0.0317  avg_val_loss: 0.1080  time: 103s\n",
            "Epoch 3 - avg_train_loss: 0.0317  avg_val_loss: 0.1080  time: 103s\n",
            "Epoch 3 - Score: 0.10797931664589544\n",
            "Epoch 3 - Score: 0.10797931664589544\n",
            "Epoch 3 - Score: 0.10797931664589544\n",
            "Epoch 3 - Score: 0.10797931664589544\n",
            "Epoch 3 - Score: 0.10797931664589544\n",
            "Epoch 3 - Score: 0.10797931664589544\n",
            "Epoch 3 - Score: 0.10797931664589544\n",
            "Epoch 3 - Score: 0.10797931664589544\n",
            "Epoch 3 - Score: 0.10797931664589544\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EVAL: [76/77] Elapsed 0m 8s (remain 0m 0s) Loss: 0.1080 \n",
            "Log loss for 2 is 0.10797931664589544\n",
            "Epoch: [4][0/306] Elapsed 0m 0s (remain 3m 4s) Loss: 0.0102 \n",
            "Epoch: [4][100/306] Elapsed 0m 31s (remain 1m 3s) Loss: 0.0188 \n",
            "Epoch: [4][200/306] Elapsed 1m 2s (remain 0m 32s) Loss: 0.0185 \n",
            "Epoch: [4][300/306] Elapsed 1m 33s (remain 0m 1s) Loss: 0.0207 \n",
            "Epoch: [4][305/306] Elapsed 1m 34s (remain 0m 0s) Loss: 0.0229 \n",
            "EVAL: [0/77] Elapsed 0m 0s (remain 0m 28s) Loss: 0.2670 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4 - avg_train_loss: 0.0229  avg_val_loss: 0.0942  time: 103s\n",
            "Epoch 4 - avg_train_loss: 0.0229  avg_val_loss: 0.0942  time: 103s\n",
            "Epoch 4 - avg_train_loss: 0.0229  avg_val_loss: 0.0942  time: 103s\n",
            "Epoch 4 - avg_train_loss: 0.0229  avg_val_loss: 0.0942  time: 103s\n",
            "Epoch 4 - avg_train_loss: 0.0229  avg_val_loss: 0.0942  time: 103s\n",
            "Epoch 4 - avg_train_loss: 0.0229  avg_val_loss: 0.0942  time: 103s\n",
            "Epoch 4 - avg_train_loss: 0.0229  avg_val_loss: 0.0942  time: 103s\n",
            "Epoch 4 - avg_train_loss: 0.0229  avg_val_loss: 0.0942  time: 103s\n",
            "Epoch 4 - avg_train_loss: 0.0229  avg_val_loss: 0.0942  time: 103s\n",
            "Epoch 4 - Score: 0.09416210613083592\n",
            "Epoch 4 - Score: 0.09416210613083592\n",
            "Epoch 4 - Score: 0.09416210613083592\n",
            "Epoch 4 - Score: 0.09416210613083592\n",
            "Epoch 4 - Score: 0.09416210613083592\n",
            "Epoch 4 - Score: 0.09416210613083592\n",
            "Epoch 4 - Score: 0.09416210613083592\n",
            "Epoch 4 - Score: 0.09416210613083592\n",
            "Epoch 4 - Score: 0.09416210613083592\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EVAL: [76/77] Elapsed 0m 8s (remain 0m 0s) Loss: 0.0942 \n",
            "Log loss for 3 is 0.09416210613083592\n",
            "Epoch: [5][0/306] Elapsed 0m 0s (remain 2m 59s) Loss: 0.0068 \n",
            "Epoch: [5][100/306] Elapsed 0m 31s (remain 1m 3s) Loss: 0.0120 \n",
            "Epoch: [5][200/306] Elapsed 1m 2s (remain 0m 32s) Loss: 0.0141 \n",
            "Epoch: [5][300/306] Elapsed 1m 33s (remain 0m 1s) Loss: 0.0174 \n",
            "Epoch: [5][305/306] Elapsed 1m 34s (remain 0m 0s) Loss: 0.0172 \n",
            "EVAL: [0/77] Elapsed 0m 0s (remain 0m 25s) Loss: 0.2026 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5 - avg_train_loss: 0.0172  avg_val_loss: 0.1254  time: 103s\n",
            "Epoch 5 - avg_train_loss: 0.0172  avg_val_loss: 0.1254  time: 103s\n",
            "Epoch 5 - avg_train_loss: 0.0172  avg_val_loss: 0.1254  time: 103s\n",
            "Epoch 5 - avg_train_loss: 0.0172  avg_val_loss: 0.1254  time: 103s\n",
            "Epoch 5 - avg_train_loss: 0.0172  avg_val_loss: 0.1254  time: 103s\n",
            "Epoch 5 - avg_train_loss: 0.0172  avg_val_loss: 0.1254  time: 103s\n",
            "Epoch 5 - avg_train_loss: 0.0172  avg_val_loss: 0.1254  time: 103s\n",
            "Epoch 5 - avg_train_loss: 0.0172  avg_val_loss: 0.1254  time: 103s\n",
            "Epoch 5 - avg_train_loss: 0.0172  avg_val_loss: 0.1254  time: 103s\n",
            "Epoch 5 - Score: 0.12538878357651004\n",
            "Epoch 5 - Score: 0.12538878357651004\n",
            "Epoch 5 - Score: 0.12538878357651004\n",
            "Epoch 5 - Score: 0.12538878357651004\n",
            "Epoch 5 - Score: 0.12538878357651004\n",
            "Epoch 5 - Score: 0.12538878357651004\n",
            "Epoch 5 - Score: 0.12538878357651004\n",
            "Epoch 5 - Score: 0.12538878357651004\n",
            "Epoch 5 - Score: 0.12538878357651004\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EVAL: [76/77] Elapsed 0m 8s (remain 0m 0s) Loss: 0.1254 \n",
            "Log loss for 4 is 0.12538878357651004\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "========== fold: 4 result ==========\n",
            "========== fold: 4 result ==========\n",
            "========== fold: 4 result ==========\n",
            "========== fold: 4 result ==========\n",
            "========== fold: 4 result ==========\n",
            "========== fold: 4 result ==========\n",
            "========== fold: 4 result ==========\n",
            "========== fold: 4 result ==========\n",
            "========== fold: 4 result ==========\n",
            "Score: 0.09144\n",
            "Score: 0.09144\n",
            "Score: 0.09144\n",
            "Score: 0.09144\n",
            "Score: 0.09144\n",
            "Score: 0.09144\n",
            "Score: 0.09144\n",
            "Score: 0.09144\n",
            "Score: 0.09144\n",
            "========== CV ==========\n",
            "========== CV ==========\n",
            "========== CV ==========\n",
            "========== CV ==========\n",
            "========== CV ==========\n",
            "========== CV ==========\n",
            "========== CV ==========\n",
            "========== CV ==========\n",
            "========== CV ==========\n",
            "Score: 0.07093\n",
            "Score: 0.07093\n",
            "Score: 0.07093\n",
            "Score: 0.07093\n",
            "Score: 0.07093\n",
            "Score: 0.07093\n",
            "Score: 0.07093\n",
            "Score: 0.07093\n",
            "Score: 0.07093\n",
            "Didn't find file /content/activebus_bert_final/added_tokens.json. We won't load it.\n",
            "loading file /content/activebus_bert_final/vocab.txt\n",
            "loading file None\n",
            "loading file /content/activebus_bert_final/special_tokens_map.json\n",
            "loading file /content/activebus_bert_final/tokenizer_config.json\n",
            "loading file /content/activebus_bert_final/tokenizer.json\n",
            "========== model: bert-base-uncased fold: 0 inference ==========\n",
            "========== model: bert-base-uncased fold: 0 inference ==========\n",
            "========== model: bert-base-uncased fold: 0 inference ==========\n",
            "========== model: bert-base-uncased fold: 0 inference ==========\n",
            "========== model: bert-base-uncased fold: 0 inference ==========\n",
            "========== model: bert-base-uncased fold: 0 inference ==========\n",
            "========== model: bert-base-uncased fold: 0 inference ==========\n",
            "========== model: bert-base-uncased fold: 0 inference ==========\n",
            "========== model: bert-base-uncased fold: 0 inference ==========\n",
            "loading configuration file /content/activebus_bert_final/config.json\n",
            "Model config BertConfig {\n",
            "  \"_name_or_path\": \"activebus/BERT-XD_Review\",\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_ids\": 0,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.15.0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "loading weights file /content/activebus_bert_final/pytorch_model.bin\n",
            "Some weights of the model checkpoint at /content/activebus_bert_final were not used when initializing BertForSequenceClassification: ['cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /content/activebus_bert_final and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "\n",
            "  0%|          | 0/165 [00:00<?, ?it/s]\u001b[A\n",
            "  1%|          | 1/165 [00:00<00:20,  8.03it/s]\u001b[A\n",
            "  1%|          | 2/165 [00:00<00:19,  8.44it/s]\u001b[A\n",
            "  2%|▏         | 3/165 [00:00<00:18,  8.86it/s]\u001b[A\n",
            "  2%|▏         | 4/165 [00:00<00:17,  9.07it/s]\u001b[A\n",
            "  3%|▎         | 5/165 [00:00<00:17,  9.20it/s]\u001b[A\n",
            "  4%|▎         | 6/165 [00:00<00:17,  9.27it/s]\u001b[A\n",
            "  4%|▍         | 7/165 [00:00<00:16,  9.32it/s]\u001b[A\n",
            "  5%|▍         | 8/165 [00:00<00:16,  9.33it/s]\u001b[A\n",
            "  5%|▌         | 9/165 [00:00<00:16,  9.33it/s]\u001b[A\n",
            "  6%|▌         | 10/165 [00:01<00:16,  9.32it/s]\u001b[A\n",
            "  7%|▋         | 11/165 [00:01<00:16,  9.34it/s]\u001b[A\n",
            "  7%|▋         | 12/165 [00:01<00:16,  9.37it/s]\u001b[A\n",
            "  8%|▊         | 13/165 [00:01<00:16,  9.37it/s]\u001b[A\n",
            "  8%|▊         | 14/165 [00:01<00:16,  9.37it/s]\u001b[A\n",
            "  9%|▉         | 15/165 [00:01<00:15,  9.38it/s]\u001b[A\n",
            " 10%|▉         | 16/165 [00:01<00:15,  9.39it/s]\u001b[A\n",
            " 10%|█         | 17/165 [00:01<00:15,  9.39it/s]\u001b[A\n",
            " 11%|█         | 18/165 [00:01<00:15,  9.35it/s]\u001b[A\n",
            " 12%|█▏        | 19/165 [00:02<00:15,  9.38it/s]\u001b[A\n",
            " 12%|█▏        | 20/165 [00:02<00:15,  9.39it/s]\u001b[A\n",
            " 13%|█▎        | 21/165 [00:02<00:15,  9.40it/s]\u001b[A\n",
            " 13%|█▎        | 22/165 [00:02<00:15,  9.40it/s]\u001b[A\n",
            " 14%|█▍        | 23/165 [00:02<00:15,  9.40it/s]\u001b[A\n",
            " 15%|█▍        | 24/165 [00:02<00:15,  9.39it/s]\u001b[A\n",
            " 15%|█▌        | 25/165 [00:02<00:14,  9.38it/s]\u001b[A\n",
            " 16%|█▌        | 26/165 [00:02<00:14,  9.38it/s]\u001b[A\n",
            " 16%|█▋        | 27/165 [00:02<00:14,  9.38it/s]\u001b[A\n",
            " 17%|█▋        | 28/165 [00:03<00:14,  9.36it/s]\u001b[A\n",
            " 18%|█▊        | 29/165 [00:03<00:14,  9.38it/s]\u001b[A\n",
            " 18%|█▊        | 30/165 [00:03<00:14,  9.39it/s]\u001b[A\n",
            " 19%|█▉        | 31/165 [00:03<00:14,  9.40it/s]\u001b[A\n",
            " 19%|█▉        | 32/165 [00:03<00:14,  9.38it/s]\u001b[A\n",
            " 20%|██        | 33/165 [00:03<00:14,  9.38it/s]\u001b[A\n",
            " 21%|██        | 34/165 [00:03<00:13,  9.39it/s]\u001b[A\n",
            " 21%|██        | 35/165 [00:03<00:13,  9.38it/s]\u001b[A\n",
            " 22%|██▏       | 36/165 [00:03<00:13,  9.38it/s]\u001b[A\n",
            " 22%|██▏       | 37/165 [00:03<00:13,  9.38it/s]\u001b[A\n",
            " 23%|██▎       | 38/165 [00:04<00:13,  9.38it/s]\u001b[A\n",
            " 24%|██▎       | 39/165 [00:04<00:13,  9.38it/s]\u001b[A\n",
            " 24%|██▍       | 40/165 [00:04<00:13,  9.39it/s]\u001b[A\n",
            " 25%|██▍       | 41/165 [00:04<00:13,  9.40it/s]\u001b[A\n",
            " 25%|██▌       | 42/165 [00:04<00:13,  9.43it/s]\u001b[A\n",
            " 26%|██▌       | 43/165 [00:04<00:12,  9.43it/s]\u001b[A\n",
            " 27%|██▋       | 44/165 [00:04<00:12,  9.42it/s]\u001b[A\n",
            " 27%|██▋       | 45/165 [00:04<00:12,  9.41it/s]\u001b[A\n",
            " 28%|██▊       | 46/165 [00:04<00:12,  9.42it/s]\u001b[A\n",
            " 28%|██▊       | 47/165 [00:05<00:12,  9.41it/s]\u001b[A\n",
            " 29%|██▉       | 48/165 [00:05<00:12,  9.40it/s]\u001b[A\n",
            " 30%|██▉       | 49/165 [00:05<00:12,  9.40it/s]\u001b[A\n",
            " 30%|███       | 50/165 [00:05<00:12,  9.41it/s]\u001b[A\n",
            " 31%|███       | 51/165 [00:05<00:12,  9.43it/s]\u001b[A\n",
            " 32%|███▏      | 52/165 [00:05<00:11,  9.42it/s]\u001b[A\n",
            " 32%|███▏      | 53/165 [00:05<00:11,  9.40it/s]\u001b[A\n",
            " 33%|███▎      | 54/165 [00:05<00:11,  9.41it/s]\u001b[A\n",
            " 33%|███▎      | 55/165 [00:05<00:11,  9.41it/s]\u001b[A\n",
            " 34%|███▍      | 56/165 [00:05<00:11,  9.41it/s]\u001b[A\n",
            " 35%|███▍      | 57/165 [00:06<00:11,  9.42it/s]\u001b[A\n",
            " 35%|███▌      | 58/165 [00:06<00:11,  9.44it/s]\u001b[A\n",
            " 36%|███▌      | 59/165 [00:06<00:11,  9.42it/s]\u001b[A\n",
            " 36%|███▋      | 60/165 [00:06<00:11,  9.40it/s]\u001b[A\n",
            " 37%|███▋      | 61/165 [00:06<00:11,  9.41it/s]\u001b[A\n",
            " 38%|███▊      | 62/165 [00:06<00:10,  9.41it/s]\u001b[A\n",
            " 38%|███▊      | 63/165 [00:06<00:10,  9.42it/s]\u001b[A\n",
            " 39%|███▉      | 64/165 [00:06<00:10,  9.39it/s]\u001b[A\n",
            " 39%|███▉      | 65/165 [00:06<00:10,  9.39it/s]\u001b[A\n",
            " 40%|████      | 66/165 [00:07<00:10,  9.40it/s]\u001b[A\n",
            " 41%|████      | 67/165 [00:07<00:10,  9.41it/s]\u001b[A\n",
            " 41%|████      | 68/165 [00:07<00:10,  9.41it/s]\u001b[A\n",
            " 42%|████▏     | 69/165 [00:07<00:10,  9.42it/s]\u001b[A\n",
            " 42%|████▏     | 70/165 [00:07<00:10,  9.42it/s]\u001b[A\n",
            " 43%|████▎     | 71/165 [00:07<00:09,  9.42it/s]\u001b[A\n",
            " 44%|████▎     | 72/165 [00:07<00:09,  9.42it/s]\u001b[A\n",
            " 44%|████▍     | 73/165 [00:07<00:09,  9.40it/s]\u001b[A\n",
            " 45%|████▍     | 74/165 [00:07<00:09,  9.40it/s]\u001b[A\n",
            " 45%|████▌     | 75/165 [00:08<00:09,  9.41it/s]\u001b[A\n",
            " 46%|████▌     | 76/165 [00:08<00:09,  9.42it/s]\u001b[A\n",
            " 47%|████▋     | 77/165 [00:08<00:09,  9.42it/s]\u001b[A\n",
            " 47%|████▋     | 78/165 [00:08<00:09,  9.41it/s]\u001b[A\n",
            " 48%|████▊     | 79/165 [00:08<00:09,  9.41it/s]\u001b[A\n",
            " 48%|████▊     | 80/165 [00:08<00:09,  9.41it/s]\u001b[A\n",
            " 49%|████▉     | 81/165 [00:08<00:08,  9.41it/s]\u001b[A\n",
            " 50%|████▉     | 82/165 [00:08<00:08,  9.41it/s]\u001b[A\n",
            " 50%|█████     | 83/165 [00:08<00:08,  9.41it/s]\u001b[A\n",
            " 51%|█████     | 84/165 [00:08<00:08,  9.40it/s]\u001b[A\n",
            " 52%|█████▏    | 85/165 [00:09<00:08,  9.40it/s]\u001b[A\n",
            " 52%|█████▏    | 86/165 [00:09<00:08,  9.40it/s]\u001b[A\n",
            " 53%|█████▎    | 87/165 [00:09<00:08,  9.40it/s]\u001b[A\n",
            " 53%|█████▎    | 88/165 [00:09<00:08,  9.38it/s]\u001b[A\n",
            " 54%|█████▍    | 89/165 [00:09<00:08,  9.38it/s]\u001b[A\n",
            " 55%|█████▍    | 90/165 [00:09<00:07,  9.40it/s]\u001b[A\n",
            " 55%|█████▌    | 91/165 [00:09<00:07,  9.40it/s]\u001b[A\n",
            " 56%|█████▌    | 92/165 [00:09<00:07,  9.40it/s]\u001b[A\n",
            " 56%|█████▋    | 93/165 [00:09<00:07,  9.41it/s]\u001b[A\n",
            " 57%|█████▋    | 94/165 [00:10<00:07,  9.40it/s]\u001b[A\n",
            " 58%|█████▊    | 95/165 [00:10<00:07,  9.41it/s]\u001b[A\n",
            " 58%|█████▊    | 96/165 [00:10<00:07,  9.40it/s]\u001b[A\n",
            " 59%|█████▉    | 97/165 [00:10<00:07,  9.44it/s]\u001b[A\n",
            " 59%|█████▉    | 98/165 [00:10<00:07,  9.43it/s]\u001b[A\n",
            " 60%|██████    | 99/165 [00:10<00:06,  9.43it/s]\u001b[A\n",
            " 61%|██████    | 100/165 [00:10<00:06,  9.42it/s]\u001b[A\n",
            " 61%|██████    | 101/165 [00:10<00:06,  9.39it/s]\u001b[A\n",
            " 62%|██████▏   | 102/165 [00:10<00:06,  9.39it/s]\u001b[A\n",
            " 62%|██████▏   | 103/165 [00:10<00:06,  9.39it/s]\u001b[A\n",
            " 63%|██████▎   | 104/165 [00:11<00:06,  9.40it/s]\u001b[A\n",
            " 64%|██████▎   | 105/165 [00:11<00:06,  9.40it/s]\u001b[A\n",
            " 64%|██████▍   | 106/165 [00:11<00:06,  9.40it/s]\u001b[A\n",
            " 65%|██████▍   | 107/165 [00:11<00:06,  9.39it/s]\u001b[A\n",
            " 65%|██████▌   | 108/165 [00:11<00:06,  9.40it/s]\u001b[A\n",
            " 66%|██████▌   | 109/165 [00:11<00:05,  9.39it/s]\u001b[A\n",
            " 67%|██████▋   | 110/165 [00:11<00:05,  9.37it/s]\u001b[A\n",
            " 67%|██████▋   | 111/165 [00:11<00:05,  9.36it/s]\u001b[A\n",
            " 68%|██████▊   | 112/165 [00:11<00:05,  9.33it/s]\u001b[A\n",
            " 68%|██████▊   | 113/165 [00:12<00:05,  9.33it/s]\u001b[A\n",
            " 69%|██████▉   | 114/165 [00:12<00:05,  9.32it/s]\u001b[A\n",
            " 70%|██████▉   | 115/165 [00:12<00:05,  9.32it/s]\u001b[A\n",
            " 70%|███████   | 116/165 [00:12<00:05,  9.32it/s]\u001b[A\n",
            " 71%|███████   | 117/165 [00:12<00:05,  9.33it/s]\u001b[A\n",
            " 72%|███████▏  | 118/165 [00:12<00:05,  9.34it/s]\u001b[A\n",
            " 72%|███████▏  | 119/165 [00:12<00:04,  9.33it/s]\u001b[A\n",
            " 73%|███████▎  | 120/165 [00:12<00:04,  9.36it/s]\u001b[A\n",
            " 73%|███████▎  | 121/165 [00:12<00:04,  9.36it/s]\u001b[A\n",
            " 74%|███████▍  | 122/165 [00:13<00:04,  9.38it/s]\u001b[A\n",
            " 75%|███████▍  | 123/165 [00:13<00:04,  9.40it/s]\u001b[A\n",
            " 75%|███████▌  | 124/165 [00:13<00:04,  9.39it/s]\u001b[A\n",
            " 76%|███████▌  | 125/165 [00:13<00:04,  9.38it/s]\u001b[A\n",
            " 76%|███████▋  | 126/165 [00:13<00:04,  9.31it/s]\u001b[A\n",
            " 77%|███████▋  | 127/165 [00:13<00:04,  9.36it/s]\u001b[A\n",
            " 78%|███████▊  | 128/165 [00:13<00:03,  9.39it/s]\u001b[A\n",
            " 78%|███████▊  | 129/165 [00:13<00:03,  9.41it/s]\u001b[A\n",
            " 79%|███████▉  | 130/165 [00:13<00:03,  9.39it/s]\u001b[A\n",
            " 79%|███████▉  | 131/165 [00:13<00:03,  9.39it/s]\u001b[A\n",
            " 80%|████████  | 132/165 [00:14<00:03,  9.39it/s]\u001b[A\n",
            " 81%|████████  | 133/165 [00:14<00:03,  9.40it/s]\u001b[A\n",
            " 81%|████████  | 134/165 [00:14<00:03,  9.41it/s]\u001b[A\n",
            " 82%|████████▏ | 135/165 [00:14<00:03,  9.41it/s]\u001b[A\n",
            " 82%|████████▏ | 136/165 [00:14<00:03,  9.42it/s]\u001b[A\n",
            " 83%|████████▎ | 137/165 [00:14<00:02,  9.42it/s]\u001b[A\n",
            " 84%|████████▎ | 138/165 [00:14<00:02,  9.41it/s]\u001b[A\n",
            " 84%|████████▍ | 139/165 [00:14<00:02,  9.40it/s]\u001b[A\n",
            " 85%|████████▍ | 140/165 [00:14<00:02,  9.39it/s]\u001b[A\n",
            " 85%|████████▌ | 141/165 [00:15<00:02,  9.39it/s]\u001b[A\n",
            " 86%|████████▌ | 142/165 [00:15<00:02,  9.39it/s]\u001b[A\n",
            " 87%|████████▋ | 143/165 [00:15<00:02,  9.38it/s]\u001b[A\n",
            " 87%|████████▋ | 144/165 [00:15<00:02,  9.39it/s]\u001b[A\n",
            " 88%|████████▊ | 145/165 [00:15<00:02,  9.40it/s]\u001b[A\n",
            " 88%|████████▊ | 146/165 [00:15<00:02,  9.41it/s]\u001b[A\n",
            " 89%|████████▉ | 147/165 [00:15<00:01,  9.41it/s]\u001b[A\n",
            " 90%|████████▉ | 148/165 [00:15<00:01,  9.42it/s]\u001b[A\n",
            " 90%|█████████ | 149/165 [00:15<00:01,  9.39it/s]\u001b[A\n",
            " 91%|█████████ | 150/165 [00:15<00:01,  9.39it/s]\u001b[A\n",
            " 92%|█████████▏| 151/165 [00:16<00:01,  9.40it/s]\u001b[A\n",
            " 92%|█████████▏| 152/165 [00:16<00:01,  9.42it/s]\u001b[A\n",
            " 93%|█████████▎| 153/165 [00:16<00:01,  9.42it/s]\u001b[A\n",
            " 93%|█████████▎| 154/165 [00:16<00:01,  9.40it/s]\u001b[A\n",
            " 94%|█████████▍| 155/165 [00:16<00:01,  9.41it/s]\u001b[A\n",
            " 95%|█████████▍| 156/165 [00:16<00:00,  9.40it/s]\u001b[A\n",
            " 95%|█████████▌| 157/165 [00:16<00:00,  9.36it/s]\u001b[A\n",
            " 96%|█████████▌| 158/165 [00:16<00:00,  9.37it/s]\u001b[A\n",
            " 96%|█████████▋| 159/165 [00:16<00:00,  9.39it/s]\u001b[A\n",
            " 97%|█████████▋| 160/165 [00:17<00:00,  9.41it/s]\u001b[A\n",
            " 98%|█████████▊| 161/165 [00:17<00:00,  9.43it/s]\u001b[A\n",
            " 98%|█████████▊| 162/165 [00:17<00:00,  9.44it/s]\u001b[A\n",
            " 99%|█████████▉| 163/165 [00:17<00:00,  9.43it/s]\u001b[A\n",
            "100%|██████████| 165/165 [00:17<00:00,  9.37it/s]\n",
            "========== model: bert-base-uncased fold: 1 inference ==========\n",
            "========== model: bert-base-uncased fold: 1 inference ==========\n",
            "========== model: bert-base-uncased fold: 1 inference ==========\n",
            "========== model: bert-base-uncased fold: 1 inference ==========\n",
            "========== model: bert-base-uncased fold: 1 inference ==========\n",
            "========== model: bert-base-uncased fold: 1 inference ==========\n",
            "========== model: bert-base-uncased fold: 1 inference ==========\n",
            "========== model: bert-base-uncased fold: 1 inference ==========\n",
            "========== model: bert-base-uncased fold: 1 inference ==========\n",
            "loading configuration file /content/activebus_bert_final/config.json\n",
            "Model config BertConfig {\n",
            "  \"_name_or_path\": \"activebus/BERT-XD_Review\",\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_ids\": 0,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.15.0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "loading weights file /content/activebus_bert_final/pytorch_model.bin\n",
            "Some weights of the model checkpoint at /content/activebus_bert_final were not used when initializing BertForSequenceClassification: ['cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /content/activebus_bert_final and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "\n",
            "  0%|          | 0/165 [00:00<?, ?it/s]\u001b[A\n",
            "  1%|          | 1/165 [00:00<00:20,  8.17it/s]\u001b[A\n",
            "  1%|          | 2/165 [00:00<00:18,  8.83it/s]\u001b[A\n",
            "  2%|▏         | 3/165 [00:00<00:17,  9.06it/s]\u001b[A\n",
            "  2%|▏         | 4/165 [00:00<00:17,  9.19it/s]\u001b[A\n",
            "  3%|▎         | 5/165 [00:00<00:17,  9.25it/s]\u001b[A\n",
            "  4%|▎         | 6/165 [00:00<00:17,  9.30it/s]\u001b[A\n",
            "  4%|▍         | 7/165 [00:00<00:16,  9.34it/s]\u001b[A\n",
            "  5%|▍         | 8/165 [00:00<00:16,  9.36it/s]\u001b[A\n",
            "  5%|▌         | 9/165 [00:00<00:16,  9.35it/s]\u001b[A\n",
            "  6%|▌         | 10/165 [00:01<00:16,  9.35it/s]\u001b[A\n",
            "  7%|▋         | 11/165 [00:01<00:16,  9.36it/s]\u001b[A\n",
            "  7%|▋         | 12/165 [00:01<00:16,  9.39it/s]\u001b[A\n",
            "  8%|▊         | 13/165 [00:01<00:16,  9.39it/s]\u001b[A\n",
            "  8%|▊         | 14/165 [00:01<00:16,  9.38it/s]\u001b[A\n",
            "  9%|▉         | 15/165 [00:01<00:15,  9.39it/s]\u001b[A\n",
            " 10%|▉         | 16/165 [00:01<00:15,  9.41it/s]\u001b[A\n",
            " 10%|█         | 17/165 [00:01<00:15,  9.39it/s]\u001b[A\n",
            " 11%|█         | 18/165 [00:01<00:15,  9.37it/s]\u001b[A\n",
            " 12%|█▏        | 19/165 [00:02<00:15,  9.39it/s]\u001b[A\n",
            " 12%|█▏        | 20/165 [00:02<00:15,  9.40it/s]\u001b[A\n",
            " 13%|█▎        | 21/165 [00:02<00:15,  9.38it/s]\u001b[A\n",
            " 13%|█▎        | 22/165 [00:02<00:15,  9.37it/s]\u001b[A\n",
            " 14%|█▍        | 23/165 [00:02<00:15,  9.38it/s]\u001b[A\n",
            " 15%|█▍        | 24/165 [00:02<00:15,  9.39it/s]\u001b[A\n",
            " 15%|█▌        | 25/165 [00:02<00:14,  9.41it/s]\u001b[A\n",
            " 16%|█▌        | 26/165 [00:02<00:14,  9.40it/s]\u001b[A\n",
            " 16%|█▋        | 27/165 [00:02<00:14,  9.39it/s]\u001b[A\n",
            " 17%|█▋        | 28/165 [00:02<00:14,  9.40it/s]\u001b[A\n",
            " 18%|█▊        | 29/165 [00:03<00:14,  9.41it/s]\u001b[A\n",
            " 18%|█▊        | 30/165 [00:03<00:14,  9.41it/s]\u001b[A\n",
            " 19%|█▉        | 31/165 [00:03<00:14,  9.35it/s]\u001b[A\n",
            " 19%|█▉        | 32/165 [00:03<00:14,  9.35it/s]\u001b[A\n",
            " 20%|██        | 33/165 [00:03<00:14,  9.38it/s]\u001b[A\n",
            " 21%|██        | 34/165 [00:03<00:13,  9.37it/s]\u001b[A\n",
            " 21%|██        | 35/165 [00:03<00:13,  9.37it/s]\u001b[A\n",
            " 22%|██▏       | 36/165 [00:03<00:13,  9.39it/s]\u001b[A\n",
            " 22%|██▏       | 37/165 [00:03<00:13,  9.40it/s]\u001b[A\n",
            " 23%|██▎       | 38/165 [00:04<00:13,  9.40it/s]\u001b[A\n",
            " 24%|██▎       | 39/165 [00:04<00:13,  9.39it/s]\u001b[A\n",
            " 24%|██▍       | 40/165 [00:04<00:13,  9.38it/s]\u001b[A\n",
            " 25%|██▍       | 41/165 [00:04<00:13,  9.38it/s]\u001b[A\n",
            " 25%|██▌       | 42/165 [00:04<00:13,  9.40it/s]\u001b[A\n",
            " 26%|██▌       | 43/165 [00:04<00:12,  9.39it/s]\u001b[A\n",
            " 27%|██▋       | 44/165 [00:04<00:12,  9.40it/s]\u001b[A\n",
            " 27%|██▋       | 45/165 [00:04<00:12,  9.41it/s]\u001b[A\n",
            " 28%|██▊       | 46/165 [00:04<00:12,  9.40it/s]\u001b[A\n",
            " 28%|██▊       | 47/165 [00:05<00:12,  9.41it/s]\u001b[A\n",
            " 29%|██▉       | 48/165 [00:05<00:12,  9.40it/s]\u001b[A\n",
            " 30%|██▉       | 49/165 [00:05<00:12,  9.40it/s]\u001b[A\n",
            " 30%|███       | 50/165 [00:05<00:12,  9.40it/s]\u001b[A\n",
            " 31%|███       | 51/165 [00:05<00:12,  9.40it/s]\u001b[A\n",
            " 32%|███▏      | 52/165 [00:05<00:12,  9.39it/s]\u001b[A\n",
            " 32%|███▏      | 53/165 [00:05<00:11,  9.40it/s]\u001b[A\n",
            " 33%|███▎      | 54/165 [00:05<00:11,  9.40it/s]\u001b[A\n",
            " 33%|███▎      | 55/165 [00:05<00:11,  9.40it/s]\u001b[A\n",
            " 34%|███▍      | 56/165 [00:05<00:11,  9.41it/s]\u001b[A\n",
            " 35%|███▍      | 57/165 [00:06<00:11,  9.40it/s]\u001b[A\n",
            " 35%|███▌      | 58/165 [00:06<00:11,  9.39it/s]\u001b[A\n",
            " 36%|███▌      | 59/165 [00:06<00:11,  9.37it/s]\u001b[A\n",
            " 36%|███▋      | 60/165 [00:06<00:11,  9.37it/s]\u001b[A\n",
            " 37%|███▋      | 61/165 [00:06<00:11,  9.36it/s]\u001b[A\n",
            " 38%|███▊      | 62/165 [00:06<00:10,  9.37it/s]\u001b[A\n",
            " 38%|███▊      | 63/165 [00:06<00:10,  9.39it/s]\u001b[A\n",
            " 39%|███▉      | 64/165 [00:06<00:10,  9.41it/s]\u001b[A\n",
            " 39%|███▉      | 65/165 [00:06<00:10,  9.39it/s]\u001b[A\n",
            " 40%|████      | 66/165 [00:07<00:10,  9.40it/s]\u001b[A\n",
            " 41%|████      | 67/165 [00:07<00:10,  9.39it/s]\u001b[A\n",
            " 41%|████      | 68/165 [00:07<00:10,  9.38it/s]\u001b[A\n",
            " 42%|████▏     | 69/165 [00:07<00:10,  9.37it/s]\u001b[A\n",
            " 42%|████▏     | 70/165 [00:07<00:10,  9.34it/s]\u001b[A\n",
            " 43%|████▎     | 71/165 [00:07<00:10,  9.36it/s]\u001b[A\n",
            " 44%|████▎     | 72/165 [00:07<00:09,  9.39it/s]\u001b[A\n",
            " 44%|████▍     | 73/165 [00:07<00:09,  9.41it/s]\u001b[A\n",
            " 45%|████▍     | 74/165 [00:07<00:09,  9.38it/s]\u001b[A\n",
            " 45%|████▌     | 75/165 [00:08<00:09,  9.38it/s]\u001b[A\n",
            " 46%|████▌     | 76/165 [00:08<00:09,  9.41it/s]\u001b[A\n",
            " 47%|████▋     | 77/165 [00:08<00:09,  9.41it/s]\u001b[A\n",
            " 47%|████▋     | 78/165 [00:08<00:09,  9.39it/s]\u001b[A\n",
            " 48%|████▊     | 79/165 [00:08<00:09,  9.39it/s]\u001b[A\n",
            " 48%|████▊     | 80/165 [00:08<00:09,  9.39it/s]\u001b[A\n",
            " 49%|████▉     | 81/165 [00:08<00:08,  9.40it/s]\u001b[A\n",
            " 50%|████▉     | 82/165 [00:08<00:08,  9.32it/s]\u001b[A\n",
            " 50%|█████     | 83/165 [00:08<00:08,  9.32it/s]\u001b[A\n",
            " 51%|█████     | 84/165 [00:08<00:08,  9.36it/s]\u001b[A\n",
            " 52%|█████▏    | 85/165 [00:09<00:08,  9.37it/s]\u001b[A\n",
            " 52%|█████▏    | 86/165 [00:09<00:08,  9.33it/s]\u001b[A\n",
            " 53%|█████▎    | 87/165 [00:09<00:08,  9.30it/s]\u001b[A\n",
            " 53%|█████▎    | 88/165 [00:09<00:08,  9.35it/s]\u001b[A\n",
            " 54%|█████▍    | 89/165 [00:09<00:08,  9.36it/s]\u001b[A\n",
            " 55%|█████▍    | 90/165 [00:09<00:08,  9.37it/s]\u001b[A\n",
            " 55%|█████▌    | 91/165 [00:09<00:07,  9.36it/s]\u001b[A\n",
            " 56%|█████▌    | 92/165 [00:09<00:07,  9.37it/s]\u001b[A\n",
            " 56%|█████▋    | 93/165 [00:09<00:07,  9.39it/s]\u001b[A\n",
            " 57%|█████▋    | 94/165 [00:10<00:07,  9.39it/s]\u001b[A\n",
            " 58%|█████▊    | 95/165 [00:10<00:07,  9.39it/s]\u001b[A\n",
            " 58%|█████▊    | 96/165 [00:10<00:07,  9.40it/s]\u001b[A\n",
            " 59%|█████▉    | 97/165 [00:10<00:07,  9.38it/s]\u001b[A\n",
            " 59%|█████▉    | 98/165 [00:10<00:07,  9.38it/s]\u001b[A\n",
            " 60%|██████    | 99/165 [00:10<00:07,  9.39it/s]\u001b[A\n",
            " 61%|██████    | 100/165 [00:10<00:06,  9.40it/s]\u001b[A\n",
            " 61%|██████    | 101/165 [00:10<00:06,  9.40it/s]\u001b[A\n",
            " 62%|██████▏   | 102/165 [00:10<00:06,  9.39it/s]\u001b[A\n",
            " 62%|██████▏   | 103/165 [00:10<00:06,  9.37it/s]\u001b[A\n",
            " 63%|██████▎   | 104/165 [00:11<00:06,  9.38it/s]\u001b[A\n",
            " 64%|██████▎   | 105/165 [00:11<00:06,  9.38it/s]\u001b[A\n",
            " 64%|██████▍   | 106/165 [00:11<00:06,  9.36it/s]\u001b[A\n",
            " 65%|██████▍   | 107/165 [00:11<00:06,  9.36it/s]\u001b[A\n",
            " 65%|██████▌   | 108/165 [00:11<00:06,  9.37it/s]\u001b[A\n",
            " 66%|██████▌   | 109/165 [00:11<00:05,  9.37it/s]\u001b[A\n",
            " 67%|██████▋   | 110/165 [00:11<00:05,  9.36it/s]\u001b[A\n",
            " 67%|██████▋   | 111/165 [00:11<00:05,  9.36it/s]\u001b[A\n",
            " 68%|██████▊   | 112/165 [00:11<00:05,  9.37it/s]\u001b[A\n",
            " 68%|██████▊   | 113/165 [00:12<00:05,  9.36it/s]\u001b[A\n",
            " 69%|██████▉   | 114/165 [00:12<00:05,  9.36it/s]\u001b[A\n",
            " 70%|██████▉   | 115/165 [00:12<00:05,  9.37it/s]\u001b[A\n",
            " 70%|███████   | 116/165 [00:12<00:05,  9.37it/s]\u001b[A\n",
            " 71%|███████   | 117/165 [00:12<00:05,  9.37it/s]\u001b[A\n",
            " 72%|███████▏  | 118/165 [00:12<00:05,  9.38it/s]\u001b[A\n",
            " 72%|███████▏  | 119/165 [00:12<00:04,  9.37it/s]\u001b[A\n",
            " 73%|███████▎  | 120/165 [00:12<00:04,  9.37it/s]\u001b[A\n",
            " 73%|███████▎  | 121/165 [00:12<00:04,  9.38it/s]\u001b[A\n",
            " 74%|███████▍  | 122/165 [00:13<00:04,  9.39it/s]\u001b[A\n",
            " 75%|███████▍  | 123/165 [00:13<00:04,  9.39it/s]\u001b[A\n",
            " 75%|███████▌  | 124/165 [00:13<00:04,  9.40it/s]\u001b[A\n",
            " 76%|███████▌  | 125/165 [00:13<00:04,  9.39it/s]\u001b[A\n",
            " 76%|███████▋  | 126/165 [00:13<00:04,  9.38it/s]\u001b[A\n",
            " 77%|███████▋  | 127/165 [00:13<00:04,  9.37it/s]\u001b[A\n",
            " 78%|███████▊  | 128/165 [00:13<00:03,  9.38it/s]\u001b[A\n",
            " 78%|███████▊  | 129/165 [00:13<00:03,  9.38it/s]\u001b[A\n",
            " 79%|███████▉  | 130/165 [00:13<00:03,  9.40it/s]\u001b[A\n",
            " 79%|███████▉  | 131/165 [00:13<00:03,  9.41it/s]\u001b[A\n",
            " 80%|████████  | 132/165 [00:14<00:03,  9.39it/s]\u001b[A\n",
            " 81%|████████  | 133/165 [00:14<00:03,  9.40it/s]\u001b[A\n",
            " 81%|████████  | 134/165 [00:14<00:03,  9.40it/s]\u001b[A\n",
            " 82%|████████▏ | 135/165 [00:14<00:03,  9.41it/s]\u001b[A\n",
            " 82%|████████▏ | 136/165 [00:14<00:03,  9.40it/s]\u001b[A\n",
            " 83%|████████▎ | 137/165 [00:14<00:02,  9.41it/s]\u001b[A\n",
            " 84%|████████▎ | 138/165 [00:14<00:02,  9.41it/s]\u001b[A\n",
            " 84%|████████▍ | 139/165 [00:14<00:02,  9.41it/s]\u001b[A\n",
            " 85%|████████▍ | 140/165 [00:14<00:02,  9.41it/s]\u001b[A\n",
            " 85%|████████▌ | 141/165 [00:15<00:02,  9.41it/s]\u001b[A\n",
            " 86%|████████▌ | 142/165 [00:15<00:02,  9.40it/s]\u001b[A\n",
            " 87%|████████▋ | 143/165 [00:15<00:02,  9.41it/s]\u001b[A\n",
            " 87%|████████▋ | 144/165 [00:15<00:02,  9.40it/s]\u001b[A\n",
            " 88%|████████▊ | 145/165 [00:15<00:02,  9.40it/s]\u001b[A\n",
            " 88%|████████▊ | 146/165 [00:15<00:02,  9.39it/s]\u001b[A\n",
            " 89%|████████▉ | 147/165 [00:15<00:01,  9.36it/s]\u001b[A\n",
            " 90%|████████▉ | 148/165 [00:15<00:01,  9.37it/s]\u001b[A\n",
            " 90%|█████████ | 149/165 [00:15<00:01,  9.38it/s]\u001b[A\n",
            " 91%|█████████ | 150/165 [00:16<00:01,  9.38it/s]\u001b[A\n",
            " 92%|█████████▏| 151/165 [00:16<00:01,  9.40it/s]\u001b[A\n",
            " 92%|█████████▏| 152/165 [00:16<00:01,  9.40it/s]\u001b[A\n",
            " 93%|█████████▎| 153/165 [00:16<00:01,  9.39it/s]\u001b[A\n",
            " 93%|█████████▎| 154/165 [00:16<00:01,  9.40it/s]\u001b[A\n",
            " 94%|█████████▍| 155/165 [00:16<00:01,  9.39it/s]\u001b[A\n",
            " 95%|█████████▍| 156/165 [00:16<00:00,  9.41it/s]\u001b[A\n",
            " 95%|█████████▌| 157/165 [00:16<00:00,  9.39it/s]\u001b[A\n",
            " 96%|█████████▌| 158/165 [00:16<00:00,  9.40it/s]\u001b[A\n",
            " 96%|█████████▋| 159/165 [00:16<00:00,  9.38it/s]\u001b[A\n",
            " 97%|█████████▋| 160/165 [00:17<00:00,  9.41it/s]\u001b[A\n",
            " 98%|█████████▊| 161/165 [00:17<00:00,  9.43it/s]\u001b[A\n",
            " 98%|█████████▊| 162/165 [00:17<00:00,  9.42it/s]\u001b[A\n",
            " 99%|█████████▉| 163/165 [00:17<00:00,  9.42it/s]\u001b[A\n",
            "100%|██████████| 165/165 [00:17<00:00,  9.36it/s]\n",
            "========== model: bert-base-uncased fold: 2 inference ==========\n",
            "========== model: bert-base-uncased fold: 2 inference ==========\n",
            "========== model: bert-base-uncased fold: 2 inference ==========\n",
            "========== model: bert-base-uncased fold: 2 inference ==========\n",
            "========== model: bert-base-uncased fold: 2 inference ==========\n",
            "========== model: bert-base-uncased fold: 2 inference ==========\n",
            "========== model: bert-base-uncased fold: 2 inference ==========\n",
            "========== model: bert-base-uncased fold: 2 inference ==========\n",
            "========== model: bert-base-uncased fold: 2 inference ==========\n",
            "loading configuration file /content/activebus_bert_final/config.json\n",
            "Model config BertConfig {\n",
            "  \"_name_or_path\": \"activebus/BERT-XD_Review\",\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_ids\": 0,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.15.0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "loading weights file /content/activebus_bert_final/pytorch_model.bin\n",
            "Some weights of the model checkpoint at /content/activebus_bert_final were not used when initializing BertForSequenceClassification: ['cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /content/activebus_bert_final and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "\n",
            "  0%|          | 0/165 [00:00<?, ?it/s]\u001b[A\n",
            "  1%|          | 1/165 [00:00<00:18,  8.77it/s]\u001b[A\n",
            "  1%|          | 2/165 [00:00<00:17,  9.08it/s]\u001b[A\n",
            "  2%|▏         | 3/165 [00:00<00:17,  9.23it/s]\u001b[A\n",
            "  2%|▏         | 4/165 [00:00<00:17,  9.30it/s]\u001b[A\n",
            "  3%|▎         | 5/165 [00:00<00:17,  9.34it/s]\u001b[A\n",
            "  4%|▎         | 6/165 [00:00<00:17,  9.35it/s]\u001b[A\n",
            "  4%|▍         | 7/165 [00:00<00:16,  9.38it/s]\u001b[A\n",
            "  5%|▍         | 8/165 [00:00<00:16,  9.39it/s]\u001b[A\n",
            "  5%|▌         | 9/165 [00:00<00:16,  9.40it/s]\u001b[A\n",
            "  6%|▌         | 10/165 [00:01<00:16,  9.41it/s]\u001b[A\n",
            "  7%|▋         | 11/165 [00:01<00:16,  9.42it/s]\u001b[A\n",
            "  7%|▋         | 12/165 [00:01<00:16,  9.43it/s]\u001b[A\n",
            "  8%|▊         | 13/165 [00:01<00:16,  9.43it/s]\u001b[A\n",
            "  8%|▊         | 14/165 [00:01<00:16,  9.32it/s]\u001b[A\n",
            "  9%|▉         | 15/165 [00:01<00:16,  9.34it/s]\u001b[A\n",
            " 10%|▉         | 16/165 [00:01<00:15,  9.35it/s]\u001b[A\n",
            " 10%|█         | 17/165 [00:01<00:15,  9.38it/s]\u001b[A\n",
            " 11%|█         | 18/165 [00:01<00:15,  9.39it/s]\u001b[A\n",
            " 12%|█▏        | 19/165 [00:02<00:15,  9.38it/s]\u001b[A\n",
            " 12%|█▏        | 20/165 [00:02<00:15,  9.36it/s]\u001b[A\n",
            " 13%|█▎        | 21/165 [00:02<00:15,  9.37it/s]\u001b[A\n",
            " 13%|█▎        | 22/165 [00:02<00:15,  9.37it/s]\u001b[A\n",
            " 14%|█▍        | 23/165 [00:02<00:15,  9.39it/s]\u001b[A\n",
            " 15%|█▍        | 24/165 [00:02<00:15,  9.37it/s]\u001b[A\n",
            " 15%|█▌        | 25/165 [00:02<00:14,  9.37it/s]\u001b[A\n",
            " 16%|█▌        | 26/165 [00:02<00:14,  9.37it/s]\u001b[A\n",
            " 16%|█▋        | 27/165 [00:02<00:14,  9.35it/s]\u001b[A\n",
            " 17%|█▋        | 28/165 [00:02<00:14,  9.37it/s]\u001b[A\n",
            " 18%|█▊        | 29/165 [00:03<00:14,  9.38it/s]\u001b[A\n",
            " 18%|█▊        | 30/165 [00:03<00:14,  9.37it/s]\u001b[A\n",
            " 19%|█▉        | 31/165 [00:03<00:14,  9.39it/s]\u001b[A\n",
            " 19%|█▉        | 32/165 [00:03<00:14,  9.40it/s]\u001b[A\n",
            " 20%|██        | 33/165 [00:03<00:14,  9.40it/s]\u001b[A\n",
            " 21%|██        | 34/165 [00:03<00:13,  9.39it/s]\u001b[A\n",
            " 21%|██        | 35/165 [00:03<00:13,  9.40it/s]\u001b[A\n",
            " 22%|██▏       | 36/165 [00:03<00:13,  9.40it/s]\u001b[A\n",
            " 22%|██▏       | 37/165 [00:03<00:13,  9.41it/s]\u001b[A\n",
            " 23%|██▎       | 38/165 [00:04<00:13,  9.40it/s]\u001b[A\n",
            " 24%|██▎       | 39/165 [00:04<00:13,  9.40it/s]\u001b[A\n",
            " 24%|██▍       | 40/165 [00:04<00:13,  9.38it/s]\u001b[A\n",
            " 25%|██▍       | 41/165 [00:04<00:13,  9.37it/s]\u001b[A\n",
            " 25%|██▌       | 42/165 [00:04<00:13,  9.38it/s]\u001b[A\n",
            " 26%|██▌       | 43/165 [00:04<00:12,  9.40it/s]\u001b[A\n",
            " 27%|██▋       | 44/165 [00:04<00:12,  9.39it/s]\u001b[A\n",
            " 27%|██▋       | 45/165 [00:04<00:12,  9.36it/s]\u001b[A\n",
            " 28%|██▊       | 46/165 [00:04<00:12,  9.38it/s]\u001b[A\n",
            " 28%|██▊       | 47/165 [00:05<00:12,  9.37it/s]\u001b[A\n",
            " 29%|██▉       | 48/165 [00:05<00:12,  9.38it/s]\u001b[A\n",
            " 30%|██▉       | 49/165 [00:05<00:12,  9.39it/s]\u001b[A\n",
            " 30%|███       | 50/165 [00:05<00:12,  9.39it/s]\u001b[A\n",
            " 31%|███       | 51/165 [00:05<00:12,  9.39it/s]\u001b[A\n",
            " 32%|███▏      | 52/165 [00:05<00:12,  9.41it/s]\u001b[A\n",
            " 32%|███▏      | 53/165 [00:05<00:11,  9.42it/s]\u001b[A\n",
            " 33%|███▎      | 54/165 [00:05<00:11,  9.42it/s]\u001b[A\n",
            " 33%|███▎      | 55/165 [00:05<00:11,  9.43it/s]\u001b[A\n",
            " 34%|███▍      | 56/165 [00:05<00:11,  9.43it/s]\u001b[A\n",
            " 35%|███▍      | 57/165 [00:06<00:11,  9.44it/s]\u001b[A\n",
            " 35%|███▌      | 58/165 [00:06<00:11,  9.44it/s]\u001b[A\n",
            " 36%|███▌      | 59/165 [00:06<00:11,  9.41it/s]\u001b[A\n",
            " 36%|███▋      | 60/165 [00:06<00:11,  9.42it/s]\u001b[A\n",
            " 37%|███▋      | 61/165 [00:06<00:11,  9.42it/s]\u001b[A\n",
            " 38%|███▊      | 62/165 [00:06<00:10,  9.39it/s]\u001b[A\n",
            " 38%|███▊      | 63/165 [00:06<00:10,  9.35it/s]\u001b[A\n",
            " 39%|███▉      | 64/165 [00:06<00:10,  9.36it/s]\u001b[A\n",
            " 39%|███▉      | 65/165 [00:06<00:10,  9.37it/s]\u001b[A\n",
            " 40%|████      | 66/165 [00:07<00:10,  9.37it/s]\u001b[A\n",
            " 41%|████      | 67/165 [00:07<00:10,  9.38it/s]\u001b[A\n",
            " 41%|████      | 68/165 [00:07<00:10,  9.31it/s]\u001b[A\n",
            " 42%|████▏     | 69/165 [00:07<00:10,  9.34it/s]\u001b[A\n",
            " 42%|████▏     | 70/165 [00:07<00:10,  9.36it/s]\u001b[A\n",
            " 43%|████▎     | 71/165 [00:07<00:10,  9.38it/s]\u001b[A\n",
            " 44%|████▎     | 72/165 [00:07<00:09,  9.39it/s]\u001b[A\n",
            " 44%|████▍     | 73/165 [00:07<00:09,  9.38it/s]\u001b[A\n",
            " 45%|████▍     | 74/165 [00:07<00:09,  9.37it/s]\u001b[A\n",
            " 45%|████▌     | 75/165 [00:07<00:09,  9.35it/s]\u001b[A\n",
            " 46%|████▌     | 76/165 [00:08<00:09,  9.36it/s]\u001b[A\n",
            " 47%|████▋     | 77/165 [00:08<00:09,  9.36it/s]\u001b[A\n",
            " 47%|████▋     | 78/165 [00:08<00:09,  9.37it/s]\u001b[A\n",
            " 48%|████▊     | 79/165 [00:08<00:09,  9.35it/s]\u001b[A\n",
            " 48%|████▊     | 80/165 [00:08<00:09,  9.35it/s]\u001b[A\n",
            " 49%|████▉     | 81/165 [00:08<00:08,  9.34it/s]\u001b[A\n",
            " 50%|████▉     | 82/165 [00:08<00:08,  9.36it/s]\u001b[A\n",
            " 50%|█████     | 83/165 [00:08<00:08,  9.39it/s]\u001b[A\n",
            " 51%|█████     | 84/165 [00:08<00:08,  9.40it/s]\u001b[A\n",
            " 52%|█████▏    | 85/165 [00:09<00:08,  9.39it/s]\u001b[A\n",
            " 52%|█████▏    | 86/165 [00:09<00:08,  9.42it/s]\u001b[A\n",
            " 53%|█████▎    | 87/165 [00:09<00:08,  9.42it/s]\u001b[A\n",
            " 53%|█████▎    | 88/165 [00:09<00:08,  9.38it/s]\u001b[A\n",
            " 54%|█████▍    | 89/165 [00:09<00:08,  9.38it/s]\u001b[A\n",
            " 55%|█████▍    | 90/165 [00:09<00:07,  9.39it/s]\u001b[A\n",
            " 55%|█████▌    | 91/165 [00:09<00:07,  9.38it/s]\u001b[A\n",
            " 56%|█████▌    | 92/165 [00:09<00:07,  9.38it/s]\u001b[A\n",
            " 56%|█████▋    | 93/165 [00:09<00:07,  9.35it/s]\u001b[A\n",
            " 57%|█████▋    | 94/165 [00:10<00:07,  9.36it/s]\u001b[A\n",
            " 58%|█████▊    | 95/165 [00:10<00:07,  9.36it/s]\u001b[A\n",
            " 58%|█████▊    | 96/165 [00:10<00:07,  9.37it/s]\u001b[A\n",
            " 59%|█████▉    | 97/165 [00:10<00:07,  9.36it/s]\u001b[A\n",
            " 59%|█████▉    | 98/165 [00:10<00:07,  9.28it/s]\u001b[A\n",
            " 60%|██████    | 99/165 [00:10<00:07,  9.32it/s]\u001b[A\n",
            " 61%|██████    | 100/165 [00:10<00:06,  9.34it/s]\u001b[A\n",
            " 61%|██████    | 101/165 [00:10<00:06,  9.37it/s]\u001b[A\n",
            " 62%|██████▏   | 102/165 [00:10<00:06,  9.38it/s]\u001b[A\n",
            " 62%|██████▏   | 103/165 [00:10<00:06,  9.38it/s]\u001b[A\n",
            " 63%|██████▎   | 104/165 [00:11<00:06,  9.37it/s]\u001b[A\n",
            " 64%|██████▎   | 105/165 [00:11<00:06,  9.37it/s]\u001b[A\n",
            " 64%|██████▍   | 106/165 [00:11<00:06,  9.38it/s]\u001b[A\n",
            " 65%|██████▍   | 107/165 [00:11<00:06,  9.39it/s]\u001b[A\n",
            " 65%|██████▌   | 108/165 [00:11<00:06,  9.36it/s]\u001b[A\n",
            " 66%|██████▌   | 109/165 [00:11<00:05,  9.39it/s]\u001b[A\n",
            " 67%|██████▋   | 110/165 [00:11<00:05,  9.40it/s]\u001b[A\n",
            " 67%|██████▋   | 111/165 [00:11<00:05,  9.39it/s]\u001b[A\n",
            " 68%|██████▊   | 112/165 [00:11<00:05,  9.36it/s]\u001b[A\n",
            " 68%|██████▊   | 113/165 [00:12<00:05,  9.35it/s]\u001b[A\n",
            " 69%|██████▉   | 114/165 [00:12<00:05,  9.38it/s]\u001b[A\n",
            " 70%|██████▉   | 115/165 [00:12<00:05,  9.39it/s]\u001b[A\n",
            " 70%|███████   | 116/165 [00:12<00:05,  9.41it/s]\u001b[A\n",
            " 71%|███████   | 117/165 [00:12<00:05,  9.41it/s]\u001b[A\n",
            " 72%|███████▏  | 118/165 [00:12<00:04,  9.42it/s]\u001b[A\n",
            " 72%|███████▏  | 119/165 [00:12<00:04,  9.40it/s]\u001b[A\n",
            " 73%|███████▎  | 120/165 [00:12<00:04,  9.41it/s]\u001b[A\n",
            " 73%|███████▎  | 121/165 [00:12<00:04,  9.41it/s]\u001b[A\n",
            " 74%|███████▍  | 122/165 [00:13<00:04,  9.42it/s]\u001b[A\n",
            " 75%|███████▍  | 123/165 [00:13<00:04,  9.40it/s]\u001b[A\n",
            " 75%|███████▌  | 124/165 [00:13<00:04,  9.30it/s]\u001b[A\n",
            " 76%|███████▌  | 125/165 [00:13<00:04,  9.33it/s]\u001b[A\n",
            " 76%|███████▋  | 126/165 [00:13<00:04,  9.35it/s]\u001b[A\n",
            " 77%|███████▋  | 127/165 [00:13<00:04,  9.36it/s]\u001b[A\n",
            " 78%|███████▊  | 128/165 [00:13<00:03,  9.37it/s]\u001b[A\n",
            " 78%|███████▊  | 129/165 [00:13<00:03,  9.37it/s]\u001b[A\n",
            " 79%|███████▉  | 130/165 [00:13<00:03,  9.36it/s]\u001b[A\n",
            " 79%|███████▉  | 131/165 [00:13<00:03,  9.36it/s]\u001b[A\n",
            " 80%|████████  | 132/165 [00:14<00:03,  9.39it/s]\u001b[A\n",
            " 81%|████████  | 133/165 [00:14<00:03,  9.41it/s]\u001b[A\n",
            " 81%|████████  | 134/165 [00:14<00:03,  9.40it/s]\u001b[A\n",
            " 82%|████████▏ | 135/165 [00:14<00:03,  9.40it/s]\u001b[A\n",
            " 82%|████████▏ | 136/165 [00:14<00:03,  9.40it/s]\u001b[A\n",
            " 83%|████████▎ | 137/165 [00:14<00:03,  9.31it/s]\u001b[A\n",
            " 84%|████████▎ | 138/165 [00:14<00:02,  9.33it/s]\u001b[A\n",
            " 84%|████████▍ | 139/165 [00:14<00:02,  9.34it/s]\u001b[A\n",
            " 85%|████████▍ | 140/165 [00:14<00:02,  9.35it/s]\u001b[A\n",
            " 85%|████████▌ | 141/165 [00:15<00:02,  9.34it/s]\u001b[A\n",
            " 86%|████████▌ | 142/165 [00:15<00:02,  9.33it/s]\u001b[A\n",
            " 87%|████████▋ | 143/165 [00:15<00:02,  9.36it/s]\u001b[A\n",
            " 87%|████████▋ | 144/165 [00:15<00:02,  9.36it/s]\u001b[A\n",
            " 88%|████████▊ | 145/165 [00:15<00:02,  9.38it/s]\u001b[A\n",
            " 88%|████████▊ | 146/165 [00:15<00:02,  9.37it/s]\u001b[A\n",
            " 89%|████████▉ | 147/165 [00:15<00:01,  9.37it/s]\u001b[A\n",
            " 90%|████████▉ | 148/165 [00:15<00:01,  9.36it/s]\u001b[A\n",
            " 90%|█████████ | 149/165 [00:15<00:01,  9.36it/s]\u001b[A\n",
            " 91%|█████████ | 150/165 [00:16<00:01,  9.36it/s]\u001b[A\n",
            " 92%|█████████▏| 151/165 [00:16<00:01,  9.38it/s]\u001b[A\n",
            " 92%|█████████▏| 152/165 [00:16<00:01,  9.39it/s]\u001b[A\n",
            " 93%|█████████▎| 153/165 [00:16<00:01,  9.39it/s]\u001b[A\n",
            " 93%|█████████▎| 154/165 [00:16<00:01,  9.39it/s]\u001b[A\n",
            " 94%|█████████▍| 155/165 [00:16<00:01,  9.37it/s]\u001b[A\n",
            " 95%|█████████▍| 156/165 [00:16<00:00,  9.38it/s]\u001b[A\n",
            " 95%|█████████▌| 157/165 [00:16<00:00,  9.37it/s]\u001b[A\n",
            " 96%|█████████▌| 158/165 [00:16<00:00,  9.41it/s]\u001b[A\n",
            " 96%|█████████▋| 159/165 [00:16<00:00,  9.40it/s]\u001b[A\n",
            " 97%|█████████▋| 160/165 [00:17<00:00,  9.42it/s]\u001b[A\n",
            " 98%|█████████▊| 161/165 [00:17<00:00,  9.44it/s]\u001b[A\n",
            " 98%|█████████▊| 162/165 [00:17<00:00,  9.43it/s]\u001b[A\n",
            " 99%|█████████▉| 163/165 [00:17<00:00,  9.44it/s]\u001b[A\n",
            "100%|██████████| 165/165 [00:17<00:00,  9.36it/s]\n",
            "========== model: bert-base-uncased fold: 3 inference ==========\n",
            "========== model: bert-base-uncased fold: 3 inference ==========\n",
            "========== model: bert-base-uncased fold: 3 inference ==========\n",
            "========== model: bert-base-uncased fold: 3 inference ==========\n",
            "========== model: bert-base-uncased fold: 3 inference ==========\n",
            "========== model: bert-base-uncased fold: 3 inference ==========\n",
            "========== model: bert-base-uncased fold: 3 inference ==========\n",
            "========== model: bert-base-uncased fold: 3 inference ==========\n",
            "========== model: bert-base-uncased fold: 3 inference ==========\n",
            "loading configuration file /content/activebus_bert_final/config.json\n",
            "Model config BertConfig {\n",
            "  \"_name_or_path\": \"activebus/BERT-XD_Review\",\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_ids\": 0,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.15.0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "loading weights file /content/activebus_bert_final/pytorch_model.bin\n",
            "Some weights of the model checkpoint at /content/activebus_bert_final were not used when initializing BertForSequenceClassification: ['cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /content/activebus_bert_final and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "\n",
            "  0%|          | 0/165 [00:00<?, ?it/s]\u001b[A\n",
            "  1%|          | 1/165 [00:00<00:18,  8.87it/s]\u001b[A\n",
            "  1%|          | 2/165 [00:00<00:17,  9.15it/s]\u001b[A\n",
            "  2%|▏         | 3/165 [00:00<00:17,  9.27it/s]\u001b[A\n",
            "  2%|▏         | 4/165 [00:00<00:17,  9.24it/s]\u001b[A\n",
            "  3%|▎         | 5/165 [00:00<00:17,  9.28it/s]\u001b[A\n",
            "  4%|▎         | 6/165 [00:00<00:17,  9.32it/s]\u001b[A\n",
            "  4%|▍         | 7/165 [00:00<00:16,  9.32it/s]\u001b[A\n",
            "  5%|▍         | 8/165 [00:00<00:16,  9.33it/s]\u001b[A\n",
            "  5%|▌         | 9/165 [00:00<00:16,  9.36it/s]\u001b[A\n",
            "  6%|▌         | 10/165 [00:01<00:16,  9.36it/s]\u001b[A\n",
            "  7%|▋         | 11/165 [00:01<00:16,  9.35it/s]\u001b[A\n",
            "  7%|▋         | 12/165 [00:01<00:16,  9.35it/s]\u001b[A\n",
            "  8%|▊         | 13/165 [00:01<00:16,  9.38it/s]\u001b[A\n",
            "  8%|▊         | 14/165 [00:01<00:16,  9.37it/s]\u001b[A\n",
            "  9%|▉         | 15/165 [00:01<00:15,  9.38it/s]\u001b[A\n",
            " 10%|▉         | 16/165 [00:01<00:15,  9.40it/s]\u001b[A\n",
            " 10%|█         | 17/165 [00:01<00:15,  9.40it/s]\u001b[A\n",
            " 11%|█         | 18/165 [00:01<00:15,  9.40it/s]\u001b[A\n",
            " 12%|█▏        | 19/165 [00:02<00:15,  9.37it/s]\u001b[A\n",
            " 12%|█▏        | 20/165 [00:02<00:15,  9.39it/s]\u001b[A\n",
            " 13%|█▎        | 21/165 [00:02<00:15,  9.39it/s]\u001b[A\n",
            " 13%|█▎        | 22/165 [00:02<00:15,  9.41it/s]\u001b[A\n",
            " 14%|█▍        | 23/165 [00:02<00:15,  9.41it/s]\u001b[A\n",
            " 15%|█▍        | 24/165 [00:02<00:15,  9.39it/s]\u001b[A\n",
            " 15%|█▌        | 25/165 [00:02<00:14,  9.40it/s]\u001b[A\n",
            " 16%|█▌        | 26/165 [00:02<00:14,  9.42it/s]\u001b[A\n",
            " 16%|█▋        | 27/165 [00:02<00:14,  9.41it/s]\u001b[A\n",
            " 17%|█▋        | 28/165 [00:02<00:14,  9.38it/s]\u001b[A\n",
            " 18%|█▊        | 29/165 [00:03<00:14,  9.36it/s]\u001b[A\n",
            " 18%|█▊        | 30/165 [00:03<00:14,  9.38it/s]\u001b[A\n",
            " 19%|█▉        | 31/165 [00:03<00:14,  9.38it/s]\u001b[A\n",
            " 19%|█▉        | 32/165 [00:03<00:14,  9.37it/s]\u001b[A\n",
            " 20%|██        | 33/165 [00:03<00:14,  9.38it/s]\u001b[A\n",
            " 21%|██        | 34/165 [00:03<00:13,  9.39it/s]\u001b[A\n",
            " 21%|██        | 35/165 [00:03<00:13,  9.43it/s]\u001b[A\n",
            " 22%|██▏       | 36/165 [00:03<00:13,  9.43it/s]\u001b[A\n",
            " 22%|██▏       | 37/165 [00:03<00:13,  9.42it/s]\u001b[A\n",
            " 23%|██▎       | 38/165 [00:04<00:13,  9.42it/s]\u001b[A\n",
            " 24%|██▎       | 39/165 [00:04<00:13,  9.41it/s]\u001b[A\n",
            " 24%|██▍       | 40/165 [00:04<00:13,  9.40it/s]\u001b[A\n",
            " 25%|██▍       | 41/165 [00:04<00:13,  9.39it/s]\u001b[A\n",
            " 25%|██▌       | 42/165 [00:04<00:13,  9.40it/s]\u001b[A\n",
            " 26%|██▌       | 43/165 [00:04<00:12,  9.41it/s]\u001b[A\n",
            " 27%|██▋       | 44/165 [00:04<00:12,  9.39it/s]\u001b[A\n",
            " 27%|██▋       | 45/165 [00:04<00:12,  9.38it/s]\u001b[A\n",
            " 28%|██▊       | 46/165 [00:04<00:12,  9.38it/s]\u001b[A\n",
            " 28%|██▊       | 47/165 [00:05<00:12,  9.40it/s]\u001b[A\n",
            " 29%|██▉       | 48/165 [00:05<00:12,  9.39it/s]\u001b[A\n",
            " 30%|██▉       | 49/165 [00:05<00:12,  9.40it/s]\u001b[A\n",
            " 30%|███       | 50/165 [00:05<00:12,  9.40it/s]\u001b[A\n",
            " 31%|███       | 51/165 [00:05<00:12,  9.40it/s]\u001b[A\n",
            " 32%|███▏      | 52/165 [00:05<00:12,  9.40it/s]\u001b[A\n",
            " 32%|███▏      | 53/165 [00:05<00:11,  9.40it/s]\u001b[A\n",
            " 33%|███▎      | 54/165 [00:05<00:11,  9.39it/s]\u001b[A\n",
            " 33%|███▎      | 55/165 [00:05<00:11,  9.39it/s]\u001b[A\n",
            " 34%|███▍      | 56/165 [00:05<00:11,  9.40it/s]\u001b[A\n",
            " 35%|███▍      | 57/165 [00:06<00:11,  9.40it/s]\u001b[A\n",
            " 35%|███▌      | 58/165 [00:06<00:11,  9.38it/s]\u001b[A\n",
            " 36%|███▌      | 59/165 [00:06<00:11,  9.38it/s]\u001b[A\n",
            " 36%|███▋      | 60/165 [00:06<00:11,  9.38it/s]\u001b[A\n",
            " 37%|███▋      | 61/165 [00:06<00:11,  9.38it/s]\u001b[A\n",
            " 38%|███▊      | 62/165 [00:06<00:10,  9.38it/s]\u001b[A\n",
            " 38%|███▊      | 63/165 [00:06<00:10,  9.38it/s]\u001b[A\n",
            " 39%|███▉      | 64/165 [00:06<00:10,  9.38it/s]\u001b[A\n",
            " 39%|███▉      | 65/165 [00:06<00:10,  9.37it/s]\u001b[A\n",
            " 40%|████      | 66/165 [00:07<00:10,  9.37it/s]\u001b[A\n",
            " 41%|████      | 67/165 [00:07<00:10,  9.38it/s]\u001b[A\n",
            " 41%|████      | 68/165 [00:07<00:10,  9.38it/s]\u001b[A\n",
            " 42%|████▏     | 69/165 [00:07<00:10,  9.40it/s]\u001b[A\n",
            " 42%|████▏     | 70/165 [00:07<00:10,  9.39it/s]\u001b[A\n",
            " 43%|████▎     | 71/165 [00:07<00:10,  9.39it/s]\u001b[A\n",
            " 44%|████▎     | 72/165 [00:07<00:09,  9.39it/s]\u001b[A\n",
            " 44%|████▍     | 73/165 [00:07<00:09,  9.40it/s]\u001b[A\n",
            " 45%|████▍     | 74/165 [00:07<00:09,  9.41it/s]\u001b[A\n",
            " 45%|████▌     | 75/165 [00:07<00:09,  9.42it/s]\u001b[A\n",
            " 46%|████▌     | 76/165 [00:08<00:09,  9.37it/s]\u001b[A\n",
            " 47%|████▋     | 77/165 [00:08<00:09,  9.38it/s]\u001b[A\n",
            " 47%|████▋     | 78/165 [00:08<00:09,  9.39it/s]\u001b[A\n",
            " 48%|████▊     | 79/165 [00:08<00:09,  9.40it/s]\u001b[A\n",
            " 48%|████▊     | 80/165 [00:08<00:09,  9.40it/s]\u001b[A\n",
            " 49%|████▉     | 81/165 [00:08<00:08,  9.40it/s]\u001b[A\n",
            " 50%|████▉     | 82/165 [00:08<00:08,  9.40it/s]\u001b[A\n",
            " 50%|█████     | 83/165 [00:08<00:08,  9.40it/s]\u001b[A\n",
            " 51%|█████     | 84/165 [00:08<00:08,  9.41it/s]\u001b[A\n",
            " 52%|█████▏    | 85/165 [00:09<00:08,  9.37it/s]\u001b[A\n",
            " 52%|█████▏    | 86/165 [00:09<00:08,  9.37it/s]\u001b[A\n",
            " 53%|█████▎    | 87/165 [00:09<00:08,  9.39it/s]\u001b[A\n",
            " 53%|█████▎    | 88/165 [00:09<00:08,  9.37it/s]\u001b[A\n",
            " 54%|█████▍    | 89/165 [00:09<00:08,  9.35it/s]\u001b[A\n",
            " 55%|█████▍    | 90/165 [00:09<00:08,  9.35it/s]\u001b[A\n",
            " 55%|█████▌    | 91/165 [00:09<00:07,  9.34it/s]\u001b[A\n",
            " 56%|█████▌    | 92/165 [00:09<00:07,  9.36it/s]\u001b[A\n",
            " 56%|█████▋    | 93/165 [00:09<00:07,  9.38it/s]\u001b[A\n",
            " 57%|█████▋    | 94/165 [00:10<00:07,  9.36it/s]\u001b[A\n",
            " 58%|█████▊    | 95/165 [00:10<00:07,  9.35it/s]\u001b[A\n",
            " 58%|█████▊    | 96/165 [00:10<00:07,  9.33it/s]\u001b[A\n",
            " 59%|█████▉    | 97/165 [00:10<00:07,  9.33it/s]\u001b[A\n",
            " 59%|█████▉    | 98/165 [00:10<00:07,  9.33it/s]\u001b[A\n",
            " 60%|██████    | 99/165 [00:10<00:07,  9.35it/s]\u001b[A\n",
            " 61%|██████    | 100/165 [00:10<00:06,  9.38it/s]\u001b[A\n",
            " 61%|██████    | 101/165 [00:10<00:06,  9.38it/s]\u001b[A\n",
            " 62%|██████▏   | 102/165 [00:10<00:06,  9.38it/s]\u001b[A\n",
            " 62%|██████▏   | 103/165 [00:10<00:06,  9.39it/s]\u001b[A\n",
            " 63%|██████▎   | 104/165 [00:11<00:06,  9.40it/s]\u001b[A\n",
            " 64%|██████▎   | 105/165 [00:11<00:06,  9.36it/s]\u001b[A\n",
            " 64%|██████▍   | 106/165 [00:11<00:06,  9.38it/s]\u001b[A\n",
            " 65%|██████▍   | 107/165 [00:11<00:06,  9.36it/s]\u001b[A\n",
            " 65%|██████▌   | 108/165 [00:11<00:06,  9.35it/s]\u001b[A\n",
            " 66%|██████▌   | 109/165 [00:11<00:05,  9.34it/s]\u001b[A\n",
            " 67%|██████▋   | 110/165 [00:11<00:05,  9.35it/s]\u001b[A\n",
            " 67%|██████▋   | 111/165 [00:11<00:05,  9.38it/s]\u001b[A\n",
            " 68%|██████▊   | 112/165 [00:11<00:05,  9.38it/s]\u001b[A\n",
            " 68%|██████▊   | 113/165 [00:12<00:05,  9.40it/s]\u001b[A\n",
            " 69%|██████▉   | 114/165 [00:12<00:05,  9.40it/s]\u001b[A\n",
            " 70%|██████▉   | 115/165 [00:12<00:05,  9.40it/s]\u001b[A\n",
            " 70%|███████   | 116/165 [00:12<00:05,  9.40it/s]\u001b[A\n",
            " 71%|███████   | 117/165 [00:12<00:05,  9.40it/s]\u001b[A\n",
            " 72%|███████▏  | 118/165 [00:12<00:04,  9.40it/s]\u001b[A\n",
            " 72%|███████▏  | 119/165 [00:12<00:04,  9.42it/s]\u001b[A\n",
            " 73%|███████▎  | 120/165 [00:12<00:04,  9.43it/s]\u001b[A\n",
            " 73%|███████▎  | 121/165 [00:12<00:04,  9.40it/s]\u001b[A\n",
            " 74%|███████▍  | 122/165 [00:13<00:04,  9.36it/s]\u001b[A\n",
            " 75%|███████▍  | 123/165 [00:13<00:04,  9.36it/s]\u001b[A\n",
            " 75%|███████▌  | 124/165 [00:13<00:04,  9.30it/s]\u001b[A\n",
            " 76%|███████▌  | 125/165 [00:13<00:04,  9.37it/s]\u001b[A\n",
            " 76%|███████▋  | 126/165 [00:13<00:04,  9.37it/s]\u001b[A\n",
            " 77%|███████▋  | 127/165 [00:13<00:04,  9.37it/s]\u001b[A\n",
            " 78%|███████▊  | 128/165 [00:13<00:03,  9.40it/s]\u001b[A\n",
            " 78%|███████▊  | 129/165 [00:13<00:03,  9.41it/s]\u001b[A\n",
            " 79%|███████▉  | 130/165 [00:13<00:03,  9.40it/s]\u001b[A\n",
            " 79%|███████▉  | 131/165 [00:13<00:03,  9.41it/s]\u001b[A\n",
            " 80%|████████  | 132/165 [00:14<00:03,  9.42it/s]\u001b[A\n",
            " 81%|████████  | 133/165 [00:14<00:03,  9.42it/s]\u001b[A\n",
            " 81%|████████  | 134/165 [00:14<00:03,  9.42it/s]\u001b[A\n",
            " 82%|████████▏ | 135/165 [00:14<00:03,  9.43it/s]\u001b[A\n",
            " 82%|████████▏ | 136/165 [00:14<00:03,  9.42it/s]\u001b[A\n",
            " 83%|████████▎ | 137/165 [00:14<00:02,  9.42it/s]\u001b[A\n",
            " 84%|████████▎ | 138/165 [00:14<00:02,  9.43it/s]\u001b[A\n",
            " 84%|████████▍ | 139/165 [00:14<00:02,  9.42it/s]\u001b[A\n",
            " 85%|████████▍ | 140/165 [00:14<00:02,  9.39it/s]\u001b[A\n",
            " 85%|████████▌ | 141/165 [00:15<00:02,  9.38it/s]\u001b[A\n",
            " 86%|████████▌ | 142/165 [00:15<00:02,  9.37it/s]\u001b[A\n",
            " 87%|████████▋ | 143/165 [00:15<00:02,  9.39it/s]\u001b[A\n",
            " 87%|████████▋ | 144/165 [00:15<00:02,  9.40it/s]\u001b[A\n",
            " 88%|████████▊ | 145/165 [00:15<00:02,  9.39it/s]\u001b[A\n",
            " 88%|████████▊ | 146/165 [00:15<00:02,  9.39it/s]\u001b[A\n",
            " 89%|████████▉ | 147/165 [00:15<00:01,  9.40it/s]\u001b[A\n",
            " 90%|████████▉ | 148/165 [00:15<00:01,  9.40it/s]\u001b[A\n",
            " 90%|█████████ | 149/165 [00:15<00:01,  9.41it/s]\u001b[A\n",
            " 91%|█████████ | 150/165 [00:15<00:01,  9.39it/s]\u001b[A\n",
            " 92%|█████████▏| 151/165 [00:16<00:01,  9.39it/s]\u001b[A\n",
            " 92%|█████████▏| 152/165 [00:16<00:01,  9.40it/s]\u001b[A\n",
            " 93%|█████████▎| 153/165 [00:16<00:01,  9.38it/s]\u001b[A\n",
            " 93%|█████████▎| 154/165 [00:16<00:01,  9.38it/s]\u001b[A\n",
            " 94%|█████████▍| 155/165 [00:16<00:01,  9.39it/s]\u001b[A\n",
            " 95%|█████████▍| 156/165 [00:16<00:00,  9.39it/s]\u001b[A\n",
            " 95%|█████████▌| 157/165 [00:16<00:00,  9.39it/s]\u001b[A\n",
            " 96%|█████████▌| 158/165 [00:16<00:00,  9.41it/s]\u001b[A\n",
            " 96%|█████████▋| 159/165 [00:16<00:00,  9.41it/s]\u001b[A\n",
            " 97%|█████████▋| 160/165 [00:17<00:00,  9.39it/s]\u001b[A\n",
            " 98%|█████████▊| 161/165 [00:17<00:00,  9.39it/s]\u001b[A\n",
            " 98%|█████████▊| 162/165 [00:17<00:00,  9.41it/s]\u001b[A\n",
            " 99%|█████████▉| 163/165 [00:17<00:00,  9.41it/s]\u001b[A\n",
            "100%|██████████| 165/165 [00:17<00:00,  9.37it/s]\n",
            "========== model: bert-base-uncased fold: 4 inference ==========\n",
            "========== model: bert-base-uncased fold: 4 inference ==========\n",
            "========== model: bert-base-uncased fold: 4 inference ==========\n",
            "========== model: bert-base-uncased fold: 4 inference ==========\n",
            "========== model: bert-base-uncased fold: 4 inference ==========\n",
            "========== model: bert-base-uncased fold: 4 inference ==========\n",
            "========== model: bert-base-uncased fold: 4 inference ==========\n",
            "========== model: bert-base-uncased fold: 4 inference ==========\n",
            "========== model: bert-base-uncased fold: 4 inference ==========\n",
            "loading configuration file /content/activebus_bert_final/config.json\n",
            "Model config BertConfig {\n",
            "  \"_name_or_path\": \"activebus/BERT-XD_Review\",\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_ids\": 0,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.15.0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "loading weights file /content/activebus_bert_final/pytorch_model.bin\n",
            "Some weights of the model checkpoint at /content/activebus_bert_final were not used when initializing BertForSequenceClassification: ['cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /content/activebus_bert_final and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "\n",
            "  0%|          | 0/165 [00:00<?, ?it/s]\u001b[A\n",
            "  1%|          | 1/165 [00:00<00:19,  8.48it/s]\u001b[A\n",
            "  1%|          | 2/165 [00:00<00:18,  8.96it/s]\u001b[A\n",
            "  2%|▏         | 3/165 [00:00<00:17,  9.10it/s]\u001b[A\n",
            "  2%|▏         | 4/165 [00:00<00:17,  9.20it/s]\u001b[A\n",
            "  3%|▎         | 5/165 [00:00<00:17,  9.21it/s]\u001b[A\n",
            "  4%|▎         | 6/165 [00:00<00:17,  9.24it/s]\u001b[A\n",
            "  4%|▍         | 7/165 [00:00<00:17,  9.26it/s]\u001b[A\n",
            "  5%|▍         | 8/165 [00:00<00:16,  9.31it/s]\u001b[A\n",
            "  5%|▌         | 9/165 [00:00<00:16,  9.31it/s]\u001b[A\n",
            "  6%|▌         | 10/165 [00:01<00:16,  9.31it/s]\u001b[A\n",
            "  7%|▋         | 11/165 [00:01<00:16,  9.33it/s]\u001b[A\n",
            "  7%|▋         | 12/165 [00:01<00:16,  9.34it/s]\u001b[A\n",
            "  8%|▊         | 13/165 [00:01<00:16,  9.32it/s]\u001b[A\n",
            "  8%|▊         | 14/165 [00:01<00:16,  9.25it/s]\u001b[A\n",
            "  9%|▉         | 15/165 [00:01<00:16,  9.28it/s]\u001b[A\n",
            " 10%|▉         | 16/165 [00:01<00:15,  9.32it/s]\u001b[A\n",
            " 10%|█         | 17/165 [00:01<00:15,  9.34it/s]\u001b[A\n",
            " 11%|█         | 18/165 [00:01<00:15,  9.39it/s]\u001b[A\n",
            " 12%|█▏        | 19/165 [00:02<00:15,  9.39it/s]\u001b[A\n",
            " 12%|█▏        | 20/165 [00:02<00:15,  9.40it/s]\u001b[A\n",
            " 13%|█▎        | 21/165 [00:02<00:15,  9.39it/s]\u001b[A\n",
            " 13%|█▎        | 22/165 [00:02<00:15,  9.38it/s]\u001b[A\n",
            " 14%|█▍        | 23/165 [00:02<00:15,  9.39it/s]\u001b[A\n",
            " 15%|█▍        | 24/165 [00:02<00:15,  9.38it/s]\u001b[A\n",
            " 15%|█▌        | 25/165 [00:02<00:14,  9.38it/s]\u001b[A\n",
            " 16%|█▌        | 26/165 [00:02<00:14,  9.39it/s]\u001b[A\n",
            " 16%|█▋        | 27/165 [00:02<00:14,  9.39it/s]\u001b[A\n",
            " 17%|█▋        | 28/165 [00:03<00:14,  9.39it/s]\u001b[A\n",
            " 18%|█▊        | 29/165 [00:03<00:14,  9.41it/s]\u001b[A\n",
            " 18%|█▊        | 30/165 [00:03<00:14,  9.38it/s]\u001b[A\n",
            " 19%|█▉        | 31/165 [00:03<00:14,  9.36it/s]\u001b[A\n",
            " 19%|█▉        | 32/165 [00:03<00:14,  9.36it/s]\u001b[A\n",
            " 20%|██        | 33/165 [00:03<00:14,  9.33it/s]\u001b[A\n",
            " 21%|██        | 34/165 [00:03<00:14,  9.34it/s]\u001b[A\n",
            " 21%|██        | 35/165 [00:03<00:13,  9.34it/s]\u001b[A\n",
            " 22%|██▏       | 36/165 [00:03<00:13,  9.34it/s]\u001b[A\n",
            " 22%|██▏       | 37/165 [00:03<00:13,  9.35it/s]\u001b[A\n",
            " 23%|██▎       | 38/165 [00:04<00:17,  7.32it/s]\u001b[A\n",
            " 24%|██▎       | 39/165 [00:04<00:16,  7.84it/s]\u001b[A\n",
            " 24%|██▍       | 40/165 [00:04<00:15,  8.23it/s]\u001b[A\n",
            " 25%|██▍       | 41/165 [00:04<00:14,  8.57it/s]\u001b[A\n",
            " 25%|██▌       | 42/165 [00:04<00:13,  8.80it/s]\u001b[A\n",
            " 26%|██▌       | 43/165 [00:04<00:13,  8.98it/s]\u001b[A\n",
            " 27%|██▋       | 44/165 [00:04<00:13,  9.10it/s]\u001b[A\n",
            " 27%|██▋       | 45/165 [00:04<00:13,  9.17it/s]\u001b[A\n",
            " 28%|██▊       | 46/165 [00:05<00:12,  9.24it/s]\u001b[A\n",
            " 28%|██▊       | 47/165 [00:05<00:12,  9.28it/s]\u001b[A\n",
            " 29%|██▉       | 48/165 [00:05<00:12,  9.33it/s]\u001b[A\n",
            " 30%|██▉       | 49/165 [00:05<00:12,  9.33it/s]\u001b[A\n",
            " 30%|███       | 50/165 [00:05<00:12,  9.34it/s]\u001b[A\n",
            " 31%|███       | 51/165 [00:05<00:12,  9.34it/s]\u001b[A\n",
            " 32%|███▏      | 52/165 [00:05<00:12,  9.36it/s]\u001b[A\n",
            " 32%|███▏      | 53/165 [00:05<00:11,  9.38it/s]\u001b[A\n",
            " 33%|███▎      | 54/165 [00:05<00:11,  9.39it/s]\u001b[A\n",
            " 33%|███▎      | 55/165 [00:05<00:11,  9.38it/s]\u001b[A\n",
            " 34%|███▍      | 56/165 [00:06<00:11,  9.37it/s]\u001b[A\n",
            " 35%|███▍      | 57/165 [00:06<00:11,  9.39it/s]\u001b[A\n",
            " 35%|███▌      | 58/165 [00:06<00:11,  9.40it/s]\u001b[A\n",
            " 36%|███▌      | 59/165 [00:06<00:11,  9.41it/s]\u001b[A\n",
            " 36%|███▋      | 60/165 [00:06<00:11,  9.41it/s]\u001b[A\n",
            " 37%|███▋      | 61/165 [00:06<00:11,  9.39it/s]\u001b[A\n",
            " 38%|███▊      | 62/165 [00:06<00:10,  9.38it/s]\u001b[A\n",
            " 38%|███▊      | 63/165 [00:06<00:10,  9.36it/s]\u001b[A\n",
            " 39%|███▉      | 64/165 [00:06<00:10,  9.35it/s]\u001b[A\n",
            " 39%|███▉      | 65/165 [00:07<00:10,  9.34it/s]\u001b[A\n",
            " 40%|████      | 66/165 [00:07<00:10,  9.34it/s]\u001b[A\n",
            " 41%|████      | 67/165 [00:07<00:10,  9.35it/s]\u001b[A\n",
            " 41%|████      | 68/165 [00:07<00:10,  9.35it/s]\u001b[A\n",
            " 42%|████▏     | 69/165 [00:07<00:10,  9.34it/s]\u001b[A\n",
            " 42%|████▏     | 70/165 [00:07<00:10,  9.37it/s]\u001b[A\n",
            " 43%|████▎     | 71/165 [00:07<00:10,  9.40it/s]\u001b[A\n",
            " 44%|████▎     | 72/165 [00:07<00:09,  9.40it/s]\u001b[A\n",
            " 44%|████▍     | 73/165 [00:07<00:09,  9.38it/s]\u001b[A\n",
            " 45%|████▍     | 74/165 [00:08<00:09,  9.40it/s]\u001b[A\n",
            " 45%|████▌     | 75/165 [00:08<00:09,  9.39it/s]\u001b[A\n",
            " 46%|████▌     | 76/165 [00:08<00:09,  9.39it/s]\u001b[A\n",
            " 47%|████▋     | 77/165 [00:08<00:09,  9.40it/s]\u001b[A\n",
            " 47%|████▋     | 78/165 [00:08<00:09,  9.42it/s]\u001b[A\n",
            " 48%|████▊     | 79/165 [00:08<00:09,  9.43it/s]\u001b[A\n",
            " 48%|████▊     | 80/165 [00:08<00:09,  9.44it/s]\u001b[A\n",
            " 49%|████▉     | 81/165 [00:08<00:08,  9.40it/s]\u001b[A\n",
            " 50%|████▉     | 82/165 [00:08<00:08,  9.40it/s]\u001b[A\n",
            " 50%|█████     | 83/165 [00:08<00:08,  9.40it/s]\u001b[A\n",
            " 51%|█████     | 84/165 [00:09<00:08,  9.40it/s]\u001b[A\n",
            " 52%|█████▏    | 85/165 [00:09<00:08,  9.42it/s]\u001b[A\n",
            " 52%|█████▏    | 86/165 [00:09<00:08,  9.42it/s]\u001b[A\n",
            " 53%|█████▎    | 87/165 [00:09<00:08,  9.42it/s]\u001b[A\n",
            " 53%|█████▎    | 88/165 [00:09<00:08,  9.40it/s]\u001b[A\n",
            " 54%|█████▍    | 89/165 [00:09<00:08,  9.37it/s]\u001b[A\n",
            " 55%|█████▍    | 90/165 [00:09<00:08,  9.36it/s]\u001b[A\n",
            " 55%|█████▌    | 91/165 [00:09<00:07,  9.40it/s]\u001b[A\n",
            " 56%|█████▌    | 92/165 [00:09<00:07,  9.39it/s]\u001b[A\n",
            " 56%|█████▋    | 93/165 [00:10<00:07,  9.39it/s]\u001b[A\n",
            " 57%|█████▋    | 94/165 [00:10<00:07,  9.41it/s]\u001b[A\n",
            " 58%|█████▊    | 95/165 [00:10<00:07,  9.41it/s]\u001b[A\n",
            " 58%|█████▊    | 96/165 [00:10<00:07,  9.39it/s]\u001b[A\n",
            " 59%|█████▉    | 97/165 [00:10<00:07,  9.39it/s]\u001b[A\n",
            " 59%|█████▉    | 98/165 [00:10<00:07,  9.39it/s]\u001b[A\n",
            " 60%|██████    | 99/165 [00:10<00:07,  9.38it/s]\u001b[A\n",
            " 61%|██████    | 100/165 [00:10<00:06,  9.39it/s]\u001b[A\n",
            " 61%|██████    | 101/165 [00:10<00:06,  9.39it/s]\u001b[A\n",
            " 62%|██████▏   | 102/165 [00:10<00:06,  9.38it/s]\u001b[A\n",
            " 62%|██████▏   | 103/165 [00:11<00:06,  9.38it/s]\u001b[A\n",
            " 63%|██████▎   | 104/165 [00:11<00:06,  9.40it/s]\u001b[A\n",
            " 64%|██████▎   | 105/165 [00:11<00:06,  9.40it/s]\u001b[A\n",
            " 64%|██████▍   | 106/165 [00:11<00:06,  9.40it/s]\u001b[A\n",
            " 65%|██████▍   | 107/165 [00:11<00:06,  9.40it/s]\u001b[A\n",
            " 65%|██████▌   | 108/165 [00:11<00:06,  9.38it/s]\u001b[A\n",
            " 66%|██████▌   | 109/165 [00:11<00:05,  9.38it/s]\u001b[A\n",
            " 67%|██████▋   | 110/165 [00:11<00:05,  9.38it/s]\u001b[A\n",
            " 67%|██████▋   | 111/165 [00:11<00:05,  9.38it/s]\u001b[A\n",
            " 68%|██████▊   | 112/165 [00:12<00:05,  9.33it/s]\u001b[A\n",
            " 68%|██████▊   | 113/165 [00:12<00:05,  9.35it/s]\u001b[A\n",
            " 69%|██████▉   | 114/165 [00:12<00:05,  9.36it/s]\u001b[A\n",
            " 70%|██████▉   | 115/165 [00:12<00:05,  9.37it/s]\u001b[A\n",
            " 70%|███████   | 116/165 [00:12<00:05,  9.39it/s]\u001b[A\n",
            " 71%|███████   | 117/165 [00:12<00:05,  9.40it/s]\u001b[A\n",
            " 72%|███████▏  | 118/165 [00:12<00:05,  9.39it/s]\u001b[A\n",
            " 72%|███████▏  | 119/165 [00:12<00:04,  9.39it/s]\u001b[A\n",
            " 73%|███████▎  | 120/165 [00:12<00:04,  9.39it/s]\u001b[A\n",
            " 73%|███████▎  | 121/165 [00:13<00:04,  9.40it/s]\u001b[A\n",
            " 74%|███████▍  | 122/165 [00:13<00:04,  9.38it/s]\u001b[A\n",
            " 75%|███████▍  | 123/165 [00:13<00:04,  9.39it/s]\u001b[A\n",
            " 75%|███████▌  | 124/165 [00:13<00:04,  9.38it/s]\u001b[A\n",
            " 76%|███████▌  | 125/165 [00:13<00:04,  9.40it/s]\u001b[A\n",
            " 76%|███████▋  | 126/165 [00:13<00:04,  9.40it/s]\u001b[A\n",
            " 77%|███████▋  | 127/165 [00:13<00:04,  9.40it/s]\u001b[A\n",
            " 78%|███████▊  | 128/165 [00:13<00:03,  9.39it/s]\u001b[A\n",
            " 78%|███████▊  | 129/165 [00:13<00:03,  9.40it/s]\u001b[A\n",
            " 79%|███████▉  | 130/165 [00:13<00:03,  9.40it/s]\u001b[A\n",
            " 79%|███████▉  | 131/165 [00:14<00:03,  9.40it/s]\u001b[A\n",
            " 80%|████████  | 132/165 [00:14<00:03,  9.37it/s]\u001b[A\n",
            " 81%|████████  | 133/165 [00:14<00:03,  9.39it/s]\u001b[A\n",
            " 81%|████████  | 134/165 [00:14<00:03,  9.40it/s]\u001b[A\n",
            " 82%|████████▏ | 135/165 [00:14<00:03,  9.38it/s]\u001b[A\n",
            " 82%|████████▏ | 136/165 [00:14<00:03,  9.38it/s]\u001b[A\n",
            " 83%|████████▎ | 137/165 [00:14<00:02,  9.36it/s]\u001b[A\n",
            " 84%|████████▎ | 138/165 [00:14<00:02,  9.38it/s]\u001b[A\n",
            " 84%|████████▍ | 139/165 [00:14<00:02,  9.37it/s]\u001b[A\n",
            " 85%|████████▍ | 140/165 [00:15<00:02,  9.37it/s]\u001b[A\n",
            " 85%|████████▌ | 141/165 [00:15<00:02,  9.38it/s]\u001b[A\n",
            " 86%|████████▌ | 142/165 [00:15<00:02,  9.38it/s]\u001b[A\n",
            " 87%|████████▋ | 143/165 [00:15<00:02,  9.36it/s]\u001b[A\n",
            " 87%|████████▋ | 144/165 [00:15<00:02,  9.34it/s]\u001b[A\n",
            " 88%|████████▊ | 145/165 [00:15<00:02,  9.36it/s]\u001b[A\n",
            " 88%|████████▊ | 146/165 [00:15<00:02,  9.37it/s]\u001b[A\n",
            " 89%|████████▉ | 147/165 [00:15<00:01,  9.37it/s]\u001b[A\n",
            " 90%|████████▉ | 148/165 [00:15<00:01,  9.38it/s]\u001b[A\n",
            " 90%|█████████ | 149/165 [00:16<00:01,  9.39it/s]\u001b[A\n",
            " 91%|█████████ | 150/165 [00:16<00:01,  9.40it/s]\u001b[A\n",
            " 92%|█████████▏| 151/165 [00:16<00:01,  9.39it/s]\u001b[A\n",
            " 92%|█████████▏| 152/165 [00:16<00:01,  9.39it/s]\u001b[A\n",
            " 93%|█████████▎| 153/165 [00:16<00:01,  9.36it/s]\u001b[A\n",
            " 93%|█████████▎| 154/165 [00:16<00:01,  9.36it/s]\u001b[A\n",
            " 94%|█████████▍| 155/165 [00:16<00:01,  9.35it/s]\u001b[A\n",
            " 95%|█████████▍| 156/165 [00:16<00:00,  9.36it/s]\u001b[A\n",
            " 95%|█████████▌| 157/165 [00:16<00:00,  9.36it/s]\u001b[A\n",
            " 96%|█████████▌| 158/165 [00:16<00:00,  9.38it/s]\u001b[A\n",
            " 96%|█████████▋| 159/165 [00:17<00:00,  9.39it/s]\u001b[A\n",
            " 97%|█████████▋| 160/165 [00:17<00:00,  9.39it/s]\u001b[A\n",
            " 98%|█████████▊| 161/165 [00:17<00:00,  9.39it/s]\u001b[A\n",
            " 98%|█████████▊| 162/165 [00:17<00:00,  9.39it/s]\u001b[A\n",
            " 99%|█████████▉| 163/165 [00:17<00:00,  9.40it/s]\u001b[A\n",
            "100%|██████████| 165/165 [00:17<00:00,  9.31it/s]\n",
            "100%|██████████| 3/3 [2:16:09<00:00, 2723.27s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "Z6WbFMriHy-1"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "TARGETS=['Features',\t'Material','Usability']"
      ],
      "metadata": {
        "id": "vB2my-jgHy7a"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for Target in tqdm(TARGETS):\n",
        "    def init_logger(log_file=OUTPUT_DIR + \"train.log\"):\n",
        "        from logging import INFO, FileHandler, Formatter, StreamHandler, getLogger\n",
        "\n",
        "        logger = getLogger(__name__)\n",
        "        logger.setLevel(INFO)\n",
        "        handler1 = StreamHandler()\n",
        "        handler1.setFormatter(Formatter(\"%(message)s\"))\n",
        "        handler2 = FileHandler(filename=log_file)\n",
        "        handler2.setFormatter(Formatter(\"%(message)s\"))\n",
        "        logger.addHandler(handler1)\n",
        "        logger.addHandler(handler2)\n",
        "        return logger\n",
        "\n",
        "    LOGGER = init_logger()\n",
        "\n",
        "    def seed_torch(seed=13):\n",
        "        random.seed(seed)\n",
        "        os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
        "        np.random.seed(seed)\n",
        "        torch.manual_seed(seed)\n",
        "        torch.cuda.manual_seed(seed)\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "        torch.backends.cudnn.deterministic = True\n",
        "\n",
        "    seed = 13\n",
        "    seed_torch(seed)\n",
        "\n",
        "    train = pd.read_csv(DATA_DIR + \"train.csv\",usecols=['Review',Target])\n",
        "    test = pd.read_csv(DATA_DIR + \"test.csv\",usecols=['Review',Target])\n",
        "    train['Review']=train['Review'].str.lower()\n",
        "    test['Review']=test['Review'].str.lower()\n",
        "    sub = pd.read_csv(\"/content/submission.csv\")\n",
        "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "    \n",
        "    def get_train_data(train):\n",
        "\n",
        "        Fold = StratifiedKFold(n_splits=5, shuffle=True, random_state=seed)\n",
        "        for n, (train_index, val_index) in enumerate(Fold.split(train, train[Target])):\n",
        "            train.loc[val_index, \"fold\"] = int(n)\n",
        "        train[\"fold\"] = train[\"fold\"].astype(np.uint8)\n",
        "\n",
        "        return train\n",
        "\n",
        "    def get_test_data(test):\n",
        "        return test\n",
        "\n",
        "    train = get_train_data(train)\n",
        "\n",
        "    class BaseDataset(Dataset):\n",
        "        def __init__(self, df, model_name, include_labels=True):\n",
        "            tokenizer = T.BertTokenizer.from_pretrained(model_name)\n",
        "\n",
        "            self.df = df\n",
        "            self.include_labels = include_labels\n",
        "\n",
        "            self.title = df[\"Review\"].tolist()\n",
        "            self.encoded = tokenizer.batch_encode_plus(\n",
        "                self.title,\n",
        "                padding = 'max_length',            \n",
        "                max_length = 208,\n",
        "                truncation = True,\n",
        "                return_attention_mask=True\n",
        "            )\n",
        "            \n",
        "            if self.include_labels:\n",
        "                self.labels = df[Target].values\n",
        "\n",
        "        def __len__(self):\n",
        "            return len(self.df)\n",
        "\n",
        "        def __getitem__(self, idx):\n",
        "            input_ids = torch.tensor(self.encoded['input_ids'][idx])\n",
        "            attention_mask = torch.tensor(self.encoded['attention_mask'][idx])\n",
        "\n",
        "            if self.include_labels:\n",
        "                label = torch.tensor(self.labels[idx]).float()\n",
        "                return input_ids, attention_mask, label\n",
        "\n",
        "            return input_ids, attention_mask\n",
        "\n",
        "    class BaseModel(nn.Module):\n",
        "        def __init__(self, model_name):\n",
        "            super().__init__()\n",
        "\n",
        "            self.model = T.BertForSequenceClassification.from_pretrained(model_name, num_labels=1)\n",
        "            self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "        def forward(self, input_ids, attention_mask):\n",
        "            out = self.model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "            out = self.sigmoid(out.logits).squeeze()\n",
        "\n",
        "            return out\n",
        "\n",
        "    class AverageMeter(object):\n",
        "        \"\"\"Computes and stores the average and current value\"\"\"\n",
        "\n",
        "        def __init__(self):\n",
        "            self.reset()\n",
        "\n",
        "        def reset(self):\n",
        "            self.val = 0\n",
        "            self.avg = 0\n",
        "            self.sum = 0\n",
        "            self.count = 0\n",
        "\n",
        "        def update(self, val, n=1):\n",
        "            self.val = val\n",
        "            self.sum += val * n\n",
        "            self.count += n\n",
        "            self.avg = self.sum / self.count\n",
        "\n",
        "\n",
        "    def asMinutes(s):\n",
        "        m = math.floor(s / 60)\n",
        "        s -= m * 60\n",
        "        return \"%dm %ds\" % (m, s)\n",
        "\n",
        "\n",
        "    def timeSince(since, percent):\n",
        "        now = time.time()\n",
        "        s = now - since\n",
        "        es = s / (percent)\n",
        "        rs = es - s\n",
        "        return \"%s (remain %s)\" % (asMinutes(s), asMinutes(rs))\n",
        "\n",
        "    def train_fn(train_loader, model, criterion, optimizer, epoch, device):\n",
        "        start = end = time.time()\n",
        "        losses = AverageMeter()\n",
        "\n",
        "        # switch to train mode\n",
        "        model.train()\n",
        "\n",
        "        for step, (input_ids, attention_mask, labels) in enumerate(train_loader):\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            input_ids = input_ids.to(device)\n",
        "            attention_mask = attention_mask.to(device)\n",
        "            labels = labels.to(device)\n",
        "            batch_size = labels.size(0)\n",
        "\n",
        "            y_preds = model(input_ids, attention_mask)\n",
        "\n",
        "            loss = criterion(y_preds, labels)\n",
        "\n",
        "            # record loss\n",
        "            losses.update(loss.item(), batch_size)\n",
        "            loss.backward()\n",
        "\n",
        "            optimizer.step()\n",
        "\n",
        "            if step % 100 == 0 or step == (len(train_loader) - 1):\n",
        "                print(\n",
        "                    f\"Epoch: [{epoch + 1}][{step}/{len(train_loader)}] \"\n",
        "                    f\"Elapsed {timeSince(start, float(step + 1) / len(train_loader)):s} \"\n",
        "                    f\"Loss: {losses.avg:.4f} \"\n",
        "                )\n",
        "\n",
        "        return losses.avg\n",
        "\n",
        "    def valid_fn(valid_loader, model, criterion, device):\n",
        "        start = end = time.time()\n",
        "        losses = AverageMeter()\n",
        "\n",
        "        # switch to evaluation mode\n",
        "        model.eval()\n",
        "        preds = []\n",
        "\n",
        "        for step, (input_ids, attention_mask, labels) in enumerate(valid_loader):\n",
        "            input_ids = input_ids.to(device)\n",
        "            attention_mask = attention_mask.to(device)\n",
        "            labels = labels.to(device)\n",
        "            batch_size = labels.size(0)\n",
        "            # compute loss\n",
        "            with torch.no_grad():\n",
        "                y_preds = model(input_ids, attention_mask)\n",
        "            \n",
        "            loss = criterion(y_preds, labels)\n",
        "            losses.update(loss.item(), batch_size)\n",
        "\n",
        "            # record score\n",
        "            preds.append(y_preds.to(\"cpu\").numpy())\n",
        "\n",
        "            if step % 100 == 0 or step == (len(valid_loader) - 1):\n",
        "                print(\n",
        "                    f\"EVAL: [{step}/{len(valid_loader)}] \"\n",
        "                    f\"Elapsed {timeSince(start, float(step + 1) / len(valid_loader)):s} \"\n",
        "                    f\"Loss: {losses.avg:.4f} \"\n",
        "                )\n",
        "\n",
        "        predictions = np.concatenate(preds)\n",
        "        return losses.avg, predictions\n",
        "\n",
        "    def inference():\n",
        "        predictions = []\n",
        "\n",
        "        test_dataset = BaseDataset(test, MODEL_NAME, include_labels=False)\n",
        "        test_loader = DataLoader(\n",
        "            test_dataset, batch_size=16, shuffle=False, num_workers=4, pin_memory=True\n",
        "        )\n",
        "\n",
        "        for fold in range(5):\n",
        "            LOGGER.info(f\"========== model: bert-base-uncased fold: {fold} inference ==========\")\n",
        "            model = BaseModel(MODEL_NAME)\n",
        "            model.to(device)\n",
        "            model.load_state_dict(torch.load(OUTPUT_DIR + f\"bert-base-uncased_fold{fold}_best.pth\")[\"model\"])\n",
        "            model.eval()\n",
        "            preds = []\n",
        "            for i, (input_ids, attention_mask) in tqdm(enumerate(test_loader), total=len(test_loader)):\n",
        "                input_ids = input_ids.to(device)\n",
        "                attention_mask = attention_mask.to(device)\n",
        "                with torch.no_grad():\n",
        "                    y_preds = model(input_ids, attention_mask)\n",
        "                preds.append(y_preds.to(\"cpu\").numpy())\n",
        "            preds = np.concatenate(preds)\n",
        "            predictions.append(preds)\n",
        "        predictions = np.mean(predictions, axis=0)\n",
        "\n",
        "        return predictions\n",
        "\n",
        "    def train_loop(train, fold):\n",
        "\n",
        "        LOGGER.info(f\"========== fold: {fold} training ==========\")\n",
        "\n",
        "        # ====================================================\n",
        "        # Data Loader\n",
        "        # ====================================================\n",
        "        trn_idx = train[train[\"fold\"] != fold].index\n",
        "        val_idx = train[train[\"fold\"] == fold].index\n",
        "\n",
        "        train_folds = train.loc[trn_idx].reset_index(drop=True)\n",
        "        valid_folds = train.loc[val_idx].reset_index(drop=True)\n",
        "\n",
        "        train_dataset = BaseDataset(train_folds,MODEL_NAME)\n",
        "        valid_dataset = BaseDataset(valid_folds, MODEL_NAME)\n",
        "\n",
        "        train_loader = DataLoader(\n",
        "            train_dataset,\n",
        "            batch_size=16,\n",
        "            shuffle=True,\n",
        "            num_workers=4,\n",
        "            pin_memory=True,\n",
        "            drop_last=True,\n",
        "        )\n",
        "        valid_loader = DataLoader(\n",
        "            valid_dataset,\n",
        "            batch_size=16,\n",
        "            shuffle=False,\n",
        "            num_workers=4,\n",
        "            pin_memory=True,\n",
        "            drop_last=False,\n",
        "        )\n",
        "\n",
        "        # ====================================================\n",
        "        # Model\n",
        "        # ====================================================\n",
        "        model = BaseModel(MODEL_NAME)\n",
        "        model.to(device)\n",
        "\n",
        "        optimizer = T.AdamW(model.parameters(),lr=2e-5)\n",
        "\n",
        "        criterion = nn.BCELoss()\n",
        "\n",
        "        # ====================================================\n",
        "        # Loop\n",
        "        # ====================================================\n",
        "        best_score = 100\n",
        "        best_loss = np.inf\n",
        "\n",
        "        for epoch in range(5):\n",
        "            start_time = time.time()\n",
        "            \n",
        "            # train\n",
        "            avg_loss = train_fn(train_loader, model, criterion, optimizer, epoch, device)\n",
        "\n",
        "            # eval\n",
        "            avg_val_loss, preds = valid_fn(valid_loader, model, criterion, device)\n",
        "            valid_labels = valid_folds[Target].values\n",
        "\n",
        "            # scoring\n",
        "            score = log_loss(valid_labels, preds)\n",
        "            print(f\"Log loss for {epoch} is {score}\")\n",
        "            elapsed = time.time() - start_time\n",
        "            LOGGER.info(\n",
        "                f\"Epoch {epoch+1} - avg_train_loss: {avg_loss:.4f}  avg_val_loss: {avg_val_loss:.4f}  time: {elapsed:.0f}s\"\n",
        "            )\n",
        "            LOGGER.info(f\"Epoch {epoch+1} - Score: {score}\")\n",
        "\n",
        "            if score < best_score:\n",
        "                best_score = score\n",
        "                LOGGER.info(f\"Epoch {epoch+1} - Save Best Score: {best_score:.4f} Model\")\n",
        "                torch.save(\n",
        "                    {\"model\": model.state_dict(), \"preds\": preds}, OUTPUT_DIR + f\"bert-base-uncased_fold{fold}_best.pth\"\n",
        "                )\n",
        "\n",
        "        check_point = torch.load(OUTPUT_DIR + f\"bert-base-uncased_fold{fold}_best.pth\")\n",
        "\n",
        "        valid_folds[\"preds\"] = check_point[\"preds\"]\n",
        "\n",
        "        return valid_folds\n",
        "\n",
        "    from sklearn.metrics import log_loss\n",
        "    def get_result(result_df):\n",
        "        preds = result_df[\"preds\"].values\n",
        "        labels = result_df[Target].values\n",
        "        score = log_loss(labels, preds)\n",
        "        LOGGER.info(f\"Score: {score:<.5f}\")\n",
        "\n",
        "    def main():\n",
        "        # Training\n",
        "        oof_df = pd.DataFrame()\n",
        "        for fold in range(5):\n",
        "            _oof_df = train_loop(train, fold)\n",
        "            oof_df = pd.concat([oof_df, _oof_df])\n",
        "            LOGGER.info(f\"========== fold: {fold} result ==========\")\n",
        "            get_result(_oof_df)\n",
        "            \n",
        "        # CV result\n",
        "        LOGGER.info(f\"========== CV ==========\")\n",
        "        get_result(oof_df)\n",
        "        \n",
        "        # Save OOF result\n",
        "        oof_df.to_csv(OUTPUT_DIR + \"oof_df.csv\", index=False)\n",
        "\n",
        "        # Inference\n",
        "        predictions = inference()\n",
        "        \n",
        "        # submission\n",
        "        sub[Target] = predictions\n",
        "        sub.to_csv('/content/submission.csv', index=False)\n",
        "\n",
        "    if __name__ == \"__main__\":\n",
        "        main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5I2cu2TrIMPk",
        "outputId": "dc3c7651-0a1c-490b-b41b-309e0c1f8f0b"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/3 [00:00<?, ?it/s]Didn't find file /content/activebus_bert_final/added_tokens.json. We won't load it.\n",
            "loading file /content/activebus_bert_final/vocab.txt\n",
            "loading file /content/activebus_bert_final/tokenizer.json\n",
            "loading file None\n",
            "loading file /content/activebus_bert_final/special_tokens_map.json\n",
            "loading file /content/activebus_bert_final/tokenizer_config.json\n",
            "========== fold: 0 training ==========\n",
            "========== fold: 0 training ==========\n",
            "========== fold: 0 training ==========\n",
            "========== fold: 0 training ==========\n",
            "========== fold: 0 training ==========\n",
            "========== fold: 0 training ==========\n",
            "========== fold: 0 training ==========\n",
            "========== fold: 0 training ==========\n",
            "========== fold: 0 training ==========\n",
            "========== fold: 0 training ==========\n",
            "Didn't find file /content/activebus_bert_final/added_tokens.json. We won't load it.\n",
            "loading file /content/activebus_bert_final/vocab.txt\n",
            "loading file None\n",
            "loading file /content/activebus_bert_final/special_tokens_map.json\n",
            "loading file /content/activebus_bert_final/tokenizer_config.json\n",
            "loading file /content/activebus_bert_final/tokenizer.json\n",
            "Didn't find file /content/activebus_bert_final/added_tokens.json. We won't load it.\n",
            "loading file /content/activebus_bert_final/vocab.txt\n",
            "loading file None\n",
            "loading file /content/activebus_bert_final/special_tokens_map.json\n",
            "loading file /content/activebus_bert_final/tokenizer_config.json\n",
            "loading file /content/activebus_bert_final/tokenizer.json\n",
            "loading configuration file /content/activebus_bert_final/config.json\n",
            "Model config BertConfig {\n",
            "  \"_name_or_path\": \"activebus/BERT-XD_Review\",\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_ids\": 0,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.15.0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "loading weights file /content/activebus_bert_final/pytorch_model.bin\n",
            "Some weights of the model checkpoint at /content/activebus_bert_final were not used when initializing BertForSequenceClassification: ['cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /content/activebus_bert_final and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: [1][0/306] Elapsed 0m 0s (remain 3m 10s) Loss: 0.6812 \n",
            "Epoch: [1][100/306] Elapsed 0m 34s (remain 1m 9s) Loss: 0.2142 \n",
            "Epoch: [1][200/306] Elapsed 1m 7s (remain 0m 35s) Loss: 0.1841 \n",
            "Epoch: [1][300/306] Elapsed 1m 40s (remain 0m 1s) Loss: 0.1668 \n",
            "Epoch: [1][305/306] Elapsed 1m 42s (remain 0m 0s) Loss: 0.1664 \n",
            "EVAL: [0/77] Elapsed 0m 0s (remain 0m 27s) Loss: 0.0588 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1 - avg_train_loss: 0.1664  avg_val_loss: 0.1021  time: 111s\n",
            "Epoch 1 - avg_train_loss: 0.1664  avg_val_loss: 0.1021  time: 111s\n",
            "Epoch 1 - avg_train_loss: 0.1664  avg_val_loss: 0.1021  time: 111s\n",
            "Epoch 1 - avg_train_loss: 0.1664  avg_val_loss: 0.1021  time: 111s\n",
            "Epoch 1 - avg_train_loss: 0.1664  avg_val_loss: 0.1021  time: 111s\n",
            "Epoch 1 - avg_train_loss: 0.1664  avg_val_loss: 0.1021  time: 111s\n",
            "Epoch 1 - avg_train_loss: 0.1664  avg_val_loss: 0.1021  time: 111s\n",
            "Epoch 1 - avg_train_loss: 0.1664  avg_val_loss: 0.1021  time: 111s\n",
            "Epoch 1 - avg_train_loss: 0.1664  avg_val_loss: 0.1021  time: 111s\n",
            "Epoch 1 - avg_train_loss: 0.1664  avg_val_loss: 0.1021  time: 111s\n",
            "Epoch 1 - Score: 0.10209462196256734\n",
            "Epoch 1 - Score: 0.10209462196256734\n",
            "Epoch 1 - Score: 0.10209462196256734\n",
            "Epoch 1 - Score: 0.10209462196256734\n",
            "Epoch 1 - Score: 0.10209462196256734\n",
            "Epoch 1 - Score: 0.10209462196256734\n",
            "Epoch 1 - Score: 0.10209462196256734\n",
            "Epoch 1 - Score: 0.10209462196256734\n",
            "Epoch 1 - Score: 0.10209462196256734\n",
            "Epoch 1 - Score: 0.10209462196256734\n",
            "Epoch 1 - Save Best Score: 0.1021 Model\n",
            "Epoch 1 - Save Best Score: 0.1021 Model\n",
            "Epoch 1 - Save Best Score: 0.1021 Model\n",
            "Epoch 1 - Save Best Score: 0.1021 Model\n",
            "Epoch 1 - Save Best Score: 0.1021 Model\n",
            "Epoch 1 - Save Best Score: 0.1021 Model\n",
            "Epoch 1 - Save Best Score: 0.1021 Model\n",
            "Epoch 1 - Save Best Score: 0.1021 Model\n",
            "Epoch 1 - Save Best Score: 0.1021 Model\n",
            "Epoch 1 - Save Best Score: 0.1021 Model\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EVAL: [76/77] Elapsed 0m 8s (remain 0m 0s) Loss: 0.1021 \n",
            "Log loss for 0 is 0.10209462196256734\n",
            "Epoch: [2][0/306] Elapsed 0m 0s (remain 3m 5s) Loss: 0.0632 \n",
            "Epoch: [2][100/306] Elapsed 0m 34s (remain 1m 9s) Loss: 0.0807 \n",
            "Epoch: [2][200/306] Elapsed 1m 7s (remain 0m 35s) Loss: 0.0892 \n",
            "Epoch: [2][300/306] Elapsed 1m 40s (remain 0m 1s) Loss: 0.0856 \n",
            "Epoch: [2][305/306] Elapsed 1m 42s (remain 0m 0s) Loss: 0.0853 \n",
            "EVAL: [0/77] Elapsed 0m 0s (remain 0m 26s) Loss: 0.1449 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2 - avg_train_loss: 0.0853  avg_val_loss: 0.1063  time: 111s\n",
            "Epoch 2 - avg_train_loss: 0.0853  avg_val_loss: 0.1063  time: 111s\n",
            "Epoch 2 - avg_train_loss: 0.0853  avg_val_loss: 0.1063  time: 111s\n",
            "Epoch 2 - avg_train_loss: 0.0853  avg_val_loss: 0.1063  time: 111s\n",
            "Epoch 2 - avg_train_loss: 0.0853  avg_val_loss: 0.1063  time: 111s\n",
            "Epoch 2 - avg_train_loss: 0.0853  avg_val_loss: 0.1063  time: 111s\n",
            "Epoch 2 - avg_train_loss: 0.0853  avg_val_loss: 0.1063  time: 111s\n",
            "Epoch 2 - avg_train_loss: 0.0853  avg_val_loss: 0.1063  time: 111s\n",
            "Epoch 2 - avg_train_loss: 0.0853  avg_val_loss: 0.1063  time: 111s\n",
            "Epoch 2 - avg_train_loss: 0.0853  avg_val_loss: 0.1063  time: 111s\n",
            "Epoch 2 - Score: 0.10625035424232529\n",
            "Epoch 2 - Score: 0.10625035424232529\n",
            "Epoch 2 - Score: 0.10625035424232529\n",
            "Epoch 2 - Score: 0.10625035424232529\n",
            "Epoch 2 - Score: 0.10625035424232529\n",
            "Epoch 2 - Score: 0.10625035424232529\n",
            "Epoch 2 - Score: 0.10625035424232529\n",
            "Epoch 2 - Score: 0.10625035424232529\n",
            "Epoch 2 - Score: 0.10625035424232529\n",
            "Epoch 2 - Score: 0.10625035424232529\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EVAL: [76/77] Elapsed 0m 8s (remain 0m 0s) Loss: 0.1063 \n",
            "Log loss for 1 is 0.10625035424232529\n",
            "Epoch: [3][0/306] Elapsed 0m 0s (remain 3m 13s) Loss: 0.1625 \n",
            "Epoch: [3][100/306] Elapsed 0m 34s (remain 1m 9s) Loss: 0.0358 \n",
            "Epoch: [3][200/306] Elapsed 1m 7s (remain 0m 35s) Loss: 0.0414 \n",
            "Epoch: [3][300/306] Elapsed 1m 40s (remain 0m 1s) Loss: 0.0461 \n",
            "Epoch: [3][305/306] Elapsed 1m 42s (remain 0m 0s) Loss: 0.0464 \n",
            "EVAL: [0/77] Elapsed 0m 0s (remain 0m 28s) Loss: 0.1033 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3 - avg_train_loss: 0.0464  avg_val_loss: 0.1281  time: 111s\n",
            "Epoch 3 - avg_train_loss: 0.0464  avg_val_loss: 0.1281  time: 111s\n",
            "Epoch 3 - avg_train_loss: 0.0464  avg_val_loss: 0.1281  time: 111s\n",
            "Epoch 3 - avg_train_loss: 0.0464  avg_val_loss: 0.1281  time: 111s\n",
            "Epoch 3 - avg_train_loss: 0.0464  avg_val_loss: 0.1281  time: 111s\n",
            "Epoch 3 - avg_train_loss: 0.0464  avg_val_loss: 0.1281  time: 111s\n",
            "Epoch 3 - avg_train_loss: 0.0464  avg_val_loss: 0.1281  time: 111s\n",
            "Epoch 3 - avg_train_loss: 0.0464  avg_val_loss: 0.1281  time: 111s\n",
            "Epoch 3 - avg_train_loss: 0.0464  avg_val_loss: 0.1281  time: 111s\n",
            "Epoch 3 - avg_train_loss: 0.0464  avg_val_loss: 0.1281  time: 111s\n",
            "Epoch 3 - Score: 0.12805250467433357\n",
            "Epoch 3 - Score: 0.12805250467433357\n",
            "Epoch 3 - Score: 0.12805250467433357\n",
            "Epoch 3 - Score: 0.12805250467433357\n",
            "Epoch 3 - Score: 0.12805250467433357\n",
            "Epoch 3 - Score: 0.12805250467433357\n",
            "Epoch 3 - Score: 0.12805250467433357\n",
            "Epoch 3 - Score: 0.12805250467433357\n",
            "Epoch 3 - Score: 0.12805250467433357\n",
            "Epoch 3 - Score: 0.12805250467433357\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EVAL: [76/77] Elapsed 0m 8s (remain 0m 0s) Loss: 0.1281 \n",
            "Log loss for 2 is 0.12805250467433357\n",
            "Epoch: [4][0/306] Elapsed 0m 0s (remain 3m 13s) Loss: 0.0409 \n",
            "Epoch: [4][100/306] Elapsed 0m 34s (remain 1m 9s) Loss: 0.0224 \n",
            "Epoch: [4][200/306] Elapsed 1m 7s (remain 0m 35s) Loss: 0.0223 \n",
            "Epoch: [4][300/306] Elapsed 1m 41s (remain 0m 1s) Loss: 0.0208 \n",
            "Epoch: [4][305/306] Elapsed 1m 42s (remain 0m 0s) Loss: 0.0216 \n",
            "EVAL: [0/77] Elapsed 0m 0s (remain 0m 26s) Loss: 0.4534 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4 - avg_train_loss: 0.0216  avg_val_loss: 0.1850  time: 111s\n",
            "Epoch 4 - avg_train_loss: 0.0216  avg_val_loss: 0.1850  time: 111s\n",
            "Epoch 4 - avg_train_loss: 0.0216  avg_val_loss: 0.1850  time: 111s\n",
            "Epoch 4 - avg_train_loss: 0.0216  avg_val_loss: 0.1850  time: 111s\n",
            "Epoch 4 - avg_train_loss: 0.0216  avg_val_loss: 0.1850  time: 111s\n",
            "Epoch 4 - avg_train_loss: 0.0216  avg_val_loss: 0.1850  time: 111s\n",
            "Epoch 4 - avg_train_loss: 0.0216  avg_val_loss: 0.1850  time: 111s\n",
            "Epoch 4 - avg_train_loss: 0.0216  avg_val_loss: 0.1850  time: 111s\n",
            "Epoch 4 - avg_train_loss: 0.0216  avg_val_loss: 0.1850  time: 111s\n",
            "Epoch 4 - avg_train_loss: 0.0216  avg_val_loss: 0.1850  time: 111s\n",
            "Epoch 4 - Score: 0.1849779967291492\n",
            "Epoch 4 - Score: 0.1849779967291492\n",
            "Epoch 4 - Score: 0.1849779967291492\n",
            "Epoch 4 - Score: 0.1849779967291492\n",
            "Epoch 4 - Score: 0.1849779967291492\n",
            "Epoch 4 - Score: 0.1849779967291492\n",
            "Epoch 4 - Score: 0.1849779967291492\n",
            "Epoch 4 - Score: 0.1849779967291492\n",
            "Epoch 4 - Score: 0.1849779967291492\n",
            "Epoch 4 - Score: 0.1849779967291492\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EVAL: [76/77] Elapsed 0m 8s (remain 0m 0s) Loss: 0.1850 \n",
            "Log loss for 3 is 0.1849779967291492\n",
            "Epoch: [5][0/306] Elapsed 0m 0s (remain 3m 6s) Loss: 0.0074 \n",
            "Epoch: [5][100/306] Elapsed 0m 34s (remain 1m 9s) Loss: 0.0159 \n",
            "Epoch: [5][200/306] Elapsed 1m 7s (remain 0m 35s) Loss: 0.0178 \n",
            "Epoch: [5][300/306] Elapsed 1m 40s (remain 0m 1s) Loss: 0.0209 \n",
            "Epoch: [5][305/306] Elapsed 1m 42s (remain 0m 0s) Loss: 0.0206 \n",
            "EVAL: [0/77] Elapsed 0m 0s (remain 0m 28s) Loss: 0.4149 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5 - avg_train_loss: 0.0206  avg_val_loss: 0.1781  time: 111s\n",
            "Epoch 5 - avg_train_loss: 0.0206  avg_val_loss: 0.1781  time: 111s\n",
            "Epoch 5 - avg_train_loss: 0.0206  avg_val_loss: 0.1781  time: 111s\n",
            "Epoch 5 - avg_train_loss: 0.0206  avg_val_loss: 0.1781  time: 111s\n",
            "Epoch 5 - avg_train_loss: 0.0206  avg_val_loss: 0.1781  time: 111s\n",
            "Epoch 5 - avg_train_loss: 0.0206  avg_val_loss: 0.1781  time: 111s\n",
            "Epoch 5 - avg_train_loss: 0.0206  avg_val_loss: 0.1781  time: 111s\n",
            "Epoch 5 - avg_train_loss: 0.0206  avg_val_loss: 0.1781  time: 111s\n",
            "Epoch 5 - avg_train_loss: 0.0206  avg_val_loss: 0.1781  time: 111s\n",
            "Epoch 5 - avg_train_loss: 0.0206  avg_val_loss: 0.1781  time: 111s\n",
            "Epoch 5 - Score: 0.1780786714123394\n",
            "Epoch 5 - Score: 0.1780786714123394\n",
            "Epoch 5 - Score: 0.1780786714123394\n",
            "Epoch 5 - Score: 0.1780786714123394\n",
            "Epoch 5 - Score: 0.1780786714123394\n",
            "Epoch 5 - Score: 0.1780786714123394\n",
            "Epoch 5 - Score: 0.1780786714123394\n",
            "Epoch 5 - Score: 0.1780786714123394\n",
            "Epoch 5 - Score: 0.1780786714123394\n",
            "Epoch 5 - Score: 0.1780786714123394\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EVAL: [76/77] Elapsed 0m 8s (remain 0m 0s) Loss: 0.1781 \n",
            "Log loss for 4 is 0.1780786714123394\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "========== fold: 0 result ==========\n",
            "========== fold: 0 result ==========\n",
            "========== fold: 0 result ==========\n",
            "========== fold: 0 result ==========\n",
            "========== fold: 0 result ==========\n",
            "========== fold: 0 result ==========\n",
            "========== fold: 0 result ==========\n",
            "========== fold: 0 result ==========\n",
            "========== fold: 0 result ==========\n",
            "========== fold: 0 result ==========\n",
            "Score: 0.10209\n",
            "Score: 0.10209\n",
            "Score: 0.10209\n",
            "Score: 0.10209\n",
            "Score: 0.10209\n",
            "Score: 0.10209\n",
            "Score: 0.10209\n",
            "Score: 0.10209\n",
            "Score: 0.10209\n",
            "Score: 0.10209\n",
            "========== fold: 1 training ==========\n",
            "========== fold: 1 training ==========\n",
            "========== fold: 1 training ==========\n",
            "========== fold: 1 training ==========\n",
            "========== fold: 1 training ==========\n",
            "========== fold: 1 training ==========\n",
            "========== fold: 1 training ==========\n",
            "========== fold: 1 training ==========\n",
            "========== fold: 1 training ==========\n",
            "========== fold: 1 training ==========\n",
            "Didn't find file /content/activebus_bert_final/added_tokens.json. We won't load it.\n",
            "loading file /content/activebus_bert_final/vocab.txt\n",
            "loading file None\n",
            "loading file /content/activebus_bert_final/special_tokens_map.json\n",
            "loading file /content/activebus_bert_final/tokenizer_config.json\n",
            "loading file /content/activebus_bert_final/tokenizer.json\n",
            "Didn't find file /content/activebus_bert_final/added_tokens.json. We won't load it.\n",
            "loading file /content/activebus_bert_final/vocab.txt\n",
            "loading file None\n",
            "loading file /content/activebus_bert_final/special_tokens_map.json\n",
            "loading file /content/activebus_bert_final/tokenizer_config.json\n",
            "loading file /content/activebus_bert_final/tokenizer.json\n",
            "loading configuration file /content/activebus_bert_final/config.json\n",
            "Model config BertConfig {\n",
            "  \"_name_or_path\": \"activebus/BERT-XD_Review\",\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_ids\": 0,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.15.0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "loading weights file /content/activebus_bert_final/pytorch_model.bin\n",
            "Some weights of the model checkpoint at /content/activebus_bert_final were not used when initializing BertForSequenceClassification: ['cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /content/activebus_bert_final and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: [1][0/306] Elapsed 0m 0s (remain 3m 6s) Loss: 0.6145 \n",
            "Epoch: [1][100/306] Elapsed 0m 34s (remain 1m 9s) Loss: 0.2016 \n",
            "Epoch: [1][200/306] Elapsed 1m 7s (remain 0m 35s) Loss: 0.1685 \n",
            "Epoch: [1][300/306] Elapsed 1m 40s (remain 0m 1s) Loss: 0.1595 \n",
            "Epoch: [1][305/306] Elapsed 1m 42s (remain 0m 0s) Loss: 0.1583 \n",
            "EVAL: [0/77] Elapsed 0m 0s (remain 0m 28s) Loss: 0.0863 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1 - avg_train_loss: 0.1583  avg_val_loss: 0.1107  time: 111s\n",
            "Epoch 1 - avg_train_loss: 0.1583  avg_val_loss: 0.1107  time: 111s\n",
            "Epoch 1 - avg_train_loss: 0.1583  avg_val_loss: 0.1107  time: 111s\n",
            "Epoch 1 - avg_train_loss: 0.1583  avg_val_loss: 0.1107  time: 111s\n",
            "Epoch 1 - avg_train_loss: 0.1583  avg_val_loss: 0.1107  time: 111s\n",
            "Epoch 1 - avg_train_loss: 0.1583  avg_val_loss: 0.1107  time: 111s\n",
            "Epoch 1 - avg_train_loss: 0.1583  avg_val_loss: 0.1107  time: 111s\n",
            "Epoch 1 - avg_train_loss: 0.1583  avg_val_loss: 0.1107  time: 111s\n",
            "Epoch 1 - avg_train_loss: 0.1583  avg_val_loss: 0.1107  time: 111s\n",
            "Epoch 1 - avg_train_loss: 0.1583  avg_val_loss: 0.1107  time: 111s\n",
            "Epoch 1 - Score: 0.11074777768572645\n",
            "Epoch 1 - Score: 0.11074777768572645\n",
            "Epoch 1 - Score: 0.11074777768572645\n",
            "Epoch 1 - Score: 0.11074777768572645\n",
            "Epoch 1 - Score: 0.11074777768572645\n",
            "Epoch 1 - Score: 0.11074777768572645\n",
            "Epoch 1 - Score: 0.11074777768572645\n",
            "Epoch 1 - Score: 0.11074777768572645\n",
            "Epoch 1 - Score: 0.11074777768572645\n",
            "Epoch 1 - Score: 0.11074777768572645\n",
            "Epoch 1 - Save Best Score: 0.1107 Model\n",
            "Epoch 1 - Save Best Score: 0.1107 Model\n",
            "Epoch 1 - Save Best Score: 0.1107 Model\n",
            "Epoch 1 - Save Best Score: 0.1107 Model\n",
            "Epoch 1 - Save Best Score: 0.1107 Model\n",
            "Epoch 1 - Save Best Score: 0.1107 Model\n",
            "Epoch 1 - Save Best Score: 0.1107 Model\n",
            "Epoch 1 - Save Best Score: 0.1107 Model\n",
            "Epoch 1 - Save Best Score: 0.1107 Model\n",
            "Epoch 1 - Save Best Score: 0.1107 Model\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EVAL: [76/77] Elapsed 0m 8s (remain 0m 0s) Loss: 0.1107 \n",
            "Log loss for 0 is 0.11074777768572645\n",
            "Epoch: [2][0/306] Elapsed 0m 0s (remain 3m 5s) Loss: 0.0320 \n",
            "Epoch: [2][100/306] Elapsed 0m 34s (remain 1m 9s) Loss: 0.0825 \n",
            "Epoch: [2][200/306] Elapsed 1m 7s (remain 0m 35s) Loss: 0.0830 \n",
            "Epoch: [2][300/306] Elapsed 1m 40s (remain 0m 1s) Loss: 0.0850 \n",
            "Epoch: [2][305/306] Elapsed 1m 42s (remain 0m 0s) Loss: 0.0844 \n",
            "EVAL: [0/77] Elapsed 0m 0s (remain 0m 26s) Loss: 0.0709 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2 - avg_train_loss: 0.0844  avg_val_loss: 0.0879  time: 111s\n",
            "Epoch 2 - avg_train_loss: 0.0844  avg_val_loss: 0.0879  time: 111s\n",
            "Epoch 2 - avg_train_loss: 0.0844  avg_val_loss: 0.0879  time: 111s\n",
            "Epoch 2 - avg_train_loss: 0.0844  avg_val_loss: 0.0879  time: 111s\n",
            "Epoch 2 - avg_train_loss: 0.0844  avg_val_loss: 0.0879  time: 111s\n",
            "Epoch 2 - avg_train_loss: 0.0844  avg_val_loss: 0.0879  time: 111s\n",
            "Epoch 2 - avg_train_loss: 0.0844  avg_val_loss: 0.0879  time: 111s\n",
            "Epoch 2 - avg_train_loss: 0.0844  avg_val_loss: 0.0879  time: 111s\n",
            "Epoch 2 - avg_train_loss: 0.0844  avg_val_loss: 0.0879  time: 111s\n",
            "Epoch 2 - avg_train_loss: 0.0844  avg_val_loss: 0.0879  time: 111s\n",
            "Epoch 2 - Score: 0.08791907529419511\n",
            "Epoch 2 - Score: 0.08791907529419511\n",
            "Epoch 2 - Score: 0.08791907529419511\n",
            "Epoch 2 - Score: 0.08791907529419511\n",
            "Epoch 2 - Score: 0.08791907529419511\n",
            "Epoch 2 - Score: 0.08791907529419511\n",
            "Epoch 2 - Score: 0.08791907529419511\n",
            "Epoch 2 - Score: 0.08791907529419511\n",
            "Epoch 2 - Score: 0.08791907529419511\n",
            "Epoch 2 - Score: 0.08791907529419511\n",
            "Epoch 2 - Save Best Score: 0.0879 Model\n",
            "Epoch 2 - Save Best Score: 0.0879 Model\n",
            "Epoch 2 - Save Best Score: 0.0879 Model\n",
            "Epoch 2 - Save Best Score: 0.0879 Model\n",
            "Epoch 2 - Save Best Score: 0.0879 Model\n",
            "Epoch 2 - Save Best Score: 0.0879 Model\n",
            "Epoch 2 - Save Best Score: 0.0879 Model\n",
            "Epoch 2 - Save Best Score: 0.0879 Model\n",
            "Epoch 2 - Save Best Score: 0.0879 Model\n",
            "Epoch 2 - Save Best Score: 0.0879 Model\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EVAL: [76/77] Elapsed 0m 8s (remain 0m 0s) Loss: 0.0879 \n",
            "Log loss for 1 is 0.08791907529419511\n",
            "Epoch: [3][0/306] Elapsed 0m 0s (remain 3m 11s) Loss: 0.0045 \n",
            "Epoch: [3][100/306] Elapsed 0m 34s (remain 1m 9s) Loss: 0.0367 \n",
            "Epoch: [3][200/306] Elapsed 1m 7s (remain 0m 35s) Loss: 0.0387 \n",
            "Epoch: [3][300/306] Elapsed 1m 41s (remain 0m 1s) Loss: 0.0422 \n",
            "Epoch: [3][305/306] Elapsed 1m 42s (remain 0m 0s) Loss: 0.0418 \n",
            "EVAL: [0/77] Elapsed 0m 0s (remain 0m 28s) Loss: 0.0611 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3 - avg_train_loss: 0.0418  avg_val_loss: 0.1176  time: 112s\n",
            "Epoch 3 - avg_train_loss: 0.0418  avg_val_loss: 0.1176  time: 112s\n",
            "Epoch 3 - avg_train_loss: 0.0418  avg_val_loss: 0.1176  time: 112s\n",
            "Epoch 3 - avg_train_loss: 0.0418  avg_val_loss: 0.1176  time: 112s\n",
            "Epoch 3 - avg_train_loss: 0.0418  avg_val_loss: 0.1176  time: 112s\n",
            "Epoch 3 - avg_train_loss: 0.0418  avg_val_loss: 0.1176  time: 112s\n",
            "Epoch 3 - avg_train_loss: 0.0418  avg_val_loss: 0.1176  time: 112s\n",
            "Epoch 3 - avg_train_loss: 0.0418  avg_val_loss: 0.1176  time: 112s\n",
            "Epoch 3 - avg_train_loss: 0.0418  avg_val_loss: 0.1176  time: 112s\n",
            "Epoch 3 - avg_train_loss: 0.0418  avg_val_loss: 0.1176  time: 112s\n",
            "Epoch 3 - Score: 0.11763757391898803\n",
            "Epoch 3 - Score: 0.11763757391898803\n",
            "Epoch 3 - Score: 0.11763757391898803\n",
            "Epoch 3 - Score: 0.11763757391898803\n",
            "Epoch 3 - Score: 0.11763757391898803\n",
            "Epoch 3 - Score: 0.11763757391898803\n",
            "Epoch 3 - Score: 0.11763757391898803\n",
            "Epoch 3 - Score: 0.11763757391898803\n",
            "Epoch 3 - Score: 0.11763757391898803\n",
            "Epoch 3 - Score: 0.11763757391898803\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EVAL: [76/77] Elapsed 0m 8s (remain 0m 0s) Loss: 0.1176 \n",
            "Log loss for 2 is 0.11763757391898803\n",
            "Epoch: [4][0/306] Elapsed 0m 0s (remain 3m 9s) Loss: 0.0017 \n",
            "Epoch: [4][100/306] Elapsed 0m 34s (remain 1m 9s) Loss: 0.0290 \n",
            "Epoch: [4][200/306] Elapsed 1m 7s (remain 0m 35s) Loss: 0.0236 \n",
            "Epoch: [4][300/306] Elapsed 1m 41s (remain 0m 1s) Loss: 0.0274 \n",
            "Epoch: [4][305/306] Elapsed 1m 42s (remain 0m 0s) Loss: 0.0273 \n",
            "EVAL: [0/77] Elapsed 0m 0s (remain 0m 26s) Loss: 0.0238 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4 - avg_train_loss: 0.0273  avg_val_loss: 0.1097  time: 111s\n",
            "Epoch 4 - avg_train_loss: 0.0273  avg_val_loss: 0.1097  time: 111s\n",
            "Epoch 4 - avg_train_loss: 0.0273  avg_val_loss: 0.1097  time: 111s\n",
            "Epoch 4 - avg_train_loss: 0.0273  avg_val_loss: 0.1097  time: 111s\n",
            "Epoch 4 - avg_train_loss: 0.0273  avg_val_loss: 0.1097  time: 111s\n",
            "Epoch 4 - avg_train_loss: 0.0273  avg_val_loss: 0.1097  time: 111s\n",
            "Epoch 4 - avg_train_loss: 0.0273  avg_val_loss: 0.1097  time: 111s\n",
            "Epoch 4 - avg_train_loss: 0.0273  avg_val_loss: 0.1097  time: 111s\n",
            "Epoch 4 - avg_train_loss: 0.0273  avg_val_loss: 0.1097  time: 111s\n",
            "Epoch 4 - avg_train_loss: 0.0273  avg_val_loss: 0.1097  time: 111s\n",
            "Epoch 4 - Score: 0.1097162224089263\n",
            "Epoch 4 - Score: 0.1097162224089263\n",
            "Epoch 4 - Score: 0.1097162224089263\n",
            "Epoch 4 - Score: 0.1097162224089263\n",
            "Epoch 4 - Score: 0.1097162224089263\n",
            "Epoch 4 - Score: 0.1097162224089263\n",
            "Epoch 4 - Score: 0.1097162224089263\n",
            "Epoch 4 - Score: 0.1097162224089263\n",
            "Epoch 4 - Score: 0.1097162224089263\n",
            "Epoch 4 - Score: 0.1097162224089263\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EVAL: [76/77] Elapsed 0m 8s (remain 0m 0s) Loss: 0.1097 \n",
            "Log loss for 3 is 0.1097162224089263\n",
            "Epoch: [5][0/306] Elapsed 0m 0s (remain 3m 4s) Loss: 0.0022 \n",
            "Epoch: [5][100/306] Elapsed 0m 34s (remain 1m 9s) Loss: 0.0102 \n",
            "Epoch: [5][200/306] Elapsed 1m 7s (remain 0m 35s) Loss: 0.0088 \n",
            "Epoch: [5][300/306] Elapsed 1m 40s (remain 0m 1s) Loss: 0.0078 \n",
            "Epoch: [5][305/306] Elapsed 1m 42s (remain 0m 0s) Loss: 0.0077 \n",
            "EVAL: [0/77] Elapsed 0m 0s (remain 0m 27s) Loss: 0.0991 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5 - avg_train_loss: 0.0077  avg_val_loss: 0.1559  time: 111s\n",
            "Epoch 5 - avg_train_loss: 0.0077  avg_val_loss: 0.1559  time: 111s\n",
            "Epoch 5 - avg_train_loss: 0.0077  avg_val_loss: 0.1559  time: 111s\n",
            "Epoch 5 - avg_train_loss: 0.0077  avg_val_loss: 0.1559  time: 111s\n",
            "Epoch 5 - avg_train_loss: 0.0077  avg_val_loss: 0.1559  time: 111s\n",
            "Epoch 5 - avg_train_loss: 0.0077  avg_val_loss: 0.1559  time: 111s\n",
            "Epoch 5 - avg_train_loss: 0.0077  avg_val_loss: 0.1559  time: 111s\n",
            "Epoch 5 - avg_train_loss: 0.0077  avg_val_loss: 0.1559  time: 111s\n",
            "Epoch 5 - avg_train_loss: 0.0077  avg_val_loss: 0.1559  time: 111s\n",
            "Epoch 5 - avg_train_loss: 0.0077  avg_val_loss: 0.1559  time: 111s\n",
            "Epoch 5 - Score: 0.15591380283256523\n",
            "Epoch 5 - Score: 0.15591380283256523\n",
            "Epoch 5 - Score: 0.15591380283256523\n",
            "Epoch 5 - Score: 0.15591380283256523\n",
            "Epoch 5 - Score: 0.15591380283256523\n",
            "Epoch 5 - Score: 0.15591380283256523\n",
            "Epoch 5 - Score: 0.15591380283256523\n",
            "Epoch 5 - Score: 0.15591380283256523\n",
            "Epoch 5 - Score: 0.15591380283256523\n",
            "Epoch 5 - Score: 0.15591380283256523\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EVAL: [76/77] Elapsed 0m 8s (remain 0m 0s) Loss: 0.1559 \n",
            "Log loss for 4 is 0.15591380283256523\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "========== fold: 1 result ==========\n",
            "========== fold: 1 result ==========\n",
            "========== fold: 1 result ==========\n",
            "========== fold: 1 result ==========\n",
            "========== fold: 1 result ==========\n",
            "========== fold: 1 result ==========\n",
            "========== fold: 1 result ==========\n",
            "========== fold: 1 result ==========\n",
            "========== fold: 1 result ==========\n",
            "========== fold: 1 result ==========\n",
            "Score: 0.08792\n",
            "Score: 0.08792\n",
            "Score: 0.08792\n",
            "Score: 0.08792\n",
            "Score: 0.08792\n",
            "Score: 0.08792\n",
            "Score: 0.08792\n",
            "Score: 0.08792\n",
            "Score: 0.08792\n",
            "Score: 0.08792\n",
            "========== fold: 2 training ==========\n",
            "========== fold: 2 training ==========\n",
            "========== fold: 2 training ==========\n",
            "========== fold: 2 training ==========\n",
            "========== fold: 2 training ==========\n",
            "========== fold: 2 training ==========\n",
            "========== fold: 2 training ==========\n",
            "========== fold: 2 training ==========\n",
            "========== fold: 2 training ==========\n",
            "========== fold: 2 training ==========\n",
            "Didn't find file /content/activebus_bert_final/added_tokens.json. We won't load it.\n",
            "loading file /content/activebus_bert_final/vocab.txt\n",
            "loading file None\n",
            "loading file /content/activebus_bert_final/special_tokens_map.json\n",
            "loading file /content/activebus_bert_final/tokenizer_config.json\n",
            "loading file /content/activebus_bert_final/tokenizer.json\n",
            "Didn't find file /content/activebus_bert_final/added_tokens.json. We won't load it.\n",
            "loading file /content/activebus_bert_final/vocab.txt\n",
            "loading file None\n",
            "loading file /content/activebus_bert_final/special_tokens_map.json\n",
            "loading file /content/activebus_bert_final/tokenizer_config.json\n",
            "loading file /content/activebus_bert_final/tokenizer.json\n",
            "loading configuration file /content/activebus_bert_final/config.json\n",
            "Model config BertConfig {\n",
            "  \"_name_or_path\": \"activebus/BERT-XD_Review\",\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_ids\": 0,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.15.0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "loading weights file /content/activebus_bert_final/pytorch_model.bin\n",
            "Some weights of the model checkpoint at /content/activebus_bert_final were not used when initializing BertForSequenceClassification: ['cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /content/activebus_bert_final and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: [1][0/306] Elapsed 0m 0s (remain 3m 4s) Loss: 0.6969 \n",
            "Epoch: [1][100/306] Elapsed 0m 34s (remain 1m 9s) Loss: 0.2252 \n",
            "Epoch: [1][200/306] Elapsed 1m 7s (remain 0m 35s) Loss: 0.1778 \n",
            "Epoch: [1][300/306] Elapsed 1m 40s (remain 0m 1s) Loss: 0.1618 \n",
            "Epoch: [1][305/306] Elapsed 1m 42s (remain 0m 0s) Loss: 0.1604 \n",
            "EVAL: [0/77] Elapsed 0m 0s (remain 0m 26s) Loss: 0.0366 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1 - avg_train_loss: 0.1604  avg_val_loss: 0.1044  time: 111s\n",
            "Epoch 1 - avg_train_loss: 0.1604  avg_val_loss: 0.1044  time: 111s\n",
            "Epoch 1 - avg_train_loss: 0.1604  avg_val_loss: 0.1044  time: 111s\n",
            "Epoch 1 - avg_train_loss: 0.1604  avg_val_loss: 0.1044  time: 111s\n",
            "Epoch 1 - avg_train_loss: 0.1604  avg_val_loss: 0.1044  time: 111s\n",
            "Epoch 1 - avg_train_loss: 0.1604  avg_val_loss: 0.1044  time: 111s\n",
            "Epoch 1 - avg_train_loss: 0.1604  avg_val_loss: 0.1044  time: 111s\n",
            "Epoch 1 - avg_train_loss: 0.1604  avg_val_loss: 0.1044  time: 111s\n",
            "Epoch 1 - avg_train_loss: 0.1604  avg_val_loss: 0.1044  time: 111s\n",
            "Epoch 1 - avg_train_loss: 0.1604  avg_val_loss: 0.1044  time: 111s\n",
            "Epoch 1 - Score: 0.10435993509623659\n",
            "Epoch 1 - Score: 0.10435993509623659\n",
            "Epoch 1 - Score: 0.10435993509623659\n",
            "Epoch 1 - Score: 0.10435993509623659\n",
            "Epoch 1 - Score: 0.10435993509623659\n",
            "Epoch 1 - Score: 0.10435993509623659\n",
            "Epoch 1 - Score: 0.10435993509623659\n",
            "Epoch 1 - Score: 0.10435993509623659\n",
            "Epoch 1 - Score: 0.10435993509623659\n",
            "Epoch 1 - Score: 0.10435993509623659\n",
            "Epoch 1 - Save Best Score: 0.1044 Model\n",
            "Epoch 1 - Save Best Score: 0.1044 Model\n",
            "Epoch 1 - Save Best Score: 0.1044 Model\n",
            "Epoch 1 - Save Best Score: 0.1044 Model\n",
            "Epoch 1 - Save Best Score: 0.1044 Model\n",
            "Epoch 1 - Save Best Score: 0.1044 Model\n",
            "Epoch 1 - Save Best Score: 0.1044 Model\n",
            "Epoch 1 - Save Best Score: 0.1044 Model\n",
            "Epoch 1 - Save Best Score: 0.1044 Model\n",
            "Epoch 1 - Save Best Score: 0.1044 Model\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EVAL: [76/77] Elapsed 0m 8s (remain 0m 0s) Loss: 0.1044 \n",
            "Log loss for 0 is 0.10435993509623659\n",
            "Epoch: [2][0/306] Elapsed 0m 0s (remain 3m 9s) Loss: 0.0617 \n",
            "Epoch: [2][100/306] Elapsed 0m 34s (remain 1m 9s) Loss: 0.0810 \n",
            "Epoch: [2][200/306] Elapsed 1m 7s (remain 0m 35s) Loss: 0.0870 \n",
            "Epoch: [2][300/306] Elapsed 1m 41s (remain 0m 1s) Loss: 0.0851 \n",
            "Epoch: [2][305/306] Elapsed 1m 42s (remain 0m 0s) Loss: 0.0852 \n",
            "EVAL: [0/77] Elapsed 0m 0s (remain 0m 26s) Loss: 0.0333 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2 - avg_train_loss: 0.0852  avg_val_loss: 0.0945  time: 112s\n",
            "Epoch 2 - avg_train_loss: 0.0852  avg_val_loss: 0.0945  time: 112s\n",
            "Epoch 2 - avg_train_loss: 0.0852  avg_val_loss: 0.0945  time: 112s\n",
            "Epoch 2 - avg_train_loss: 0.0852  avg_val_loss: 0.0945  time: 112s\n",
            "Epoch 2 - avg_train_loss: 0.0852  avg_val_loss: 0.0945  time: 112s\n",
            "Epoch 2 - avg_train_loss: 0.0852  avg_val_loss: 0.0945  time: 112s\n",
            "Epoch 2 - avg_train_loss: 0.0852  avg_val_loss: 0.0945  time: 112s\n",
            "Epoch 2 - avg_train_loss: 0.0852  avg_val_loss: 0.0945  time: 112s\n",
            "Epoch 2 - avg_train_loss: 0.0852  avg_val_loss: 0.0945  time: 112s\n",
            "Epoch 2 - avg_train_loss: 0.0852  avg_val_loss: 0.0945  time: 112s\n",
            "Epoch 2 - Score: 0.09445734017683434\n",
            "Epoch 2 - Score: 0.09445734017683434\n",
            "Epoch 2 - Score: 0.09445734017683434\n",
            "Epoch 2 - Score: 0.09445734017683434\n",
            "Epoch 2 - Score: 0.09445734017683434\n",
            "Epoch 2 - Score: 0.09445734017683434\n",
            "Epoch 2 - Score: 0.09445734017683434\n",
            "Epoch 2 - Score: 0.09445734017683434\n",
            "Epoch 2 - Score: 0.09445734017683434\n",
            "Epoch 2 - Score: 0.09445734017683434\n",
            "Epoch 2 - Save Best Score: 0.0945 Model\n",
            "Epoch 2 - Save Best Score: 0.0945 Model\n",
            "Epoch 2 - Save Best Score: 0.0945 Model\n",
            "Epoch 2 - Save Best Score: 0.0945 Model\n",
            "Epoch 2 - Save Best Score: 0.0945 Model\n",
            "Epoch 2 - Save Best Score: 0.0945 Model\n",
            "Epoch 2 - Save Best Score: 0.0945 Model\n",
            "Epoch 2 - Save Best Score: 0.0945 Model\n",
            "Epoch 2 - Save Best Score: 0.0945 Model\n",
            "Epoch 2 - Save Best Score: 0.0945 Model\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EVAL: [76/77] Elapsed 0m 8s (remain 0m 0s) Loss: 0.0945 \n",
            "Log loss for 1 is 0.09445734017683434\n",
            "Epoch: [3][0/306] Elapsed 0m 0s (remain 3m 1s) Loss: 0.0272 \n",
            "Epoch: [3][100/306] Elapsed 0m 34s (remain 1m 9s) Loss: 0.0404 \n",
            "Epoch: [3][200/306] Elapsed 1m 7s (remain 0m 35s) Loss: 0.0441 \n",
            "Epoch: [3][300/306] Elapsed 1m 40s (remain 0m 1s) Loss: 0.0475 \n",
            "Epoch: [3][305/306] Elapsed 1m 42s (remain 0m 0s) Loss: 0.0473 \n",
            "EVAL: [0/77] Elapsed 0m 0s (remain 0m 27s) Loss: 0.0565 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3 - avg_train_loss: 0.0473  avg_val_loss: 0.1202  time: 111s\n",
            "Epoch 3 - avg_train_loss: 0.0473  avg_val_loss: 0.1202  time: 111s\n",
            "Epoch 3 - avg_train_loss: 0.0473  avg_val_loss: 0.1202  time: 111s\n",
            "Epoch 3 - avg_train_loss: 0.0473  avg_val_loss: 0.1202  time: 111s\n",
            "Epoch 3 - avg_train_loss: 0.0473  avg_val_loss: 0.1202  time: 111s\n",
            "Epoch 3 - avg_train_loss: 0.0473  avg_val_loss: 0.1202  time: 111s\n",
            "Epoch 3 - avg_train_loss: 0.0473  avg_val_loss: 0.1202  time: 111s\n",
            "Epoch 3 - avg_train_loss: 0.0473  avg_val_loss: 0.1202  time: 111s\n",
            "Epoch 3 - avg_train_loss: 0.0473  avg_val_loss: 0.1202  time: 111s\n",
            "Epoch 3 - avg_train_loss: 0.0473  avg_val_loss: 0.1202  time: 111s\n",
            "Epoch 3 - Score: 0.12023044901828076\n",
            "Epoch 3 - Score: 0.12023044901828076\n",
            "Epoch 3 - Score: 0.12023044901828076\n",
            "Epoch 3 - Score: 0.12023044901828076\n",
            "Epoch 3 - Score: 0.12023044901828076\n",
            "Epoch 3 - Score: 0.12023044901828076\n",
            "Epoch 3 - Score: 0.12023044901828076\n",
            "Epoch 3 - Score: 0.12023044901828076\n",
            "Epoch 3 - Score: 0.12023044901828076\n",
            "Epoch 3 - Score: 0.12023044901828076\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EVAL: [76/77] Elapsed 0m 8s (remain 0m 0s) Loss: 0.1202 \n",
            "Log loss for 2 is 0.12023044901828076\n",
            "Epoch: [4][0/306] Elapsed 0m 0s (remain 3m 3s) Loss: 0.0238 \n",
            "Epoch: [4][100/306] Elapsed 0m 34s (remain 1m 9s) Loss: 0.0148 \n",
            "Epoch: [4][200/306] Elapsed 1m 7s (remain 0m 35s) Loss: 0.0235 \n",
            "Epoch: [4][300/306] Elapsed 1m 41s (remain 0m 1s) Loss: 0.0244 \n",
            "Epoch: [4][305/306] Elapsed 1m 42s (remain 0m 0s) Loss: 0.0241 \n",
            "EVAL: [0/77] Elapsed 0m 0s (remain 0m 26s) Loss: 0.0069 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4 - avg_train_loss: 0.0241  avg_val_loss: 0.1397  time: 111s\n",
            "Epoch 4 - avg_train_loss: 0.0241  avg_val_loss: 0.1397  time: 111s\n",
            "Epoch 4 - avg_train_loss: 0.0241  avg_val_loss: 0.1397  time: 111s\n",
            "Epoch 4 - avg_train_loss: 0.0241  avg_val_loss: 0.1397  time: 111s\n",
            "Epoch 4 - avg_train_loss: 0.0241  avg_val_loss: 0.1397  time: 111s\n",
            "Epoch 4 - avg_train_loss: 0.0241  avg_val_loss: 0.1397  time: 111s\n",
            "Epoch 4 - avg_train_loss: 0.0241  avg_val_loss: 0.1397  time: 111s\n",
            "Epoch 4 - avg_train_loss: 0.0241  avg_val_loss: 0.1397  time: 111s\n",
            "Epoch 4 - avg_train_loss: 0.0241  avg_val_loss: 0.1397  time: 111s\n",
            "Epoch 4 - avg_train_loss: 0.0241  avg_val_loss: 0.1397  time: 111s\n",
            "Epoch 4 - Score: 0.1397175268634312\n",
            "Epoch 4 - Score: 0.1397175268634312\n",
            "Epoch 4 - Score: 0.1397175268634312\n",
            "Epoch 4 - Score: 0.1397175268634312\n",
            "Epoch 4 - Score: 0.1397175268634312\n",
            "Epoch 4 - Score: 0.1397175268634312\n",
            "Epoch 4 - Score: 0.1397175268634312\n",
            "Epoch 4 - Score: 0.1397175268634312\n",
            "Epoch 4 - Score: 0.1397175268634312\n",
            "Epoch 4 - Score: 0.1397175268634312\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EVAL: [76/77] Elapsed 0m 8s (remain 0m 0s) Loss: 0.1397 \n",
            "Log loss for 3 is 0.1397175268634312\n",
            "Epoch: [5][0/306] Elapsed 0m 0s (remain 3m 6s) Loss: 0.0070 \n",
            "Epoch: [5][100/306] Elapsed 0m 34s (remain 1m 9s) Loss: 0.0141 \n",
            "Epoch: [5][200/306] Elapsed 1m 7s (remain 0m 35s) Loss: 0.0132 \n",
            "Epoch: [5][300/306] Elapsed 1m 40s (remain 0m 1s) Loss: 0.0116 \n",
            "Epoch: [5][305/306] Elapsed 1m 42s (remain 0m 0s) Loss: 0.0115 \n",
            "EVAL: [0/77] Elapsed 0m 0s (remain 0m 26s) Loss: 0.1609 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5 - avg_train_loss: 0.0115  avg_val_loss: 0.1992  time: 111s\n",
            "Epoch 5 - avg_train_loss: 0.0115  avg_val_loss: 0.1992  time: 111s\n",
            "Epoch 5 - avg_train_loss: 0.0115  avg_val_loss: 0.1992  time: 111s\n",
            "Epoch 5 - avg_train_loss: 0.0115  avg_val_loss: 0.1992  time: 111s\n",
            "Epoch 5 - avg_train_loss: 0.0115  avg_val_loss: 0.1992  time: 111s\n",
            "Epoch 5 - avg_train_loss: 0.0115  avg_val_loss: 0.1992  time: 111s\n",
            "Epoch 5 - avg_train_loss: 0.0115  avg_val_loss: 0.1992  time: 111s\n",
            "Epoch 5 - avg_train_loss: 0.0115  avg_val_loss: 0.1992  time: 111s\n",
            "Epoch 5 - avg_train_loss: 0.0115  avg_val_loss: 0.1992  time: 111s\n",
            "Epoch 5 - avg_train_loss: 0.0115  avg_val_loss: 0.1992  time: 111s\n",
            "Epoch 5 - Score: 0.19920193568463518\n",
            "Epoch 5 - Score: 0.19920193568463518\n",
            "Epoch 5 - Score: 0.19920193568463518\n",
            "Epoch 5 - Score: 0.19920193568463518\n",
            "Epoch 5 - Score: 0.19920193568463518\n",
            "Epoch 5 - Score: 0.19920193568463518\n",
            "Epoch 5 - Score: 0.19920193568463518\n",
            "Epoch 5 - Score: 0.19920193568463518\n",
            "Epoch 5 - Score: 0.19920193568463518\n",
            "Epoch 5 - Score: 0.19920193568463518\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EVAL: [76/77] Elapsed 0m 8s (remain 0m 0s) Loss: 0.1992 \n",
            "Log loss for 4 is 0.19920193568463518\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "========== fold: 2 result ==========\n",
            "========== fold: 2 result ==========\n",
            "========== fold: 2 result ==========\n",
            "========== fold: 2 result ==========\n",
            "========== fold: 2 result ==========\n",
            "========== fold: 2 result ==========\n",
            "========== fold: 2 result ==========\n",
            "========== fold: 2 result ==========\n",
            "========== fold: 2 result ==========\n",
            "========== fold: 2 result ==========\n",
            "Score: 0.09446\n",
            "Score: 0.09446\n",
            "Score: 0.09446\n",
            "Score: 0.09446\n",
            "Score: 0.09446\n",
            "Score: 0.09446\n",
            "Score: 0.09446\n",
            "Score: 0.09446\n",
            "Score: 0.09446\n",
            "Score: 0.09446\n",
            "========== fold: 3 training ==========\n",
            "========== fold: 3 training ==========\n",
            "========== fold: 3 training ==========\n",
            "========== fold: 3 training ==========\n",
            "========== fold: 3 training ==========\n",
            "========== fold: 3 training ==========\n",
            "========== fold: 3 training ==========\n",
            "========== fold: 3 training ==========\n",
            "========== fold: 3 training ==========\n",
            "========== fold: 3 training ==========\n",
            "Didn't find file /content/activebus_bert_final/added_tokens.json. We won't load it.\n",
            "loading file /content/activebus_bert_final/vocab.txt\n",
            "loading file None\n",
            "loading file /content/activebus_bert_final/special_tokens_map.json\n",
            "loading file /content/activebus_bert_final/tokenizer_config.json\n",
            "loading file /content/activebus_bert_final/tokenizer.json\n",
            "Didn't find file /content/activebus_bert_final/added_tokens.json. We won't load it.\n",
            "loading file /content/activebus_bert_final/vocab.txt\n",
            "loading file None\n",
            "loading file /content/activebus_bert_final/special_tokens_map.json\n",
            "loading file /content/activebus_bert_final/tokenizer_config.json\n",
            "loading file /content/activebus_bert_final/tokenizer.json\n",
            "loading configuration file /content/activebus_bert_final/config.json\n",
            "Model config BertConfig {\n",
            "  \"_name_or_path\": \"activebus/BERT-XD_Review\",\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_ids\": 0,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.15.0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "loading weights file /content/activebus_bert_final/pytorch_model.bin\n",
            "Some weights of the model checkpoint at /content/activebus_bert_final were not used when initializing BertForSequenceClassification: ['cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /content/activebus_bert_final and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: [1][0/306] Elapsed 0m 0s (remain 3m 6s) Loss: 0.7021 \n",
            "Epoch: [1][100/306] Elapsed 0m 34s (remain 1m 9s) Loss: 0.2103 \n",
            "Epoch: [1][200/306] Elapsed 1m 7s (remain 0m 35s) Loss: 0.1874 \n",
            "Epoch: [1][300/306] Elapsed 1m 40s (remain 0m 1s) Loss: 0.1643 \n",
            "Epoch: [1][305/306] Elapsed 1m 42s (remain 0m 0s) Loss: 0.1632 \n",
            "EVAL: [0/77] Elapsed 0m 0s (remain 0m 27s) Loss: 0.0521 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1 - avg_train_loss: 0.1632  avg_val_loss: 0.1149  time: 111s\n",
            "Epoch 1 - avg_train_loss: 0.1632  avg_val_loss: 0.1149  time: 111s\n",
            "Epoch 1 - avg_train_loss: 0.1632  avg_val_loss: 0.1149  time: 111s\n",
            "Epoch 1 - avg_train_loss: 0.1632  avg_val_loss: 0.1149  time: 111s\n",
            "Epoch 1 - avg_train_loss: 0.1632  avg_val_loss: 0.1149  time: 111s\n",
            "Epoch 1 - avg_train_loss: 0.1632  avg_val_loss: 0.1149  time: 111s\n",
            "Epoch 1 - avg_train_loss: 0.1632  avg_val_loss: 0.1149  time: 111s\n",
            "Epoch 1 - avg_train_loss: 0.1632  avg_val_loss: 0.1149  time: 111s\n",
            "Epoch 1 - avg_train_loss: 0.1632  avg_val_loss: 0.1149  time: 111s\n",
            "Epoch 1 - avg_train_loss: 0.1632  avg_val_loss: 0.1149  time: 111s\n",
            "Epoch 1 - Score: 0.11487200318136614\n",
            "Epoch 1 - Score: 0.11487200318136614\n",
            "Epoch 1 - Score: 0.11487200318136614\n",
            "Epoch 1 - Score: 0.11487200318136614\n",
            "Epoch 1 - Score: 0.11487200318136614\n",
            "Epoch 1 - Score: 0.11487200318136614\n",
            "Epoch 1 - Score: 0.11487200318136614\n",
            "Epoch 1 - Score: 0.11487200318136614\n",
            "Epoch 1 - Score: 0.11487200318136614\n",
            "Epoch 1 - Score: 0.11487200318136614\n",
            "Epoch 1 - Save Best Score: 0.1149 Model\n",
            "Epoch 1 - Save Best Score: 0.1149 Model\n",
            "Epoch 1 - Save Best Score: 0.1149 Model\n",
            "Epoch 1 - Save Best Score: 0.1149 Model\n",
            "Epoch 1 - Save Best Score: 0.1149 Model\n",
            "Epoch 1 - Save Best Score: 0.1149 Model\n",
            "Epoch 1 - Save Best Score: 0.1149 Model\n",
            "Epoch 1 - Save Best Score: 0.1149 Model\n",
            "Epoch 1 - Save Best Score: 0.1149 Model\n",
            "Epoch 1 - Save Best Score: 0.1149 Model\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EVAL: [76/77] Elapsed 0m 8s (remain 0m 0s) Loss: 0.1149 \n",
            "Log loss for 0 is 0.11487200318136614\n",
            "Epoch: [2][0/306] Elapsed 0m 0s (remain 3m 3s) Loss: 0.0478 \n",
            "Epoch: [2][100/306] Elapsed 0m 34s (remain 1m 9s) Loss: 0.0892 \n",
            "Epoch: [2][200/306] Elapsed 1m 7s (remain 0m 35s) Loss: 0.0948 \n",
            "Epoch: [2][300/306] Elapsed 1m 40s (remain 0m 1s) Loss: 0.0869 \n",
            "Epoch: [2][305/306] Elapsed 1m 42s (remain 0m 0s) Loss: 0.0883 \n",
            "EVAL: [0/77] Elapsed 0m 0s (remain 0m 28s) Loss: 0.1692 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2 - avg_train_loss: 0.0883  avg_val_loss: 0.1232  time: 111s\n",
            "Epoch 2 - avg_train_loss: 0.0883  avg_val_loss: 0.1232  time: 111s\n",
            "Epoch 2 - avg_train_loss: 0.0883  avg_val_loss: 0.1232  time: 111s\n",
            "Epoch 2 - avg_train_loss: 0.0883  avg_val_loss: 0.1232  time: 111s\n",
            "Epoch 2 - avg_train_loss: 0.0883  avg_val_loss: 0.1232  time: 111s\n",
            "Epoch 2 - avg_train_loss: 0.0883  avg_val_loss: 0.1232  time: 111s\n",
            "Epoch 2 - avg_train_loss: 0.0883  avg_val_loss: 0.1232  time: 111s\n",
            "Epoch 2 - avg_train_loss: 0.0883  avg_val_loss: 0.1232  time: 111s\n",
            "Epoch 2 - avg_train_loss: 0.0883  avg_val_loss: 0.1232  time: 111s\n",
            "Epoch 2 - avg_train_loss: 0.0883  avg_val_loss: 0.1232  time: 111s\n",
            "Epoch 2 - Score: 0.1231783054067511\n",
            "Epoch 2 - Score: 0.1231783054067511\n",
            "Epoch 2 - Score: 0.1231783054067511\n",
            "Epoch 2 - Score: 0.1231783054067511\n",
            "Epoch 2 - Score: 0.1231783054067511\n",
            "Epoch 2 - Score: 0.1231783054067511\n",
            "Epoch 2 - Score: 0.1231783054067511\n",
            "Epoch 2 - Score: 0.1231783054067511\n",
            "Epoch 2 - Score: 0.1231783054067511\n",
            "Epoch 2 - Score: 0.1231783054067511\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EVAL: [76/77] Elapsed 0m 8s (remain 0m 0s) Loss: 0.1232 \n",
            "Log loss for 1 is 0.1231783054067511\n",
            "Epoch: [3][0/306] Elapsed 0m 0s (remain 3m 7s) Loss: 0.0268 \n",
            "Epoch: [3][100/306] Elapsed 0m 34s (remain 1m 9s) Loss: 0.0417 \n",
            "Epoch: [3][200/306] Elapsed 1m 7s (remain 0m 35s) Loss: 0.0471 \n",
            "Epoch: [3][300/306] Elapsed 1m 40s (remain 0m 1s) Loss: 0.0518 \n",
            "Epoch: [3][305/306] Elapsed 1m 42s (remain 0m 0s) Loss: 0.0511 \n",
            "EVAL: [0/77] Elapsed 0m 0s (remain 0m 27s) Loss: 0.1106 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3 - avg_train_loss: 0.0511  avg_val_loss: 0.1221  time: 111s\n",
            "Epoch 3 - avg_train_loss: 0.0511  avg_val_loss: 0.1221  time: 111s\n",
            "Epoch 3 - avg_train_loss: 0.0511  avg_val_loss: 0.1221  time: 111s\n",
            "Epoch 3 - avg_train_loss: 0.0511  avg_val_loss: 0.1221  time: 111s\n",
            "Epoch 3 - avg_train_loss: 0.0511  avg_val_loss: 0.1221  time: 111s\n",
            "Epoch 3 - avg_train_loss: 0.0511  avg_val_loss: 0.1221  time: 111s\n",
            "Epoch 3 - avg_train_loss: 0.0511  avg_val_loss: 0.1221  time: 111s\n",
            "Epoch 3 - avg_train_loss: 0.0511  avg_val_loss: 0.1221  time: 111s\n",
            "Epoch 3 - avg_train_loss: 0.0511  avg_val_loss: 0.1221  time: 111s\n",
            "Epoch 3 - avg_train_loss: 0.0511  avg_val_loss: 0.1221  time: 111s\n",
            "Epoch 3 - Score: 0.1221124874213016\n",
            "Epoch 3 - Score: 0.1221124874213016\n",
            "Epoch 3 - Score: 0.1221124874213016\n",
            "Epoch 3 - Score: 0.1221124874213016\n",
            "Epoch 3 - Score: 0.1221124874213016\n",
            "Epoch 3 - Score: 0.1221124874213016\n",
            "Epoch 3 - Score: 0.1221124874213016\n",
            "Epoch 3 - Score: 0.1221124874213016\n",
            "Epoch 3 - Score: 0.1221124874213016\n",
            "Epoch 3 - Score: 0.1221124874213016\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EVAL: [76/77] Elapsed 0m 8s (remain 0m 0s) Loss: 0.1221 \n",
            "Log loss for 2 is 0.1221124874213016\n",
            "Epoch: [4][0/306] Elapsed 0m 0s (remain 3m 1s) Loss: 0.0214 \n",
            "Epoch: [4][100/306] Elapsed 0m 34s (remain 1m 9s) Loss: 0.0248 \n",
            "Epoch: [4][200/306] Elapsed 1m 7s (remain 0m 35s) Loss: 0.0226 \n",
            "Epoch: [4][300/306] Elapsed 1m 41s (remain 0m 1s) Loss: 0.0240 \n",
            "Epoch: [4][305/306] Elapsed 1m 42s (remain 0m 0s) Loss: 0.0245 \n",
            "EVAL: [0/77] Elapsed 0m 0s (remain 0m 27s) Loss: 0.0176 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4 - avg_train_loss: 0.0245  avg_val_loss: 0.1426  time: 112s\n",
            "Epoch 4 - avg_train_loss: 0.0245  avg_val_loss: 0.1426  time: 112s\n",
            "Epoch 4 - avg_train_loss: 0.0245  avg_val_loss: 0.1426  time: 112s\n",
            "Epoch 4 - avg_train_loss: 0.0245  avg_val_loss: 0.1426  time: 112s\n",
            "Epoch 4 - avg_train_loss: 0.0245  avg_val_loss: 0.1426  time: 112s\n",
            "Epoch 4 - avg_train_loss: 0.0245  avg_val_loss: 0.1426  time: 112s\n",
            "Epoch 4 - avg_train_loss: 0.0245  avg_val_loss: 0.1426  time: 112s\n",
            "Epoch 4 - avg_train_loss: 0.0245  avg_val_loss: 0.1426  time: 112s\n",
            "Epoch 4 - avg_train_loss: 0.0245  avg_val_loss: 0.1426  time: 112s\n",
            "Epoch 4 - avg_train_loss: 0.0245  avg_val_loss: 0.1426  time: 112s\n",
            "Epoch 4 - Score: 0.14256457521694202\n",
            "Epoch 4 - Score: 0.14256457521694202\n",
            "Epoch 4 - Score: 0.14256457521694202\n",
            "Epoch 4 - Score: 0.14256457521694202\n",
            "Epoch 4 - Score: 0.14256457521694202\n",
            "Epoch 4 - Score: 0.14256457521694202\n",
            "Epoch 4 - Score: 0.14256457521694202\n",
            "Epoch 4 - Score: 0.14256457521694202\n",
            "Epoch 4 - Score: 0.14256457521694202\n",
            "Epoch 4 - Score: 0.14256457521694202\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EVAL: [76/77] Elapsed 0m 8s (remain 0m 0s) Loss: 0.1426 \n",
            "Log loss for 3 is 0.14256457521694202\n",
            "Epoch: [5][0/306] Elapsed 0m 0s (remain 3m 9s) Loss: 0.0070 \n",
            "Epoch: [5][100/306] Elapsed 0m 34s (remain 1m 9s) Loss: 0.0166 \n",
            "Epoch: [5][200/306] Elapsed 1m 7s (remain 0m 35s) Loss: 0.0168 \n",
            "Epoch: [5][300/306] Elapsed 1m 40s (remain 0m 1s) Loss: 0.0192 \n",
            "Epoch: [5][305/306] Elapsed 1m 42s (remain 0m 0s) Loss: 0.0192 \n",
            "EVAL: [0/77] Elapsed 0m 0s (remain 0m 27s) Loss: 0.0044 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5 - avg_train_loss: 0.0192  avg_val_loss: 0.1779  time: 111s\n",
            "Epoch 5 - avg_train_loss: 0.0192  avg_val_loss: 0.1779  time: 111s\n",
            "Epoch 5 - avg_train_loss: 0.0192  avg_val_loss: 0.1779  time: 111s\n",
            "Epoch 5 - avg_train_loss: 0.0192  avg_val_loss: 0.1779  time: 111s\n",
            "Epoch 5 - avg_train_loss: 0.0192  avg_val_loss: 0.1779  time: 111s\n",
            "Epoch 5 - avg_train_loss: 0.0192  avg_val_loss: 0.1779  time: 111s\n",
            "Epoch 5 - avg_train_loss: 0.0192  avg_val_loss: 0.1779  time: 111s\n",
            "Epoch 5 - avg_train_loss: 0.0192  avg_val_loss: 0.1779  time: 111s\n",
            "Epoch 5 - avg_train_loss: 0.0192  avg_val_loss: 0.1779  time: 111s\n",
            "Epoch 5 - avg_train_loss: 0.0192  avg_val_loss: 0.1779  time: 111s\n",
            "Epoch 5 - Score: 0.17794011458477496\n",
            "Epoch 5 - Score: 0.17794011458477496\n",
            "Epoch 5 - Score: 0.17794011458477496\n",
            "Epoch 5 - Score: 0.17794011458477496\n",
            "Epoch 5 - Score: 0.17794011458477496\n",
            "Epoch 5 - Score: 0.17794011458477496\n",
            "Epoch 5 - Score: 0.17794011458477496\n",
            "Epoch 5 - Score: 0.17794011458477496\n",
            "Epoch 5 - Score: 0.17794011458477496\n",
            "Epoch 5 - Score: 0.17794011458477496\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EVAL: [76/77] Elapsed 0m 8s (remain 0m 0s) Loss: 0.1779 \n",
            "Log loss for 4 is 0.17794011458477496\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "========== fold: 3 result ==========\n",
            "========== fold: 3 result ==========\n",
            "========== fold: 3 result ==========\n",
            "========== fold: 3 result ==========\n",
            "========== fold: 3 result ==========\n",
            "========== fold: 3 result ==========\n",
            "========== fold: 3 result ==========\n",
            "========== fold: 3 result ==========\n",
            "========== fold: 3 result ==========\n",
            "========== fold: 3 result ==========\n",
            "Score: 0.11487\n",
            "Score: 0.11487\n",
            "Score: 0.11487\n",
            "Score: 0.11487\n",
            "Score: 0.11487\n",
            "Score: 0.11487\n",
            "Score: 0.11487\n",
            "Score: 0.11487\n",
            "Score: 0.11487\n",
            "Score: 0.11487\n",
            "========== fold: 4 training ==========\n",
            "========== fold: 4 training ==========\n",
            "========== fold: 4 training ==========\n",
            "========== fold: 4 training ==========\n",
            "========== fold: 4 training ==========\n",
            "========== fold: 4 training ==========\n",
            "========== fold: 4 training ==========\n",
            "========== fold: 4 training ==========\n",
            "========== fold: 4 training ==========\n",
            "========== fold: 4 training ==========\n",
            "Didn't find file /content/activebus_bert_final/added_tokens.json. We won't load it.\n",
            "loading file /content/activebus_bert_final/vocab.txt\n",
            "loading file None\n",
            "loading file /content/activebus_bert_final/special_tokens_map.json\n",
            "loading file /content/activebus_bert_final/tokenizer_config.json\n",
            "loading file /content/activebus_bert_final/tokenizer.json\n",
            "Didn't find file /content/activebus_bert_final/added_tokens.json. We won't load it.\n",
            "loading file /content/activebus_bert_final/vocab.txt\n",
            "loading file None\n",
            "loading file /content/activebus_bert_final/special_tokens_map.json\n",
            "loading file /content/activebus_bert_final/tokenizer_config.json\n",
            "loading file /content/activebus_bert_final/tokenizer.json\n",
            "loading configuration file /content/activebus_bert_final/config.json\n",
            "Model config BertConfig {\n",
            "  \"_name_or_path\": \"activebus/BERT-XD_Review\",\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_ids\": 0,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.15.0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "loading weights file /content/activebus_bert_final/pytorch_model.bin\n",
            "Some weights of the model checkpoint at /content/activebus_bert_final were not used when initializing BertForSequenceClassification: ['cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /content/activebus_bert_final and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: [1][0/306] Elapsed 0m 0s (remain 3m 5s) Loss: 0.6502 \n",
            "Epoch: [1][100/306] Elapsed 0m 34s (remain 1m 9s) Loss: 0.2040 \n",
            "Epoch: [1][200/306] Elapsed 1m 7s (remain 0m 35s) Loss: 0.1746 \n",
            "Epoch: [1][300/306] Elapsed 1m 40s (remain 0m 1s) Loss: 0.1541 \n",
            "Epoch: [1][305/306] Elapsed 1m 42s (remain 0m 0s) Loss: 0.1540 \n",
            "EVAL: [0/77] Elapsed 0m 0s (remain 0m 28s) Loss: 0.2516 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1 - avg_train_loss: 0.1540  avg_val_loss: 0.1232  time: 111s\n",
            "Epoch 1 - avg_train_loss: 0.1540  avg_val_loss: 0.1232  time: 111s\n",
            "Epoch 1 - avg_train_loss: 0.1540  avg_val_loss: 0.1232  time: 111s\n",
            "Epoch 1 - avg_train_loss: 0.1540  avg_val_loss: 0.1232  time: 111s\n",
            "Epoch 1 - avg_train_loss: 0.1540  avg_val_loss: 0.1232  time: 111s\n",
            "Epoch 1 - avg_train_loss: 0.1540  avg_val_loss: 0.1232  time: 111s\n",
            "Epoch 1 - avg_train_loss: 0.1540  avg_val_loss: 0.1232  time: 111s\n",
            "Epoch 1 - avg_train_loss: 0.1540  avg_val_loss: 0.1232  time: 111s\n",
            "Epoch 1 - avg_train_loss: 0.1540  avg_val_loss: 0.1232  time: 111s\n",
            "Epoch 1 - avg_train_loss: 0.1540  avg_val_loss: 0.1232  time: 111s\n",
            "Epoch 1 - Score: 0.12323454065026736\n",
            "Epoch 1 - Score: 0.12323454065026736\n",
            "Epoch 1 - Score: 0.12323454065026736\n",
            "Epoch 1 - Score: 0.12323454065026736\n",
            "Epoch 1 - Score: 0.12323454065026736\n",
            "Epoch 1 - Score: 0.12323454065026736\n",
            "Epoch 1 - Score: 0.12323454065026736\n",
            "Epoch 1 - Score: 0.12323454065026736\n",
            "Epoch 1 - Score: 0.12323454065026736\n",
            "Epoch 1 - Score: 0.12323454065026736\n",
            "Epoch 1 - Save Best Score: 0.1232 Model\n",
            "Epoch 1 - Save Best Score: 0.1232 Model\n",
            "Epoch 1 - Save Best Score: 0.1232 Model\n",
            "Epoch 1 - Save Best Score: 0.1232 Model\n",
            "Epoch 1 - Save Best Score: 0.1232 Model\n",
            "Epoch 1 - Save Best Score: 0.1232 Model\n",
            "Epoch 1 - Save Best Score: 0.1232 Model\n",
            "Epoch 1 - Save Best Score: 0.1232 Model\n",
            "Epoch 1 - Save Best Score: 0.1232 Model\n",
            "Epoch 1 - Save Best Score: 0.1232 Model\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EVAL: [76/77] Elapsed 0m 8s (remain 0m 0s) Loss: 0.1232 \n",
            "Log loss for 0 is 0.12323454065026736\n",
            "Epoch: [2][0/306] Elapsed 0m 0s (remain 3m 9s) Loss: 0.0155 \n",
            "Epoch: [2][100/306] Elapsed 0m 34s (remain 1m 9s) Loss: 0.0806 \n",
            "Epoch: [2][200/306] Elapsed 1m 7s (remain 0m 35s) Loss: 0.0816 \n",
            "Epoch: [2][300/306] Elapsed 1m 40s (remain 0m 1s) Loss: 0.0824 \n",
            "Epoch: [2][305/306] Elapsed 1m 42s (remain 0m 0s) Loss: 0.0816 \n",
            "EVAL: [0/77] Elapsed 0m 0s (remain 0m 28s) Loss: 0.3083 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2 - avg_train_loss: 0.0816  avg_val_loss: 0.1472  time: 111s\n",
            "Epoch 2 - avg_train_loss: 0.0816  avg_val_loss: 0.1472  time: 111s\n",
            "Epoch 2 - avg_train_loss: 0.0816  avg_val_loss: 0.1472  time: 111s\n",
            "Epoch 2 - avg_train_loss: 0.0816  avg_val_loss: 0.1472  time: 111s\n",
            "Epoch 2 - avg_train_loss: 0.0816  avg_val_loss: 0.1472  time: 111s\n",
            "Epoch 2 - avg_train_loss: 0.0816  avg_val_loss: 0.1472  time: 111s\n",
            "Epoch 2 - avg_train_loss: 0.0816  avg_val_loss: 0.1472  time: 111s\n",
            "Epoch 2 - avg_train_loss: 0.0816  avg_val_loss: 0.1472  time: 111s\n",
            "Epoch 2 - avg_train_loss: 0.0816  avg_val_loss: 0.1472  time: 111s\n",
            "Epoch 2 - avg_train_loss: 0.0816  avg_val_loss: 0.1472  time: 111s\n",
            "Epoch 2 - Score: 0.14718948382073627\n",
            "Epoch 2 - Score: 0.14718948382073627\n",
            "Epoch 2 - Score: 0.14718948382073627\n",
            "Epoch 2 - Score: 0.14718948382073627\n",
            "Epoch 2 - Score: 0.14718948382073627\n",
            "Epoch 2 - Score: 0.14718948382073627\n",
            "Epoch 2 - Score: 0.14718948382073627\n",
            "Epoch 2 - Score: 0.14718948382073627\n",
            "Epoch 2 - Score: 0.14718948382073627\n",
            "Epoch 2 - Score: 0.14718948382073627\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EVAL: [76/77] Elapsed 0m 8s (remain 0m 0s) Loss: 0.1472 \n",
            "Log loss for 1 is 0.14718948382073627\n",
            "Epoch: [3][0/306] Elapsed 0m 0s (remain 3m 5s) Loss: 0.0109 \n",
            "Epoch: [3][100/306] Elapsed 0m 34s (remain 1m 9s) Loss: 0.0435 \n",
            "Epoch: [3][200/306] Elapsed 1m 7s (remain 0m 35s) Loss: 0.0422 \n",
            "Epoch: [3][300/306] Elapsed 1m 41s (remain 0m 1s) Loss: 0.0420 \n",
            "Epoch: [3][305/306] Elapsed 1m 42s (remain 0m 0s) Loss: 0.0423 \n",
            "EVAL: [0/77] Elapsed 0m 0s (remain 0m 29s) Loss: 0.4000 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3 - avg_train_loss: 0.0423  avg_val_loss: 0.1639  time: 112s\n",
            "Epoch 3 - avg_train_loss: 0.0423  avg_val_loss: 0.1639  time: 112s\n",
            "Epoch 3 - avg_train_loss: 0.0423  avg_val_loss: 0.1639  time: 112s\n",
            "Epoch 3 - avg_train_loss: 0.0423  avg_val_loss: 0.1639  time: 112s\n",
            "Epoch 3 - avg_train_loss: 0.0423  avg_val_loss: 0.1639  time: 112s\n",
            "Epoch 3 - avg_train_loss: 0.0423  avg_val_loss: 0.1639  time: 112s\n",
            "Epoch 3 - avg_train_loss: 0.0423  avg_val_loss: 0.1639  time: 112s\n",
            "Epoch 3 - avg_train_loss: 0.0423  avg_val_loss: 0.1639  time: 112s\n",
            "Epoch 3 - avg_train_loss: 0.0423  avg_val_loss: 0.1639  time: 112s\n",
            "Epoch 3 - avg_train_loss: 0.0423  avg_val_loss: 0.1639  time: 112s\n",
            "Epoch 3 - Score: 0.16390082481602425\n",
            "Epoch 3 - Score: 0.16390082481602425\n",
            "Epoch 3 - Score: 0.16390082481602425\n",
            "Epoch 3 - Score: 0.16390082481602425\n",
            "Epoch 3 - Score: 0.16390082481602425\n",
            "Epoch 3 - Score: 0.16390082481602425\n",
            "Epoch 3 - Score: 0.16390082481602425\n",
            "Epoch 3 - Score: 0.16390082481602425\n",
            "Epoch 3 - Score: 0.16390082481602425\n",
            "Epoch 3 - Score: 0.16390082481602425\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EVAL: [76/77] Elapsed 0m 8s (remain 0m 0s) Loss: 0.1639 \n",
            "Log loss for 2 is 0.16390082481602425\n",
            "Epoch: [4][0/306] Elapsed 0m 0s (remain 3m 1s) Loss: 0.0016 \n",
            "Epoch: [4][100/306] Elapsed 0m 34s (remain 1m 9s) Loss: 0.0215 \n",
            "Epoch: [4][200/306] Elapsed 1m 7s (remain 0m 35s) Loss: 0.0189 \n",
            "Epoch: [4][300/306] Elapsed 1m 40s (remain 0m 1s) Loss: 0.0218 \n",
            "Epoch: [4][305/306] Elapsed 1m 42s (remain 0m 0s) Loss: 0.0222 \n",
            "EVAL: [0/77] Elapsed 0m 0s (remain 0m 27s) Loss: 0.5223 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4 - avg_train_loss: 0.0222  avg_val_loss: 0.1814  time: 111s\n",
            "Epoch 4 - avg_train_loss: 0.0222  avg_val_loss: 0.1814  time: 111s\n",
            "Epoch 4 - avg_train_loss: 0.0222  avg_val_loss: 0.1814  time: 111s\n",
            "Epoch 4 - avg_train_loss: 0.0222  avg_val_loss: 0.1814  time: 111s\n",
            "Epoch 4 - avg_train_loss: 0.0222  avg_val_loss: 0.1814  time: 111s\n",
            "Epoch 4 - avg_train_loss: 0.0222  avg_val_loss: 0.1814  time: 111s\n",
            "Epoch 4 - avg_train_loss: 0.0222  avg_val_loss: 0.1814  time: 111s\n",
            "Epoch 4 - avg_train_loss: 0.0222  avg_val_loss: 0.1814  time: 111s\n",
            "Epoch 4 - avg_train_loss: 0.0222  avg_val_loss: 0.1814  time: 111s\n",
            "Epoch 4 - avg_train_loss: 0.0222  avg_val_loss: 0.1814  time: 111s\n",
            "Epoch 4 - Score: 0.1814228393568296\n",
            "Epoch 4 - Score: 0.1814228393568296\n",
            "Epoch 4 - Score: 0.1814228393568296\n",
            "Epoch 4 - Score: 0.1814228393568296\n",
            "Epoch 4 - Score: 0.1814228393568296\n",
            "Epoch 4 - Score: 0.1814228393568296\n",
            "Epoch 4 - Score: 0.1814228393568296\n",
            "Epoch 4 - Score: 0.1814228393568296\n",
            "Epoch 4 - Score: 0.1814228393568296\n",
            "Epoch 4 - Score: 0.1814228393568296\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EVAL: [76/77] Elapsed 0m 8s (remain 0m 0s) Loss: 0.1814 \n",
            "Log loss for 3 is 0.1814228393568296\n",
            "Epoch: [5][0/306] Elapsed 0m 0s (remain 3m 10s) Loss: 0.0008 \n",
            "Epoch: [5][100/306] Elapsed 0m 34s (remain 1m 9s) Loss: 0.0070 \n",
            "Epoch: [5][200/306] Elapsed 1m 7s (remain 0m 35s) Loss: 0.0127 \n",
            "Epoch: [5][300/306] Elapsed 1m 40s (remain 0m 1s) Loss: 0.0165 \n",
            "Epoch: [5][305/306] Elapsed 1m 42s (remain 0m 0s) Loss: 0.0166 \n",
            "EVAL: [0/77] Elapsed 0m 0s (remain 0m 29s) Loss: 0.6083 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5 - avg_train_loss: 0.0166  avg_val_loss: 0.2097  time: 111s\n",
            "Epoch 5 - avg_train_loss: 0.0166  avg_val_loss: 0.2097  time: 111s\n",
            "Epoch 5 - avg_train_loss: 0.0166  avg_val_loss: 0.2097  time: 111s\n",
            "Epoch 5 - avg_train_loss: 0.0166  avg_val_loss: 0.2097  time: 111s\n",
            "Epoch 5 - avg_train_loss: 0.0166  avg_val_loss: 0.2097  time: 111s\n",
            "Epoch 5 - avg_train_loss: 0.0166  avg_val_loss: 0.2097  time: 111s\n",
            "Epoch 5 - avg_train_loss: 0.0166  avg_val_loss: 0.2097  time: 111s\n",
            "Epoch 5 - avg_train_loss: 0.0166  avg_val_loss: 0.2097  time: 111s\n",
            "Epoch 5 - avg_train_loss: 0.0166  avg_val_loss: 0.2097  time: 111s\n",
            "Epoch 5 - avg_train_loss: 0.0166  avg_val_loss: 0.2097  time: 111s\n",
            "Epoch 5 - Score: 0.2097125785829628\n",
            "Epoch 5 - Score: 0.2097125785829628\n",
            "Epoch 5 - Score: 0.2097125785829628\n",
            "Epoch 5 - Score: 0.2097125785829628\n",
            "Epoch 5 - Score: 0.2097125785829628\n",
            "Epoch 5 - Score: 0.2097125785829628\n",
            "Epoch 5 - Score: 0.2097125785829628\n",
            "Epoch 5 - Score: 0.2097125785829628\n",
            "Epoch 5 - Score: 0.2097125785829628\n",
            "Epoch 5 - Score: 0.2097125785829628\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EVAL: [76/77] Elapsed 0m 8s (remain 0m 0s) Loss: 0.2097 \n",
            "Log loss for 4 is 0.2097125785829628\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "========== fold: 4 result ==========\n",
            "========== fold: 4 result ==========\n",
            "========== fold: 4 result ==========\n",
            "========== fold: 4 result ==========\n",
            "========== fold: 4 result ==========\n",
            "========== fold: 4 result ==========\n",
            "========== fold: 4 result ==========\n",
            "========== fold: 4 result ==========\n",
            "========== fold: 4 result ==========\n",
            "========== fold: 4 result ==========\n",
            "Score: 0.12323\n",
            "Score: 0.12323\n",
            "Score: 0.12323\n",
            "Score: 0.12323\n",
            "Score: 0.12323\n",
            "Score: 0.12323\n",
            "Score: 0.12323\n",
            "Score: 0.12323\n",
            "Score: 0.12323\n",
            "Score: 0.12323\n",
            "========== CV ==========\n",
            "========== CV ==========\n",
            "========== CV ==========\n",
            "========== CV ==========\n",
            "========== CV ==========\n",
            "========== CV ==========\n",
            "========== CV ==========\n",
            "========== CV ==========\n",
            "========== CV ==========\n",
            "========== CV ==========\n",
            "Score: 0.10452\n",
            "Score: 0.10452\n",
            "Score: 0.10452\n",
            "Score: 0.10452\n",
            "Score: 0.10452\n",
            "Score: 0.10452\n",
            "Score: 0.10452\n",
            "Score: 0.10452\n",
            "Score: 0.10452\n",
            "Score: 0.10452\n",
            "Didn't find file /content/activebus_bert_final/added_tokens.json. We won't load it.\n",
            "loading file /content/activebus_bert_final/vocab.txt\n",
            "loading file None\n",
            "loading file /content/activebus_bert_final/special_tokens_map.json\n",
            "loading file /content/activebus_bert_final/tokenizer_config.json\n",
            "loading file /content/activebus_bert_final/tokenizer.json\n",
            "========== model: bert-base-uncased fold: 0 inference ==========\n",
            "========== model: bert-base-uncased fold: 0 inference ==========\n",
            "========== model: bert-base-uncased fold: 0 inference ==========\n",
            "========== model: bert-base-uncased fold: 0 inference ==========\n",
            "========== model: bert-base-uncased fold: 0 inference ==========\n",
            "========== model: bert-base-uncased fold: 0 inference ==========\n",
            "========== model: bert-base-uncased fold: 0 inference ==========\n",
            "========== model: bert-base-uncased fold: 0 inference ==========\n",
            "========== model: bert-base-uncased fold: 0 inference ==========\n",
            "========== model: bert-base-uncased fold: 0 inference ==========\n",
            "loading configuration file /content/activebus_bert_final/config.json\n",
            "Model config BertConfig {\n",
            "  \"_name_or_path\": \"activebus/BERT-XD_Review\",\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_ids\": 0,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.15.0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "loading weights file /content/activebus_bert_final/pytorch_model.bin\n",
            "Some weights of the model checkpoint at /content/activebus_bert_final were not used when initializing BertForSequenceClassification: ['cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /content/activebus_bert_final and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "\n",
            "  0%|          | 0/165 [00:00<?, ?it/s]\u001b[A\n",
            "  1%|          | 1/165 [00:00<00:22,  7.35it/s]\u001b[A\n",
            "  1%|          | 2/165 [00:00<00:20,  7.84it/s]\u001b[A\n",
            "  2%|▏         | 3/165 [00:00<00:19,  8.15it/s]\u001b[A\n",
            "  2%|▏         | 4/165 [00:00<00:19,  8.36it/s]\u001b[A\n",
            "  3%|▎         | 5/165 [00:00<00:18,  8.54it/s]\u001b[A\n",
            "  4%|▎         | 6/165 [00:00<00:18,  8.66it/s]\u001b[A\n",
            "  4%|▍         | 7/165 [00:00<00:18,  8.71it/s]\u001b[A\n",
            "  5%|▍         | 8/165 [00:00<00:17,  8.76it/s]\u001b[A\n",
            "  5%|▌         | 9/165 [00:01<00:17,  8.79it/s]\u001b[A\n",
            "  6%|▌         | 10/165 [00:01<00:17,  8.79it/s]\u001b[A\n",
            "  7%|▋         | 11/165 [00:01<00:17,  8.81it/s]\u001b[A\n",
            "  7%|▋         | 12/165 [00:01<00:17,  8.78it/s]\u001b[A\n",
            "  8%|▊         | 13/165 [00:01<00:17,  8.79it/s]\u001b[A\n",
            "  8%|▊         | 14/165 [00:01<00:17,  8.82it/s]\u001b[A\n",
            "  9%|▉         | 15/165 [00:01<00:16,  8.85it/s]\u001b[A\n",
            " 10%|▉         | 16/165 [00:01<00:16,  8.85it/s]\u001b[A\n",
            " 10%|█         | 17/165 [00:01<00:16,  8.86it/s]\u001b[A\n",
            " 11%|█         | 18/165 [00:02<00:16,  8.87it/s]\u001b[A\n",
            " 12%|█▏        | 19/165 [00:02<00:16,  8.85it/s]\u001b[A\n",
            " 12%|█▏        | 20/165 [00:02<00:16,  8.87it/s]\u001b[A\n",
            " 13%|█▎        | 21/165 [00:02<00:16,  8.88it/s]\u001b[A\n",
            " 13%|█▎        | 22/165 [00:02<00:16,  8.88it/s]\u001b[A\n",
            " 14%|█▍        | 23/165 [00:02<00:15,  8.90it/s]\u001b[A\n",
            " 15%|█▍        | 24/165 [00:02<00:16,  8.80it/s]\u001b[A\n",
            " 15%|█▌        | 25/165 [00:02<00:15,  8.83it/s]\u001b[A\n",
            " 16%|█▌        | 26/165 [00:02<00:15,  8.82it/s]\u001b[A\n",
            " 16%|█▋        | 27/165 [00:03<00:15,  8.83it/s]\u001b[A\n",
            " 17%|█▋        | 28/165 [00:03<00:15,  8.85it/s]\u001b[A\n",
            " 18%|█▊        | 29/165 [00:03<00:15,  8.85it/s]\u001b[A\n",
            " 18%|█▊        | 30/165 [00:03<00:15,  8.87it/s]\u001b[A\n",
            " 19%|█▉        | 31/165 [00:03<00:15,  8.88it/s]\u001b[A\n",
            " 19%|█▉        | 32/165 [00:03<00:14,  8.88it/s]\u001b[A\n",
            " 20%|██        | 33/165 [00:03<00:14,  8.88it/s]\u001b[A\n",
            " 21%|██        | 34/165 [00:03<00:14,  8.87it/s]\u001b[A\n",
            " 21%|██        | 35/165 [00:03<00:14,  8.88it/s]\u001b[A\n",
            " 22%|██▏       | 36/165 [00:04<00:14,  8.86it/s]\u001b[A\n",
            " 22%|██▏       | 37/165 [00:04<00:14,  8.86it/s]\u001b[A\n",
            " 23%|██▎       | 38/165 [00:04<00:14,  8.87it/s]\u001b[A\n",
            " 24%|██▎       | 39/165 [00:04<00:14,  8.87it/s]\u001b[A\n",
            " 24%|██▍       | 40/165 [00:04<00:14,  8.84it/s]\u001b[A\n",
            " 25%|██▍       | 41/165 [00:04<00:13,  8.86it/s]\u001b[A\n",
            " 25%|██▌       | 42/165 [00:04<00:13,  8.88it/s]\u001b[A\n",
            " 26%|██▌       | 43/165 [00:04<00:13,  8.88it/s]\u001b[A\n",
            " 27%|██▋       | 44/165 [00:05<00:13,  8.88it/s]\u001b[A\n",
            " 27%|██▋       | 45/165 [00:05<00:13,  8.89it/s]\u001b[A\n",
            " 28%|██▊       | 46/165 [00:05<00:13,  8.88it/s]\u001b[A\n",
            " 28%|██▊       | 47/165 [00:05<00:13,  8.90it/s]\u001b[A\n",
            " 29%|██▉       | 48/165 [00:05<00:13,  8.90it/s]\u001b[A\n",
            " 30%|██▉       | 49/165 [00:05<00:13,  8.90it/s]\u001b[A\n",
            " 30%|███       | 50/165 [00:05<00:12,  8.90it/s]\u001b[A\n",
            " 31%|███       | 51/165 [00:05<00:12,  8.91it/s]\u001b[A\n",
            " 32%|███▏      | 52/165 [00:05<00:12,  8.91it/s]\u001b[A\n",
            " 32%|███▏      | 53/165 [00:06<00:12,  8.86it/s]\u001b[A\n",
            " 33%|███▎      | 54/165 [00:06<00:12,  8.86it/s]\u001b[A\n",
            " 33%|███▎      | 55/165 [00:06<00:12,  8.84it/s]\u001b[A\n",
            " 34%|███▍      | 56/165 [00:06<00:12,  8.85it/s]\u001b[A\n",
            " 35%|███▍      | 57/165 [00:06<00:12,  8.86it/s]\u001b[A\n",
            " 35%|███▌      | 58/165 [00:06<00:12,  8.87it/s]\u001b[A\n",
            " 36%|███▌      | 59/165 [00:06<00:11,  8.84it/s]\u001b[A\n",
            " 36%|███▋      | 60/165 [00:06<00:11,  8.83it/s]\u001b[A\n",
            " 37%|███▋      | 61/165 [00:06<00:11,  8.82it/s]\u001b[A\n",
            " 38%|███▊      | 62/165 [00:07<00:11,  8.82it/s]\u001b[A\n",
            " 38%|███▊      | 63/165 [00:07<00:11,  8.85it/s]\u001b[A\n",
            " 39%|███▉      | 64/165 [00:07<00:11,  8.86it/s]\u001b[A\n",
            " 39%|███▉      | 65/165 [00:07<00:11,  8.88it/s]\u001b[A\n",
            " 40%|████      | 66/165 [00:07<00:11,  8.84it/s]\u001b[A\n",
            " 41%|████      | 67/165 [00:07<00:11,  8.83it/s]\u001b[A\n",
            " 41%|████      | 68/165 [00:07<00:10,  8.83it/s]\u001b[A\n",
            " 42%|████▏     | 69/165 [00:07<00:10,  8.83it/s]\u001b[A\n",
            " 42%|████▏     | 70/165 [00:07<00:10,  8.84it/s]\u001b[A\n",
            " 43%|████▎     | 71/165 [00:08<00:10,  8.86it/s]\u001b[A\n",
            " 44%|████▎     | 72/165 [00:08<00:10,  8.87it/s]\u001b[A\n",
            " 44%|████▍     | 73/165 [00:08<00:10,  8.88it/s]\u001b[A\n",
            " 45%|████▍     | 74/165 [00:08<00:10,  8.88it/s]\u001b[A\n",
            " 45%|████▌     | 75/165 [00:08<00:10,  8.87it/s]\u001b[A\n",
            " 46%|████▌     | 76/165 [00:08<00:10,  8.86it/s]\u001b[A\n",
            " 47%|████▋     | 77/165 [00:08<00:09,  8.88it/s]\u001b[A\n",
            " 47%|████▋     | 78/165 [00:08<00:09,  8.88it/s]\u001b[A\n",
            " 48%|████▊     | 79/165 [00:08<00:09,  8.88it/s]\u001b[A\n",
            " 48%|████▊     | 80/165 [00:09<00:09,  8.87it/s]\u001b[A\n",
            " 49%|████▉     | 81/165 [00:09<00:09,  8.87it/s]\u001b[A\n",
            " 50%|████▉     | 82/165 [00:09<00:09,  8.84it/s]\u001b[A\n",
            " 50%|█████     | 83/165 [00:09<00:09,  8.85it/s]\u001b[A\n",
            " 51%|█████     | 84/165 [00:09<00:09,  8.84it/s]\u001b[A\n",
            " 52%|█████▏    | 85/165 [00:09<00:09,  8.85it/s]\u001b[A\n",
            " 52%|█████▏    | 86/165 [00:09<00:08,  8.87it/s]\u001b[A\n",
            " 53%|█████▎    | 87/165 [00:09<00:08,  8.86it/s]\u001b[A\n",
            " 53%|█████▎    | 88/165 [00:09<00:08,  8.86it/s]\u001b[A\n",
            " 54%|█████▍    | 89/165 [00:10<00:08,  8.86it/s]\u001b[A\n",
            " 55%|█████▍    | 90/165 [00:10<00:08,  8.87it/s]\u001b[A\n",
            " 55%|█████▌    | 91/165 [00:10<00:08,  8.86it/s]\u001b[A\n",
            " 56%|█████▌    | 92/165 [00:10<00:08,  8.86it/s]\u001b[A\n",
            " 56%|█████▋    | 93/165 [00:10<00:08,  8.86it/s]\u001b[A\n",
            " 57%|█████▋    | 94/165 [00:10<00:08,  8.87it/s]\u001b[A\n",
            " 58%|█████▊    | 95/165 [00:10<00:07,  8.88it/s]\u001b[A\n",
            " 58%|█████▊    | 96/165 [00:10<00:07,  8.88it/s]\u001b[A\n",
            " 59%|█████▉    | 97/165 [00:10<00:07,  8.88it/s]\u001b[A\n",
            " 59%|█████▉    | 98/165 [00:11<00:07,  8.90it/s]\u001b[A\n",
            " 60%|██████    | 99/165 [00:11<00:07,  8.90it/s]\u001b[A\n",
            " 61%|██████    | 100/165 [00:11<00:07,  8.88it/s]\u001b[A\n",
            " 61%|██████    | 101/165 [00:11<00:07,  8.90it/s]\u001b[A\n",
            " 62%|██████▏   | 102/165 [00:11<00:07,  8.87it/s]\u001b[A\n",
            " 62%|██████▏   | 103/165 [00:11<00:06,  8.87it/s]\u001b[A\n",
            " 63%|██████▎   | 104/165 [00:11<00:06,  8.88it/s]\u001b[A\n",
            " 64%|██████▎   | 105/165 [00:11<00:06,  8.89it/s]\u001b[A\n",
            " 64%|██████▍   | 106/165 [00:11<00:06,  8.86it/s]\u001b[A\n",
            " 65%|██████▍   | 107/165 [00:12<00:06,  8.85it/s]\u001b[A\n",
            " 65%|██████▌   | 108/165 [00:12<00:06,  8.84it/s]\u001b[A\n",
            " 66%|██████▌   | 109/165 [00:12<00:06,  8.82it/s]\u001b[A\n",
            " 67%|██████▋   | 110/165 [00:12<00:06,  8.84it/s]\u001b[A\n",
            " 67%|██████▋   | 111/165 [00:12<00:06,  8.87it/s]\u001b[A\n",
            " 68%|██████▊   | 112/165 [00:12<00:05,  8.87it/s]\u001b[A\n",
            " 68%|██████▊   | 113/165 [00:12<00:05,  8.87it/s]\u001b[A\n",
            " 69%|██████▉   | 114/165 [00:12<00:05,  8.89it/s]\u001b[A\n",
            " 70%|██████▉   | 115/165 [00:13<00:05,  8.89it/s]\u001b[A\n",
            " 70%|███████   | 116/165 [00:13<00:05,  8.86it/s]\u001b[A\n",
            " 71%|███████   | 117/165 [00:13<00:05,  8.87it/s]\u001b[A\n",
            " 72%|███████▏  | 118/165 [00:13<00:05,  8.85it/s]\u001b[A\n",
            " 72%|███████▏  | 119/165 [00:13<00:05,  8.86it/s]\u001b[A\n",
            " 73%|███████▎  | 120/165 [00:13<00:05,  8.86it/s]\u001b[A\n",
            " 73%|███████▎  | 121/165 [00:13<00:04,  8.85it/s]\u001b[A\n",
            " 74%|███████▍  | 122/165 [00:13<00:04,  8.85it/s]\u001b[A\n",
            " 75%|███████▍  | 123/165 [00:13<00:04,  8.87it/s]\u001b[A\n",
            " 75%|███████▌  | 124/165 [00:14<00:04,  8.87it/s]\u001b[A\n",
            " 76%|███████▌  | 125/165 [00:14<00:04,  8.87it/s]\u001b[A\n",
            " 76%|███████▋  | 126/165 [00:14<00:04,  8.88it/s]\u001b[A\n",
            " 77%|███████▋  | 127/165 [00:14<00:04,  8.89it/s]\u001b[A\n",
            " 78%|███████▊  | 128/165 [00:14<00:04,  8.89it/s]\u001b[A\n",
            " 78%|███████▊  | 129/165 [00:14<00:04,  8.88it/s]\u001b[A\n",
            " 79%|███████▉  | 130/165 [00:14<00:03,  8.88it/s]\u001b[A\n",
            " 79%|███████▉  | 131/165 [00:14<00:03,  8.89it/s]\u001b[A\n",
            " 80%|████████  | 132/165 [00:14<00:03,  8.90it/s]\u001b[A\n",
            " 81%|████████  | 133/165 [00:15<00:03,  8.90it/s]\u001b[A\n",
            " 81%|████████  | 134/165 [00:15<00:03,  8.89it/s]\u001b[A\n",
            " 82%|████████▏ | 135/165 [00:15<00:03,  8.90it/s]\u001b[A\n",
            " 82%|████████▏ | 136/165 [00:15<00:03,  8.89it/s]\u001b[A\n",
            " 83%|████████▎ | 137/165 [00:15<00:03,  8.89it/s]\u001b[A\n",
            " 84%|████████▎ | 138/165 [00:15<00:03,  8.90it/s]\u001b[A\n",
            " 84%|████████▍ | 139/165 [00:15<00:02,  8.89it/s]\u001b[A\n",
            " 85%|████████▍ | 140/165 [00:15<00:02,  8.89it/s]\u001b[A\n",
            " 85%|████████▌ | 141/165 [00:15<00:02,  8.89it/s]\u001b[A\n",
            " 86%|████████▌ | 142/165 [00:16<00:02,  8.90it/s]\u001b[A\n",
            " 87%|████████▋ | 143/165 [00:16<00:02,  8.92it/s]\u001b[A\n",
            " 87%|████████▋ | 144/165 [00:16<00:02,  8.87it/s]\u001b[A\n",
            " 88%|████████▊ | 145/165 [00:16<00:02,  8.87it/s]\u001b[A\n",
            " 88%|████████▊ | 146/165 [00:16<00:02,  8.86it/s]\u001b[A\n",
            " 89%|████████▉ | 147/165 [00:16<00:02,  8.88it/s]\u001b[A\n",
            " 90%|████████▉ | 148/165 [00:16<00:01,  8.82it/s]\u001b[A\n",
            " 90%|█████████ | 149/165 [00:16<00:01,  8.83it/s]\u001b[A\n",
            " 91%|█████████ | 150/165 [00:16<00:01,  8.84it/s]\u001b[A\n",
            " 92%|█████████▏| 151/165 [00:17<00:01,  8.86it/s]\u001b[A\n",
            " 92%|█████████▏| 152/165 [00:17<00:01,  8.85it/s]\u001b[A\n",
            " 93%|█████████▎| 153/165 [00:17<00:01,  8.84it/s]\u001b[A\n",
            " 93%|█████████▎| 154/165 [00:17<00:01,  8.86it/s]\u001b[A\n",
            " 94%|█████████▍| 155/165 [00:17<00:01,  8.87it/s]\u001b[A\n",
            " 95%|█████████▍| 156/165 [00:17<00:01,  8.87it/s]\u001b[A\n",
            " 95%|█████████▌| 157/165 [00:17<00:00,  8.86it/s]\u001b[A\n",
            " 96%|█████████▌| 158/165 [00:17<00:00,  8.88it/s]\u001b[A\n",
            " 96%|█████████▋| 159/165 [00:17<00:00,  8.87it/s]\u001b[A\n",
            " 97%|█████████▋| 160/165 [00:18<00:00,  8.88it/s]\u001b[A\n",
            " 98%|█████████▊| 161/165 [00:18<00:00,  8.88it/s]\u001b[A\n",
            " 98%|█████████▊| 162/165 [00:18<00:00,  8.90it/s]\u001b[A\n",
            " 99%|█████████▉| 163/165 [00:18<00:00,  8.91it/s]\u001b[A\n",
            "100%|██████████| 165/165 [00:18<00:00,  8.84it/s]\n",
            "========== model: bert-base-uncased fold: 1 inference ==========\n",
            "========== model: bert-base-uncased fold: 1 inference ==========\n",
            "========== model: bert-base-uncased fold: 1 inference ==========\n",
            "========== model: bert-base-uncased fold: 1 inference ==========\n",
            "========== model: bert-base-uncased fold: 1 inference ==========\n",
            "========== model: bert-base-uncased fold: 1 inference ==========\n",
            "========== model: bert-base-uncased fold: 1 inference ==========\n",
            "========== model: bert-base-uncased fold: 1 inference ==========\n",
            "========== model: bert-base-uncased fold: 1 inference ==========\n",
            "========== model: bert-base-uncased fold: 1 inference ==========\n",
            "loading configuration file /content/activebus_bert_final/config.json\n",
            "Model config BertConfig {\n",
            "  \"_name_or_path\": \"activebus/BERT-XD_Review\",\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_ids\": 0,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.15.0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "loading weights file /content/activebus_bert_final/pytorch_model.bin\n",
            "Some weights of the model checkpoint at /content/activebus_bert_final were not used when initializing BertForSequenceClassification: ['cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /content/activebus_bert_final and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "\n",
            "  0%|          | 0/165 [00:00<?, ?it/s]\u001b[A\n",
            "  1%|          | 1/165 [00:00<00:19,  8.56it/s]\u001b[A\n",
            "  1%|          | 2/165 [00:00<00:18,  8.69it/s]\u001b[A\n",
            "  2%|▏         | 3/165 [00:00<00:18,  8.77it/s]\u001b[A\n",
            "  2%|▏         | 4/165 [00:00<00:18,  8.77it/s]\u001b[A\n",
            "  3%|▎         | 5/165 [00:00<00:18,  8.82it/s]\u001b[A\n",
            "  4%|▎         | 6/165 [00:00<00:17,  8.86it/s]\u001b[A\n",
            "  4%|▍         | 7/165 [00:00<00:17,  8.87it/s]\u001b[A\n",
            "  5%|▍         | 8/165 [00:00<00:17,  8.88it/s]\u001b[A\n",
            "  5%|▌         | 9/165 [00:01<00:17,  8.86it/s]\u001b[A\n",
            "  6%|▌         | 10/165 [00:01<00:17,  8.86it/s]\u001b[A\n",
            "  7%|▋         | 11/165 [00:01<00:17,  8.88it/s]\u001b[A\n",
            "  7%|▋         | 12/165 [00:01<00:17,  8.86it/s]\u001b[A\n",
            "  8%|▊         | 13/165 [00:01<00:17,  8.87it/s]\u001b[A\n",
            "  8%|▊         | 14/165 [00:01<00:16,  8.89it/s]\u001b[A\n",
            "  9%|▉         | 15/165 [00:01<00:16,  8.84it/s]\u001b[A\n",
            " 10%|▉         | 16/165 [00:01<00:16,  8.81it/s]\u001b[A\n",
            " 10%|█         | 17/165 [00:01<00:16,  8.81it/s]\u001b[A\n",
            " 11%|█         | 18/165 [00:02<00:16,  8.82it/s]\u001b[A\n",
            " 12%|█▏        | 19/165 [00:02<00:16,  8.82it/s]\u001b[A\n",
            " 12%|█▏        | 20/165 [00:02<00:16,  8.83it/s]\u001b[A\n",
            " 13%|█▎        | 21/165 [00:02<00:16,  8.82it/s]\u001b[A\n",
            " 13%|█▎        | 22/165 [00:02<00:16,  8.85it/s]\u001b[A\n",
            " 14%|█▍        | 23/165 [00:02<00:15,  8.88it/s]\u001b[A\n",
            " 15%|█▍        | 24/165 [00:02<00:15,  8.88it/s]\u001b[A\n",
            " 15%|█▌        | 25/165 [00:02<00:15,  8.86it/s]\u001b[A\n",
            " 16%|█▌        | 26/165 [00:02<00:15,  8.84it/s]\u001b[A\n",
            " 16%|█▋        | 27/165 [00:03<00:15,  8.83it/s]\u001b[A\n",
            " 17%|█▋        | 28/165 [00:03<00:15,  8.84it/s]\u001b[A\n",
            " 18%|█▊        | 29/165 [00:03<00:15,  8.85it/s]\u001b[A\n",
            " 18%|█▊        | 30/165 [00:03<00:15,  8.86it/s]\u001b[A\n",
            " 19%|█▉        | 31/165 [00:03<00:15,  8.87it/s]\u001b[A\n",
            " 19%|█▉        | 32/165 [00:03<00:14,  8.87it/s]\u001b[A\n",
            " 20%|██        | 33/165 [00:03<00:14,  8.88it/s]\u001b[A\n",
            " 21%|██        | 34/165 [00:03<00:14,  8.89it/s]\u001b[A\n",
            " 21%|██        | 35/165 [00:03<00:14,  8.86it/s]\u001b[A\n",
            " 22%|██▏       | 36/165 [00:04<00:14,  8.87it/s]\u001b[A\n",
            " 22%|██▏       | 37/165 [00:04<00:14,  8.90it/s]\u001b[A\n",
            " 23%|██▎       | 38/165 [00:04<00:14,  8.90it/s]\u001b[A\n",
            " 24%|██▎       | 39/165 [00:04<00:14,  8.88it/s]\u001b[A\n",
            " 24%|██▍       | 40/165 [00:04<00:14,  8.87it/s]\u001b[A\n",
            " 25%|██▍       | 41/165 [00:04<00:14,  8.84it/s]\u001b[A\n",
            " 25%|██▌       | 42/165 [00:04<00:13,  8.83it/s]\u001b[A\n",
            " 26%|██▌       | 43/165 [00:04<00:13,  8.83it/s]\u001b[A\n",
            " 27%|██▋       | 44/165 [00:04<00:13,  8.80it/s]\u001b[A\n",
            " 27%|██▋       | 45/165 [00:05<00:13,  8.80it/s]\u001b[A\n",
            " 28%|██▊       | 46/165 [00:05<00:13,  8.83it/s]\u001b[A\n",
            " 28%|██▊       | 47/165 [00:05<00:13,  8.85it/s]\u001b[A\n",
            " 29%|██▉       | 48/165 [00:05<00:13,  8.85it/s]\u001b[A\n",
            " 30%|██▉       | 49/165 [00:05<00:13,  8.86it/s]\u001b[A\n",
            " 30%|███       | 50/165 [00:05<00:12,  8.86it/s]\u001b[A\n",
            " 31%|███       | 51/165 [00:05<00:12,  8.86it/s]\u001b[A\n",
            " 32%|███▏      | 52/165 [00:05<00:12,  8.87it/s]\u001b[A\n",
            " 32%|███▏      | 53/165 [00:05<00:12,  8.88it/s]\u001b[A\n",
            " 33%|███▎      | 54/165 [00:06<00:12,  8.88it/s]\u001b[A\n",
            " 33%|███▎      | 55/165 [00:06<00:12,  8.87it/s]\u001b[A\n",
            " 34%|███▍      | 56/165 [00:06<00:12,  8.86it/s]\u001b[A\n",
            " 35%|███▍      | 57/165 [00:06<00:12,  8.86it/s]\u001b[A\n",
            " 35%|███▌      | 58/165 [00:06<00:12,  8.85it/s]\u001b[A\n",
            " 36%|███▌      | 59/165 [00:06<00:11,  8.87it/s]\u001b[A\n",
            " 36%|███▋      | 60/165 [00:06<00:11,  8.89it/s]\u001b[A\n",
            " 37%|███▋      | 61/165 [00:06<00:11,  8.88it/s]\u001b[A\n",
            " 38%|███▊      | 62/165 [00:07<00:11,  8.89it/s]\u001b[A\n",
            " 38%|███▊      | 63/165 [00:07<00:11,  8.89it/s]\u001b[A\n",
            " 39%|███▉      | 64/165 [00:07<00:11,  8.84it/s]\u001b[A\n",
            " 39%|███▉      | 65/165 [00:07<00:11,  8.84it/s]\u001b[A\n",
            " 40%|████      | 66/165 [00:07<00:11,  8.85it/s]\u001b[A\n",
            " 41%|████      | 67/165 [00:07<00:11,  8.85it/s]\u001b[A\n",
            " 41%|████      | 68/165 [00:07<00:10,  8.84it/s]\u001b[A\n",
            " 42%|████▏     | 69/165 [00:07<00:10,  8.84it/s]\u001b[A\n",
            " 42%|████▏     | 70/165 [00:07<00:10,  8.81it/s]\u001b[A\n",
            " 43%|████▎     | 71/165 [00:08<00:10,  8.79it/s]\u001b[A\n",
            " 44%|████▎     | 72/165 [00:08<00:10,  8.83it/s]\u001b[A\n",
            " 44%|████▍     | 73/165 [00:08<00:10,  8.82it/s]\u001b[A\n",
            " 45%|████▍     | 74/165 [00:08<00:10,  8.83it/s]\u001b[A\n",
            " 45%|████▌     | 75/165 [00:08<00:10,  8.85it/s]\u001b[A\n",
            " 46%|████▌     | 76/165 [00:08<00:10,  8.84it/s]\u001b[A\n",
            " 47%|████▋     | 77/165 [00:08<00:09,  8.86it/s]\u001b[A\n",
            " 47%|████▋     | 78/165 [00:08<00:09,  8.89it/s]\u001b[A\n",
            " 48%|████▊     | 79/165 [00:08<00:09,  8.88it/s]\u001b[A\n",
            " 48%|████▊     | 80/165 [00:09<00:09,  8.85it/s]\u001b[A\n",
            " 49%|████▉     | 81/165 [00:09<00:09,  8.87it/s]\u001b[A\n",
            " 50%|████▉     | 82/165 [00:09<00:09,  8.86it/s]\u001b[A\n",
            " 50%|█████     | 83/165 [00:09<00:09,  8.83it/s]\u001b[A\n",
            " 51%|█████     | 84/165 [00:09<00:09,  8.84it/s]\u001b[A\n",
            " 52%|█████▏    | 85/165 [00:09<00:09,  8.84it/s]\u001b[A\n",
            " 52%|█████▏    | 86/165 [00:09<00:08,  8.86it/s]\u001b[A\n",
            " 53%|█████▎    | 87/165 [00:09<00:08,  8.88it/s]\u001b[A\n",
            " 53%|█████▎    | 88/165 [00:09<00:08,  8.91it/s]\u001b[A\n",
            " 54%|█████▍    | 89/165 [00:10<00:08,  8.91it/s]\u001b[A\n",
            " 55%|█████▍    | 90/165 [00:10<00:08,  8.92it/s]\u001b[A\n",
            " 55%|█████▌    | 91/165 [00:10<00:08,  8.90it/s]\u001b[A\n",
            " 56%|█████▌    | 92/165 [00:10<00:08,  8.90it/s]\u001b[A\n",
            " 56%|█████▋    | 93/165 [00:10<00:08,  8.87it/s]\u001b[A\n",
            " 57%|█████▋    | 94/165 [00:10<00:08,  8.86it/s]\u001b[A\n",
            " 58%|█████▊    | 95/165 [00:10<00:07,  8.84it/s]\u001b[A\n",
            " 58%|█████▊    | 96/165 [00:10<00:07,  8.83it/s]\u001b[A\n",
            " 59%|█████▉    | 97/165 [00:10<00:07,  8.79it/s]\u001b[A\n",
            " 59%|█████▉    | 98/165 [00:11<00:07,  8.79it/s]\u001b[A\n",
            " 60%|██████    | 99/165 [00:11<00:07,  8.78it/s]\u001b[A\n",
            " 61%|██████    | 100/165 [00:11<00:07,  8.78it/s]\u001b[A\n",
            " 61%|██████    | 101/165 [00:11<00:07,  8.79it/s]\u001b[A\n",
            " 62%|██████▏   | 102/165 [00:11<00:07,  8.80it/s]\u001b[A\n",
            " 62%|██████▏   | 103/165 [00:11<00:07,  8.79it/s]\u001b[A\n",
            " 63%|██████▎   | 104/165 [00:11<00:06,  8.80it/s]\u001b[A\n",
            " 64%|██████▎   | 105/165 [00:11<00:06,  8.82it/s]\u001b[A\n",
            " 64%|██████▍   | 106/165 [00:11<00:06,  8.82it/s]\u001b[A\n",
            " 65%|██████▍   | 107/165 [00:12<00:06,  8.82it/s]\u001b[A\n",
            " 65%|██████▌   | 108/165 [00:12<00:06,  8.85it/s]\u001b[A\n",
            " 66%|██████▌   | 109/165 [00:12<00:06,  8.84it/s]\u001b[A\n",
            " 67%|██████▋   | 110/165 [00:12<00:06,  8.82it/s]\u001b[A\n",
            " 67%|██████▋   | 111/165 [00:12<00:06,  8.82it/s]\u001b[A\n",
            " 68%|██████▊   | 112/165 [00:12<00:06,  8.81it/s]\u001b[A\n",
            " 68%|██████▊   | 113/165 [00:12<00:05,  8.82it/s]\u001b[A\n",
            " 69%|██████▉   | 114/165 [00:12<00:05,  8.85it/s]\u001b[A\n",
            " 70%|██████▉   | 115/165 [00:12<00:05,  8.86it/s]\u001b[A\n",
            " 70%|███████   | 116/165 [00:13<00:05,  8.85it/s]\u001b[A\n",
            " 71%|███████   | 117/165 [00:13<00:05,  8.84it/s]\u001b[A\n",
            " 72%|███████▏  | 118/165 [00:13<00:05,  8.86it/s]\u001b[A\n",
            " 72%|███████▏  | 119/165 [00:13<00:05,  8.86it/s]\u001b[A\n",
            " 73%|███████▎  | 120/165 [00:13<00:05,  8.83it/s]\u001b[A\n",
            " 73%|███████▎  | 121/165 [00:13<00:04,  8.84it/s]\u001b[A\n",
            " 74%|███████▍  | 122/165 [00:13<00:04,  8.84it/s]\u001b[A\n",
            " 75%|███████▍  | 123/165 [00:13<00:04,  8.86it/s]\u001b[A\n",
            " 75%|███████▌  | 124/165 [00:14<00:04,  8.85it/s]\u001b[A\n",
            " 76%|███████▌  | 125/165 [00:14<00:04,  8.87it/s]\u001b[A\n",
            " 76%|███████▋  | 126/165 [00:14<00:04,  8.87it/s]\u001b[A\n",
            " 77%|███████▋  | 127/165 [00:14<00:04,  8.88it/s]\u001b[A\n",
            " 78%|███████▊  | 128/165 [00:14<00:04,  8.88it/s]\u001b[A\n",
            " 78%|███████▊  | 129/165 [00:14<00:04,  8.90it/s]\u001b[A\n",
            " 79%|███████▉  | 130/165 [00:14<00:03,  8.88it/s]\u001b[A\n",
            " 79%|███████▉  | 131/165 [00:14<00:03,  8.88it/s]\u001b[A\n",
            " 80%|████████  | 132/165 [00:14<00:03,  8.88it/s]\u001b[A\n",
            " 81%|████████  | 133/165 [00:15<00:03,  8.90it/s]\u001b[A\n",
            " 81%|████████  | 134/165 [00:15<00:03,  8.88it/s]\u001b[A\n",
            " 82%|████████▏ | 135/165 [00:15<00:03,  8.87it/s]\u001b[A\n",
            " 82%|████████▏ | 136/165 [00:15<00:03,  8.86it/s]\u001b[A\n",
            " 83%|████████▎ | 137/165 [00:15<00:03,  8.87it/s]\u001b[A\n",
            " 84%|████████▎ | 138/165 [00:15<00:03,  8.86it/s]\u001b[A\n",
            " 84%|████████▍ | 139/165 [00:15<00:02,  8.86it/s]\u001b[A\n",
            " 85%|████████▍ | 140/165 [00:15<00:02,  8.88it/s]\u001b[A\n",
            " 85%|████████▌ | 141/165 [00:15<00:02,  8.88it/s]\u001b[A\n",
            " 86%|████████▌ | 142/165 [00:16<00:02,  8.88it/s]\u001b[A\n",
            " 87%|████████▋ | 143/165 [00:16<00:02,  8.88it/s]\u001b[A\n",
            " 87%|████████▋ | 144/165 [00:16<00:02,  8.86it/s]\u001b[A\n",
            " 88%|████████▊ | 145/165 [00:16<00:02,  8.87it/s]\u001b[A\n",
            " 88%|████████▊ | 146/165 [00:16<00:02,  8.85it/s]\u001b[A\n",
            " 89%|████████▉ | 147/165 [00:16<00:02,  8.86it/s]\u001b[A\n",
            " 90%|████████▉ | 148/165 [00:16<00:01,  8.87it/s]\u001b[A\n",
            " 90%|█████████ | 149/165 [00:16<00:01,  8.89it/s]\u001b[A\n",
            " 91%|█████████ | 150/165 [00:16<00:01,  8.86it/s]\u001b[A\n",
            " 92%|█████████▏| 151/165 [00:17<00:01,  8.88it/s]\u001b[A\n",
            " 92%|█████████▏| 152/165 [00:17<00:01,  8.87it/s]\u001b[A\n",
            " 93%|█████████▎| 153/165 [00:17<00:01,  8.86it/s]\u001b[A\n",
            " 93%|█████████▎| 154/165 [00:17<00:01,  8.86it/s]\u001b[A\n",
            " 94%|█████████▍| 155/165 [00:17<00:01,  8.86it/s]\u001b[A\n",
            " 95%|█████████▍| 156/165 [00:17<00:01,  8.88it/s]\u001b[A\n",
            " 95%|█████████▌| 157/165 [00:17<00:00,  8.87it/s]\u001b[A\n",
            " 96%|█████████▌| 158/165 [00:17<00:00,  8.85it/s]\u001b[A\n",
            " 96%|█████████▋| 159/165 [00:17<00:00,  8.86it/s]\u001b[A\n",
            " 97%|█████████▋| 160/165 [00:18<00:00,  8.87it/s]\u001b[A\n",
            " 98%|█████████▊| 161/165 [00:18<00:00,  8.88it/s]\u001b[A\n",
            " 98%|█████████▊| 162/165 [00:18<00:00,  8.89it/s]\u001b[A\n",
            " 99%|█████████▉| 163/165 [00:18<00:00,  8.89it/s]\u001b[A\n",
            "100%|██████████| 165/165 [00:18<00:00,  8.84it/s]\n",
            "========== model: bert-base-uncased fold: 2 inference ==========\n",
            "========== model: bert-base-uncased fold: 2 inference ==========\n",
            "========== model: bert-base-uncased fold: 2 inference ==========\n",
            "========== model: bert-base-uncased fold: 2 inference ==========\n",
            "========== model: bert-base-uncased fold: 2 inference ==========\n",
            "========== model: bert-base-uncased fold: 2 inference ==========\n",
            "========== model: bert-base-uncased fold: 2 inference ==========\n",
            "========== model: bert-base-uncased fold: 2 inference ==========\n",
            "========== model: bert-base-uncased fold: 2 inference ==========\n",
            "========== model: bert-base-uncased fold: 2 inference ==========\n",
            "loading configuration file /content/activebus_bert_final/config.json\n",
            "Model config BertConfig {\n",
            "  \"_name_or_path\": \"activebus/BERT-XD_Review\",\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_ids\": 0,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.15.0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "loading weights file /content/activebus_bert_final/pytorch_model.bin\n",
            "Some weights of the model checkpoint at /content/activebus_bert_final were not used when initializing BertForSequenceClassification: ['cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /content/activebus_bert_final and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "\n",
            "  0%|          | 0/165 [00:00<?, ?it/s]\u001b[A\n",
            "  1%|          | 1/165 [00:00<00:20,  7.96it/s]\u001b[A\n",
            "  1%|          | 2/165 [00:00<00:19,  8.43it/s]\u001b[A\n",
            "  2%|▏         | 3/165 [00:00<00:18,  8.64it/s]\u001b[A\n",
            "  2%|▏         | 4/165 [00:00<00:18,  8.71it/s]\u001b[A\n",
            "  3%|▎         | 5/165 [00:00<00:18,  8.77it/s]\u001b[A\n",
            "  4%|▎         | 6/165 [00:00<00:18,  8.77it/s]\u001b[A\n",
            "  4%|▍         | 7/165 [00:00<00:17,  8.81it/s]\u001b[A\n",
            "  5%|▍         | 8/165 [00:00<00:17,  8.81it/s]\u001b[A\n",
            "  5%|▌         | 9/165 [00:01<00:17,  8.81it/s]\u001b[A\n",
            "  6%|▌         | 10/165 [00:01<00:17,  8.80it/s]\u001b[A\n",
            "  7%|▋         | 11/165 [00:01<00:17,  8.83it/s]\u001b[A\n",
            "  7%|▋         | 12/165 [00:01<00:17,  8.82it/s]\u001b[A\n",
            "  8%|▊         | 13/165 [00:01<00:17,  8.82it/s]\u001b[A\n",
            "  8%|▊         | 14/165 [00:01<00:17,  8.83it/s]\u001b[A\n",
            "  9%|▉         | 15/165 [00:01<00:17,  8.81it/s]\u001b[A\n",
            " 10%|▉         | 16/165 [00:01<00:16,  8.84it/s]\u001b[A\n",
            " 10%|█         | 17/165 [00:01<00:16,  8.84it/s]\u001b[A\n",
            " 11%|█         | 18/165 [00:02<00:16,  8.82it/s]\u001b[A\n",
            " 12%|█▏        | 19/165 [00:02<00:16,  8.84it/s]\u001b[A\n",
            " 12%|█▏        | 20/165 [00:02<00:16,  8.84it/s]\u001b[A\n",
            " 13%|█▎        | 21/165 [00:02<00:16,  8.85it/s]\u001b[A\n",
            " 13%|█▎        | 22/165 [00:02<00:16,  8.87it/s]\u001b[A\n",
            " 14%|█▍        | 23/165 [00:02<00:16,  8.86it/s]\u001b[A\n",
            " 15%|█▍        | 24/165 [00:02<00:15,  8.88it/s]\u001b[A\n",
            " 15%|█▌        | 25/165 [00:02<00:15,  8.87it/s]\u001b[A\n",
            " 16%|█▌        | 26/165 [00:02<00:15,  8.85it/s]\u001b[A\n",
            " 16%|█▋        | 27/165 [00:03<00:15,  8.83it/s]\u001b[A\n",
            " 17%|█▋        | 28/165 [00:03<00:15,  8.80it/s]\u001b[A\n",
            " 18%|█▊        | 29/165 [00:03<00:15,  8.82it/s]\u001b[A\n",
            " 18%|█▊        | 30/165 [00:03<00:15,  8.81it/s]\u001b[A\n",
            " 19%|█▉        | 31/165 [00:03<00:15,  8.81it/s]\u001b[A\n",
            " 19%|█▉        | 32/165 [00:03<00:15,  8.81it/s]\u001b[A\n",
            " 20%|██        | 33/165 [00:03<00:14,  8.84it/s]\u001b[A\n",
            " 21%|██        | 34/165 [00:03<00:14,  8.84it/s]\u001b[A\n",
            " 21%|██        | 35/165 [00:03<00:14,  8.84it/s]\u001b[A\n",
            " 22%|██▏       | 36/165 [00:04<00:14,  8.82it/s]\u001b[A\n",
            " 22%|██▏       | 37/165 [00:04<00:14,  8.82it/s]\u001b[A\n",
            " 23%|██▎       | 38/165 [00:04<00:14,  8.82it/s]\u001b[A\n",
            " 24%|██▎       | 39/165 [00:04<00:14,  8.83it/s]\u001b[A\n",
            " 24%|██▍       | 40/165 [00:04<00:14,  8.84it/s]\u001b[A\n",
            " 25%|██▍       | 41/165 [00:04<00:14,  8.84it/s]\u001b[A\n",
            " 25%|██▌       | 42/165 [00:04<00:13,  8.83it/s]\u001b[A\n",
            " 26%|██▌       | 43/165 [00:04<00:13,  8.83it/s]\u001b[A\n",
            " 27%|██▋       | 44/165 [00:04<00:13,  8.82it/s]\u001b[A\n",
            " 27%|██▋       | 45/165 [00:05<00:13,  8.82it/s]\u001b[A\n",
            " 28%|██▊       | 46/165 [00:05<00:13,  8.79it/s]\u001b[A\n",
            " 28%|██▊       | 47/165 [00:05<00:13,  8.81it/s]\u001b[A\n",
            " 29%|██▉       | 48/165 [00:05<00:13,  8.84it/s]\u001b[A\n",
            " 30%|██▉       | 49/165 [00:05<00:13,  8.85it/s]\u001b[A\n",
            " 30%|███       | 50/165 [00:05<00:13,  8.85it/s]\u001b[A\n",
            " 31%|███       | 51/165 [00:05<00:12,  8.86it/s]\u001b[A\n",
            " 32%|███▏      | 52/165 [00:05<00:12,  8.86it/s]\u001b[A\n",
            " 32%|███▏      | 53/165 [00:06<00:12,  8.86it/s]\u001b[A\n",
            " 33%|███▎      | 54/165 [00:06<00:12,  8.87it/s]\u001b[A\n",
            " 33%|███▎      | 55/165 [00:06<00:12,  8.85it/s]\u001b[A\n",
            " 34%|███▍      | 56/165 [00:06<00:12,  8.87it/s]\u001b[A\n",
            " 35%|███▍      | 57/165 [00:06<00:12,  8.88it/s]\u001b[A\n",
            " 35%|███▌      | 58/165 [00:06<00:12,  8.89it/s]\u001b[A\n",
            " 36%|███▌      | 59/165 [00:06<00:11,  8.85it/s]\u001b[A\n",
            " 36%|███▋      | 60/165 [00:06<00:11,  8.83it/s]\u001b[A\n",
            " 37%|███▋      | 61/165 [00:06<00:11,  8.84it/s]\u001b[A\n",
            " 38%|███▊      | 62/165 [00:07<00:11,  8.84it/s]\u001b[A\n",
            " 38%|███▊      | 63/165 [00:07<00:11,  8.83it/s]\u001b[A\n",
            " 39%|███▉      | 64/165 [00:07<00:11,  8.83it/s]\u001b[A\n",
            " 39%|███▉      | 65/165 [00:07<00:11,  8.82it/s]\u001b[A\n",
            " 40%|████      | 66/165 [00:07<00:11,  8.79it/s]\u001b[A\n",
            " 41%|████      | 67/165 [00:07<00:11,  8.80it/s]\u001b[A\n",
            " 41%|████      | 68/165 [00:07<00:11,  8.80it/s]\u001b[A\n",
            " 42%|████▏     | 69/165 [00:07<00:10,  8.81it/s]\u001b[A\n",
            " 42%|████▏     | 70/165 [00:07<00:10,  8.81it/s]\u001b[A\n",
            " 43%|████▎     | 71/165 [00:08<00:10,  8.83it/s]\u001b[A\n",
            " 44%|████▎     | 72/165 [00:08<00:10,  8.86it/s]\u001b[A\n",
            " 44%|████▍     | 73/165 [00:08<00:10,  8.85it/s]\u001b[A\n",
            " 45%|████▍     | 74/165 [00:08<00:10,  8.83it/s]\u001b[A\n",
            " 45%|████▌     | 75/165 [00:08<00:10,  8.82it/s]\u001b[A\n",
            " 46%|████▌     | 76/165 [00:08<00:10,  8.82it/s]\u001b[A\n",
            " 47%|████▋     | 77/165 [00:08<00:09,  8.83it/s]\u001b[A\n",
            " 47%|████▋     | 78/165 [00:08<00:09,  8.82it/s]\u001b[A\n",
            " 48%|████▊     | 79/165 [00:08<00:09,  8.83it/s]\u001b[A\n",
            " 48%|████▊     | 80/165 [00:09<00:09,  8.86it/s]\u001b[A\n",
            " 49%|████▉     | 81/165 [00:09<00:09,  8.88it/s]\u001b[A\n",
            " 50%|████▉     | 82/165 [00:09<00:09,  8.89it/s]\u001b[A\n",
            " 50%|█████     | 83/165 [00:09<00:09,  8.86it/s]\u001b[A\n",
            " 51%|█████     | 84/165 [00:09<00:09,  8.85it/s]\u001b[A\n",
            " 52%|█████▏    | 85/165 [00:09<00:09,  8.86it/s]\u001b[A\n",
            " 52%|█████▏    | 86/165 [00:09<00:08,  8.87it/s]\u001b[A\n",
            " 53%|█████▎    | 87/165 [00:09<00:08,  8.83it/s]\u001b[A\n",
            " 53%|█████▎    | 88/165 [00:09<00:08,  8.83it/s]\u001b[A\n",
            " 54%|█████▍    | 89/165 [00:10<00:08,  8.85it/s]\u001b[A\n",
            " 55%|█████▍    | 90/165 [00:10<00:08,  8.84it/s]\u001b[A\n",
            " 55%|█████▌    | 91/165 [00:10<00:08,  8.82it/s]\u001b[A\n",
            " 56%|█████▌    | 92/165 [00:10<00:08,  8.82it/s]\u001b[A\n",
            " 56%|█████▋    | 93/165 [00:10<00:08,  8.82it/s]\u001b[A\n",
            " 57%|█████▋    | 94/165 [00:10<00:08,  8.81it/s]\u001b[A\n",
            " 58%|█████▊    | 95/165 [00:10<00:07,  8.78it/s]\u001b[A\n",
            " 58%|█████▊    | 96/165 [00:10<00:07,  8.79it/s]\u001b[A\n",
            " 59%|█████▉    | 97/165 [00:10<00:07,  8.81it/s]\u001b[A\n",
            " 59%|█████▉    | 98/165 [00:11<00:07,  8.83it/s]\u001b[A\n",
            " 60%|██████    | 99/165 [00:11<00:07,  8.82it/s]\u001b[A\n",
            " 61%|██████    | 100/165 [00:11<00:07,  8.81it/s]\u001b[A\n",
            " 61%|██████    | 101/165 [00:11<00:07,  8.81it/s]\u001b[A\n",
            " 62%|██████▏   | 102/165 [00:11<00:07,  8.84it/s]\u001b[A\n",
            " 62%|██████▏   | 103/165 [00:11<00:07,  8.85it/s]\u001b[A\n",
            " 63%|██████▎   | 104/165 [00:11<00:06,  8.86it/s]\u001b[A\n",
            " 64%|██████▎   | 105/165 [00:11<00:06,  8.86it/s]\u001b[A\n",
            " 64%|██████▍   | 106/165 [00:12<00:06,  8.84it/s]\u001b[A\n",
            " 65%|██████▍   | 107/165 [00:12<00:06,  8.86it/s]\u001b[A\n",
            " 65%|██████▌   | 108/165 [00:12<00:06,  8.83it/s]\u001b[A\n",
            " 66%|██████▌   | 109/165 [00:12<00:06,  8.82it/s]\u001b[A\n",
            " 67%|██████▋   | 110/165 [00:12<00:06,  8.84it/s]\u001b[A\n",
            " 67%|██████▋   | 111/165 [00:12<00:06,  8.82it/s]\u001b[A\n",
            " 68%|██████▊   | 112/165 [00:12<00:06,  8.81it/s]\u001b[A\n",
            " 68%|██████▊   | 113/165 [00:12<00:05,  8.82it/s]\u001b[A\n",
            " 69%|██████▉   | 114/165 [00:12<00:05,  8.85it/s]\u001b[A\n",
            " 70%|██████▉   | 115/165 [00:13<00:05,  8.87it/s]\u001b[A\n",
            " 70%|███████   | 116/165 [00:13<00:05,  8.88it/s]\u001b[A\n",
            " 71%|███████   | 117/165 [00:13<00:05,  8.88it/s]\u001b[A\n",
            " 72%|███████▏  | 118/165 [00:13<00:05,  8.86it/s]\u001b[A\n",
            " 72%|███████▏  | 119/165 [00:13<00:05,  8.87it/s]\u001b[A\n",
            " 73%|███████▎  | 120/165 [00:13<00:05,  8.86it/s]\u001b[A\n",
            " 73%|███████▎  | 121/165 [00:13<00:04,  8.84it/s]\u001b[A\n",
            " 74%|███████▍  | 122/165 [00:13<00:04,  8.86it/s]\u001b[A\n",
            " 75%|███████▍  | 123/165 [00:13<00:04,  8.86it/s]\u001b[A\n",
            " 75%|███████▌  | 124/165 [00:14<00:04,  8.86it/s]\u001b[A\n",
            " 76%|███████▌  | 125/165 [00:14<00:04,  8.87it/s]\u001b[A\n",
            " 76%|███████▋  | 126/165 [00:14<00:04,  8.85it/s]\u001b[A\n",
            " 77%|███████▋  | 127/165 [00:14<00:04,  8.85it/s]\u001b[A\n",
            " 78%|███████▊  | 128/165 [00:14<00:04,  8.84it/s]\u001b[A\n",
            " 78%|███████▊  | 129/165 [00:14<00:04,  8.86it/s]\u001b[A\n",
            " 79%|███████▉  | 130/165 [00:14<00:03,  8.86it/s]\u001b[A\n",
            " 79%|███████▉  | 131/165 [00:14<00:03,  8.82it/s]\u001b[A\n",
            " 80%|████████  | 132/165 [00:14<00:03,  8.83it/s]\u001b[A\n",
            " 81%|████████  | 133/165 [00:15<00:03,  8.85it/s]\u001b[A\n",
            " 81%|████████  | 134/165 [00:15<00:03,  8.85it/s]\u001b[A\n",
            " 82%|████████▏ | 135/165 [00:15<00:03,  8.87it/s]\u001b[A\n",
            " 82%|████████▏ | 136/165 [00:15<00:03,  8.87it/s]\u001b[A\n",
            " 83%|████████▎ | 137/165 [00:15<00:03,  8.86it/s]\u001b[A\n",
            " 84%|████████▎ | 138/165 [00:15<00:03,  8.88it/s]\u001b[A\n",
            " 84%|████████▍ | 139/165 [00:15<00:02,  8.89it/s]\u001b[A\n",
            " 85%|████████▍ | 140/165 [00:15<00:02,  8.90it/s]\u001b[A\n",
            " 85%|████████▌ | 141/165 [00:15<00:02,  8.89it/s]\u001b[A\n",
            " 86%|████████▌ | 142/165 [00:16<00:02,  8.88it/s]\u001b[A\n",
            " 87%|████████▋ | 143/165 [00:16<00:02,  8.87it/s]\u001b[A\n",
            " 87%|████████▋ | 144/165 [00:16<00:02,  8.87it/s]\u001b[A\n",
            " 88%|████████▊ | 145/165 [00:16<00:02,  8.84it/s]\u001b[A\n",
            " 88%|████████▊ | 146/165 [00:16<00:02,  8.84it/s]\u001b[A\n",
            " 89%|████████▉ | 147/165 [00:16<00:02,  8.84it/s]\u001b[A\n",
            " 90%|████████▉ | 148/165 [00:16<00:01,  8.87it/s]\u001b[A\n",
            " 90%|█████████ | 149/165 [00:16<00:01,  8.88it/s]\u001b[A\n",
            " 91%|█████████ | 150/165 [00:16<00:01,  8.86it/s]\u001b[A\n",
            " 92%|█████████▏| 151/165 [00:17<00:01,  8.87it/s]\u001b[A\n",
            " 92%|█████████▏| 152/165 [00:17<00:01,  8.86it/s]\u001b[A\n",
            " 93%|█████████▎| 153/165 [00:17<00:01,  8.86it/s]\u001b[A\n",
            " 93%|█████████▎| 154/165 [00:17<00:01,  8.83it/s]\u001b[A\n",
            " 94%|█████████▍| 155/165 [00:17<00:01,  8.82it/s]\u001b[A\n",
            " 95%|█████████▍| 156/165 [00:17<00:01,  8.80it/s]\u001b[A\n",
            " 95%|█████████▌| 157/165 [00:17<00:00,  8.79it/s]\u001b[A\n",
            " 96%|█████████▌| 158/165 [00:17<00:00,  8.81it/s]\u001b[A\n",
            " 96%|█████████▋| 159/165 [00:17<00:00,  8.83it/s]\u001b[A\n",
            " 97%|█████████▋| 160/165 [00:18<00:00,  8.86it/s]\u001b[A\n",
            " 98%|█████████▊| 161/165 [00:18<00:00,  8.88it/s]\u001b[A\n",
            " 98%|█████████▊| 162/165 [00:18<00:00,  8.88it/s]\u001b[A\n",
            " 99%|█████████▉| 163/165 [00:18<00:00,  8.87it/s]\u001b[A\n",
            "100%|██████████| 165/165 [00:18<00:00,  8.82it/s]\n",
            "========== model: bert-base-uncased fold: 3 inference ==========\n",
            "========== model: bert-base-uncased fold: 3 inference ==========\n",
            "========== model: bert-base-uncased fold: 3 inference ==========\n",
            "========== model: bert-base-uncased fold: 3 inference ==========\n",
            "========== model: bert-base-uncased fold: 3 inference ==========\n",
            "========== model: bert-base-uncased fold: 3 inference ==========\n",
            "========== model: bert-base-uncased fold: 3 inference ==========\n",
            "========== model: bert-base-uncased fold: 3 inference ==========\n",
            "========== model: bert-base-uncased fold: 3 inference ==========\n",
            "========== model: bert-base-uncased fold: 3 inference ==========\n",
            "loading configuration file /content/activebus_bert_final/config.json\n",
            "Model config BertConfig {\n",
            "  \"_name_or_path\": \"activebus/BERT-XD_Review\",\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_ids\": 0,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.15.0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "loading weights file /content/activebus_bert_final/pytorch_model.bin\n",
            "Some weights of the model checkpoint at /content/activebus_bert_final were not used when initializing BertForSequenceClassification: ['cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /content/activebus_bert_final and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "\n",
            "  0%|          | 0/165 [00:00<?, ?it/s]\u001b[A\n",
            "  1%|          | 1/165 [00:00<00:21,  7.66it/s]\u001b[A\n",
            "  1%|          | 2/165 [00:00<00:19,  8.30it/s]\u001b[A\n",
            "  2%|▏         | 3/165 [00:00<00:19,  8.52it/s]\u001b[A\n",
            "  2%|▏         | 4/165 [00:00<00:18,  8.68it/s]\u001b[A\n",
            "  3%|▎         | 5/165 [00:00<00:18,  8.76it/s]\u001b[A\n",
            "  4%|▎         | 6/165 [00:00<00:18,  8.79it/s]\u001b[A\n",
            "  4%|▍         | 7/165 [00:00<00:17,  8.83it/s]\u001b[A\n",
            "  5%|▍         | 8/165 [00:00<00:17,  8.85it/s]\u001b[A\n",
            "  5%|▌         | 9/165 [00:01<00:17,  8.87it/s]\u001b[A\n",
            "  6%|▌         | 10/165 [00:01<00:17,  8.88it/s]\u001b[A\n",
            "  7%|▋         | 11/165 [00:01<00:17,  8.89it/s]\u001b[A\n",
            "  7%|▋         | 12/165 [00:01<00:17,  8.87it/s]\u001b[A\n",
            "  8%|▊         | 13/165 [00:01<00:17,  8.84it/s]\u001b[A\n",
            "  8%|▊         | 14/165 [00:01<00:17,  8.84it/s]\u001b[A\n",
            "  9%|▉         | 15/165 [00:01<00:16,  8.86it/s]\u001b[A\n",
            " 10%|▉         | 16/165 [00:01<00:16,  8.86it/s]\u001b[A\n",
            " 10%|█         | 17/165 [00:01<00:16,  8.87it/s]\u001b[A\n",
            " 11%|█         | 18/165 [00:02<00:16,  8.86it/s]\u001b[A\n",
            " 12%|█▏        | 19/165 [00:02<00:16,  8.86it/s]\u001b[A\n",
            " 12%|█▏        | 20/165 [00:02<00:16,  8.85it/s]\u001b[A\n",
            " 13%|█▎        | 21/165 [00:02<00:16,  8.86it/s]\u001b[A\n",
            " 13%|█▎        | 22/165 [00:02<00:16,  8.86it/s]\u001b[A\n",
            " 14%|█▍        | 23/165 [00:02<00:16,  8.85it/s]\u001b[A\n",
            " 15%|█▍        | 24/165 [00:02<00:15,  8.86it/s]\u001b[A\n",
            " 15%|█▌        | 25/165 [00:02<00:15,  8.85it/s]\u001b[A\n",
            " 16%|█▌        | 26/165 [00:02<00:15,  8.85it/s]\u001b[A\n",
            " 16%|█▋        | 27/165 [00:03<00:15,  8.84it/s]\u001b[A\n",
            " 17%|█▋        | 28/165 [00:03<00:15,  8.82it/s]\u001b[A\n",
            " 18%|█▊        | 29/165 [00:03<00:15,  8.84it/s]\u001b[A\n",
            " 18%|█▊        | 30/165 [00:03<00:15,  8.85it/s]\u001b[A\n",
            " 19%|█▉        | 31/165 [00:03<00:15,  8.82it/s]\u001b[A\n",
            " 19%|█▉        | 32/165 [00:03<00:15,  8.84it/s]\u001b[A\n",
            " 20%|██        | 33/165 [00:03<00:14,  8.85it/s]\u001b[A\n",
            " 21%|██        | 34/165 [00:03<00:14,  8.86it/s]\u001b[A\n",
            " 21%|██        | 35/165 [00:03<00:14,  8.86it/s]\u001b[A\n",
            " 22%|██▏       | 36/165 [00:04<00:14,  8.86it/s]\u001b[A\n",
            " 22%|██▏       | 37/165 [00:04<00:14,  8.88it/s]\u001b[A\n",
            " 23%|██▎       | 38/165 [00:04<00:14,  8.88it/s]\u001b[A\n",
            " 24%|██▎       | 39/165 [00:04<00:14,  8.87it/s]\u001b[A\n",
            " 24%|██▍       | 40/165 [00:04<00:14,  8.86it/s]\u001b[A\n",
            " 25%|██▍       | 41/165 [00:04<00:13,  8.87it/s]\u001b[A\n",
            " 25%|██▌       | 42/165 [00:04<00:13,  8.88it/s]\u001b[A\n",
            " 26%|██▌       | 43/165 [00:04<00:13,  8.89it/s]\u001b[A\n",
            " 27%|██▋       | 44/165 [00:04<00:13,  8.87it/s]\u001b[A\n",
            " 27%|██▋       | 45/165 [00:05<00:13,  8.88it/s]\u001b[A\n",
            " 28%|██▊       | 46/165 [00:05<00:13,  8.87it/s]\u001b[A\n",
            " 28%|██▊       | 47/165 [00:05<00:13,  8.84it/s]\u001b[A\n",
            " 29%|██▉       | 48/165 [00:05<00:13,  8.85it/s]\u001b[A\n",
            " 30%|██▉       | 49/165 [00:05<00:13,  8.83it/s]\u001b[A\n",
            " 30%|███       | 50/165 [00:05<00:12,  8.85it/s]\u001b[A\n",
            " 31%|███       | 51/165 [00:05<00:12,  8.86it/s]\u001b[A\n",
            " 32%|███▏      | 52/165 [00:05<00:12,  8.84it/s]\u001b[A\n",
            " 32%|███▏      | 53/165 [00:06<00:12,  8.85it/s]\u001b[A\n",
            " 33%|███▎      | 54/165 [00:06<00:12,  8.86it/s]\u001b[A\n",
            " 33%|███▎      | 55/165 [00:06<00:12,  8.87it/s]\u001b[A\n",
            " 34%|███▍      | 56/165 [00:06<00:12,  8.87it/s]\u001b[A\n",
            " 35%|███▍      | 57/165 [00:06<00:12,  8.85it/s]\u001b[A\n",
            " 35%|███▌      | 58/165 [00:06<00:12,  8.83it/s]\u001b[A\n",
            " 36%|███▌      | 59/165 [00:06<00:12,  8.83it/s]\u001b[A\n",
            " 36%|███▋      | 60/165 [00:06<00:11,  8.81it/s]\u001b[A\n",
            " 37%|███▋      | 61/165 [00:06<00:11,  8.81it/s]\u001b[A\n",
            " 38%|███▊      | 62/165 [00:07<00:11,  8.82it/s]\u001b[A\n",
            " 38%|███▊      | 63/165 [00:07<00:11,  8.85it/s]\u001b[A\n",
            " 39%|███▉      | 64/165 [00:07<00:11,  8.85it/s]\u001b[A\n",
            " 39%|███▉      | 65/165 [00:07<00:11,  8.88it/s]\u001b[A\n",
            " 40%|████      | 66/165 [00:07<00:11,  8.89it/s]\u001b[A\n",
            " 41%|████      | 67/165 [00:07<00:11,  8.91it/s]\u001b[A\n",
            " 41%|████      | 68/165 [00:07<00:10,  8.90it/s]\u001b[A\n",
            " 42%|████▏     | 69/165 [00:07<00:10,  8.86it/s]\u001b[A\n",
            " 42%|████▏     | 70/165 [00:07<00:10,  8.85it/s]\u001b[A\n",
            " 43%|████▎     | 71/165 [00:08<00:10,  8.88it/s]\u001b[A\n",
            " 44%|████▎     | 72/165 [00:08<00:10,  8.88it/s]\u001b[A\n",
            " 44%|████▍     | 73/165 [00:08<00:10,  8.86it/s]\u001b[A\n",
            " 45%|████▍     | 74/165 [00:08<00:10,  8.87it/s]\u001b[A\n",
            " 45%|████▌     | 75/165 [00:08<00:10,  8.89it/s]\u001b[A\n",
            " 46%|████▌     | 76/165 [00:08<00:10,  8.89it/s]\u001b[A\n",
            " 47%|████▋     | 77/165 [00:08<00:09,  8.89it/s]\u001b[A\n",
            " 47%|████▋     | 78/165 [00:08<00:09,  8.86it/s]\u001b[A\n",
            " 48%|████▊     | 79/165 [00:08<00:09,  8.85it/s]\u001b[A\n",
            " 48%|████▊     | 80/165 [00:09<00:09,  8.85it/s]\u001b[A\n",
            " 49%|████▉     | 81/165 [00:09<00:09,  8.83it/s]\u001b[A\n",
            " 50%|████▉     | 82/165 [00:09<00:09,  8.86it/s]\u001b[A\n",
            " 50%|█████     | 83/165 [00:09<00:09,  8.85it/s]\u001b[A\n",
            " 51%|█████     | 84/165 [00:09<00:09,  8.87it/s]\u001b[A\n",
            " 52%|█████▏    | 85/165 [00:09<00:08,  8.89it/s]\u001b[A\n",
            " 52%|█████▏    | 86/165 [00:09<00:08,  8.87it/s]\u001b[A\n",
            " 53%|█████▎    | 87/165 [00:09<00:08,  8.87it/s]\u001b[A\n",
            " 53%|█████▎    | 88/165 [00:09<00:08,  8.87it/s]\u001b[A\n",
            " 54%|█████▍    | 89/165 [00:10<00:08,  8.86it/s]\u001b[A\n",
            " 55%|█████▍    | 90/165 [00:10<00:08,  8.86it/s]\u001b[A\n",
            " 55%|█████▌    | 91/165 [00:10<00:08,  8.85it/s]\u001b[A\n",
            " 56%|█████▌    | 92/165 [00:10<00:08,  8.86it/s]\u001b[A\n",
            " 56%|█████▋    | 93/165 [00:10<00:10,  7.18it/s]\u001b[A\n",
            " 57%|█████▋    | 94/165 [00:10<00:09,  7.61it/s]\u001b[A\n",
            " 58%|█████▊    | 95/165 [00:10<00:08,  7.96it/s]\u001b[A\n",
            " 58%|█████▊    | 96/165 [00:10<00:08,  8.23it/s]\u001b[A\n",
            " 59%|█████▉    | 97/165 [00:11<00:08,  8.40it/s]\u001b[A\n",
            " 59%|█████▉    | 98/165 [00:11<00:07,  8.53it/s]\u001b[A\n",
            " 60%|██████    | 99/165 [00:11<00:07,  8.65it/s]\u001b[A\n",
            " 61%|██████    | 100/165 [00:11<00:07,  8.69it/s]\u001b[A\n",
            " 61%|██████    | 101/165 [00:11<00:07,  8.75it/s]\u001b[A\n",
            " 62%|██████▏   | 102/165 [00:11<00:07,  8.77it/s]\u001b[A\n",
            " 62%|██████▏   | 103/165 [00:11<00:07,  8.79it/s]\u001b[A\n",
            " 63%|██████▎   | 104/165 [00:11<00:06,  8.83it/s]\u001b[A\n",
            " 64%|██████▎   | 105/165 [00:11<00:06,  8.85it/s]\u001b[A\n",
            " 64%|██████▍   | 106/165 [00:12<00:06,  8.85it/s]\u001b[A\n",
            " 65%|██████▍   | 107/165 [00:12<00:06,  8.84it/s]\u001b[A\n",
            " 65%|██████▌   | 108/165 [00:12<00:06,  8.87it/s]\u001b[A\n",
            " 66%|██████▌   | 109/165 [00:12<00:06,  8.89it/s]\u001b[A\n",
            " 67%|██████▋   | 110/165 [00:12<00:06,  8.89it/s]\u001b[A\n",
            " 67%|██████▋   | 111/165 [00:12<00:06,  8.87it/s]\u001b[A\n",
            " 68%|██████▊   | 112/165 [00:12<00:05,  8.86it/s]\u001b[A\n",
            " 68%|██████▊   | 113/165 [00:12<00:05,  8.87it/s]\u001b[A\n",
            " 69%|██████▉   | 114/165 [00:12<00:05,  8.86it/s]\u001b[A\n",
            " 70%|██████▉   | 115/165 [00:13<00:05,  8.88it/s]\u001b[A\n",
            " 70%|███████   | 116/165 [00:13<00:05,  8.89it/s]\u001b[A\n",
            " 71%|███████   | 117/165 [00:13<00:05,  8.88it/s]\u001b[A\n",
            " 72%|███████▏  | 118/165 [00:13<00:05,  8.88it/s]\u001b[A\n",
            " 72%|███████▏  | 119/165 [00:13<00:05,  8.87it/s]\u001b[A\n",
            " 73%|███████▎  | 120/165 [00:13<00:05,  8.82it/s]\u001b[A\n",
            " 73%|███████▎  | 121/165 [00:13<00:04,  8.84it/s]\u001b[A\n",
            " 74%|███████▍  | 122/165 [00:13<00:04,  8.83it/s]\u001b[A\n",
            " 75%|███████▍  | 123/165 [00:13<00:04,  8.85it/s]\u001b[A\n",
            " 75%|███████▌  | 124/165 [00:14<00:04,  8.86it/s]\u001b[A\n",
            " 76%|███████▌  | 125/165 [00:14<00:04,  8.86it/s]\u001b[A\n",
            " 76%|███████▋  | 126/165 [00:14<00:04,  8.87it/s]\u001b[A\n",
            " 77%|███████▋  | 127/165 [00:14<00:04,  8.88it/s]\u001b[A\n",
            " 78%|███████▊  | 128/165 [00:14<00:04,  8.81it/s]\u001b[A\n",
            " 78%|███████▊  | 129/165 [00:14<00:04,  8.82it/s]\u001b[A\n",
            " 79%|███████▉  | 130/165 [00:14<00:03,  8.82it/s]\u001b[A\n",
            " 79%|███████▉  | 131/165 [00:14<00:03,  8.82it/s]\u001b[A\n",
            " 80%|████████  | 132/165 [00:15<00:03,  8.82it/s]\u001b[A\n",
            " 81%|████████  | 133/165 [00:15<00:03,  8.83it/s]\u001b[A\n",
            " 81%|████████  | 134/165 [00:15<00:03,  8.83it/s]\u001b[A\n",
            " 82%|████████▏ | 135/165 [00:15<00:03,  8.82it/s]\u001b[A\n",
            " 82%|████████▏ | 136/165 [00:15<00:03,  8.82it/s]\u001b[A\n",
            " 83%|████████▎ | 137/165 [00:15<00:03,  8.83it/s]\u001b[A\n",
            " 84%|████████▎ | 138/165 [00:15<00:03,  8.80it/s]\u001b[A\n",
            " 84%|████████▍ | 139/165 [00:15<00:02,  8.81it/s]\u001b[A\n",
            " 85%|████████▍ | 140/165 [00:15<00:02,  8.81it/s]\u001b[A\n",
            " 85%|████████▌ | 141/165 [00:16<00:02,  8.83it/s]\u001b[A\n",
            " 86%|████████▌ | 142/165 [00:16<00:02,  8.83it/s]\u001b[A\n",
            " 87%|████████▋ | 143/165 [00:16<00:02,  8.85it/s]\u001b[A\n",
            " 87%|████████▋ | 144/165 [00:16<00:02,  8.87it/s]\u001b[A\n",
            " 88%|████████▊ | 145/165 [00:16<00:02,  8.88it/s]\u001b[A\n",
            " 88%|████████▊ | 146/165 [00:16<00:02,  8.88it/s]\u001b[A\n",
            " 89%|████████▉ | 147/165 [00:16<00:02,  8.86it/s]\u001b[A\n",
            " 90%|████████▉ | 148/165 [00:16<00:01,  8.85it/s]\u001b[A\n",
            " 90%|█████████ | 149/165 [00:16<00:01,  8.84it/s]\u001b[A\n",
            " 91%|█████████ | 150/165 [00:17<00:01,  8.84it/s]\u001b[A\n",
            " 92%|█████████▏| 151/165 [00:17<00:01,  8.85it/s]\u001b[A\n",
            " 92%|█████████▏| 152/165 [00:17<00:01,  8.85it/s]\u001b[A\n",
            " 93%|█████████▎| 153/165 [00:17<00:01,  8.86it/s]\u001b[A\n",
            " 93%|█████████▎| 154/165 [00:17<00:01,  8.88it/s]\u001b[A\n",
            " 94%|█████████▍| 155/165 [00:17<00:01,  8.88it/s]\u001b[A\n",
            " 95%|█████████▍| 156/165 [00:17<00:01,  8.86it/s]\u001b[A\n",
            " 95%|█████████▌| 157/165 [00:17<00:00,  8.89it/s]\u001b[A\n",
            " 96%|█████████▌| 158/165 [00:17<00:00,  8.88it/s]\u001b[A\n",
            " 96%|█████████▋| 159/165 [00:18<00:00,  8.89it/s]\u001b[A\n",
            " 97%|█████████▋| 160/165 [00:18<00:00,  8.89it/s]\u001b[A\n",
            " 98%|█████████▊| 161/165 [00:18<00:00,  8.91it/s]\u001b[A\n",
            " 98%|█████████▊| 162/165 [00:18<00:00,  8.90it/s]\u001b[A\n",
            " 99%|█████████▉| 163/165 [00:18<00:00,  8.90it/s]\u001b[A\n",
            "100%|██████████| 165/165 [00:18<00:00,  8.80it/s]\n",
            "========== model: bert-base-uncased fold: 4 inference ==========\n",
            "========== model: bert-base-uncased fold: 4 inference ==========\n",
            "========== model: bert-base-uncased fold: 4 inference ==========\n",
            "========== model: bert-base-uncased fold: 4 inference ==========\n",
            "========== model: bert-base-uncased fold: 4 inference ==========\n",
            "========== model: bert-base-uncased fold: 4 inference ==========\n",
            "========== model: bert-base-uncased fold: 4 inference ==========\n",
            "========== model: bert-base-uncased fold: 4 inference ==========\n",
            "========== model: bert-base-uncased fold: 4 inference ==========\n",
            "========== model: bert-base-uncased fold: 4 inference ==========\n",
            "loading configuration file /content/activebus_bert_final/config.json\n",
            "Model config BertConfig {\n",
            "  \"_name_or_path\": \"activebus/BERT-XD_Review\",\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_ids\": 0,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.15.0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "loading weights file /content/activebus_bert_final/pytorch_model.bin\n",
            "Some weights of the model checkpoint at /content/activebus_bert_final were not used when initializing BertForSequenceClassification: ['cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /content/activebus_bert_final and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "\n",
            "  0%|          | 0/165 [00:00<?, ?it/s]\u001b[A\n",
            "  1%|          | 1/165 [00:00<00:20,  7.99it/s]\u001b[A\n",
            "  1%|          | 2/165 [00:00<00:19,  8.50it/s]\u001b[A\n",
            "  2%|▏         | 3/165 [00:00<00:18,  8.58it/s]\u001b[A\n",
            "  2%|▏         | 4/165 [00:00<00:18,  8.68it/s]\u001b[A\n",
            "  3%|▎         | 5/165 [00:00<00:18,  8.75it/s]\u001b[A\n",
            "  4%|▎         | 6/165 [00:00<00:18,  8.77it/s]\u001b[A\n",
            "  4%|▍         | 7/165 [00:00<00:17,  8.80it/s]\u001b[A\n",
            "  5%|▍         | 8/165 [00:00<00:17,  8.82it/s]\u001b[A\n",
            "  5%|▌         | 9/165 [00:01<00:17,  8.81it/s]\u001b[A\n",
            "  6%|▌         | 10/165 [00:01<00:17,  8.83it/s]\u001b[A\n",
            "  7%|▋         | 11/165 [00:01<00:17,  8.85it/s]\u001b[A\n",
            "  7%|▋         | 12/165 [00:01<00:17,  8.87it/s]\u001b[A\n",
            "  8%|▊         | 13/165 [00:01<00:17,  8.89it/s]\u001b[A\n",
            "  8%|▊         | 14/165 [00:01<00:16,  8.90it/s]\u001b[A\n",
            "  9%|▉         | 15/165 [00:01<00:16,  8.89it/s]\u001b[A\n",
            " 10%|▉         | 16/165 [00:01<00:16,  8.90it/s]\u001b[A\n",
            " 10%|█         | 17/165 [00:01<00:16,  8.87it/s]\u001b[A\n",
            " 11%|█         | 18/165 [00:02<00:16,  8.87it/s]\u001b[A\n",
            " 12%|█▏        | 19/165 [00:02<00:16,  8.86it/s]\u001b[A\n",
            " 12%|█▏        | 20/165 [00:02<00:16,  8.87it/s]\u001b[A\n",
            " 13%|█▎        | 21/165 [00:02<00:16,  8.85it/s]\u001b[A\n",
            " 13%|█▎        | 22/165 [00:02<00:16,  8.86it/s]\u001b[A\n",
            " 14%|█▍        | 23/165 [00:02<00:16,  8.85it/s]\u001b[A\n",
            " 15%|█▍        | 24/165 [00:02<00:15,  8.84it/s]\u001b[A\n",
            " 15%|█▌        | 25/165 [00:02<00:15,  8.84it/s]\u001b[A\n",
            " 16%|█▌        | 26/165 [00:02<00:15,  8.83it/s]\u001b[A\n",
            " 16%|█▋        | 27/165 [00:03<00:15,  8.82it/s]\u001b[A\n",
            " 17%|█▋        | 28/165 [00:03<00:15,  8.84it/s]\u001b[A\n",
            " 18%|█▊        | 29/165 [00:03<00:15,  8.85it/s]\u001b[A\n",
            " 18%|█▊        | 30/165 [00:03<00:15,  8.82it/s]\u001b[A\n",
            " 19%|█▉        | 31/165 [00:03<00:15,  8.84it/s]\u001b[A\n",
            " 19%|█▉        | 32/165 [00:03<00:15,  8.85it/s]\u001b[A\n",
            " 20%|██        | 33/165 [00:03<00:14,  8.86it/s]\u001b[A\n",
            " 21%|██        | 34/165 [00:03<00:14,  8.87it/s]\u001b[A\n",
            " 21%|██        | 35/165 [00:03<00:14,  8.86it/s]\u001b[A\n",
            " 22%|██▏       | 36/165 [00:04<00:14,  8.87it/s]\u001b[A\n",
            " 22%|██▏       | 37/165 [00:04<00:14,  8.86it/s]\u001b[A\n",
            " 23%|██▎       | 38/165 [00:04<00:14,  8.85it/s]\u001b[A\n",
            " 24%|██▎       | 39/165 [00:04<00:14,  8.86it/s]\u001b[A\n",
            " 24%|██▍       | 40/165 [00:04<00:14,  8.88it/s]\u001b[A\n",
            " 25%|██▍       | 41/165 [00:04<00:13,  8.89it/s]\u001b[A\n",
            " 25%|██▌       | 42/165 [00:04<00:13,  8.88it/s]\u001b[A\n",
            " 26%|██▌       | 43/165 [00:04<00:13,  8.86it/s]\u001b[A\n",
            " 27%|██▋       | 44/165 [00:04<00:13,  8.88it/s]\u001b[A\n",
            " 27%|██▋       | 45/165 [00:05<00:13,  8.86it/s]\u001b[A\n",
            " 28%|██▊       | 46/165 [00:05<00:13,  8.86it/s]\u001b[A\n",
            " 28%|██▊       | 47/165 [00:05<00:13,  8.88it/s]\u001b[A\n",
            " 29%|██▉       | 48/165 [00:05<00:13,  8.90it/s]\u001b[A\n",
            " 30%|██▉       | 49/165 [00:05<00:13,  8.89it/s]\u001b[A\n",
            " 30%|███       | 50/165 [00:05<00:12,  8.89it/s]\u001b[A\n",
            " 31%|███       | 51/165 [00:05<00:12,  8.89it/s]\u001b[A\n",
            " 32%|███▏      | 52/165 [00:05<00:12,  8.87it/s]\u001b[A\n",
            " 32%|███▏      | 53/165 [00:05<00:12,  8.86it/s]\u001b[A\n",
            " 33%|███▎      | 54/165 [00:06<00:12,  8.89it/s]\u001b[A\n",
            " 33%|███▎      | 55/165 [00:06<00:12,  8.87it/s]\u001b[A\n",
            " 34%|███▍      | 56/165 [00:06<00:12,  8.87it/s]\u001b[A\n",
            " 35%|███▍      | 57/165 [00:06<00:12,  8.89it/s]\u001b[A\n",
            " 35%|███▌      | 58/165 [00:06<00:12,  8.88it/s]\u001b[A\n",
            " 36%|███▌      | 59/165 [00:06<00:11,  8.87it/s]\u001b[A\n",
            " 36%|███▋      | 60/165 [00:06<00:11,  8.88it/s]\u001b[A\n",
            " 37%|███▋      | 61/165 [00:06<00:11,  8.88it/s]\u001b[A\n",
            " 38%|███▊      | 62/165 [00:07<00:11,  8.89it/s]\u001b[A\n",
            " 38%|███▊      | 63/165 [00:07<00:11,  8.88it/s]\u001b[A\n",
            " 39%|███▉      | 64/165 [00:07<00:11,  8.87it/s]\u001b[A\n",
            " 39%|███▉      | 65/165 [00:07<00:11,  8.86it/s]\u001b[A\n",
            " 40%|████      | 66/165 [00:07<00:11,  8.86it/s]\u001b[A\n",
            " 41%|████      | 67/165 [00:07<00:11,  8.84it/s]\u001b[A\n",
            " 41%|████      | 68/165 [00:07<00:10,  8.84it/s]\u001b[A\n",
            " 42%|████▏     | 69/165 [00:07<00:10,  8.85it/s]\u001b[A\n",
            " 42%|████▏     | 70/165 [00:07<00:10,  8.87it/s]\u001b[A\n",
            " 43%|████▎     | 71/165 [00:08<00:10,  8.90it/s]\u001b[A\n",
            " 44%|████▎     | 72/165 [00:08<00:10,  8.87it/s]\u001b[A\n",
            " 44%|████▍     | 73/165 [00:08<00:10,  8.85it/s]\u001b[A\n",
            " 45%|████▍     | 74/165 [00:08<00:10,  8.86it/s]\u001b[A\n",
            " 45%|████▌     | 75/165 [00:08<00:10,  8.85it/s]\u001b[A\n",
            " 46%|████▌     | 76/165 [00:08<00:10,  8.86it/s]\u001b[A\n",
            " 47%|████▋     | 77/165 [00:08<00:09,  8.85it/s]\u001b[A\n",
            " 47%|████▋     | 78/165 [00:08<00:09,  8.88it/s]\u001b[A\n",
            " 48%|████▊     | 79/165 [00:08<00:09,  8.87it/s]\u001b[A\n",
            " 48%|████▊     | 80/165 [00:09<00:09,  8.88it/s]\u001b[A\n",
            " 49%|████▉     | 81/165 [00:09<00:09,  8.87it/s]\u001b[A\n",
            " 50%|████▉     | 82/165 [00:09<00:09,  8.88it/s]\u001b[A\n",
            " 50%|█████     | 83/165 [00:09<00:09,  8.88it/s]\u001b[A\n",
            " 51%|█████     | 84/165 [00:09<00:09,  8.89it/s]\u001b[A\n",
            " 52%|█████▏    | 85/165 [00:09<00:09,  8.86it/s]\u001b[A\n",
            " 52%|█████▏    | 86/165 [00:09<00:08,  8.85it/s]\u001b[A\n",
            " 53%|█████▎    | 87/165 [00:09<00:08,  8.87it/s]\u001b[A\n",
            " 53%|█████▎    | 88/165 [00:09<00:08,  8.88it/s]\u001b[A\n",
            " 54%|█████▍    | 89/165 [00:10<00:08,  8.85it/s]\u001b[A\n",
            " 55%|█████▍    | 90/165 [00:10<00:08,  8.80it/s]\u001b[A\n",
            " 55%|█████▌    | 91/165 [00:10<00:08,  8.80it/s]\u001b[A\n",
            " 56%|█████▌    | 92/165 [00:10<00:08,  8.83it/s]\u001b[A\n",
            " 56%|█████▋    | 93/165 [00:10<00:08,  8.87it/s]\u001b[A\n",
            " 57%|█████▋    | 94/165 [00:10<00:07,  8.89it/s]\u001b[A\n",
            " 58%|█████▊    | 95/165 [00:10<00:07,  8.89it/s]\u001b[A\n",
            " 58%|█████▊    | 96/165 [00:10<00:07,  8.88it/s]\u001b[A\n",
            " 59%|█████▉    | 97/165 [00:10<00:07,  8.90it/s]\u001b[A\n",
            " 59%|█████▉    | 98/165 [00:11<00:07,  8.90it/s]\u001b[A\n",
            " 60%|██████    | 99/165 [00:11<00:07,  8.91it/s]\u001b[A\n",
            " 61%|██████    | 100/165 [00:11<00:07,  8.90it/s]\u001b[A\n",
            " 61%|██████    | 101/165 [00:11<00:07,  8.90it/s]\u001b[A\n",
            " 62%|██████▏   | 102/165 [00:11<00:07,  8.88it/s]\u001b[A\n",
            " 62%|██████▏   | 103/165 [00:11<00:06,  8.86it/s]\u001b[A\n",
            " 63%|██████▎   | 104/165 [00:11<00:06,  8.86it/s]\u001b[A\n",
            " 64%|██████▎   | 105/165 [00:11<00:06,  8.86it/s]\u001b[A\n",
            " 64%|██████▍   | 106/165 [00:11<00:06,  8.88it/s]\u001b[A\n",
            " 65%|██████▍   | 107/165 [00:12<00:06,  8.90it/s]\u001b[A\n",
            " 65%|██████▌   | 108/165 [00:12<00:06,  8.91it/s]\u001b[A\n",
            " 66%|██████▌   | 109/165 [00:12<00:06,  8.92it/s]\u001b[A\n",
            " 67%|██████▋   | 110/165 [00:12<00:06,  8.92it/s]\u001b[A\n",
            " 67%|██████▋   | 111/165 [00:12<00:06,  8.91it/s]\u001b[A\n",
            " 68%|██████▊   | 112/165 [00:12<00:05,  8.86it/s]\u001b[A\n",
            " 68%|██████▊   | 113/165 [00:12<00:05,  8.85it/s]\u001b[A\n",
            " 69%|██████▉   | 114/165 [00:12<00:05,  8.86it/s]\u001b[A\n",
            " 70%|██████▉   | 115/165 [00:12<00:05,  8.86it/s]\u001b[A\n",
            " 70%|███████   | 116/165 [00:13<00:05,  8.87it/s]\u001b[A\n",
            " 71%|███████   | 117/165 [00:13<00:05,  8.84it/s]\u001b[A\n",
            " 72%|███████▏  | 118/165 [00:13<00:05,  8.83it/s]\u001b[A\n",
            " 72%|███████▏  | 119/165 [00:13<00:05,  8.80it/s]\u001b[A\n",
            " 73%|███████▎  | 120/165 [00:13<00:05,  8.79it/s]\u001b[A\n",
            " 73%|███████▎  | 121/165 [00:13<00:05,  8.79it/s]\u001b[A\n",
            " 74%|███████▍  | 122/165 [00:13<00:04,  8.82it/s]\u001b[A\n",
            " 75%|███████▍  | 123/165 [00:13<00:04,  8.83it/s]\u001b[A\n",
            " 75%|███████▌  | 124/165 [00:14<00:04,  8.85it/s]\u001b[A\n",
            " 76%|███████▌  | 125/165 [00:14<00:04,  8.86it/s]\u001b[A\n",
            " 76%|███████▋  | 126/165 [00:14<00:04,  8.86it/s]\u001b[A\n",
            " 77%|███████▋  | 127/165 [00:14<00:04,  8.85it/s]\u001b[A\n",
            " 78%|███████▊  | 128/165 [00:14<00:04,  8.88it/s]\u001b[A\n",
            " 78%|███████▊  | 129/165 [00:14<00:04,  8.88it/s]\u001b[A\n",
            " 79%|███████▉  | 130/165 [00:14<00:03,  8.86it/s]\u001b[A\n",
            " 79%|███████▉  | 131/165 [00:14<00:03,  8.85it/s]\u001b[A\n",
            " 80%|████████  | 132/165 [00:14<00:03,  8.86it/s]\u001b[A\n",
            " 81%|████████  | 133/165 [00:15<00:03,  8.86it/s]\u001b[A\n",
            " 81%|████████  | 134/165 [00:15<00:03,  8.88it/s]\u001b[A\n",
            " 82%|████████▏ | 135/165 [00:15<00:03,  8.89it/s]\u001b[A\n",
            " 82%|████████▏ | 136/165 [00:15<00:03,  8.89it/s]\u001b[A\n",
            " 83%|████████▎ | 137/165 [00:15<00:03,  8.87it/s]\u001b[A\n",
            " 84%|████████▎ | 138/165 [00:15<00:03,  8.88it/s]\u001b[A\n",
            " 84%|████████▍ | 139/165 [00:15<00:02,  8.89it/s]\u001b[A\n",
            " 85%|████████▍ | 140/165 [00:15<00:02,  8.89it/s]\u001b[A\n",
            " 85%|████████▌ | 141/165 [00:15<00:02,  8.86it/s]\u001b[A\n",
            " 86%|████████▌ | 142/165 [00:16<00:02,  8.85it/s]\u001b[A\n",
            " 87%|████████▋ | 143/165 [00:16<00:02,  8.81it/s]\u001b[A\n",
            " 87%|████████▋ | 144/165 [00:16<00:02,  8.80it/s]\u001b[A\n",
            " 88%|████████▊ | 145/165 [00:16<00:02,  8.78it/s]\u001b[A\n",
            " 88%|████████▊ | 146/165 [00:16<00:02,  8.80it/s]\u001b[A\n",
            " 89%|████████▉ | 147/165 [00:16<00:02,  8.82it/s]\u001b[A\n",
            " 90%|████████▉ | 148/165 [00:16<00:01,  8.84it/s]\u001b[A\n",
            " 90%|█████████ | 149/165 [00:16<00:01,  8.84it/s]\u001b[A\n",
            " 91%|█████████ | 150/165 [00:16<00:01,  8.85it/s]\u001b[A\n",
            " 92%|█████████▏| 151/165 [00:17<00:01,  8.83it/s]\u001b[A\n",
            " 92%|█████████▏| 152/165 [00:17<00:01,  8.83it/s]\u001b[A\n",
            " 93%|█████████▎| 153/165 [00:17<00:01,  8.83it/s]\u001b[A\n",
            " 93%|█████████▎| 154/165 [00:17<00:01,  8.86it/s]\u001b[A\n",
            " 94%|█████████▍| 155/165 [00:17<00:01,  8.80it/s]\u001b[A\n",
            " 95%|█████████▍| 156/165 [00:17<00:01,  8.82it/s]\u001b[A\n",
            " 95%|█████████▌| 157/165 [00:17<00:00,  8.84it/s]\u001b[A\n",
            " 96%|█████████▌| 158/165 [00:17<00:00,  8.85it/s]\u001b[A\n",
            " 96%|█████████▋| 159/165 [00:17<00:00,  8.88it/s]\u001b[A\n",
            " 97%|█████████▋| 160/165 [00:18<00:00,  8.88it/s]\u001b[A\n",
            " 98%|█████████▊| 161/165 [00:18<00:00,  8.87it/s]\u001b[A\n",
            " 98%|█████████▊| 162/165 [00:18<00:00,  8.89it/s]\u001b[A\n",
            " 99%|█████████▉| 163/165 [00:18<00:00,  8.91it/s]\u001b[A\n",
            "100%|██████████| 165/165 [00:18<00:00,  8.84it/s]\n",
            " 33%|███▎      | 1/3 [49:04<1:38:08, 2944.07s/it]Didn't find file /content/activebus_bert_final/added_tokens.json. We won't load it.\n",
            "loading file /content/activebus_bert_final/vocab.txt\n",
            "loading file /content/activebus_bert_final/tokenizer.json\n",
            "loading file None\n",
            "loading file /content/activebus_bert_final/special_tokens_map.json\n",
            "loading file /content/activebus_bert_final/tokenizer_config.json\n",
            "========== fold: 0 training ==========\n",
            "========== fold: 0 training ==========\n",
            "========== fold: 0 training ==========\n",
            "========== fold: 0 training ==========\n",
            "========== fold: 0 training ==========\n",
            "========== fold: 0 training ==========\n",
            "========== fold: 0 training ==========\n",
            "========== fold: 0 training ==========\n",
            "========== fold: 0 training ==========\n",
            "========== fold: 0 training ==========\n",
            "========== fold: 0 training ==========\n",
            "Didn't find file /content/activebus_bert_final/added_tokens.json. We won't load it.\n",
            "loading file /content/activebus_bert_final/vocab.txt\n",
            "loading file None\n",
            "loading file /content/activebus_bert_final/special_tokens_map.json\n",
            "loading file /content/activebus_bert_final/tokenizer_config.json\n",
            "loading file /content/activebus_bert_final/tokenizer.json\n",
            "Didn't find file /content/activebus_bert_final/added_tokens.json. We won't load it.\n",
            "loading file /content/activebus_bert_final/vocab.txt\n",
            "loading file None\n",
            "loading file /content/activebus_bert_final/special_tokens_map.json\n",
            "loading file /content/activebus_bert_final/tokenizer_config.json\n",
            "loading file /content/activebus_bert_final/tokenizer.json\n",
            "loading configuration file /content/activebus_bert_final/config.json\n",
            "Model config BertConfig {\n",
            "  \"_name_or_path\": \"activebus/BERT-XD_Review\",\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_ids\": 0,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.15.0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "loading weights file /content/activebus_bert_final/pytorch_model.bin\n",
            "Some weights of the model checkpoint at /content/activebus_bert_final were not used when initializing BertForSequenceClassification: ['cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /content/activebus_bert_final and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: [1][0/306] Elapsed 0m 0s (remain 3m 4s) Loss: 0.6921 \n",
            "Epoch: [1][100/306] Elapsed 0m 34s (remain 1m 9s) Loss: 0.1557 \n",
            "Epoch: [1][200/306] Elapsed 1m 7s (remain 0m 35s) Loss: 0.1288 \n",
            "Epoch: [1][300/306] Elapsed 1m 40s (remain 0m 1s) Loss: 0.1062 \n",
            "Epoch: [1][305/306] Elapsed 1m 42s (remain 0m 0s) Loss: 0.1045 \n",
            "EVAL: [0/77] Elapsed 0m 0s (remain 0m 28s) Loss: 0.0287 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1 - avg_train_loss: 0.1045  avg_val_loss: 0.0644  time: 111s\n",
            "Epoch 1 - avg_train_loss: 0.1045  avg_val_loss: 0.0644  time: 111s\n",
            "Epoch 1 - avg_train_loss: 0.1045  avg_val_loss: 0.0644  time: 111s\n",
            "Epoch 1 - avg_train_loss: 0.1045  avg_val_loss: 0.0644  time: 111s\n",
            "Epoch 1 - avg_train_loss: 0.1045  avg_val_loss: 0.0644  time: 111s\n",
            "Epoch 1 - avg_train_loss: 0.1045  avg_val_loss: 0.0644  time: 111s\n",
            "Epoch 1 - avg_train_loss: 0.1045  avg_val_loss: 0.0644  time: 111s\n",
            "Epoch 1 - avg_train_loss: 0.1045  avg_val_loss: 0.0644  time: 111s\n",
            "Epoch 1 - avg_train_loss: 0.1045  avg_val_loss: 0.0644  time: 111s\n",
            "Epoch 1 - avg_train_loss: 0.1045  avg_val_loss: 0.0644  time: 111s\n",
            "Epoch 1 - avg_train_loss: 0.1045  avg_val_loss: 0.0644  time: 111s\n",
            "Epoch 1 - Score: 0.06444366857582394\n",
            "Epoch 1 - Score: 0.06444366857582394\n",
            "Epoch 1 - Score: 0.06444366857582394\n",
            "Epoch 1 - Score: 0.06444366857582394\n",
            "Epoch 1 - Score: 0.06444366857582394\n",
            "Epoch 1 - Score: 0.06444366857582394\n",
            "Epoch 1 - Score: 0.06444366857582394\n",
            "Epoch 1 - Score: 0.06444366857582394\n",
            "Epoch 1 - Score: 0.06444366857582394\n",
            "Epoch 1 - Score: 0.06444366857582394\n",
            "Epoch 1 - Score: 0.06444366857582394\n",
            "Epoch 1 - Save Best Score: 0.0644 Model\n",
            "Epoch 1 - Save Best Score: 0.0644 Model\n",
            "Epoch 1 - Save Best Score: 0.0644 Model\n",
            "Epoch 1 - Save Best Score: 0.0644 Model\n",
            "Epoch 1 - Save Best Score: 0.0644 Model\n",
            "Epoch 1 - Save Best Score: 0.0644 Model\n",
            "Epoch 1 - Save Best Score: 0.0644 Model\n",
            "Epoch 1 - Save Best Score: 0.0644 Model\n",
            "Epoch 1 - Save Best Score: 0.0644 Model\n",
            "Epoch 1 - Save Best Score: 0.0644 Model\n",
            "Epoch 1 - Save Best Score: 0.0644 Model\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EVAL: [76/77] Elapsed 0m 8s (remain 0m 0s) Loss: 0.0644 \n",
            "Log loss for 0 is 0.06444366857582394\n",
            "Epoch: [2][0/306] Elapsed 0m 0s (remain 3m 6s) Loss: 0.0072 \n",
            "Epoch: [2][100/306] Elapsed 0m 34s (remain 1m 9s) Loss: 0.0651 \n",
            "Epoch: [2][200/306] Elapsed 1m 7s (remain 0m 35s) Loss: 0.0583 \n",
            "Epoch: [2][300/306] Elapsed 1m 40s (remain 0m 1s) Loss: 0.0519 \n",
            "Epoch: [2][305/306] Elapsed 1m 42s (remain 0m 0s) Loss: 0.0513 \n",
            "EVAL: [0/77] Elapsed 0m 0s (remain 0m 27s) Loss: 0.0804 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2 - avg_train_loss: 0.0513  avg_val_loss: 0.0562  time: 111s\n",
            "Epoch 2 - avg_train_loss: 0.0513  avg_val_loss: 0.0562  time: 111s\n",
            "Epoch 2 - avg_train_loss: 0.0513  avg_val_loss: 0.0562  time: 111s\n",
            "Epoch 2 - avg_train_loss: 0.0513  avg_val_loss: 0.0562  time: 111s\n",
            "Epoch 2 - avg_train_loss: 0.0513  avg_val_loss: 0.0562  time: 111s\n",
            "Epoch 2 - avg_train_loss: 0.0513  avg_val_loss: 0.0562  time: 111s\n",
            "Epoch 2 - avg_train_loss: 0.0513  avg_val_loss: 0.0562  time: 111s\n",
            "Epoch 2 - avg_train_loss: 0.0513  avg_val_loss: 0.0562  time: 111s\n",
            "Epoch 2 - avg_train_loss: 0.0513  avg_val_loss: 0.0562  time: 111s\n",
            "Epoch 2 - avg_train_loss: 0.0513  avg_val_loss: 0.0562  time: 111s\n",
            "Epoch 2 - avg_train_loss: 0.0513  avg_val_loss: 0.0562  time: 111s\n",
            "Epoch 2 - Score: 0.05620239900386398\n",
            "Epoch 2 - Score: 0.05620239900386398\n",
            "Epoch 2 - Score: 0.05620239900386398\n",
            "Epoch 2 - Score: 0.05620239900386398\n",
            "Epoch 2 - Score: 0.05620239900386398\n",
            "Epoch 2 - Score: 0.05620239900386398\n",
            "Epoch 2 - Score: 0.05620239900386398\n",
            "Epoch 2 - Score: 0.05620239900386398\n",
            "Epoch 2 - Score: 0.05620239900386398\n",
            "Epoch 2 - Score: 0.05620239900386398\n",
            "Epoch 2 - Score: 0.05620239900386398\n",
            "Epoch 2 - Save Best Score: 0.0562 Model\n",
            "Epoch 2 - Save Best Score: 0.0562 Model\n",
            "Epoch 2 - Save Best Score: 0.0562 Model\n",
            "Epoch 2 - Save Best Score: 0.0562 Model\n",
            "Epoch 2 - Save Best Score: 0.0562 Model\n",
            "Epoch 2 - Save Best Score: 0.0562 Model\n",
            "Epoch 2 - Save Best Score: 0.0562 Model\n",
            "Epoch 2 - Save Best Score: 0.0562 Model\n",
            "Epoch 2 - Save Best Score: 0.0562 Model\n",
            "Epoch 2 - Save Best Score: 0.0562 Model\n",
            "Epoch 2 - Save Best Score: 0.0562 Model\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EVAL: [76/77] Elapsed 0m 8s (remain 0m 0s) Loss: 0.0562 \n",
            "Log loss for 1 is 0.05620239900386398\n",
            "Epoch: [3][0/306] Elapsed 0m 0s (remain 3m 10s) Loss: 0.1425 \n",
            "Epoch: [3][100/306] Elapsed 0m 34s (remain 1m 9s) Loss: 0.0261 \n",
            "Epoch: [3][200/306] Elapsed 1m 7s (remain 0m 35s) Loss: 0.0333 \n",
            "Epoch: [3][300/306] Elapsed 1m 40s (remain 0m 1s) Loss: 0.0339 \n",
            "Epoch: [3][305/306] Elapsed 1m 42s (remain 0m 0s) Loss: 0.0335 \n",
            "EVAL: [0/77] Elapsed 0m 0s (remain 0m 26s) Loss: 0.0157 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3 - avg_train_loss: 0.0335  avg_val_loss: 0.0460  time: 111s\n",
            "Epoch 3 - avg_train_loss: 0.0335  avg_val_loss: 0.0460  time: 111s\n",
            "Epoch 3 - avg_train_loss: 0.0335  avg_val_loss: 0.0460  time: 111s\n",
            "Epoch 3 - avg_train_loss: 0.0335  avg_val_loss: 0.0460  time: 111s\n",
            "Epoch 3 - avg_train_loss: 0.0335  avg_val_loss: 0.0460  time: 111s\n",
            "Epoch 3 - avg_train_loss: 0.0335  avg_val_loss: 0.0460  time: 111s\n",
            "Epoch 3 - avg_train_loss: 0.0335  avg_val_loss: 0.0460  time: 111s\n",
            "Epoch 3 - avg_train_loss: 0.0335  avg_val_loss: 0.0460  time: 111s\n",
            "Epoch 3 - avg_train_loss: 0.0335  avg_val_loss: 0.0460  time: 111s\n",
            "Epoch 3 - avg_train_loss: 0.0335  avg_val_loss: 0.0460  time: 111s\n",
            "Epoch 3 - avg_train_loss: 0.0335  avg_val_loss: 0.0460  time: 111s\n",
            "Epoch 3 - Score: 0.04599593694762445\n",
            "Epoch 3 - Score: 0.04599593694762445\n",
            "Epoch 3 - Score: 0.04599593694762445\n",
            "Epoch 3 - Score: 0.04599593694762445\n",
            "Epoch 3 - Score: 0.04599593694762445\n",
            "Epoch 3 - Score: 0.04599593694762445\n",
            "Epoch 3 - Score: 0.04599593694762445\n",
            "Epoch 3 - Score: 0.04599593694762445\n",
            "Epoch 3 - Score: 0.04599593694762445\n",
            "Epoch 3 - Score: 0.04599593694762445\n",
            "Epoch 3 - Score: 0.04599593694762445\n",
            "Epoch 3 - Save Best Score: 0.0460 Model\n",
            "Epoch 3 - Save Best Score: 0.0460 Model\n",
            "Epoch 3 - Save Best Score: 0.0460 Model\n",
            "Epoch 3 - Save Best Score: 0.0460 Model\n",
            "Epoch 3 - Save Best Score: 0.0460 Model\n",
            "Epoch 3 - Save Best Score: 0.0460 Model\n",
            "Epoch 3 - Save Best Score: 0.0460 Model\n",
            "Epoch 3 - Save Best Score: 0.0460 Model\n",
            "Epoch 3 - Save Best Score: 0.0460 Model\n",
            "Epoch 3 - Save Best Score: 0.0460 Model\n",
            "Epoch 3 - Save Best Score: 0.0460 Model\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EVAL: [76/77] Elapsed 0m 8s (remain 0m 0s) Loss: 0.0460 \n",
            "Log loss for 2 is 0.04599593694762445\n",
            "Epoch: [4][0/306] Elapsed 0m 0s (remain 3m 0s) Loss: 0.0013 \n",
            "Epoch: [4][100/306] Elapsed 0m 34s (remain 1m 9s) Loss: 0.0129 \n",
            "Epoch: [4][200/306] Elapsed 1m 7s (remain 0m 35s) Loss: 0.0152 \n",
            "Epoch: [4][300/306] Elapsed 1m 40s (remain 0m 1s) Loss: 0.0196 \n",
            "Epoch: [4][305/306] Elapsed 1m 42s (remain 0m 0s) Loss: 0.0193 \n",
            "EVAL: [0/77] Elapsed 0m 0s (remain 0m 26s) Loss: 0.0082 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4 - avg_train_loss: 0.0193  avg_val_loss: 0.0501  time: 111s\n",
            "Epoch 4 - avg_train_loss: 0.0193  avg_val_loss: 0.0501  time: 111s\n",
            "Epoch 4 - avg_train_loss: 0.0193  avg_val_loss: 0.0501  time: 111s\n",
            "Epoch 4 - avg_train_loss: 0.0193  avg_val_loss: 0.0501  time: 111s\n",
            "Epoch 4 - avg_train_loss: 0.0193  avg_val_loss: 0.0501  time: 111s\n",
            "Epoch 4 - avg_train_loss: 0.0193  avg_val_loss: 0.0501  time: 111s\n",
            "Epoch 4 - avg_train_loss: 0.0193  avg_val_loss: 0.0501  time: 111s\n",
            "Epoch 4 - avg_train_loss: 0.0193  avg_val_loss: 0.0501  time: 111s\n",
            "Epoch 4 - avg_train_loss: 0.0193  avg_val_loss: 0.0501  time: 111s\n",
            "Epoch 4 - avg_train_loss: 0.0193  avg_val_loss: 0.0501  time: 111s\n",
            "Epoch 4 - avg_train_loss: 0.0193  avg_val_loss: 0.0501  time: 111s\n",
            "Epoch 4 - Score: 0.050055672306646495\n",
            "Epoch 4 - Score: 0.050055672306646495\n",
            "Epoch 4 - Score: 0.050055672306646495\n",
            "Epoch 4 - Score: 0.050055672306646495\n",
            "Epoch 4 - Score: 0.050055672306646495\n",
            "Epoch 4 - Score: 0.050055672306646495\n",
            "Epoch 4 - Score: 0.050055672306646495\n",
            "Epoch 4 - Score: 0.050055672306646495\n",
            "Epoch 4 - Score: 0.050055672306646495\n",
            "Epoch 4 - Score: 0.050055672306646495\n",
            "Epoch 4 - Score: 0.050055672306646495\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EVAL: [76/77] Elapsed 0m 8s (remain 0m 0s) Loss: 0.0501 \n",
            "Log loss for 3 is 0.050055672306646495\n",
            "Epoch: [5][0/306] Elapsed 0m 0s (remain 3m 19s) Loss: 0.0009 \n",
            "Epoch: [5][100/306] Elapsed 0m 34s (remain 1m 9s) Loss: 0.0161 \n",
            "Epoch: [5][200/306] Elapsed 1m 7s (remain 0m 35s) Loss: 0.0104 \n",
            "Epoch: [5][300/306] Elapsed 1m 41s (remain 0m 1s) Loss: 0.0109 \n",
            "Epoch: [5][305/306] Elapsed 1m 42s (remain 0m 0s) Loss: 0.0108 \n",
            "EVAL: [0/77] Elapsed 0m 0s (remain 0m 27s) Loss: 0.0052 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5 - avg_train_loss: 0.0108  avg_val_loss: 0.0661  time: 111s\n",
            "Epoch 5 - avg_train_loss: 0.0108  avg_val_loss: 0.0661  time: 111s\n",
            "Epoch 5 - avg_train_loss: 0.0108  avg_val_loss: 0.0661  time: 111s\n",
            "Epoch 5 - avg_train_loss: 0.0108  avg_val_loss: 0.0661  time: 111s\n",
            "Epoch 5 - avg_train_loss: 0.0108  avg_val_loss: 0.0661  time: 111s\n",
            "Epoch 5 - avg_train_loss: 0.0108  avg_val_loss: 0.0661  time: 111s\n",
            "Epoch 5 - avg_train_loss: 0.0108  avg_val_loss: 0.0661  time: 111s\n",
            "Epoch 5 - avg_train_loss: 0.0108  avg_val_loss: 0.0661  time: 111s\n",
            "Epoch 5 - avg_train_loss: 0.0108  avg_val_loss: 0.0661  time: 111s\n",
            "Epoch 5 - avg_train_loss: 0.0108  avg_val_loss: 0.0661  time: 111s\n",
            "Epoch 5 - avg_train_loss: 0.0108  avg_val_loss: 0.0661  time: 111s\n",
            "Epoch 5 - Score: 0.06613348183894348\n",
            "Epoch 5 - Score: 0.06613348183894348\n",
            "Epoch 5 - Score: 0.06613348183894348\n",
            "Epoch 5 - Score: 0.06613348183894348\n",
            "Epoch 5 - Score: 0.06613348183894348\n",
            "Epoch 5 - Score: 0.06613348183894348\n",
            "Epoch 5 - Score: 0.06613348183894348\n",
            "Epoch 5 - Score: 0.06613348183894348\n",
            "Epoch 5 - Score: 0.06613348183894348\n",
            "Epoch 5 - Score: 0.06613348183894348\n",
            "Epoch 5 - Score: 0.06613348183894348\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EVAL: [76/77] Elapsed 0m 8s (remain 0m 0s) Loss: 0.0661 \n",
            "Log loss for 4 is 0.06613348183894348\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "========== fold: 0 result ==========\n",
            "========== fold: 0 result ==========\n",
            "========== fold: 0 result ==========\n",
            "========== fold: 0 result ==========\n",
            "========== fold: 0 result ==========\n",
            "========== fold: 0 result ==========\n",
            "========== fold: 0 result ==========\n",
            "========== fold: 0 result ==========\n",
            "========== fold: 0 result ==========\n",
            "========== fold: 0 result ==========\n",
            "========== fold: 0 result ==========\n",
            "Score: 0.04600\n",
            "Score: 0.04600\n",
            "Score: 0.04600\n",
            "Score: 0.04600\n",
            "Score: 0.04600\n",
            "Score: 0.04600\n",
            "Score: 0.04600\n",
            "Score: 0.04600\n",
            "Score: 0.04600\n",
            "Score: 0.04600\n",
            "Score: 0.04600\n",
            "========== fold: 1 training ==========\n",
            "========== fold: 1 training ==========\n",
            "========== fold: 1 training ==========\n",
            "========== fold: 1 training ==========\n",
            "========== fold: 1 training ==========\n",
            "========== fold: 1 training ==========\n",
            "========== fold: 1 training ==========\n",
            "========== fold: 1 training ==========\n",
            "========== fold: 1 training ==========\n",
            "========== fold: 1 training ==========\n",
            "========== fold: 1 training ==========\n",
            "Didn't find file /content/activebus_bert_final/added_tokens.json. We won't load it.\n",
            "loading file /content/activebus_bert_final/vocab.txt\n",
            "loading file None\n",
            "loading file /content/activebus_bert_final/special_tokens_map.json\n",
            "loading file /content/activebus_bert_final/tokenizer_config.json\n",
            "loading file /content/activebus_bert_final/tokenizer.json\n",
            "Didn't find file /content/activebus_bert_final/added_tokens.json. We won't load it.\n",
            "loading file /content/activebus_bert_final/vocab.txt\n",
            "loading file None\n",
            "loading file /content/activebus_bert_final/special_tokens_map.json\n",
            "loading file /content/activebus_bert_final/tokenizer_config.json\n",
            "loading file /content/activebus_bert_final/tokenizer.json\n",
            "loading configuration file /content/activebus_bert_final/config.json\n",
            "Model config BertConfig {\n",
            "  \"_name_or_path\": \"activebus/BERT-XD_Review\",\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_ids\": 0,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.15.0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "loading weights file /content/activebus_bert_final/pytorch_model.bin\n",
            "Some weights of the model checkpoint at /content/activebus_bert_final were not used when initializing BertForSequenceClassification: ['cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /content/activebus_bert_final and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: [1][0/306] Elapsed 0m 0s (remain 3m 4s) Loss: 0.6392 \n",
            "Epoch: [1][100/306] Elapsed 0m 34s (remain 1m 9s) Loss: 0.1450 \n",
            "Epoch: [1][200/306] Elapsed 1m 7s (remain 0m 35s) Loss: 0.1175 \n",
            "Epoch: [1][300/306] Elapsed 1m 41s (remain 0m 1s) Loss: 0.1017 \n",
            "Epoch: [1][305/306] Elapsed 1m 42s (remain 0m 0s) Loss: 0.1017 \n",
            "EVAL: [0/77] Elapsed 0m 0s (remain 0m 26s) Loss: 0.0213 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1 - avg_train_loss: 0.1017  avg_val_loss: 0.0497  time: 111s\n",
            "Epoch 1 - avg_train_loss: 0.1017  avg_val_loss: 0.0497  time: 111s\n",
            "Epoch 1 - avg_train_loss: 0.1017  avg_val_loss: 0.0497  time: 111s\n",
            "Epoch 1 - avg_train_loss: 0.1017  avg_val_loss: 0.0497  time: 111s\n",
            "Epoch 1 - avg_train_loss: 0.1017  avg_val_loss: 0.0497  time: 111s\n",
            "Epoch 1 - avg_train_loss: 0.1017  avg_val_loss: 0.0497  time: 111s\n",
            "Epoch 1 - avg_train_loss: 0.1017  avg_val_loss: 0.0497  time: 111s\n",
            "Epoch 1 - avg_train_loss: 0.1017  avg_val_loss: 0.0497  time: 111s\n",
            "Epoch 1 - avg_train_loss: 0.1017  avg_val_loss: 0.0497  time: 111s\n",
            "Epoch 1 - avg_train_loss: 0.1017  avg_val_loss: 0.0497  time: 111s\n",
            "Epoch 1 - avg_train_loss: 0.1017  avg_val_loss: 0.0497  time: 111s\n",
            "Epoch 1 - Score: 0.04965760426166199\n",
            "Epoch 1 - Score: 0.04965760426166199\n",
            "Epoch 1 - Score: 0.04965760426166199\n",
            "Epoch 1 - Score: 0.04965760426166199\n",
            "Epoch 1 - Score: 0.04965760426166199\n",
            "Epoch 1 - Score: 0.04965760426166199\n",
            "Epoch 1 - Score: 0.04965760426166199\n",
            "Epoch 1 - Score: 0.04965760426166199\n",
            "Epoch 1 - Score: 0.04965760426166199\n",
            "Epoch 1 - Score: 0.04965760426166199\n",
            "Epoch 1 - Score: 0.04965760426166199\n",
            "Epoch 1 - Save Best Score: 0.0497 Model\n",
            "Epoch 1 - Save Best Score: 0.0497 Model\n",
            "Epoch 1 - Save Best Score: 0.0497 Model\n",
            "Epoch 1 - Save Best Score: 0.0497 Model\n",
            "Epoch 1 - Save Best Score: 0.0497 Model\n",
            "Epoch 1 - Save Best Score: 0.0497 Model\n",
            "Epoch 1 - Save Best Score: 0.0497 Model\n",
            "Epoch 1 - Save Best Score: 0.0497 Model\n",
            "Epoch 1 - Save Best Score: 0.0497 Model\n",
            "Epoch 1 - Save Best Score: 0.0497 Model\n",
            "Epoch 1 - Save Best Score: 0.0497 Model\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EVAL: [76/77] Elapsed 0m 8s (remain 0m 0s) Loss: 0.0497 \n",
            "Log loss for 0 is 0.04965760426166199\n",
            "Epoch: [2][0/306] Elapsed 0m 0s (remain 3m 6s) Loss: 0.0994 \n",
            "Epoch: [2][100/306] Elapsed 0m 34s (remain 1m 9s) Loss: 0.0484 \n",
            "Epoch: [2][200/306] Elapsed 1m 7s (remain 0m 35s) Loss: 0.0459 \n",
            "Epoch: [2][300/306] Elapsed 1m 41s (remain 0m 1s) Loss: 0.0469 \n",
            "Epoch: [2][305/306] Elapsed 1m 42s (remain 0m 0s) Loss: 0.0463 \n",
            "EVAL: [0/77] Elapsed 0m 0s (remain 0m 26s) Loss: 0.0037 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2 - avg_train_loss: 0.0463  avg_val_loss: 0.0437  time: 112s\n",
            "Epoch 2 - avg_train_loss: 0.0463  avg_val_loss: 0.0437  time: 112s\n",
            "Epoch 2 - avg_train_loss: 0.0463  avg_val_loss: 0.0437  time: 112s\n",
            "Epoch 2 - avg_train_loss: 0.0463  avg_val_loss: 0.0437  time: 112s\n",
            "Epoch 2 - avg_train_loss: 0.0463  avg_val_loss: 0.0437  time: 112s\n",
            "Epoch 2 - avg_train_loss: 0.0463  avg_val_loss: 0.0437  time: 112s\n",
            "Epoch 2 - avg_train_loss: 0.0463  avg_val_loss: 0.0437  time: 112s\n",
            "Epoch 2 - avg_train_loss: 0.0463  avg_val_loss: 0.0437  time: 112s\n",
            "Epoch 2 - avg_train_loss: 0.0463  avg_val_loss: 0.0437  time: 112s\n",
            "Epoch 2 - avg_train_loss: 0.0463  avg_val_loss: 0.0437  time: 112s\n",
            "Epoch 2 - avg_train_loss: 0.0463  avg_val_loss: 0.0437  time: 112s\n",
            "Epoch 2 - Score: 0.043702487558746575\n",
            "Epoch 2 - Score: 0.043702487558746575\n",
            "Epoch 2 - Score: 0.043702487558746575\n",
            "Epoch 2 - Score: 0.043702487558746575\n",
            "Epoch 2 - Score: 0.043702487558746575\n",
            "Epoch 2 - Score: 0.043702487558746575\n",
            "Epoch 2 - Score: 0.043702487558746575\n",
            "Epoch 2 - Score: 0.043702487558746575\n",
            "Epoch 2 - Score: 0.043702487558746575\n",
            "Epoch 2 - Score: 0.043702487558746575\n",
            "Epoch 2 - Score: 0.043702487558746575\n",
            "Epoch 2 - Save Best Score: 0.0437 Model\n",
            "Epoch 2 - Save Best Score: 0.0437 Model\n",
            "Epoch 2 - Save Best Score: 0.0437 Model\n",
            "Epoch 2 - Save Best Score: 0.0437 Model\n",
            "Epoch 2 - Save Best Score: 0.0437 Model\n",
            "Epoch 2 - Save Best Score: 0.0437 Model\n",
            "Epoch 2 - Save Best Score: 0.0437 Model\n",
            "Epoch 2 - Save Best Score: 0.0437 Model\n",
            "Epoch 2 - Save Best Score: 0.0437 Model\n",
            "Epoch 2 - Save Best Score: 0.0437 Model\n",
            "Epoch 2 - Save Best Score: 0.0437 Model\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EVAL: [76/77] Elapsed 0m 8s (remain 0m 0s) Loss: 0.0437 \n",
            "Log loss for 1 is 0.043702487558746575\n",
            "Epoch: [3][0/306] Elapsed 0m 0s (remain 2m 59s) Loss: 0.1806 \n",
            "Epoch: [3][100/306] Elapsed 0m 34s (remain 1m 9s) Loss: 0.0280 \n",
            "Epoch: [3][200/306] Elapsed 1m 7s (remain 0m 35s) Loss: 0.0282 \n",
            "Epoch: [3][300/306] Elapsed 1m 40s (remain 0m 1s) Loss: 0.0271 \n",
            "Epoch: [3][305/306] Elapsed 1m 42s (remain 0m 0s) Loss: 0.0272 \n",
            "EVAL: [0/77] Elapsed 0m 0s (remain 0m 29s) Loss: 0.0049 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3 - avg_train_loss: 0.0272  avg_val_loss: 0.0568  time: 111s\n",
            "Epoch 3 - avg_train_loss: 0.0272  avg_val_loss: 0.0568  time: 111s\n",
            "Epoch 3 - avg_train_loss: 0.0272  avg_val_loss: 0.0568  time: 111s\n",
            "Epoch 3 - avg_train_loss: 0.0272  avg_val_loss: 0.0568  time: 111s\n",
            "Epoch 3 - avg_train_loss: 0.0272  avg_val_loss: 0.0568  time: 111s\n",
            "Epoch 3 - avg_train_loss: 0.0272  avg_val_loss: 0.0568  time: 111s\n",
            "Epoch 3 - avg_train_loss: 0.0272  avg_val_loss: 0.0568  time: 111s\n",
            "Epoch 3 - avg_train_loss: 0.0272  avg_val_loss: 0.0568  time: 111s\n",
            "Epoch 3 - avg_train_loss: 0.0272  avg_val_loss: 0.0568  time: 111s\n",
            "Epoch 3 - avg_train_loss: 0.0272  avg_val_loss: 0.0568  time: 111s\n",
            "Epoch 3 - avg_train_loss: 0.0272  avg_val_loss: 0.0568  time: 111s\n",
            "Epoch 3 - Score: 0.05678449675318206\n",
            "Epoch 3 - Score: 0.05678449675318206\n",
            "Epoch 3 - Score: 0.05678449675318206\n",
            "Epoch 3 - Score: 0.05678449675318206\n",
            "Epoch 3 - Score: 0.05678449675318206\n",
            "Epoch 3 - Score: 0.05678449675318206\n",
            "Epoch 3 - Score: 0.05678449675318206\n",
            "Epoch 3 - Score: 0.05678449675318206\n",
            "Epoch 3 - Score: 0.05678449675318206\n",
            "Epoch 3 - Score: 0.05678449675318206\n",
            "Epoch 3 - Score: 0.05678449675318206\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EVAL: [76/77] Elapsed 0m 8s (remain 0m 0s) Loss: 0.0568 \n",
            "Log loss for 2 is 0.05678449675318206\n",
            "Epoch: [4][0/306] Elapsed 0m 0s (remain 3m 13s) Loss: 0.0026 \n",
            "Epoch: [4][100/306] Elapsed 0m 34s (remain 1m 9s) Loss: 0.0108 \n",
            "Epoch: [4][200/306] Elapsed 1m 7s (remain 0m 35s) Loss: 0.0163 \n",
            "Epoch: [4][300/306] Elapsed 1m 40s (remain 0m 1s) Loss: 0.0141 \n",
            "Epoch: [4][305/306] Elapsed 1m 42s (remain 0m 0s) Loss: 0.0140 \n",
            "EVAL: [0/77] Elapsed 0m 0s (remain 0m 27s) Loss: 0.0077 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4 - avg_train_loss: 0.0140  avg_val_loss: 0.0917  time: 111s\n",
            "Epoch 4 - avg_train_loss: 0.0140  avg_val_loss: 0.0917  time: 111s\n",
            "Epoch 4 - avg_train_loss: 0.0140  avg_val_loss: 0.0917  time: 111s\n",
            "Epoch 4 - avg_train_loss: 0.0140  avg_val_loss: 0.0917  time: 111s\n",
            "Epoch 4 - avg_train_loss: 0.0140  avg_val_loss: 0.0917  time: 111s\n",
            "Epoch 4 - avg_train_loss: 0.0140  avg_val_loss: 0.0917  time: 111s\n",
            "Epoch 4 - avg_train_loss: 0.0140  avg_val_loss: 0.0917  time: 111s\n",
            "Epoch 4 - avg_train_loss: 0.0140  avg_val_loss: 0.0917  time: 111s\n",
            "Epoch 4 - avg_train_loss: 0.0140  avg_val_loss: 0.0917  time: 111s\n",
            "Epoch 4 - avg_train_loss: 0.0140  avg_val_loss: 0.0917  time: 111s\n",
            "Epoch 4 - avg_train_loss: 0.0140  avg_val_loss: 0.0917  time: 111s\n",
            "Epoch 4 - Score: 0.09166138714313933\n",
            "Epoch 4 - Score: 0.09166138714313933\n",
            "Epoch 4 - Score: 0.09166138714313933\n",
            "Epoch 4 - Score: 0.09166138714313933\n",
            "Epoch 4 - Score: 0.09166138714313933\n",
            "Epoch 4 - Score: 0.09166138714313933\n",
            "Epoch 4 - Score: 0.09166138714313933\n",
            "Epoch 4 - Score: 0.09166138714313933\n",
            "Epoch 4 - Score: 0.09166138714313933\n",
            "Epoch 4 - Score: 0.09166138714313933\n",
            "Epoch 4 - Score: 0.09166138714313933\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EVAL: [76/77] Elapsed 0m 8s (remain 0m 0s) Loss: 0.0917 \n",
            "Log loss for 3 is 0.09166138714313933\n",
            "Epoch: [5][0/306] Elapsed 0m 0s (remain 3m 8s) Loss: 0.0007 \n",
            "Epoch: [5][100/306] Elapsed 0m 34s (remain 1m 9s) Loss: 0.0130 \n",
            "Epoch: [5][200/306] Elapsed 1m 7s (remain 0m 35s) Loss: 0.0124 \n",
            "Epoch: [5][300/306] Elapsed 1m 41s (remain 0m 1s) Loss: 0.0120 \n",
            "Epoch: [5][305/306] Elapsed 1m 42s (remain 0m 0s) Loss: 0.0124 \n",
            "EVAL: [0/77] Elapsed 0m 0s (remain 0m 26s) Loss: 0.0010 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5 - avg_train_loss: 0.0124  avg_val_loss: 0.0946  time: 111s\n",
            "Epoch 5 - avg_train_loss: 0.0124  avg_val_loss: 0.0946  time: 111s\n",
            "Epoch 5 - avg_train_loss: 0.0124  avg_val_loss: 0.0946  time: 111s\n",
            "Epoch 5 - avg_train_loss: 0.0124  avg_val_loss: 0.0946  time: 111s\n",
            "Epoch 5 - avg_train_loss: 0.0124  avg_val_loss: 0.0946  time: 111s\n",
            "Epoch 5 - avg_train_loss: 0.0124  avg_val_loss: 0.0946  time: 111s\n",
            "Epoch 5 - avg_train_loss: 0.0124  avg_val_loss: 0.0946  time: 111s\n",
            "Epoch 5 - avg_train_loss: 0.0124  avg_val_loss: 0.0946  time: 111s\n",
            "Epoch 5 - avg_train_loss: 0.0124  avg_val_loss: 0.0946  time: 111s\n",
            "Epoch 5 - avg_train_loss: 0.0124  avg_val_loss: 0.0946  time: 111s\n",
            "Epoch 5 - avg_train_loss: 0.0124  avg_val_loss: 0.0946  time: 111s\n",
            "Epoch 5 - Score: 0.09455119340199793\n",
            "Epoch 5 - Score: 0.09455119340199793\n",
            "Epoch 5 - Score: 0.09455119340199793\n",
            "Epoch 5 - Score: 0.09455119340199793\n",
            "Epoch 5 - Score: 0.09455119340199793\n",
            "Epoch 5 - Score: 0.09455119340199793\n",
            "Epoch 5 - Score: 0.09455119340199793\n",
            "Epoch 5 - Score: 0.09455119340199793\n",
            "Epoch 5 - Score: 0.09455119340199793\n",
            "Epoch 5 - Score: 0.09455119340199793\n",
            "Epoch 5 - Score: 0.09455119340199793\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EVAL: [76/77] Elapsed 0m 8s (remain 0m 0s) Loss: 0.0946 \n",
            "Log loss for 4 is 0.09455119340199793\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "========== fold: 1 result ==========\n",
            "========== fold: 1 result ==========\n",
            "========== fold: 1 result ==========\n",
            "========== fold: 1 result ==========\n",
            "========== fold: 1 result ==========\n",
            "========== fold: 1 result ==========\n",
            "========== fold: 1 result ==========\n",
            "========== fold: 1 result ==========\n",
            "========== fold: 1 result ==========\n",
            "========== fold: 1 result ==========\n",
            "========== fold: 1 result ==========\n",
            "Score: 0.04370\n",
            "Score: 0.04370\n",
            "Score: 0.04370\n",
            "Score: 0.04370\n",
            "Score: 0.04370\n",
            "Score: 0.04370\n",
            "Score: 0.04370\n",
            "Score: 0.04370\n",
            "Score: 0.04370\n",
            "Score: 0.04370\n",
            "Score: 0.04370\n",
            "========== fold: 2 training ==========\n",
            "========== fold: 2 training ==========\n",
            "========== fold: 2 training ==========\n",
            "========== fold: 2 training ==========\n",
            "========== fold: 2 training ==========\n",
            "========== fold: 2 training ==========\n",
            "========== fold: 2 training ==========\n",
            "========== fold: 2 training ==========\n",
            "========== fold: 2 training ==========\n",
            "========== fold: 2 training ==========\n",
            "========== fold: 2 training ==========\n",
            "Didn't find file /content/activebus_bert_final/added_tokens.json. We won't load it.\n",
            "loading file /content/activebus_bert_final/vocab.txt\n",
            "loading file None\n",
            "loading file /content/activebus_bert_final/special_tokens_map.json\n",
            "loading file /content/activebus_bert_final/tokenizer_config.json\n",
            "loading file /content/activebus_bert_final/tokenizer.json\n",
            "Didn't find file /content/activebus_bert_final/added_tokens.json. We won't load it.\n",
            "loading file /content/activebus_bert_final/vocab.txt\n",
            "loading file None\n",
            "loading file /content/activebus_bert_final/special_tokens_map.json\n",
            "loading file /content/activebus_bert_final/tokenizer_config.json\n",
            "loading file /content/activebus_bert_final/tokenizer.json\n",
            "loading configuration file /content/activebus_bert_final/config.json\n",
            "Model config BertConfig {\n",
            "  \"_name_or_path\": \"activebus/BERT-XD_Review\",\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_ids\": 0,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.15.0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "loading weights file /content/activebus_bert_final/pytorch_model.bin\n",
            "Some weights of the model checkpoint at /content/activebus_bert_final were not used when initializing BertForSequenceClassification: ['cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /content/activebus_bert_final and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: [1][0/306] Elapsed 0m 0s (remain 3m 11s) Loss: 0.6874 \n",
            "Epoch: [1][100/306] Elapsed 0m 34s (remain 1m 9s) Loss: 0.1431 \n",
            "Epoch: [1][200/306] Elapsed 1m 7s (remain 0m 35s) Loss: 0.1183 \n",
            "Epoch: [1][300/306] Elapsed 1m 40s (remain 0m 1s) Loss: 0.0956 \n",
            "Epoch: [1][305/306] Elapsed 1m 42s (remain 0m 0s) Loss: 0.0946 \n",
            "EVAL: [0/77] Elapsed 0m 0s (remain 0m 27s) Loss: 0.0083 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1 - avg_train_loss: 0.0946  avg_val_loss: 0.0384  time: 111s\n",
            "Epoch 1 - avg_train_loss: 0.0946  avg_val_loss: 0.0384  time: 111s\n",
            "Epoch 1 - avg_train_loss: 0.0946  avg_val_loss: 0.0384  time: 111s\n",
            "Epoch 1 - avg_train_loss: 0.0946  avg_val_loss: 0.0384  time: 111s\n",
            "Epoch 1 - avg_train_loss: 0.0946  avg_val_loss: 0.0384  time: 111s\n",
            "Epoch 1 - avg_train_loss: 0.0946  avg_val_loss: 0.0384  time: 111s\n",
            "Epoch 1 - avg_train_loss: 0.0946  avg_val_loss: 0.0384  time: 111s\n",
            "Epoch 1 - avg_train_loss: 0.0946  avg_val_loss: 0.0384  time: 111s\n",
            "Epoch 1 - avg_train_loss: 0.0946  avg_val_loss: 0.0384  time: 111s\n",
            "Epoch 1 - avg_train_loss: 0.0946  avg_val_loss: 0.0384  time: 111s\n",
            "Epoch 1 - avg_train_loss: 0.0946  avg_val_loss: 0.0384  time: 111s\n",
            "Epoch 1 - Score: 0.03840338866649152\n",
            "Epoch 1 - Score: 0.03840338866649152\n",
            "Epoch 1 - Score: 0.03840338866649152\n",
            "Epoch 1 - Score: 0.03840338866649152\n",
            "Epoch 1 - Score: 0.03840338866649152\n",
            "Epoch 1 - Score: 0.03840338866649152\n",
            "Epoch 1 - Score: 0.03840338866649152\n",
            "Epoch 1 - Score: 0.03840338866649152\n",
            "Epoch 1 - Score: 0.03840338866649152\n",
            "Epoch 1 - Score: 0.03840338866649152\n",
            "Epoch 1 - Score: 0.03840338866649152\n",
            "Epoch 1 - Save Best Score: 0.0384 Model\n",
            "Epoch 1 - Save Best Score: 0.0384 Model\n",
            "Epoch 1 - Save Best Score: 0.0384 Model\n",
            "Epoch 1 - Save Best Score: 0.0384 Model\n",
            "Epoch 1 - Save Best Score: 0.0384 Model\n",
            "Epoch 1 - Save Best Score: 0.0384 Model\n",
            "Epoch 1 - Save Best Score: 0.0384 Model\n",
            "Epoch 1 - Save Best Score: 0.0384 Model\n",
            "Epoch 1 - Save Best Score: 0.0384 Model\n",
            "Epoch 1 - Save Best Score: 0.0384 Model\n",
            "Epoch 1 - Save Best Score: 0.0384 Model\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EVAL: [76/77] Elapsed 0m 8s (remain 0m 0s) Loss: 0.0384 \n",
            "Log loss for 0 is 0.03840338866649152\n",
            "Epoch: [2][0/306] Elapsed 0m 0s (remain 3m 10s) Loss: 0.0445 \n",
            "Epoch: [2][100/306] Elapsed 0m 34s (remain 1m 9s) Loss: 0.0422 \n",
            "Epoch: [2][200/306] Elapsed 1m 7s (remain 0m 35s) Loss: 0.0399 \n",
            "Epoch: [2][300/306] Elapsed 1m 41s (remain 0m 1s) Loss: 0.0388 \n",
            "Epoch: [2][305/306] Elapsed 1m 42s (remain 0m 0s) Loss: 0.0390 \n",
            "EVAL: [0/77] Elapsed 0m 0s (remain 0m 27s) Loss: 0.0078 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2 - avg_train_loss: 0.0390  avg_val_loss: 0.0365  time: 111s\n",
            "Epoch 2 - avg_train_loss: 0.0390  avg_val_loss: 0.0365  time: 111s\n",
            "Epoch 2 - avg_train_loss: 0.0390  avg_val_loss: 0.0365  time: 111s\n",
            "Epoch 2 - avg_train_loss: 0.0390  avg_val_loss: 0.0365  time: 111s\n",
            "Epoch 2 - avg_train_loss: 0.0390  avg_val_loss: 0.0365  time: 111s\n",
            "Epoch 2 - avg_train_loss: 0.0390  avg_val_loss: 0.0365  time: 111s\n",
            "Epoch 2 - avg_train_loss: 0.0390  avg_val_loss: 0.0365  time: 111s\n",
            "Epoch 2 - avg_train_loss: 0.0390  avg_val_loss: 0.0365  time: 111s\n",
            "Epoch 2 - avg_train_loss: 0.0390  avg_val_loss: 0.0365  time: 111s\n",
            "Epoch 2 - avg_train_loss: 0.0390  avg_val_loss: 0.0365  time: 111s\n",
            "Epoch 2 - avg_train_loss: 0.0390  avg_val_loss: 0.0365  time: 111s\n",
            "Epoch 2 - Score: 0.03654548899496783\n",
            "Epoch 2 - Score: 0.03654548899496783\n",
            "Epoch 2 - Score: 0.03654548899496783\n",
            "Epoch 2 - Score: 0.03654548899496783\n",
            "Epoch 2 - Score: 0.03654548899496783\n",
            "Epoch 2 - Score: 0.03654548899496783\n",
            "Epoch 2 - Score: 0.03654548899496783\n",
            "Epoch 2 - Score: 0.03654548899496783\n",
            "Epoch 2 - Score: 0.03654548899496783\n",
            "Epoch 2 - Score: 0.03654548899496783\n",
            "Epoch 2 - Score: 0.03654548899496783\n",
            "Epoch 2 - Save Best Score: 0.0365 Model\n",
            "Epoch 2 - Save Best Score: 0.0365 Model\n",
            "Epoch 2 - Save Best Score: 0.0365 Model\n",
            "Epoch 2 - Save Best Score: 0.0365 Model\n",
            "Epoch 2 - Save Best Score: 0.0365 Model\n",
            "Epoch 2 - Save Best Score: 0.0365 Model\n",
            "Epoch 2 - Save Best Score: 0.0365 Model\n",
            "Epoch 2 - Save Best Score: 0.0365 Model\n",
            "Epoch 2 - Save Best Score: 0.0365 Model\n",
            "Epoch 2 - Save Best Score: 0.0365 Model\n",
            "Epoch 2 - Save Best Score: 0.0365 Model\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EVAL: [76/77] Elapsed 0m 8s (remain 0m 0s) Loss: 0.0365 \n",
            "Log loss for 1 is 0.03654548899496783\n",
            "Epoch: [3][0/306] Elapsed 0m 0s (remain 3m 5s) Loss: 0.1105 \n",
            "Epoch: [3][100/306] Elapsed 0m 34s (remain 1m 9s) Loss: 0.0199 \n",
            "Epoch: [3][200/306] Elapsed 1m 7s (remain 0m 35s) Loss: 0.0201 \n",
            "Epoch: [3][300/306] Elapsed 1m 40s (remain 0m 1s) Loss: 0.0191 \n",
            "Epoch: [3][305/306] Elapsed 1m 42s (remain 0m 0s) Loss: 0.0189 \n",
            "EVAL: [0/77] Elapsed 0m 0s (remain 0m 28s) Loss: 0.0005 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3 - avg_train_loss: 0.0189  avg_val_loss: 0.0555  time: 111s\n",
            "Epoch 3 - avg_train_loss: 0.0189  avg_val_loss: 0.0555  time: 111s\n",
            "Epoch 3 - avg_train_loss: 0.0189  avg_val_loss: 0.0555  time: 111s\n",
            "Epoch 3 - avg_train_loss: 0.0189  avg_val_loss: 0.0555  time: 111s\n",
            "Epoch 3 - avg_train_loss: 0.0189  avg_val_loss: 0.0555  time: 111s\n",
            "Epoch 3 - avg_train_loss: 0.0189  avg_val_loss: 0.0555  time: 111s\n",
            "Epoch 3 - avg_train_loss: 0.0189  avg_val_loss: 0.0555  time: 111s\n",
            "Epoch 3 - avg_train_loss: 0.0189  avg_val_loss: 0.0555  time: 111s\n",
            "Epoch 3 - avg_train_loss: 0.0189  avg_val_loss: 0.0555  time: 111s\n",
            "Epoch 3 - avg_train_loss: 0.0189  avg_val_loss: 0.0555  time: 111s\n",
            "Epoch 3 - avg_train_loss: 0.0189  avg_val_loss: 0.0555  time: 111s\n",
            "Epoch 3 - Score: 0.055473162850712104\n",
            "Epoch 3 - Score: 0.055473162850712104\n",
            "Epoch 3 - Score: 0.055473162850712104\n",
            "Epoch 3 - Score: 0.055473162850712104\n",
            "Epoch 3 - Score: 0.055473162850712104\n",
            "Epoch 3 - Score: 0.055473162850712104\n",
            "Epoch 3 - Score: 0.055473162850712104\n",
            "Epoch 3 - Score: 0.055473162850712104\n",
            "Epoch 3 - Score: 0.055473162850712104\n",
            "Epoch 3 - Score: 0.055473162850712104\n",
            "Epoch 3 - Score: 0.055473162850712104\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EVAL: [76/77] Elapsed 0m 8s (remain 0m 0s) Loss: 0.0555 \n",
            "Log loss for 2 is 0.055473162850712104\n",
            "Epoch: [4][0/306] Elapsed 0m 0s (remain 3m 4s) Loss: 0.0171 \n",
            "Epoch: [4][100/306] Elapsed 0m 34s (remain 1m 9s) Loss: 0.0094 \n",
            "Epoch: [4][200/306] Elapsed 1m 7s (remain 0m 35s) Loss: 0.0165 \n",
            "Epoch: [4][300/306] Elapsed 1m 40s (remain 0m 1s) Loss: 0.0163 \n",
            "Epoch: [4][305/306] Elapsed 1m 42s (remain 0m 0s) Loss: 0.0163 \n",
            "EVAL: [0/77] Elapsed 0m 0s (remain 0m 27s) Loss: 0.0018 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4 - avg_train_loss: 0.0163  avg_val_loss: 0.0458  time: 111s\n",
            "Epoch 4 - avg_train_loss: 0.0163  avg_val_loss: 0.0458  time: 111s\n",
            "Epoch 4 - avg_train_loss: 0.0163  avg_val_loss: 0.0458  time: 111s\n",
            "Epoch 4 - avg_train_loss: 0.0163  avg_val_loss: 0.0458  time: 111s\n",
            "Epoch 4 - avg_train_loss: 0.0163  avg_val_loss: 0.0458  time: 111s\n",
            "Epoch 4 - avg_train_loss: 0.0163  avg_val_loss: 0.0458  time: 111s\n",
            "Epoch 4 - avg_train_loss: 0.0163  avg_val_loss: 0.0458  time: 111s\n",
            "Epoch 4 - avg_train_loss: 0.0163  avg_val_loss: 0.0458  time: 111s\n",
            "Epoch 4 - avg_train_loss: 0.0163  avg_val_loss: 0.0458  time: 111s\n",
            "Epoch 4 - avg_train_loss: 0.0163  avg_val_loss: 0.0458  time: 111s\n",
            "Epoch 4 - avg_train_loss: 0.0163  avg_val_loss: 0.0458  time: 111s\n",
            "Epoch 4 - Score: 0.045792890044226944\n",
            "Epoch 4 - Score: 0.045792890044226944\n",
            "Epoch 4 - Score: 0.045792890044226944\n",
            "Epoch 4 - Score: 0.045792890044226944\n",
            "Epoch 4 - Score: 0.045792890044226944\n",
            "Epoch 4 - Score: 0.045792890044226944\n",
            "Epoch 4 - Score: 0.045792890044226944\n",
            "Epoch 4 - Score: 0.045792890044226944\n",
            "Epoch 4 - Score: 0.045792890044226944\n",
            "Epoch 4 - Score: 0.045792890044226944\n",
            "Epoch 4 - Score: 0.045792890044226944\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EVAL: [76/77] Elapsed 0m 8s (remain 0m 0s) Loss: 0.0458 \n",
            "Log loss for 3 is 0.045792890044226944\n",
            "Epoch: [5][0/306] Elapsed 0m 0s (remain 2m 58s) Loss: 0.0006 \n",
            "Epoch: [5][100/306] Elapsed 0m 34s (remain 1m 9s) Loss: 0.0036 \n",
            "Epoch: [5][200/306] Elapsed 1m 7s (remain 0m 35s) Loss: 0.0080 \n",
            "Epoch: [5][300/306] Elapsed 1m 40s (remain 0m 1s) Loss: 0.0101 \n",
            "Epoch: [5][305/306] Elapsed 1m 42s (remain 0m 0s) Loss: 0.0099 \n",
            "EVAL: [0/77] Elapsed 0m 0s (remain 0m 26s) Loss: 0.0003 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5 - avg_train_loss: 0.0099  avg_val_loss: 0.0779  time: 111s\n",
            "Epoch 5 - avg_train_loss: 0.0099  avg_val_loss: 0.0779  time: 111s\n",
            "Epoch 5 - avg_train_loss: 0.0099  avg_val_loss: 0.0779  time: 111s\n",
            "Epoch 5 - avg_train_loss: 0.0099  avg_val_loss: 0.0779  time: 111s\n",
            "Epoch 5 - avg_train_loss: 0.0099  avg_val_loss: 0.0779  time: 111s\n",
            "Epoch 5 - avg_train_loss: 0.0099  avg_val_loss: 0.0779  time: 111s\n",
            "Epoch 5 - avg_train_loss: 0.0099  avg_val_loss: 0.0779  time: 111s\n",
            "Epoch 5 - avg_train_loss: 0.0099  avg_val_loss: 0.0779  time: 111s\n",
            "Epoch 5 - avg_train_loss: 0.0099  avg_val_loss: 0.0779  time: 111s\n",
            "Epoch 5 - avg_train_loss: 0.0099  avg_val_loss: 0.0779  time: 111s\n",
            "Epoch 5 - avg_train_loss: 0.0099  avg_val_loss: 0.0779  time: 111s\n",
            "Epoch 5 - Score: 0.07793941195669869\n",
            "Epoch 5 - Score: 0.07793941195669869\n",
            "Epoch 5 - Score: 0.07793941195669869\n",
            "Epoch 5 - Score: 0.07793941195669869\n",
            "Epoch 5 - Score: 0.07793941195669869\n",
            "Epoch 5 - Score: 0.07793941195669869\n",
            "Epoch 5 - Score: 0.07793941195669869\n",
            "Epoch 5 - Score: 0.07793941195669869\n",
            "Epoch 5 - Score: 0.07793941195669869\n",
            "Epoch 5 - Score: 0.07793941195669869\n",
            "Epoch 5 - Score: 0.07793941195669869\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EVAL: [76/77] Elapsed 0m 8s (remain 0m 0s) Loss: 0.0779 \n",
            "Log loss for 4 is 0.07793941195669869\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "========== fold: 2 result ==========\n",
            "========== fold: 2 result ==========\n",
            "========== fold: 2 result ==========\n",
            "========== fold: 2 result ==========\n",
            "========== fold: 2 result ==========\n",
            "========== fold: 2 result ==========\n",
            "========== fold: 2 result ==========\n",
            "========== fold: 2 result ==========\n",
            "========== fold: 2 result ==========\n",
            "========== fold: 2 result ==========\n",
            "========== fold: 2 result ==========\n",
            "Score: 0.03655\n",
            "Score: 0.03655\n",
            "Score: 0.03655\n",
            "Score: 0.03655\n",
            "Score: 0.03655\n",
            "Score: 0.03655\n",
            "Score: 0.03655\n",
            "Score: 0.03655\n",
            "Score: 0.03655\n",
            "Score: 0.03655\n",
            "Score: 0.03655\n",
            "========== fold: 3 training ==========\n",
            "========== fold: 3 training ==========\n",
            "========== fold: 3 training ==========\n",
            "========== fold: 3 training ==========\n",
            "========== fold: 3 training ==========\n",
            "========== fold: 3 training ==========\n",
            "========== fold: 3 training ==========\n",
            "========== fold: 3 training ==========\n",
            "========== fold: 3 training ==========\n",
            "========== fold: 3 training ==========\n",
            "========== fold: 3 training ==========\n",
            "Didn't find file /content/activebus_bert_final/added_tokens.json. We won't load it.\n",
            "loading file /content/activebus_bert_final/vocab.txt\n",
            "loading file None\n",
            "loading file /content/activebus_bert_final/special_tokens_map.json\n",
            "loading file /content/activebus_bert_final/tokenizer_config.json\n",
            "loading file /content/activebus_bert_final/tokenizer.json\n",
            "Didn't find file /content/activebus_bert_final/added_tokens.json. We won't load it.\n",
            "loading file /content/activebus_bert_final/vocab.txt\n",
            "loading file None\n",
            "loading file /content/activebus_bert_final/special_tokens_map.json\n",
            "loading file /content/activebus_bert_final/tokenizer_config.json\n",
            "loading file /content/activebus_bert_final/tokenizer.json\n",
            "loading configuration file /content/activebus_bert_final/config.json\n",
            "Model config BertConfig {\n",
            "  \"_name_or_path\": \"activebus/BERT-XD_Review\",\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_ids\": 0,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.15.0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "loading weights file /content/activebus_bert_final/pytorch_model.bin\n",
            "Some weights of the model checkpoint at /content/activebus_bert_final were not used when initializing BertForSequenceClassification: ['cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /content/activebus_bert_final and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: [1][0/306] Elapsed 0m 0s (remain 3m 1s) Loss: 0.7054 \n",
            "Epoch: [1][100/306] Elapsed 0m 34s (remain 1m 9s) Loss: 0.1509 \n",
            "Epoch: [1][200/306] Elapsed 1m 7s (remain 0m 35s) Loss: 0.1162 \n",
            "Epoch: [1][300/306] Elapsed 1m 40s (remain 0m 1s) Loss: 0.0935 \n",
            "Epoch: [1][305/306] Elapsed 1m 42s (remain 0m 0s) Loss: 0.0936 \n",
            "EVAL: [0/77] Elapsed 0m 0s (remain 0m 27s) Loss: 0.1468 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1 - avg_train_loss: 0.0936  avg_val_loss: 0.0921  time: 111s\n",
            "Epoch 1 - avg_train_loss: 0.0936  avg_val_loss: 0.0921  time: 111s\n",
            "Epoch 1 - avg_train_loss: 0.0936  avg_val_loss: 0.0921  time: 111s\n",
            "Epoch 1 - avg_train_loss: 0.0936  avg_val_loss: 0.0921  time: 111s\n",
            "Epoch 1 - avg_train_loss: 0.0936  avg_val_loss: 0.0921  time: 111s\n",
            "Epoch 1 - avg_train_loss: 0.0936  avg_val_loss: 0.0921  time: 111s\n",
            "Epoch 1 - avg_train_loss: 0.0936  avg_val_loss: 0.0921  time: 111s\n",
            "Epoch 1 - avg_train_loss: 0.0936  avg_val_loss: 0.0921  time: 111s\n",
            "Epoch 1 - avg_train_loss: 0.0936  avg_val_loss: 0.0921  time: 111s\n",
            "Epoch 1 - avg_train_loss: 0.0936  avg_val_loss: 0.0921  time: 111s\n",
            "Epoch 1 - avg_train_loss: 0.0936  avg_val_loss: 0.0921  time: 111s\n",
            "Epoch 1 - Score: 0.0920634834200581\n",
            "Epoch 1 - Score: 0.0920634834200581\n",
            "Epoch 1 - Score: 0.0920634834200581\n",
            "Epoch 1 - Score: 0.0920634834200581\n",
            "Epoch 1 - Score: 0.0920634834200581\n",
            "Epoch 1 - Score: 0.0920634834200581\n",
            "Epoch 1 - Score: 0.0920634834200581\n",
            "Epoch 1 - Score: 0.0920634834200581\n",
            "Epoch 1 - Score: 0.0920634834200581\n",
            "Epoch 1 - Score: 0.0920634834200581\n",
            "Epoch 1 - Score: 0.0920634834200581\n",
            "Epoch 1 - Save Best Score: 0.0921 Model\n",
            "Epoch 1 - Save Best Score: 0.0921 Model\n",
            "Epoch 1 - Save Best Score: 0.0921 Model\n",
            "Epoch 1 - Save Best Score: 0.0921 Model\n",
            "Epoch 1 - Save Best Score: 0.0921 Model\n",
            "Epoch 1 - Save Best Score: 0.0921 Model\n",
            "Epoch 1 - Save Best Score: 0.0921 Model\n",
            "Epoch 1 - Save Best Score: 0.0921 Model\n",
            "Epoch 1 - Save Best Score: 0.0921 Model\n",
            "Epoch 1 - Save Best Score: 0.0921 Model\n",
            "Epoch 1 - Save Best Score: 0.0921 Model\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EVAL: [76/77] Elapsed 0m 8s (remain 0m 0s) Loss: 0.0921 \n",
            "Log loss for 0 is 0.0920634834200581\n",
            "Epoch: [2][0/306] Elapsed 0m 0s (remain 3m 9s) Loss: 0.0088 \n",
            "Epoch: [2][100/306] Elapsed 0m 34s (remain 1m 9s) Loss: 0.0367 \n",
            "Epoch: [2][200/306] Elapsed 1m 7s (remain 0m 35s) Loss: 0.0425 \n",
            "Epoch: [2][300/306] Elapsed 1m 41s (remain 0m 1s) Loss: 0.0435 \n",
            "Epoch: [2][305/306] Elapsed 1m 42s (remain 0m 0s) Loss: 0.0432 \n",
            "EVAL: [0/77] Elapsed 0m 0s (remain 0m 28s) Loss: 0.0326 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2 - avg_train_loss: 0.0432  avg_val_loss: 0.0483  time: 112s\n",
            "Epoch 2 - avg_train_loss: 0.0432  avg_val_loss: 0.0483  time: 112s\n",
            "Epoch 2 - avg_train_loss: 0.0432  avg_val_loss: 0.0483  time: 112s\n",
            "Epoch 2 - avg_train_loss: 0.0432  avg_val_loss: 0.0483  time: 112s\n",
            "Epoch 2 - avg_train_loss: 0.0432  avg_val_loss: 0.0483  time: 112s\n",
            "Epoch 2 - avg_train_loss: 0.0432  avg_val_loss: 0.0483  time: 112s\n",
            "Epoch 2 - avg_train_loss: 0.0432  avg_val_loss: 0.0483  time: 112s\n",
            "Epoch 2 - avg_train_loss: 0.0432  avg_val_loss: 0.0483  time: 112s\n",
            "Epoch 2 - avg_train_loss: 0.0432  avg_val_loss: 0.0483  time: 112s\n",
            "Epoch 2 - avg_train_loss: 0.0432  avg_val_loss: 0.0483  time: 112s\n",
            "Epoch 2 - avg_train_loss: 0.0432  avg_val_loss: 0.0483  time: 112s\n",
            "Epoch 2 - Score: 0.048320706932503385\n",
            "Epoch 2 - Score: 0.048320706932503385\n",
            "Epoch 2 - Score: 0.048320706932503385\n",
            "Epoch 2 - Score: 0.048320706932503385\n",
            "Epoch 2 - Score: 0.048320706932503385\n",
            "Epoch 2 - Score: 0.048320706932503385\n",
            "Epoch 2 - Score: 0.048320706932503385\n",
            "Epoch 2 - Score: 0.048320706932503385\n",
            "Epoch 2 - Score: 0.048320706932503385\n",
            "Epoch 2 - Score: 0.048320706932503385\n",
            "Epoch 2 - Score: 0.048320706932503385\n",
            "Epoch 2 - Save Best Score: 0.0483 Model\n",
            "Epoch 2 - Save Best Score: 0.0483 Model\n",
            "Epoch 2 - Save Best Score: 0.0483 Model\n",
            "Epoch 2 - Save Best Score: 0.0483 Model\n",
            "Epoch 2 - Save Best Score: 0.0483 Model\n",
            "Epoch 2 - Save Best Score: 0.0483 Model\n",
            "Epoch 2 - Save Best Score: 0.0483 Model\n",
            "Epoch 2 - Save Best Score: 0.0483 Model\n",
            "Epoch 2 - Save Best Score: 0.0483 Model\n",
            "Epoch 2 - Save Best Score: 0.0483 Model\n",
            "Epoch 2 - Save Best Score: 0.0483 Model\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EVAL: [76/77] Elapsed 0m 8s (remain 0m 0s) Loss: 0.0483 \n",
            "Log loss for 1 is 0.048320706932503385\n",
            "Epoch: [3][0/306] Elapsed 0m 0s (remain 3m 1s) Loss: 0.0562 \n",
            "Epoch: [3][100/306] Elapsed 0m 34s (remain 1m 9s) Loss: 0.0267 \n",
            "Epoch: [3][200/306] Elapsed 1m 7s (remain 0m 35s) Loss: 0.0251 \n",
            "Epoch: [3][300/306] Elapsed 1m 40s (remain 0m 1s) Loss: 0.0302 \n",
            "Epoch: [3][305/306] Elapsed 1m 42s (remain 0m 0s) Loss: 0.0298 \n",
            "EVAL: [0/77] Elapsed 0m 0s (remain 0m 28s) Loss: 0.1560 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3 - avg_train_loss: 0.0298  avg_val_loss: 0.0702  time: 111s\n",
            "Epoch 3 - avg_train_loss: 0.0298  avg_val_loss: 0.0702  time: 111s\n",
            "Epoch 3 - avg_train_loss: 0.0298  avg_val_loss: 0.0702  time: 111s\n",
            "Epoch 3 - avg_train_loss: 0.0298  avg_val_loss: 0.0702  time: 111s\n",
            "Epoch 3 - avg_train_loss: 0.0298  avg_val_loss: 0.0702  time: 111s\n",
            "Epoch 3 - avg_train_loss: 0.0298  avg_val_loss: 0.0702  time: 111s\n",
            "Epoch 3 - avg_train_loss: 0.0298  avg_val_loss: 0.0702  time: 111s\n",
            "Epoch 3 - avg_train_loss: 0.0298  avg_val_loss: 0.0702  time: 111s\n",
            "Epoch 3 - avg_train_loss: 0.0298  avg_val_loss: 0.0702  time: 111s\n",
            "Epoch 3 - avg_train_loss: 0.0298  avg_val_loss: 0.0702  time: 111s\n",
            "Epoch 3 - avg_train_loss: 0.0298  avg_val_loss: 0.0702  time: 111s\n",
            "Epoch 3 - Score: 0.07016011484708491\n",
            "Epoch 3 - Score: 0.07016011484708491\n",
            "Epoch 3 - Score: 0.07016011484708491\n",
            "Epoch 3 - Score: 0.07016011484708491\n",
            "Epoch 3 - Score: 0.07016011484708491\n",
            "Epoch 3 - Score: 0.07016011484708491\n",
            "Epoch 3 - Score: 0.07016011484708491\n",
            "Epoch 3 - Score: 0.07016011484708491\n",
            "Epoch 3 - Score: 0.07016011484708491\n",
            "Epoch 3 - Score: 0.07016011484708491\n",
            "Epoch 3 - Score: 0.07016011484708491\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EVAL: [76/77] Elapsed 0m 8s (remain 0m 0s) Loss: 0.0702 \n",
            "Log loss for 2 is 0.07016011484708491\n",
            "Epoch: [4][0/306] Elapsed 0m 0s (remain 3m 6s) Loss: 0.0292 \n",
            "Epoch: [4][100/306] Elapsed 0m 34s (remain 1m 9s) Loss: 0.0218 \n",
            "Epoch: [4][200/306] Elapsed 1m 7s (remain 0m 35s) Loss: 0.0215 \n",
            "Epoch: [4][300/306] Elapsed 1m 40s (remain 0m 1s) Loss: 0.0188 \n",
            "Epoch: [4][305/306] Elapsed 1m 42s (remain 0m 0s) Loss: 0.0185 \n",
            "EVAL: [0/77] Elapsed 0m 0s (remain 0m 26s) Loss: 0.2323 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4 - avg_train_loss: 0.0185  avg_val_loss: 0.0996  time: 111s\n",
            "Epoch 4 - avg_train_loss: 0.0185  avg_val_loss: 0.0996  time: 111s\n",
            "Epoch 4 - avg_train_loss: 0.0185  avg_val_loss: 0.0996  time: 111s\n",
            "Epoch 4 - avg_train_loss: 0.0185  avg_val_loss: 0.0996  time: 111s\n",
            "Epoch 4 - avg_train_loss: 0.0185  avg_val_loss: 0.0996  time: 111s\n",
            "Epoch 4 - avg_train_loss: 0.0185  avg_val_loss: 0.0996  time: 111s\n",
            "Epoch 4 - avg_train_loss: 0.0185  avg_val_loss: 0.0996  time: 111s\n",
            "Epoch 4 - avg_train_loss: 0.0185  avg_val_loss: 0.0996  time: 111s\n",
            "Epoch 4 - avg_train_loss: 0.0185  avg_val_loss: 0.0996  time: 111s\n",
            "Epoch 4 - avg_train_loss: 0.0185  avg_val_loss: 0.0996  time: 111s\n",
            "Epoch 4 - avg_train_loss: 0.0185  avg_val_loss: 0.0996  time: 111s\n",
            "Epoch 4 - Score: 0.09958782091263067\n",
            "Epoch 4 - Score: 0.09958782091263067\n",
            "Epoch 4 - Score: 0.09958782091263067\n",
            "Epoch 4 - Score: 0.09958782091263067\n",
            "Epoch 4 - Score: 0.09958782091263067\n",
            "Epoch 4 - Score: 0.09958782091263067\n",
            "Epoch 4 - Score: 0.09958782091263067\n",
            "Epoch 4 - Score: 0.09958782091263067\n",
            "Epoch 4 - Score: 0.09958782091263067\n",
            "Epoch 4 - Score: 0.09958782091263067\n",
            "Epoch 4 - Score: 0.09958782091263067\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EVAL: [76/77] Elapsed 0m 8s (remain 0m 0s) Loss: 0.0996 \n",
            "Log loss for 3 is 0.09958782091263067\n",
            "Epoch: [5][0/306] Elapsed 0m 0s (remain 3m 13s) Loss: 0.0015 \n",
            "Epoch: [5][100/306] Elapsed 0m 33s (remain 1m 8s) Loss: 0.0069 \n",
            "Epoch: [5][200/306] Elapsed 1m 7s (remain 0m 35s) Loss: 0.0079 \n",
            "Epoch: [5][300/306] Elapsed 1m 40s (remain 0m 1s) Loss: 0.0083 \n",
            "Epoch: [5][305/306] Elapsed 1m 42s (remain 0m 0s) Loss: 0.0082 \n",
            "EVAL: [0/77] Elapsed 0m 0s (remain 0m 26s) Loss: 0.2213 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5 - avg_train_loss: 0.0082  avg_val_loss: 0.0826  time: 111s\n",
            "Epoch 5 - avg_train_loss: 0.0082  avg_val_loss: 0.0826  time: 111s\n",
            "Epoch 5 - avg_train_loss: 0.0082  avg_val_loss: 0.0826  time: 111s\n",
            "Epoch 5 - avg_train_loss: 0.0082  avg_val_loss: 0.0826  time: 111s\n",
            "Epoch 5 - avg_train_loss: 0.0082  avg_val_loss: 0.0826  time: 111s\n",
            "Epoch 5 - avg_train_loss: 0.0082  avg_val_loss: 0.0826  time: 111s\n",
            "Epoch 5 - avg_train_loss: 0.0082  avg_val_loss: 0.0826  time: 111s\n",
            "Epoch 5 - avg_train_loss: 0.0082  avg_val_loss: 0.0826  time: 111s\n",
            "Epoch 5 - avg_train_loss: 0.0082  avg_val_loss: 0.0826  time: 111s\n",
            "Epoch 5 - avg_train_loss: 0.0082  avg_val_loss: 0.0826  time: 111s\n",
            "Epoch 5 - avg_train_loss: 0.0082  avg_val_loss: 0.0826  time: 111s\n",
            "Epoch 5 - Score: 0.08256684128921718\n",
            "Epoch 5 - Score: 0.08256684128921718\n",
            "Epoch 5 - Score: 0.08256684128921718\n",
            "Epoch 5 - Score: 0.08256684128921718\n",
            "Epoch 5 - Score: 0.08256684128921718\n",
            "Epoch 5 - Score: 0.08256684128921718\n",
            "Epoch 5 - Score: 0.08256684128921718\n",
            "Epoch 5 - Score: 0.08256684128921718\n",
            "Epoch 5 - Score: 0.08256684128921718\n",
            "Epoch 5 - Score: 0.08256684128921718\n",
            "Epoch 5 - Score: 0.08256684128921718\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EVAL: [76/77] Elapsed 0m 8s (remain 0m 0s) Loss: 0.0826 \n",
            "Log loss for 4 is 0.08256684128921718\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "========== fold: 3 result ==========\n",
            "========== fold: 3 result ==========\n",
            "========== fold: 3 result ==========\n",
            "========== fold: 3 result ==========\n",
            "========== fold: 3 result ==========\n",
            "========== fold: 3 result ==========\n",
            "========== fold: 3 result ==========\n",
            "========== fold: 3 result ==========\n",
            "========== fold: 3 result ==========\n",
            "========== fold: 3 result ==========\n",
            "========== fold: 3 result ==========\n",
            "Score: 0.04832\n",
            "Score: 0.04832\n",
            "Score: 0.04832\n",
            "Score: 0.04832\n",
            "Score: 0.04832\n",
            "Score: 0.04832\n",
            "Score: 0.04832\n",
            "Score: 0.04832\n",
            "Score: 0.04832\n",
            "Score: 0.04832\n",
            "Score: 0.04832\n",
            "========== fold: 4 training ==========\n",
            "========== fold: 4 training ==========\n",
            "========== fold: 4 training ==========\n",
            "========== fold: 4 training ==========\n",
            "========== fold: 4 training ==========\n",
            "========== fold: 4 training ==========\n",
            "========== fold: 4 training ==========\n",
            "========== fold: 4 training ==========\n",
            "========== fold: 4 training ==========\n",
            "========== fold: 4 training ==========\n",
            "========== fold: 4 training ==========\n",
            "Didn't find file /content/activebus_bert_final/added_tokens.json. We won't load it.\n",
            "loading file /content/activebus_bert_final/vocab.txt\n",
            "loading file None\n",
            "loading file /content/activebus_bert_final/special_tokens_map.json\n",
            "loading file /content/activebus_bert_final/tokenizer_config.json\n",
            "loading file /content/activebus_bert_final/tokenizer.json\n",
            "Didn't find file /content/activebus_bert_final/added_tokens.json. We won't load it.\n",
            "loading file /content/activebus_bert_final/vocab.txt\n",
            "loading file None\n",
            "loading file /content/activebus_bert_final/special_tokens_map.json\n",
            "loading file /content/activebus_bert_final/tokenizer_config.json\n",
            "loading file /content/activebus_bert_final/tokenizer.json\n",
            "loading configuration file /content/activebus_bert_final/config.json\n",
            "Model config BertConfig {\n",
            "  \"_name_or_path\": \"activebus/BERT-XD_Review\",\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_ids\": 0,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.15.0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "loading weights file /content/activebus_bert_final/pytorch_model.bin\n",
            "Some weights of the model checkpoint at /content/activebus_bert_final were not used when initializing BertForSequenceClassification: ['cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /content/activebus_bert_final and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: [1][0/306] Elapsed 0m 0s (remain 3m 7s) Loss: 0.6337 \n",
            "Epoch: [1][100/306] Elapsed 0m 34s (remain 1m 9s) Loss: 0.1598 \n",
            "Epoch: [1][200/306] Elapsed 1m 7s (remain 0m 35s) Loss: 0.1224 \n",
            "Epoch: [1][300/306] Elapsed 1m 40s (remain 0m 1s) Loss: 0.1059 \n",
            "Epoch: [1][305/306] Elapsed 1m 42s (remain 0m 0s) Loss: 0.1047 \n",
            "EVAL: [0/77] Elapsed 0m 0s (remain 0m 27s) Loss: 0.0057 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1 - avg_train_loss: 0.1047  avg_val_loss: 0.0503  time: 111s\n",
            "Epoch 1 - avg_train_loss: 0.1047  avg_val_loss: 0.0503  time: 111s\n",
            "Epoch 1 - avg_train_loss: 0.1047  avg_val_loss: 0.0503  time: 111s\n",
            "Epoch 1 - avg_train_loss: 0.1047  avg_val_loss: 0.0503  time: 111s\n",
            "Epoch 1 - avg_train_loss: 0.1047  avg_val_loss: 0.0503  time: 111s\n",
            "Epoch 1 - avg_train_loss: 0.1047  avg_val_loss: 0.0503  time: 111s\n",
            "Epoch 1 - avg_train_loss: 0.1047  avg_val_loss: 0.0503  time: 111s\n",
            "Epoch 1 - avg_train_loss: 0.1047  avg_val_loss: 0.0503  time: 111s\n",
            "Epoch 1 - avg_train_loss: 0.1047  avg_val_loss: 0.0503  time: 111s\n",
            "Epoch 1 - avg_train_loss: 0.1047  avg_val_loss: 0.0503  time: 111s\n",
            "Epoch 1 - avg_train_loss: 0.1047  avg_val_loss: 0.0503  time: 111s\n",
            "Epoch 1 - Score: 0.05031886965623877\n",
            "Epoch 1 - Score: 0.05031886965623877\n",
            "Epoch 1 - Score: 0.05031886965623877\n",
            "Epoch 1 - Score: 0.05031886965623877\n",
            "Epoch 1 - Score: 0.05031886965623877\n",
            "Epoch 1 - Score: 0.05031886965623877\n",
            "Epoch 1 - Score: 0.05031886965623877\n",
            "Epoch 1 - Score: 0.05031886965623877\n",
            "Epoch 1 - Score: 0.05031886965623877\n",
            "Epoch 1 - Score: 0.05031886965623877\n",
            "Epoch 1 - Score: 0.05031886965623877\n",
            "Epoch 1 - Save Best Score: 0.0503 Model\n",
            "Epoch 1 - Save Best Score: 0.0503 Model\n",
            "Epoch 1 - Save Best Score: 0.0503 Model\n",
            "Epoch 1 - Save Best Score: 0.0503 Model\n",
            "Epoch 1 - Save Best Score: 0.0503 Model\n",
            "Epoch 1 - Save Best Score: 0.0503 Model\n",
            "Epoch 1 - Save Best Score: 0.0503 Model\n",
            "Epoch 1 - Save Best Score: 0.0503 Model\n",
            "Epoch 1 - Save Best Score: 0.0503 Model\n",
            "Epoch 1 - Save Best Score: 0.0503 Model\n",
            "Epoch 1 - Save Best Score: 0.0503 Model\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EVAL: [76/77] Elapsed 0m 8s (remain 0m 0s) Loss: 0.0503 \n",
            "Log loss for 0 is 0.05031886965623877\n",
            "Epoch: [2][0/306] Elapsed 0m 0s (remain 3m 2s) Loss: 0.0090 \n",
            "Epoch: [2][100/306] Elapsed 0m 34s (remain 1m 9s) Loss: 0.0412 \n",
            "Epoch: [2][200/306] Elapsed 1m 7s (remain 0m 35s) Loss: 0.0360 \n",
            "Epoch: [2][300/306] Elapsed 1m 41s (remain 0m 1s) Loss: 0.0450 \n",
            "Epoch: [2][305/306] Elapsed 1m 42s (remain 0m 0s) Loss: 0.0448 \n",
            "EVAL: [0/77] Elapsed 0m 0s (remain 0m 27s) Loss: 0.0013 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2 - avg_train_loss: 0.0448  avg_val_loss: 0.0511  time: 112s\n",
            "Epoch 2 - avg_train_loss: 0.0448  avg_val_loss: 0.0511  time: 112s\n",
            "Epoch 2 - avg_train_loss: 0.0448  avg_val_loss: 0.0511  time: 112s\n",
            "Epoch 2 - avg_train_loss: 0.0448  avg_val_loss: 0.0511  time: 112s\n",
            "Epoch 2 - avg_train_loss: 0.0448  avg_val_loss: 0.0511  time: 112s\n",
            "Epoch 2 - avg_train_loss: 0.0448  avg_val_loss: 0.0511  time: 112s\n",
            "Epoch 2 - avg_train_loss: 0.0448  avg_val_loss: 0.0511  time: 112s\n",
            "Epoch 2 - avg_train_loss: 0.0448  avg_val_loss: 0.0511  time: 112s\n",
            "Epoch 2 - avg_train_loss: 0.0448  avg_val_loss: 0.0511  time: 112s\n",
            "Epoch 2 - avg_train_loss: 0.0448  avg_val_loss: 0.0511  time: 112s\n",
            "Epoch 2 - avg_train_loss: 0.0448  avg_val_loss: 0.0511  time: 112s\n",
            "Epoch 2 - Score: 0.05109527567638723\n",
            "Epoch 2 - Score: 0.05109527567638723\n",
            "Epoch 2 - Score: 0.05109527567638723\n",
            "Epoch 2 - Score: 0.05109527567638723\n",
            "Epoch 2 - Score: 0.05109527567638723\n",
            "Epoch 2 - Score: 0.05109527567638723\n",
            "Epoch 2 - Score: 0.05109527567638723\n",
            "Epoch 2 - Score: 0.05109527567638723\n",
            "Epoch 2 - Score: 0.05109527567638723\n",
            "Epoch 2 - Score: 0.05109527567638723\n",
            "Epoch 2 - Score: 0.05109527567638723\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EVAL: [76/77] Elapsed 0m 8s (remain 0m 0s) Loss: 0.0511 \n",
            "Log loss for 1 is 0.05109527567638723\n",
            "Epoch: [3][0/306] Elapsed 0m 0s (remain 3m 6s) Loss: 0.0022 \n",
            "Epoch: [3][100/306] Elapsed 0m 34s (remain 1m 9s) Loss: 0.0256 \n",
            "Epoch: [3][200/306] Elapsed 1m 7s (remain 0m 35s) Loss: 0.0294 \n",
            "Epoch: [3][300/306] Elapsed 1m 41s (remain 0m 1s) Loss: 0.0338 \n",
            "Epoch: [3][305/306] Elapsed 1m 42s (remain 0m 0s) Loss: 0.0337 \n",
            "EVAL: [0/77] Elapsed 0m 0s (remain 0m 26s) Loss: 0.0033 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3 - avg_train_loss: 0.0337  avg_val_loss: 0.0453  time: 111s\n",
            "Epoch 3 - avg_train_loss: 0.0337  avg_val_loss: 0.0453  time: 111s\n",
            "Epoch 3 - avg_train_loss: 0.0337  avg_val_loss: 0.0453  time: 111s\n",
            "Epoch 3 - avg_train_loss: 0.0337  avg_val_loss: 0.0453  time: 111s\n",
            "Epoch 3 - avg_train_loss: 0.0337  avg_val_loss: 0.0453  time: 111s\n",
            "Epoch 3 - avg_train_loss: 0.0337  avg_val_loss: 0.0453  time: 111s\n",
            "Epoch 3 - avg_train_loss: 0.0337  avg_val_loss: 0.0453  time: 111s\n",
            "Epoch 3 - avg_train_loss: 0.0337  avg_val_loss: 0.0453  time: 111s\n",
            "Epoch 3 - avg_train_loss: 0.0337  avg_val_loss: 0.0453  time: 111s\n",
            "Epoch 3 - avg_train_loss: 0.0337  avg_val_loss: 0.0453  time: 111s\n",
            "Epoch 3 - avg_train_loss: 0.0337  avg_val_loss: 0.0453  time: 111s\n",
            "Epoch 3 - Score: 0.045258198916466534\n",
            "Epoch 3 - Score: 0.045258198916466534\n",
            "Epoch 3 - Score: 0.045258198916466534\n",
            "Epoch 3 - Score: 0.045258198916466534\n",
            "Epoch 3 - Score: 0.045258198916466534\n",
            "Epoch 3 - Score: 0.045258198916466534\n",
            "Epoch 3 - Score: 0.045258198916466534\n",
            "Epoch 3 - Score: 0.045258198916466534\n",
            "Epoch 3 - Score: 0.045258198916466534\n",
            "Epoch 3 - Score: 0.045258198916466534\n",
            "Epoch 3 - Score: 0.045258198916466534\n",
            "Epoch 3 - Save Best Score: 0.0453 Model\n",
            "Epoch 3 - Save Best Score: 0.0453 Model\n",
            "Epoch 3 - Save Best Score: 0.0453 Model\n",
            "Epoch 3 - Save Best Score: 0.0453 Model\n",
            "Epoch 3 - Save Best Score: 0.0453 Model\n",
            "Epoch 3 - Save Best Score: 0.0453 Model\n",
            "Epoch 3 - Save Best Score: 0.0453 Model\n",
            "Epoch 3 - Save Best Score: 0.0453 Model\n",
            "Epoch 3 - Save Best Score: 0.0453 Model\n",
            "Epoch 3 - Save Best Score: 0.0453 Model\n",
            "Epoch 3 - Save Best Score: 0.0453 Model\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EVAL: [76/77] Elapsed 0m 8s (remain 0m 0s) Loss: 0.0453 \n",
            "Log loss for 2 is 0.045258198916466534\n",
            "Epoch: [4][0/306] Elapsed 0m 0s (remain 3m 1s) Loss: 0.0073 \n",
            "Epoch: [4][100/306] Elapsed 0m 34s (remain 1m 9s) Loss: 0.0155 \n",
            "Epoch: [4][200/306] Elapsed 1m 7s (remain 0m 35s) Loss: 0.0163 \n",
            "Epoch: [4][300/306] Elapsed 1m 40s (remain 0m 1s) Loss: 0.0147 \n",
            "Epoch: [4][305/306] Elapsed 1m 42s (remain 0m 0s) Loss: 0.0146 \n",
            "EVAL: [0/77] Elapsed 0m 0s (remain 0m 28s) Loss: 0.0005 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4 - avg_train_loss: 0.0146  avg_val_loss: 0.0630  time: 111s\n",
            "Epoch 4 - avg_train_loss: 0.0146  avg_val_loss: 0.0630  time: 111s\n",
            "Epoch 4 - avg_train_loss: 0.0146  avg_val_loss: 0.0630  time: 111s\n",
            "Epoch 4 - avg_train_loss: 0.0146  avg_val_loss: 0.0630  time: 111s\n",
            "Epoch 4 - avg_train_loss: 0.0146  avg_val_loss: 0.0630  time: 111s\n",
            "Epoch 4 - avg_train_loss: 0.0146  avg_val_loss: 0.0630  time: 111s\n",
            "Epoch 4 - avg_train_loss: 0.0146  avg_val_loss: 0.0630  time: 111s\n",
            "Epoch 4 - avg_train_loss: 0.0146  avg_val_loss: 0.0630  time: 111s\n",
            "Epoch 4 - avg_train_loss: 0.0146  avg_val_loss: 0.0630  time: 111s\n",
            "Epoch 4 - avg_train_loss: 0.0146  avg_val_loss: 0.0630  time: 111s\n",
            "Epoch 4 - avg_train_loss: 0.0146  avg_val_loss: 0.0630  time: 111s\n",
            "Epoch 4 - Score: 0.063006604672291\n",
            "Epoch 4 - Score: 0.063006604672291\n",
            "Epoch 4 - Score: 0.063006604672291\n",
            "Epoch 4 - Score: 0.063006604672291\n",
            "Epoch 4 - Score: 0.063006604672291\n",
            "Epoch 4 - Score: 0.063006604672291\n",
            "Epoch 4 - Score: 0.063006604672291\n",
            "Epoch 4 - Score: 0.063006604672291\n",
            "Epoch 4 - Score: 0.063006604672291\n",
            "Epoch 4 - Score: 0.063006604672291\n",
            "Epoch 4 - Score: 0.063006604672291\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EVAL: [76/77] Elapsed 0m 8s (remain 0m 0s) Loss: 0.0630 \n",
            "Log loss for 3 is 0.063006604672291\n",
            "Epoch: [5][0/306] Elapsed 0m 0s (remain 3m 10s) Loss: 0.0007 \n",
            "Epoch: [5][100/306] Elapsed 0m 34s (remain 1m 9s) Loss: 0.0127 \n",
            "Epoch: [5][200/306] Elapsed 1m 7s (remain 0m 35s) Loss: 0.0104 \n",
            "Epoch: [5][300/306] Elapsed 1m 40s (remain 0m 1s) Loss: 0.0112 \n",
            "Epoch: [5][305/306] Elapsed 1m 42s (remain 0m 0s) Loss: 0.0110 \n",
            "EVAL: [0/77] Elapsed 0m 0s (remain 0m 26s) Loss: 0.0009 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5 - avg_train_loss: 0.0110  avg_val_loss: 0.0768  time: 111s\n",
            "Epoch 5 - avg_train_loss: 0.0110  avg_val_loss: 0.0768  time: 111s\n",
            "Epoch 5 - avg_train_loss: 0.0110  avg_val_loss: 0.0768  time: 111s\n",
            "Epoch 5 - avg_train_loss: 0.0110  avg_val_loss: 0.0768  time: 111s\n",
            "Epoch 5 - avg_train_loss: 0.0110  avg_val_loss: 0.0768  time: 111s\n",
            "Epoch 5 - avg_train_loss: 0.0110  avg_val_loss: 0.0768  time: 111s\n",
            "Epoch 5 - avg_train_loss: 0.0110  avg_val_loss: 0.0768  time: 111s\n",
            "Epoch 5 - avg_train_loss: 0.0110  avg_val_loss: 0.0768  time: 111s\n",
            "Epoch 5 - avg_train_loss: 0.0110  avg_val_loss: 0.0768  time: 111s\n",
            "Epoch 5 - avg_train_loss: 0.0110  avg_val_loss: 0.0768  time: 111s\n",
            "Epoch 5 - avg_train_loss: 0.0110  avg_val_loss: 0.0768  time: 111s\n",
            "Epoch 5 - Score: 0.07679434871391569\n",
            "Epoch 5 - Score: 0.07679434871391569\n",
            "Epoch 5 - Score: 0.07679434871391569\n",
            "Epoch 5 - Score: 0.07679434871391569\n",
            "Epoch 5 - Score: 0.07679434871391569\n",
            "Epoch 5 - Score: 0.07679434871391569\n",
            "Epoch 5 - Score: 0.07679434871391569\n",
            "Epoch 5 - Score: 0.07679434871391569\n",
            "Epoch 5 - Score: 0.07679434871391569\n",
            "Epoch 5 - Score: 0.07679434871391569\n",
            "Epoch 5 - Score: 0.07679434871391569\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EVAL: [76/77] Elapsed 0m 8s (remain 0m 0s) Loss: 0.0768 \n",
            "Log loss for 4 is 0.07679434871391569\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "========== fold: 4 result ==========\n",
            "========== fold: 4 result ==========\n",
            "========== fold: 4 result ==========\n",
            "========== fold: 4 result ==========\n",
            "========== fold: 4 result ==========\n",
            "========== fold: 4 result ==========\n",
            "========== fold: 4 result ==========\n",
            "========== fold: 4 result ==========\n",
            "========== fold: 4 result ==========\n",
            "========== fold: 4 result ==========\n",
            "========== fold: 4 result ==========\n",
            "Score: 0.04526\n",
            "Score: 0.04526\n",
            "Score: 0.04526\n",
            "Score: 0.04526\n",
            "Score: 0.04526\n",
            "Score: 0.04526\n",
            "Score: 0.04526\n",
            "Score: 0.04526\n",
            "Score: 0.04526\n",
            "Score: 0.04526\n",
            "Score: 0.04526\n",
            "========== CV ==========\n",
            "========== CV ==========\n",
            "========== CV ==========\n",
            "========== CV ==========\n",
            "========== CV ==========\n",
            "========== CV ==========\n",
            "========== CV ==========\n",
            "========== CV ==========\n",
            "========== CV ==========\n",
            "========== CV ==========\n",
            "========== CV ==========\n",
            "Score: 0.04396\n",
            "Score: 0.04396\n",
            "Score: 0.04396\n",
            "Score: 0.04396\n",
            "Score: 0.04396\n",
            "Score: 0.04396\n",
            "Score: 0.04396\n",
            "Score: 0.04396\n",
            "Score: 0.04396\n",
            "Score: 0.04396\n",
            "Score: 0.04396\n",
            "Didn't find file /content/activebus_bert_final/added_tokens.json. We won't load it.\n",
            "loading file /content/activebus_bert_final/vocab.txt\n",
            "loading file None\n",
            "loading file /content/activebus_bert_final/special_tokens_map.json\n",
            "loading file /content/activebus_bert_final/tokenizer_config.json\n",
            "loading file /content/activebus_bert_final/tokenizer.json\n",
            "========== model: bert-base-uncased fold: 0 inference ==========\n",
            "========== model: bert-base-uncased fold: 0 inference ==========\n",
            "========== model: bert-base-uncased fold: 0 inference ==========\n",
            "========== model: bert-base-uncased fold: 0 inference ==========\n",
            "========== model: bert-base-uncased fold: 0 inference ==========\n",
            "========== model: bert-base-uncased fold: 0 inference ==========\n",
            "========== model: bert-base-uncased fold: 0 inference ==========\n",
            "========== model: bert-base-uncased fold: 0 inference ==========\n",
            "========== model: bert-base-uncased fold: 0 inference ==========\n",
            "========== model: bert-base-uncased fold: 0 inference ==========\n",
            "========== model: bert-base-uncased fold: 0 inference ==========\n",
            "loading configuration file /content/activebus_bert_final/config.json\n",
            "Model config BertConfig {\n",
            "  \"_name_or_path\": \"activebus/BERT-XD_Review\",\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_ids\": 0,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.15.0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "loading weights file /content/activebus_bert_final/pytorch_model.bin\n",
            "Some weights of the model checkpoint at /content/activebus_bert_final were not used when initializing BertForSequenceClassification: ['cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /content/activebus_bert_final and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "\n",
            "  0%|          | 0/165 [00:00<?, ?it/s]\u001b[A\n",
            "  1%|          | 1/165 [00:00<00:21,  7.64it/s]\u001b[A\n",
            "  1%|          | 2/165 [00:00<00:20,  8.02it/s]\u001b[A\n",
            "  2%|▏         | 3/165 [00:00<00:19,  8.27it/s]\u001b[A\n",
            "  2%|▏         | 4/165 [00:00<00:19,  8.46it/s]\u001b[A\n",
            "  3%|▎         | 5/165 [00:00<00:18,  8.53it/s]\u001b[A\n",
            "  4%|▎         | 6/165 [00:00<00:18,  8.63it/s]\u001b[A\n",
            "  4%|▍         | 7/165 [00:00<00:18,  8.70it/s]\u001b[A\n",
            "  5%|▍         | 8/165 [00:00<00:17,  8.75it/s]\u001b[A\n",
            "  5%|▌         | 9/165 [00:01<00:17,  8.79it/s]\u001b[A\n",
            "  6%|▌         | 10/165 [00:01<00:17,  8.83it/s]\u001b[A\n",
            "  7%|▋         | 11/165 [00:01<00:17,  8.84it/s]\u001b[A\n",
            "  7%|▋         | 12/165 [00:01<00:17,  8.86it/s]\u001b[A\n",
            "  8%|▊         | 13/165 [00:01<00:17,  8.88it/s]\u001b[A\n",
            "  8%|▊         | 14/165 [00:01<00:17,  8.88it/s]\u001b[A\n",
            "  9%|▉         | 15/165 [00:01<00:16,  8.87it/s]\u001b[A\n",
            " 10%|▉         | 16/165 [00:01<00:16,  8.87it/s]\u001b[A\n",
            " 10%|█         | 17/165 [00:01<00:16,  8.83it/s]\u001b[A\n",
            " 11%|█         | 18/165 [00:02<00:16,  8.77it/s]\u001b[A\n",
            " 12%|█▏        | 19/165 [00:02<00:16,  8.78it/s]\u001b[A\n",
            " 12%|█▏        | 20/165 [00:02<00:16,  8.78it/s]\u001b[A\n",
            " 13%|█▎        | 21/165 [00:02<00:16,  8.81it/s]\u001b[A\n",
            " 13%|█▎        | 22/165 [00:02<00:16,  8.79it/s]\u001b[A\n",
            " 14%|█▍        | 23/165 [00:02<00:16,  8.83it/s]\u001b[A\n",
            " 15%|█▍        | 24/165 [00:02<00:15,  8.86it/s]\u001b[A\n",
            " 15%|█▌        | 25/165 [00:02<00:15,  8.86it/s]\u001b[A\n",
            " 16%|█▌        | 26/165 [00:02<00:15,  8.85it/s]\u001b[A\n",
            " 16%|█▋        | 27/165 [00:03<00:15,  8.85it/s]\u001b[A\n",
            " 17%|█▋        | 28/165 [00:03<00:15,  8.85it/s]\u001b[A\n",
            " 18%|█▊        | 29/165 [00:03<00:15,  8.87it/s]\u001b[A\n",
            " 18%|█▊        | 30/165 [00:03<00:15,  8.85it/s]\u001b[A\n",
            " 19%|█▉        | 31/165 [00:03<00:15,  8.85it/s]\u001b[A\n",
            " 19%|█▉        | 32/165 [00:03<00:15,  8.82it/s]\u001b[A\n",
            " 20%|██        | 33/165 [00:03<00:14,  8.81it/s]\u001b[A\n",
            " 21%|██        | 34/165 [00:03<00:14,  8.85it/s]\u001b[A\n",
            " 21%|██        | 35/165 [00:03<00:14,  8.81it/s]\u001b[A\n",
            " 22%|██▏       | 36/165 [00:04<00:14,  8.83it/s]\u001b[A\n",
            " 22%|██▏       | 37/165 [00:04<00:14,  8.85it/s]\u001b[A\n",
            " 23%|██▎       | 38/165 [00:04<00:14,  8.87it/s]\u001b[A\n",
            " 24%|██▎       | 39/165 [00:04<00:14,  8.87it/s]\u001b[A\n",
            " 24%|██▍       | 40/165 [00:04<00:14,  8.88it/s]\u001b[A\n",
            " 25%|██▍       | 41/165 [00:04<00:13,  8.90it/s]\u001b[A\n",
            " 25%|██▌       | 42/165 [00:04<00:13,  8.88it/s]\u001b[A\n",
            " 26%|██▌       | 43/165 [00:04<00:13,  8.87it/s]\u001b[A\n",
            " 27%|██▋       | 44/165 [00:05<00:13,  8.84it/s]\u001b[A\n",
            " 27%|██▋       | 45/165 [00:05<00:13,  8.83it/s]\u001b[A\n",
            " 28%|██▊       | 46/165 [00:05<00:13,  8.82it/s]\u001b[A\n",
            " 28%|██▊       | 47/165 [00:05<00:13,  8.83it/s]\u001b[A\n",
            " 29%|██▉       | 48/165 [00:05<00:13,  8.85it/s]\u001b[A\n",
            " 30%|██▉       | 49/165 [00:05<00:13,  8.87it/s]\u001b[A\n",
            " 30%|███       | 50/165 [00:05<00:12,  8.88it/s]\u001b[A\n",
            " 31%|███       | 51/165 [00:05<00:12,  8.87it/s]\u001b[A\n",
            " 32%|███▏      | 52/165 [00:05<00:12,  8.90it/s]\u001b[A\n",
            " 32%|███▏      | 53/165 [00:06<00:12,  8.88it/s]\u001b[A\n",
            " 33%|███▎      | 54/165 [00:06<00:12,  8.85it/s]\u001b[A\n",
            " 33%|███▎      | 55/165 [00:06<00:12,  8.88it/s]\u001b[A\n",
            " 34%|███▍      | 56/165 [00:06<00:12,  8.85it/s]\u001b[A\n",
            " 35%|███▍      | 57/165 [00:06<00:12,  8.86it/s]\u001b[A\n",
            " 35%|███▌      | 58/165 [00:06<00:12,  8.85it/s]\u001b[A\n",
            " 36%|███▌      | 59/165 [00:06<00:11,  8.86it/s]\u001b[A\n",
            " 36%|███▋      | 60/165 [00:06<00:11,  8.88it/s]\u001b[A\n",
            " 37%|███▋      | 61/165 [00:06<00:11,  8.88it/s]\u001b[A\n",
            " 38%|███▊      | 62/165 [00:07<00:11,  8.88it/s]\u001b[A\n",
            " 38%|███▊      | 63/165 [00:07<00:11,  8.87it/s]\u001b[A\n",
            " 39%|███▉      | 64/165 [00:07<00:11,  8.88it/s]\u001b[A\n",
            " 39%|███▉      | 65/165 [00:07<00:11,  8.89it/s]\u001b[A\n",
            " 40%|████      | 66/165 [00:07<00:11,  8.90it/s]\u001b[A\n",
            " 41%|████      | 67/165 [00:07<00:11,  8.90it/s]\u001b[A\n",
            " 41%|████      | 68/165 [00:07<00:10,  8.90it/s]\u001b[A\n",
            " 42%|████▏     | 69/165 [00:07<00:10,  8.87it/s]\u001b[A\n",
            " 42%|████▏     | 70/165 [00:07<00:10,  8.88it/s]\u001b[A\n",
            " 43%|████▎     | 71/165 [00:08<00:10,  8.88it/s]\u001b[A\n",
            " 44%|████▎     | 72/165 [00:08<00:10,  8.88it/s]\u001b[A\n",
            " 44%|████▍     | 73/165 [00:08<00:10,  8.90it/s]\u001b[A\n",
            " 45%|████▍     | 74/165 [00:08<00:10,  8.87it/s]\u001b[A\n",
            " 45%|████▌     | 75/165 [00:08<00:10,  8.86it/s]\u001b[A\n",
            " 46%|████▌     | 76/165 [00:08<00:10,  8.89it/s]\u001b[A\n",
            " 47%|████▋     | 77/165 [00:08<00:09,  8.90it/s]\u001b[A\n",
            " 47%|████▋     | 78/165 [00:08<00:09,  8.89it/s]\u001b[A\n",
            " 48%|████▊     | 79/165 [00:08<00:09,  8.84it/s]\u001b[A\n",
            " 48%|████▊     | 80/165 [00:09<00:09,  8.84it/s]\u001b[A\n",
            " 49%|████▉     | 81/165 [00:09<00:09,  8.81it/s]\u001b[A\n",
            " 50%|████▉     | 82/165 [00:09<00:09,  8.82it/s]\u001b[A\n",
            " 50%|█████     | 83/165 [00:09<00:09,  8.84it/s]\u001b[A\n",
            " 51%|█████     | 84/165 [00:09<00:09,  8.86it/s]\u001b[A\n",
            " 52%|█████▏    | 85/165 [00:09<00:09,  8.86it/s]\u001b[A\n",
            " 52%|█████▏    | 86/165 [00:09<00:08,  8.88it/s]\u001b[A\n",
            " 53%|█████▎    | 87/165 [00:09<00:08,  8.87it/s]\u001b[A\n",
            " 53%|█████▎    | 88/165 [00:09<00:08,  8.85it/s]\u001b[A\n",
            " 54%|█████▍    | 89/165 [00:10<00:08,  8.86it/s]\u001b[A\n",
            " 55%|█████▍    | 90/165 [00:10<00:08,  8.86it/s]\u001b[A\n",
            " 55%|█████▌    | 91/165 [00:10<00:08,  8.82it/s]\u001b[A\n",
            " 56%|█████▌    | 92/165 [00:10<00:08,  8.85it/s]\u001b[A\n",
            " 56%|█████▋    | 93/165 [00:10<00:08,  8.85it/s]\u001b[A\n",
            " 57%|█████▋    | 94/165 [00:10<00:07,  8.88it/s]\u001b[A\n",
            " 58%|█████▊    | 95/165 [00:10<00:07,  8.90it/s]\u001b[A\n",
            " 58%|█████▊    | 96/165 [00:10<00:07,  8.87it/s]\u001b[A\n",
            " 59%|█████▉    | 97/165 [00:10<00:07,  8.85it/s]\u001b[A\n",
            " 59%|█████▉    | 98/165 [00:11<00:07,  8.85it/s]\u001b[A\n",
            " 60%|██████    | 99/165 [00:11<00:07,  8.87it/s]\u001b[A\n",
            " 61%|██████    | 100/165 [00:11<00:07,  8.87it/s]\u001b[A\n",
            " 61%|██████    | 101/165 [00:11<00:07,  8.89it/s]\u001b[A\n",
            " 62%|██████▏   | 102/165 [00:11<00:07,  8.89it/s]\u001b[A\n",
            " 62%|██████▏   | 103/165 [00:11<00:06,  8.88it/s]\u001b[A\n",
            " 63%|██████▎   | 104/165 [00:11<00:06,  8.87it/s]\u001b[A\n",
            " 64%|██████▎   | 105/165 [00:11<00:06,  8.87it/s]\u001b[A\n",
            " 64%|██████▍   | 106/165 [00:11<00:06,  8.88it/s]\u001b[A\n",
            " 65%|██████▍   | 107/165 [00:12<00:06,  8.89it/s]\u001b[A\n",
            " 65%|██████▌   | 108/165 [00:12<00:06,  8.86it/s]\u001b[A\n",
            " 66%|██████▌   | 109/165 [00:12<00:06,  8.87it/s]\u001b[A\n",
            " 67%|██████▋   | 110/165 [00:12<00:06,  8.86it/s]\u001b[A\n",
            " 67%|██████▋   | 111/165 [00:12<00:06,  8.87it/s]\u001b[A\n",
            " 68%|██████▊   | 112/165 [00:12<00:05,  8.88it/s]\u001b[A\n",
            " 68%|██████▊   | 113/165 [00:12<00:05,  8.87it/s]\u001b[A\n",
            " 69%|██████▉   | 114/165 [00:12<00:05,  8.88it/s]\u001b[A\n",
            " 70%|██████▉   | 115/165 [00:13<00:05,  8.88it/s]\u001b[A\n",
            " 70%|███████   | 116/165 [00:13<00:05,  8.85it/s]\u001b[A\n",
            " 71%|███████   | 117/165 [00:13<00:05,  8.84it/s]\u001b[A\n",
            " 72%|███████▏  | 118/165 [00:13<00:05,  8.84it/s]\u001b[A\n",
            " 72%|███████▏  | 119/165 [00:13<00:05,  8.84it/s]\u001b[A\n",
            " 73%|███████▎  | 120/165 [00:13<00:05,  8.84it/s]\u001b[A\n",
            " 73%|███████▎  | 121/165 [00:13<00:04,  8.85it/s]\u001b[A\n",
            " 74%|███████▍  | 122/165 [00:13<00:04,  8.87it/s]\u001b[A\n",
            " 75%|███████▍  | 123/165 [00:13<00:04,  8.88it/s]\u001b[A\n",
            " 75%|███████▌  | 124/165 [00:14<00:04,  8.86it/s]\u001b[A\n",
            " 76%|███████▌  | 125/165 [00:14<00:04,  8.87it/s]\u001b[A\n",
            " 76%|███████▋  | 126/165 [00:14<00:04,  8.88it/s]\u001b[A\n",
            " 77%|███████▋  | 127/165 [00:14<00:04,  8.87it/s]\u001b[A\n",
            " 78%|███████▊  | 128/165 [00:14<00:04,  8.87it/s]\u001b[A\n",
            " 78%|███████▊  | 129/165 [00:14<00:04,  8.84it/s]\u001b[A\n",
            " 79%|███████▉  | 130/165 [00:14<00:03,  8.85it/s]\u001b[A\n",
            " 79%|███████▉  | 131/165 [00:14<00:03,  8.84it/s]\u001b[A\n",
            " 80%|████████  | 132/165 [00:14<00:03,  8.84it/s]\u001b[A\n",
            " 81%|████████  | 133/165 [00:15<00:03,  8.83it/s]\u001b[A\n",
            " 81%|████████  | 134/165 [00:15<00:03,  8.79it/s]\u001b[A\n",
            " 82%|████████▏ | 135/165 [00:15<00:03,  8.81it/s]\u001b[A\n",
            " 82%|████████▏ | 136/165 [00:15<00:03,  8.85it/s]\u001b[A\n",
            " 83%|████████▎ | 137/165 [00:15<00:03,  8.86it/s]\u001b[A\n",
            " 84%|████████▎ | 138/165 [00:15<00:03,  8.85it/s]\u001b[A\n",
            " 84%|████████▍ | 139/165 [00:15<00:02,  8.87it/s]\u001b[A\n",
            " 85%|████████▍ | 140/165 [00:15<00:02,  8.88it/s]\u001b[A\n",
            " 85%|████████▌ | 141/165 [00:15<00:02,  8.88it/s]\u001b[A\n",
            " 86%|████████▌ | 142/165 [00:16<00:02,  8.89it/s]\u001b[A\n",
            " 87%|████████▋ | 143/165 [00:16<00:02,  8.90it/s]\u001b[A\n",
            " 87%|████████▋ | 144/165 [00:16<00:02,  8.89it/s]\u001b[A\n",
            " 88%|████████▊ | 145/165 [00:16<00:02,  8.87it/s]\u001b[A\n",
            " 88%|████████▊ | 146/165 [00:16<00:02,  8.89it/s]\u001b[A\n",
            " 89%|████████▉ | 147/165 [00:16<00:02,  8.90it/s]\u001b[A\n",
            " 90%|████████▉ | 148/165 [00:16<00:01,  8.90it/s]\u001b[A\n",
            " 90%|█████████ | 149/165 [00:16<00:01,  8.89it/s]\u001b[A\n",
            " 91%|█████████ | 150/165 [00:16<00:01,  8.85it/s]\u001b[A\n",
            " 92%|█████████▏| 151/165 [00:17<00:01,  8.83it/s]\u001b[A\n",
            " 92%|█████████▏| 152/165 [00:17<00:01,  8.81it/s]\u001b[A\n",
            " 93%|█████████▎| 153/165 [00:17<00:01,  8.83it/s]\u001b[A\n",
            " 93%|█████████▎| 154/165 [00:17<00:01,  8.84it/s]\u001b[A\n",
            " 94%|█████████▍| 155/165 [00:17<00:01,  8.85it/s]\u001b[A\n",
            " 95%|█████████▍| 156/165 [00:17<00:01,  8.87it/s]\u001b[A\n",
            " 95%|█████████▌| 157/165 [00:17<00:00,  8.87it/s]\u001b[A\n",
            " 96%|█████████▌| 158/165 [00:17<00:00,  8.87it/s]\u001b[A\n",
            " 96%|█████████▋| 159/165 [00:17<00:00,  8.88it/s]\u001b[A\n",
            " 97%|█████████▋| 160/165 [00:18<00:00,  8.90it/s]\u001b[A\n",
            " 98%|█████████▊| 161/165 [00:18<00:00,  8.89it/s]\u001b[A\n",
            " 98%|█████████▊| 162/165 [00:18<00:00,  8.85it/s]\u001b[A\n",
            " 99%|█████████▉| 163/165 [00:18<00:00,  8.83it/s]\u001b[A\n",
            "100%|██████████| 165/165 [00:18<00:00,  8.83it/s]\n",
            "========== model: bert-base-uncased fold: 1 inference ==========\n",
            "========== model: bert-base-uncased fold: 1 inference ==========\n",
            "========== model: bert-base-uncased fold: 1 inference ==========\n",
            "========== model: bert-base-uncased fold: 1 inference ==========\n",
            "========== model: bert-base-uncased fold: 1 inference ==========\n",
            "========== model: bert-base-uncased fold: 1 inference ==========\n",
            "========== model: bert-base-uncased fold: 1 inference ==========\n",
            "========== model: bert-base-uncased fold: 1 inference ==========\n",
            "========== model: bert-base-uncased fold: 1 inference ==========\n",
            "========== model: bert-base-uncased fold: 1 inference ==========\n",
            "========== model: bert-base-uncased fold: 1 inference ==========\n",
            "loading configuration file /content/activebus_bert_final/config.json\n",
            "Model config BertConfig {\n",
            "  \"_name_or_path\": \"activebus/BERT-XD_Review\",\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_ids\": 0,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.15.0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "loading weights file /content/activebus_bert_final/pytorch_model.bin\n",
            "Some weights of the model checkpoint at /content/activebus_bert_final were not used when initializing BertForSequenceClassification: ['cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /content/activebus_bert_final and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "\n",
            "  0%|          | 0/165 [00:00<?, ?it/s]\u001b[A\n",
            "  1%|          | 1/165 [00:00<00:19,  8.54it/s]\u001b[A\n",
            "  1%|          | 2/165 [00:00<00:18,  8.68it/s]\u001b[A\n",
            "  2%|▏         | 3/165 [00:00<00:18,  8.75it/s]\u001b[A\n",
            "  2%|▏         | 4/165 [00:00<00:18,  8.79it/s]\u001b[A\n",
            "  3%|▎         | 5/165 [00:00<00:18,  8.83it/s]\u001b[A\n",
            "  4%|▎         | 6/165 [00:00<00:17,  8.84it/s]\u001b[A\n",
            "  4%|▍         | 7/165 [00:00<00:17,  8.84it/s]\u001b[A\n",
            "  5%|▍         | 8/165 [00:00<00:17,  8.85it/s]\u001b[A\n",
            "  5%|▌         | 9/165 [00:01<00:17,  8.85it/s]\u001b[A\n",
            "  6%|▌         | 10/165 [00:01<00:17,  8.88it/s]\u001b[A\n",
            "  7%|▋         | 11/165 [00:01<00:17,  8.85it/s]\u001b[A\n",
            "  7%|▋         | 12/165 [00:01<00:17,  8.85it/s]\u001b[A\n",
            "  8%|▊         | 13/165 [00:01<00:17,  8.85it/s]\u001b[A\n",
            "  8%|▊         | 14/165 [00:01<00:17,  8.86it/s]\u001b[A\n",
            "  9%|▉         | 15/165 [00:01<00:16,  8.85it/s]\u001b[A\n",
            " 10%|▉         | 16/165 [00:01<00:16,  8.85it/s]\u001b[A\n",
            " 10%|█         | 17/165 [00:01<00:16,  8.85it/s]\u001b[A\n",
            " 11%|█         | 18/165 [00:02<00:16,  8.82it/s]\u001b[A\n",
            " 12%|█▏        | 19/165 [00:02<00:16,  8.84it/s]\u001b[A\n",
            " 12%|█▏        | 20/165 [00:02<00:16,  8.86it/s]\u001b[A\n",
            " 13%|█▎        | 21/165 [00:02<00:16,  8.86it/s]\u001b[A\n",
            " 13%|█▎        | 22/165 [00:02<00:16,  8.87it/s]\u001b[A\n",
            " 14%|█▍        | 23/165 [00:02<00:15,  8.89it/s]\u001b[A\n",
            " 15%|█▍        | 24/165 [00:02<00:15,  8.86it/s]\u001b[A\n",
            " 15%|█▌        | 25/165 [00:02<00:20,  6.88it/s]\u001b[A\n",
            " 16%|█▌        | 26/165 [00:03<00:18,  7.36it/s]\u001b[A\n",
            " 16%|█▋        | 27/165 [00:03<00:17,  7.75it/s]\u001b[A\n",
            " 17%|█▋        | 28/165 [00:03<00:17,  8.05it/s]\u001b[A\n",
            " 18%|█▊        | 29/165 [00:03<00:16,  8.28it/s]\u001b[A\n",
            " 18%|█▊        | 30/165 [00:03<00:15,  8.46it/s]\u001b[A\n",
            " 19%|█▉        | 31/165 [00:03<00:15,  8.52it/s]\u001b[A\n",
            " 19%|█▉        | 32/165 [00:03<00:15,  8.60it/s]\u001b[A\n",
            " 20%|██        | 33/165 [00:03<00:15,  8.67it/s]\u001b[A\n",
            " 21%|██        | 34/165 [00:03<00:15,  8.72it/s]\u001b[A\n",
            " 21%|██        | 35/165 [00:04<00:14,  8.77it/s]\u001b[A\n",
            " 22%|██▏       | 36/165 [00:04<00:14,  8.80it/s]\u001b[A\n",
            " 22%|██▏       | 37/165 [00:04<00:14,  8.83it/s]\u001b[A\n",
            " 23%|██▎       | 38/165 [00:04<00:14,  8.85it/s]\u001b[A\n",
            " 24%|██▎       | 39/165 [00:04<00:14,  8.86it/s]\u001b[A\n",
            " 24%|██▍       | 40/165 [00:04<00:14,  8.87it/s]\u001b[A\n",
            " 25%|██▍       | 41/165 [00:04<00:14,  8.83it/s]\u001b[A\n",
            " 25%|██▌       | 42/165 [00:04<00:13,  8.85it/s]\u001b[A\n",
            " 26%|██▌       | 43/165 [00:04<00:13,  8.85it/s]\u001b[A\n",
            " 27%|██▋       | 44/165 [00:05<00:13,  8.87it/s]\u001b[A\n",
            " 27%|██▋       | 45/165 [00:05<00:13,  8.89it/s]\u001b[A\n",
            " 28%|██▊       | 46/165 [00:05<00:13,  8.86it/s]\u001b[A\n",
            " 28%|██▊       | 47/165 [00:05<00:13,  8.88it/s]\u001b[A\n",
            " 29%|██▉       | 48/165 [00:05<00:13,  8.87it/s]\u001b[A\n",
            " 30%|██▉       | 49/165 [00:05<00:13,  8.86it/s]\u001b[A\n",
            " 30%|███       | 50/165 [00:05<00:12,  8.86it/s]\u001b[A\n",
            " 31%|███       | 51/165 [00:05<00:12,  8.87it/s]\u001b[A\n",
            " 32%|███▏      | 52/165 [00:05<00:12,  8.87it/s]\u001b[A\n",
            " 32%|███▏      | 53/165 [00:06<00:12,  8.86it/s]\u001b[A\n",
            " 33%|███▎      | 54/165 [00:06<00:12,  8.82it/s]\u001b[A\n",
            " 33%|███▎      | 55/165 [00:06<00:12,  8.79it/s]\u001b[A\n",
            " 34%|███▍      | 56/165 [00:06<00:12,  8.82it/s]\u001b[A\n",
            " 35%|███▍      | 57/165 [00:06<00:12,  8.80it/s]\u001b[A\n",
            " 35%|███▌      | 58/165 [00:06<00:12,  8.82it/s]\u001b[A\n",
            " 36%|███▌      | 59/165 [00:06<00:12,  8.82it/s]\u001b[A\n",
            " 36%|███▋      | 60/165 [00:06<00:11,  8.83it/s]\u001b[A\n",
            " 37%|███▋      | 61/165 [00:07<00:11,  8.83it/s]\u001b[A\n",
            " 38%|███▊      | 62/165 [00:07<00:11,  8.87it/s]\u001b[A\n",
            " 38%|███▊      | 63/165 [00:07<00:11,  8.89it/s]\u001b[A\n",
            " 39%|███▉      | 64/165 [00:07<00:11,  8.90it/s]\u001b[A\n",
            " 39%|███▉      | 65/165 [00:07<00:11,  8.89it/s]\u001b[A\n",
            " 40%|████      | 66/165 [00:07<00:11,  8.90it/s]\u001b[A\n",
            " 41%|████      | 67/165 [00:07<00:11,  8.90it/s]\u001b[A\n",
            " 41%|████      | 68/165 [00:07<00:10,  8.89it/s]\u001b[A\n",
            " 42%|████▏     | 69/165 [00:07<00:10,  8.90it/s]\u001b[A\n",
            " 42%|████▏     | 70/165 [00:08<00:10,  8.88it/s]\u001b[A\n",
            " 43%|████▎     | 71/165 [00:08<00:10,  8.90it/s]\u001b[A\n",
            " 44%|████▎     | 72/165 [00:08<00:10,  8.87it/s]\u001b[A\n",
            " 44%|████▍     | 73/165 [00:08<00:10,  8.89it/s]\u001b[A\n",
            " 45%|████▍     | 74/165 [00:08<00:10,  8.88it/s]\u001b[A\n",
            " 45%|████▌     | 75/165 [00:08<00:10,  8.87it/s]\u001b[A\n",
            " 46%|████▌     | 76/165 [00:08<00:10,  8.88it/s]\u001b[A\n",
            " 47%|████▋     | 77/165 [00:08<00:09,  8.89it/s]\u001b[A\n",
            " 47%|████▋     | 78/165 [00:08<00:09,  8.88it/s]\u001b[A\n",
            " 48%|████▊     | 79/165 [00:09<00:09,  8.86it/s]\u001b[A\n",
            " 48%|████▊     | 80/165 [00:09<00:09,  8.86it/s]\u001b[A\n",
            " 49%|████▉     | 81/165 [00:09<00:09,  8.87it/s]\u001b[A\n",
            " 50%|████▉     | 82/165 [00:09<00:09,  8.87it/s]\u001b[A\n",
            " 50%|█████     | 83/165 [00:09<00:09,  8.87it/s]\u001b[A\n",
            " 51%|█████     | 84/165 [00:09<00:09,  8.84it/s]\u001b[A\n",
            " 52%|█████▏    | 85/165 [00:09<00:09,  8.86it/s]\u001b[A\n",
            " 52%|█████▏    | 86/165 [00:09<00:08,  8.87it/s]\u001b[A\n",
            " 53%|█████▎    | 87/165 [00:09<00:08,  8.85it/s]\u001b[A\n",
            " 53%|█████▎    | 88/165 [00:10<00:08,  8.85it/s]\u001b[A\n",
            " 54%|█████▍    | 89/165 [00:10<00:08,  8.86it/s]\u001b[A\n",
            " 55%|█████▍    | 90/165 [00:10<00:08,  8.88it/s]\u001b[A\n",
            " 55%|█████▌    | 91/165 [00:10<00:08,  8.86it/s]\u001b[A\n",
            " 56%|█████▌    | 92/165 [00:10<00:08,  8.87it/s]\u001b[A\n",
            " 56%|█████▋    | 93/165 [00:10<00:08,  8.88it/s]\u001b[A\n",
            " 57%|█████▋    | 94/165 [00:10<00:07,  8.88it/s]\u001b[A\n",
            " 58%|█████▊    | 95/165 [00:10<00:07,  8.89it/s]\u001b[A\n",
            " 58%|█████▊    | 96/165 [00:10<00:07,  8.84it/s]\u001b[A\n",
            " 59%|█████▉    | 97/165 [00:11<00:07,  8.84it/s]\u001b[A\n",
            " 59%|█████▉    | 98/165 [00:11<00:07,  8.85it/s]\u001b[A\n",
            " 60%|██████    | 99/165 [00:11<00:07,  8.84it/s]\u001b[A\n",
            " 61%|██████    | 100/165 [00:11<00:07,  8.84it/s]\u001b[A\n",
            " 61%|██████    | 101/165 [00:11<00:07,  8.83it/s]\u001b[A\n",
            " 62%|██████▏   | 102/165 [00:11<00:07,  8.84it/s]\u001b[A\n",
            " 62%|██████▏   | 103/165 [00:11<00:07,  8.85it/s]\u001b[A\n",
            " 63%|██████▎   | 104/165 [00:11<00:06,  8.87it/s]\u001b[A\n",
            " 64%|██████▎   | 105/165 [00:11<00:06,  8.87it/s]\u001b[A\n",
            " 64%|██████▍   | 106/165 [00:12<00:06,  8.84it/s]\u001b[A\n",
            " 65%|██████▍   | 107/165 [00:12<00:06,  8.84it/s]\u001b[A\n",
            " 65%|██████▌   | 108/165 [00:12<00:06,  8.84it/s]\u001b[A\n",
            " 66%|██████▌   | 109/165 [00:12<00:06,  8.86it/s]\u001b[A\n",
            " 67%|██████▋   | 110/165 [00:12<00:06,  8.86it/s]\u001b[A\n",
            " 67%|██████▋   | 111/165 [00:12<00:06,  8.84it/s]\u001b[A\n",
            " 68%|██████▊   | 112/165 [00:12<00:05,  8.84it/s]\u001b[A\n",
            " 68%|██████▊   | 113/165 [00:12<00:05,  8.86it/s]\u001b[A\n",
            " 69%|██████▉   | 114/165 [00:12<00:05,  8.86it/s]\u001b[A\n",
            " 70%|██████▉   | 115/165 [00:13<00:05,  8.87it/s]\u001b[A\n",
            " 70%|███████   | 116/165 [00:13<00:05,  8.84it/s]\u001b[A\n",
            " 71%|███████   | 117/165 [00:13<00:05,  8.86it/s]\u001b[A\n",
            " 72%|███████▏  | 118/165 [00:13<00:05,  8.85it/s]\u001b[A\n",
            " 72%|███████▏  | 119/165 [00:13<00:05,  8.87it/s]\u001b[A\n",
            " 73%|███████▎  | 120/165 [00:13<00:05,  8.89it/s]\u001b[A\n",
            " 73%|███████▎  | 121/165 [00:13<00:04,  8.89it/s]\u001b[A\n",
            " 74%|███████▍  | 122/165 [00:13<00:04,  8.89it/s]\u001b[A\n",
            " 75%|███████▍  | 123/165 [00:13<00:04,  8.88it/s]\u001b[A\n",
            " 75%|███████▌  | 124/165 [00:14<00:04,  8.86it/s]\u001b[A\n",
            " 76%|███████▌  | 125/165 [00:14<00:04,  8.85it/s]\u001b[A\n",
            " 76%|███████▋  | 126/165 [00:14<00:04,  8.87it/s]\u001b[A\n",
            " 77%|███████▋  | 127/165 [00:14<00:04,  8.86it/s]\u001b[A\n",
            " 78%|███████▊  | 128/165 [00:14<00:04,  8.88it/s]\u001b[A\n",
            " 78%|███████▊  | 129/165 [00:14<00:04,  8.85it/s]\u001b[A\n",
            " 79%|███████▉  | 130/165 [00:14<00:03,  8.86it/s]\u001b[A\n",
            " 79%|███████▉  | 131/165 [00:14<00:03,  8.85it/s]\u001b[A\n",
            " 80%|████████  | 132/165 [00:15<00:03,  8.81it/s]\u001b[A\n",
            " 81%|████████  | 133/165 [00:15<00:03,  8.82it/s]\u001b[A\n",
            " 81%|████████  | 134/165 [00:15<00:03,  8.82it/s]\u001b[A\n",
            " 82%|████████▏ | 135/165 [00:15<00:03,  8.81it/s]\u001b[A\n",
            " 82%|████████▏ | 136/165 [00:15<00:03,  8.83it/s]\u001b[A\n",
            " 83%|████████▎ | 137/165 [00:15<00:03,  8.85it/s]\u001b[A\n",
            " 84%|████████▎ | 138/165 [00:15<00:03,  8.86it/s]\u001b[A\n",
            " 84%|████████▍ | 139/165 [00:15<00:02,  8.84it/s]\u001b[A\n",
            " 85%|████████▍ | 140/165 [00:15<00:02,  8.86it/s]\u001b[A\n",
            " 85%|████████▌ | 141/165 [00:16<00:02,  8.88it/s]\u001b[A\n",
            " 86%|████████▌ | 142/165 [00:16<00:02,  8.84it/s]\u001b[A\n",
            " 87%|████████▋ | 143/165 [00:16<00:02,  8.87it/s]\u001b[A\n",
            " 87%|████████▋ | 144/165 [00:16<00:02,  8.88it/s]\u001b[A\n",
            " 88%|████████▊ | 145/165 [00:16<00:02,  8.88it/s]\u001b[A\n",
            " 88%|████████▊ | 146/165 [00:16<00:02,  8.86it/s]\u001b[A\n",
            " 89%|████████▉ | 147/165 [00:16<00:02,  8.87it/s]\u001b[A\n",
            " 90%|████████▉ | 148/165 [00:16<00:01,  8.89it/s]\u001b[A\n",
            " 90%|█████████ | 149/165 [00:16<00:01,  8.90it/s]\u001b[A\n",
            " 91%|█████████ | 150/165 [00:17<00:01,  8.89it/s]\u001b[A\n",
            " 92%|█████████▏| 151/165 [00:17<00:01,  8.87it/s]\u001b[A\n",
            " 92%|█████████▏| 152/165 [00:17<00:01,  8.87it/s]\u001b[A\n",
            " 93%|█████████▎| 153/165 [00:17<00:01,  8.84it/s]\u001b[A\n",
            " 93%|█████████▎| 154/165 [00:17<00:01,  8.84it/s]\u001b[A\n",
            " 94%|█████████▍| 155/165 [00:17<00:01,  8.83it/s]\u001b[A\n",
            " 95%|█████████▍| 156/165 [00:17<00:01,  8.81it/s]\u001b[A\n",
            " 95%|█████████▌| 157/165 [00:17<00:00,  8.85it/s]\u001b[A\n",
            " 96%|█████████▌| 158/165 [00:17<00:00,  8.88it/s]\u001b[A\n",
            " 96%|█████████▋| 159/165 [00:18<00:00,  8.87it/s]\u001b[A\n",
            " 97%|█████████▋| 160/165 [00:18<00:00,  8.85it/s]\u001b[A\n",
            " 98%|█████████▊| 161/165 [00:18<00:00,  8.87it/s]\u001b[A\n",
            " 98%|█████████▊| 162/165 [00:18<00:00,  8.85it/s]\u001b[A\n",
            " 99%|█████████▉| 163/165 [00:18<00:00,  8.87it/s]\u001b[A\n",
            "100%|██████████| 165/165 [00:18<00:00,  8.79it/s]\n",
            "========== model: bert-base-uncased fold: 2 inference ==========\n",
            "========== model: bert-base-uncased fold: 2 inference ==========\n",
            "========== model: bert-base-uncased fold: 2 inference ==========\n",
            "========== model: bert-base-uncased fold: 2 inference ==========\n",
            "========== model: bert-base-uncased fold: 2 inference ==========\n",
            "========== model: bert-base-uncased fold: 2 inference ==========\n",
            "========== model: bert-base-uncased fold: 2 inference ==========\n",
            "========== model: bert-base-uncased fold: 2 inference ==========\n",
            "========== model: bert-base-uncased fold: 2 inference ==========\n",
            "========== model: bert-base-uncased fold: 2 inference ==========\n",
            "========== model: bert-base-uncased fold: 2 inference ==========\n",
            "loading configuration file /content/activebus_bert_final/config.json\n",
            "Model config BertConfig {\n",
            "  \"_name_or_path\": \"activebus/BERT-XD_Review\",\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_ids\": 0,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.15.0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "loading weights file /content/activebus_bert_final/pytorch_model.bin\n",
            "Some weights of the model checkpoint at /content/activebus_bert_final were not used when initializing BertForSequenceClassification: ['cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /content/activebus_bert_final and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "\n",
            "  0%|          | 0/165 [00:00<?, ?it/s]\u001b[A\n",
            "  1%|          | 1/165 [00:00<00:19,  8.31it/s]\u001b[A\n",
            "  1%|          | 2/165 [00:00<00:19,  8.57it/s]\u001b[A\n",
            "  2%|▏         | 3/165 [00:00<00:18,  8.73it/s]\u001b[A\n",
            "  2%|▏         | 4/165 [00:00<00:18,  8.79it/s]\u001b[A\n",
            "  3%|▎         | 5/165 [00:00<00:18,  8.81it/s]\u001b[A\n",
            "  4%|▎         | 6/165 [00:00<00:18,  8.83it/s]\u001b[A\n",
            "  4%|▍         | 7/165 [00:00<00:17,  8.83it/s]\u001b[A\n",
            "  5%|▍         | 8/165 [00:00<00:17,  8.84it/s]\u001b[A\n",
            "  5%|▌         | 9/165 [00:01<00:17,  8.86it/s]\u001b[A\n",
            "  6%|▌         | 10/165 [00:01<00:17,  8.87it/s]\u001b[A\n",
            "  7%|▋         | 11/165 [00:01<00:17,  8.86it/s]\u001b[A\n",
            "  7%|▋         | 12/165 [00:01<00:17,  8.86it/s]\u001b[A\n",
            "  8%|▊         | 13/165 [00:01<00:17,  8.88it/s]\u001b[A\n",
            "  8%|▊         | 14/165 [00:01<00:17,  8.87it/s]\u001b[A\n",
            "  9%|▉         | 15/165 [00:01<00:16,  8.85it/s]\u001b[A\n",
            " 10%|▉         | 16/165 [00:01<00:16,  8.85it/s]\u001b[A\n",
            " 10%|█         | 17/165 [00:01<00:16,  8.87it/s]\u001b[A\n",
            " 11%|█         | 18/165 [00:02<00:16,  8.86it/s]\u001b[A\n",
            " 12%|█▏        | 19/165 [00:02<00:16,  8.85it/s]\u001b[A\n",
            " 12%|█▏        | 20/165 [00:02<00:16,  8.84it/s]\u001b[A\n",
            " 13%|█▎        | 21/165 [00:02<00:16,  8.85it/s]\u001b[A\n",
            " 13%|█▎        | 22/165 [00:02<00:16,  8.87it/s]\u001b[A\n",
            " 14%|█▍        | 23/165 [00:02<00:16,  8.86it/s]\u001b[A\n",
            " 15%|█▍        | 24/165 [00:02<00:15,  8.86it/s]\u001b[A\n",
            " 15%|█▌        | 25/165 [00:02<00:15,  8.89it/s]\u001b[A\n",
            " 16%|█▌        | 26/165 [00:02<00:15,  8.88it/s]\u001b[A\n",
            " 16%|█▋        | 27/165 [00:03<00:15,  8.88it/s]\u001b[A\n",
            " 17%|█▋        | 28/165 [00:03<00:15,  8.86it/s]\u001b[A\n",
            " 18%|█▊        | 29/165 [00:03<00:15,  8.85it/s]\u001b[A\n",
            " 18%|█▊        | 30/165 [00:03<00:15,  8.86it/s]\u001b[A\n",
            " 19%|█▉        | 31/165 [00:03<00:15,  8.85it/s]\u001b[A\n",
            " 19%|█▉        | 32/165 [00:03<00:15,  8.86it/s]\u001b[A\n",
            " 20%|██        | 33/165 [00:03<00:14,  8.87it/s]\u001b[A\n",
            " 21%|██        | 34/165 [00:03<00:14,  8.87it/s]\u001b[A\n",
            " 21%|██        | 35/165 [00:03<00:14,  8.88it/s]\u001b[A\n",
            " 22%|██▏       | 36/165 [00:04<00:14,  8.86it/s]\u001b[A\n",
            " 22%|██▏       | 37/165 [00:04<00:14,  8.85it/s]\u001b[A\n",
            " 23%|██▎       | 38/165 [00:04<00:14,  8.88it/s]\u001b[A\n",
            " 24%|██▎       | 39/165 [00:04<00:14,  8.85it/s]\u001b[A\n",
            " 24%|██▍       | 40/165 [00:04<00:14,  8.85it/s]\u001b[A\n",
            " 25%|██▍       | 41/165 [00:04<00:13,  8.87it/s]\u001b[A\n",
            " 25%|██▌       | 42/165 [00:04<00:13,  8.90it/s]\u001b[A\n",
            " 26%|██▌       | 43/165 [00:04<00:13,  8.89it/s]\u001b[A\n",
            " 27%|██▋       | 44/165 [00:04<00:13,  8.89it/s]\u001b[A\n",
            " 27%|██▋       | 45/165 [00:05<00:13,  8.89it/s]\u001b[A\n",
            " 28%|██▊       | 46/165 [00:05<00:13,  8.88it/s]\u001b[A\n",
            " 28%|██▊       | 47/165 [00:05<00:13,  8.85it/s]\u001b[A\n",
            " 29%|██▉       | 48/165 [00:05<00:13,  8.87it/s]\u001b[A\n",
            " 30%|██▉       | 49/165 [00:05<00:13,  8.87it/s]\u001b[A\n",
            " 30%|███       | 50/165 [00:05<00:12,  8.90it/s]\u001b[A\n",
            " 31%|███       | 51/165 [00:05<00:12,  8.85it/s]\u001b[A\n",
            " 32%|███▏      | 52/165 [00:05<00:12,  8.87it/s]\u001b[A\n",
            " 32%|███▏      | 53/165 [00:05<00:12,  8.89it/s]\u001b[A\n",
            " 33%|███▎      | 54/165 [00:06<00:12,  8.90it/s]\u001b[A\n",
            " 33%|███▎      | 55/165 [00:06<00:12,  8.87it/s]\u001b[A\n",
            " 34%|███▍      | 56/165 [00:06<00:12,  8.88it/s]\u001b[A\n",
            " 35%|███▍      | 57/165 [00:06<00:12,  8.88it/s]\u001b[A\n",
            " 35%|███▌      | 58/165 [00:06<00:12,  8.88it/s]\u001b[A\n",
            " 36%|███▌      | 59/165 [00:06<00:11,  8.88it/s]\u001b[A\n",
            " 36%|███▋      | 60/165 [00:06<00:11,  8.90it/s]\u001b[A\n",
            " 37%|███▋      | 61/165 [00:06<00:11,  8.91it/s]\u001b[A\n",
            " 38%|███▊      | 62/165 [00:06<00:11,  8.91it/s]\u001b[A\n",
            " 38%|███▊      | 63/165 [00:07<00:11,  8.89it/s]\u001b[A\n",
            " 39%|███▉      | 64/165 [00:07<00:11,  8.89it/s]\u001b[A\n",
            " 39%|███▉      | 65/165 [00:07<00:11,  8.89it/s]\u001b[A\n",
            " 40%|████      | 66/165 [00:07<00:11,  8.86it/s]\u001b[A\n",
            " 41%|████      | 67/165 [00:07<00:11,  8.87it/s]\u001b[A\n",
            " 41%|████      | 68/165 [00:07<00:10,  8.87it/s]\u001b[A\n",
            " 42%|████▏     | 69/165 [00:07<00:10,  8.89it/s]\u001b[A\n",
            " 42%|████▏     | 70/165 [00:07<00:10,  8.88it/s]\u001b[A\n",
            " 43%|████▎     | 71/165 [00:08<00:10,  8.87it/s]\u001b[A\n",
            " 44%|████▎     | 72/165 [00:08<00:10,  8.87it/s]\u001b[A\n",
            " 44%|████▍     | 73/165 [00:08<00:10,  8.88it/s]\u001b[A\n",
            " 45%|████▍     | 74/165 [00:08<00:10,  8.89it/s]\u001b[A\n",
            " 45%|████▌     | 75/165 [00:08<00:10,  8.86it/s]\u001b[A\n",
            " 46%|████▌     | 76/165 [00:08<00:10,  8.87it/s]\u001b[A\n",
            " 47%|████▋     | 77/165 [00:08<00:09,  8.88it/s]\u001b[A\n",
            " 47%|████▋     | 78/165 [00:08<00:09,  8.89it/s]\u001b[A\n",
            " 48%|████▊     | 79/165 [00:08<00:09,  8.88it/s]\u001b[A\n",
            " 48%|████▊     | 80/165 [00:09<00:09,  8.87it/s]\u001b[A\n",
            " 49%|████▉     | 81/165 [00:09<00:09,  8.84it/s]\u001b[A\n",
            " 50%|████▉     | 82/165 [00:09<00:09,  8.82it/s]\u001b[A\n",
            " 50%|█████     | 83/165 [00:09<00:09,  8.80it/s]\u001b[A\n",
            " 51%|█████     | 84/165 [00:09<00:09,  8.80it/s]\u001b[A\n",
            " 52%|█████▏    | 85/165 [00:09<00:09,  8.82it/s]\u001b[A\n",
            " 52%|█████▏    | 86/165 [00:09<00:08,  8.83it/s]\u001b[A\n",
            " 53%|█████▎    | 87/165 [00:09<00:08,  8.82it/s]\u001b[A\n",
            " 53%|█████▎    | 88/165 [00:09<00:08,  8.82it/s]\u001b[A\n",
            " 54%|█████▍    | 89/165 [00:10<00:08,  8.83it/s]\u001b[A\n",
            " 55%|█████▍    | 90/165 [00:10<00:08,  8.81it/s]\u001b[A\n",
            " 55%|█████▌    | 91/165 [00:10<00:08,  8.80it/s]\u001b[A\n",
            " 56%|█████▌    | 92/165 [00:10<00:08,  8.80it/s]\u001b[A\n",
            " 56%|█████▋    | 93/165 [00:10<00:08,  8.79it/s]\u001b[A\n",
            " 57%|█████▋    | 94/165 [00:10<00:08,  8.81it/s]\u001b[A\n",
            " 58%|█████▊    | 95/165 [00:10<00:07,  8.83it/s]\u001b[A\n",
            " 58%|█████▊    | 96/165 [00:10<00:07,  8.85it/s]\u001b[A\n",
            " 59%|█████▉    | 97/165 [00:10<00:07,  8.87it/s]\u001b[A\n",
            " 59%|█████▉    | 98/165 [00:11<00:07,  8.87it/s]\u001b[A\n",
            " 60%|██████    | 99/165 [00:11<00:07,  8.82it/s]\u001b[A\n",
            " 61%|██████    | 100/165 [00:11<00:07,  8.77it/s]\u001b[A\n",
            " 61%|██████    | 101/165 [00:11<00:07,  8.73it/s]\u001b[A\n",
            " 62%|██████▏   | 102/165 [00:11<00:07,  8.76it/s]\u001b[A\n",
            " 62%|██████▏   | 103/165 [00:11<00:07,  8.78it/s]\u001b[A\n",
            " 63%|██████▎   | 104/165 [00:11<00:06,  8.78it/s]\u001b[A\n",
            " 64%|██████▎   | 105/165 [00:11<00:06,  8.78it/s]\u001b[A\n",
            " 64%|██████▍   | 106/165 [00:11<00:06,  8.72it/s]\u001b[A\n",
            " 65%|██████▍   | 107/165 [00:12<00:06,  8.76it/s]\u001b[A\n",
            " 65%|██████▌   | 108/165 [00:12<00:06,  8.79it/s]\u001b[A\n",
            " 66%|██████▌   | 109/165 [00:12<00:06,  8.80it/s]\u001b[A\n",
            " 67%|██████▋   | 110/165 [00:12<00:06,  8.81it/s]\u001b[A\n",
            " 67%|██████▋   | 111/165 [00:12<00:06,  8.81it/s]\u001b[A\n",
            " 68%|██████▊   | 112/165 [00:12<00:06,  8.82it/s]\u001b[A\n",
            " 68%|██████▊   | 113/165 [00:12<00:05,  8.84it/s]\u001b[A\n",
            " 69%|██████▉   | 114/165 [00:12<00:05,  8.84it/s]\u001b[A\n",
            " 70%|██████▉   | 115/165 [00:13<00:05,  8.80it/s]\u001b[A\n",
            " 70%|███████   | 116/165 [00:13<00:05,  8.79it/s]\u001b[A\n",
            " 71%|███████   | 117/165 [00:13<00:05,  8.77it/s]\u001b[A\n",
            " 72%|███████▏  | 118/165 [00:13<00:05,  8.78it/s]\u001b[A\n",
            " 72%|███████▏  | 119/165 [00:13<00:05,  8.80it/s]\u001b[A\n",
            " 73%|███████▎  | 120/165 [00:13<00:05,  8.82it/s]\u001b[A\n",
            " 73%|███████▎  | 121/165 [00:13<00:04,  8.81it/s]\u001b[A\n",
            " 74%|███████▍  | 122/165 [00:13<00:04,  8.77it/s]\u001b[A\n",
            " 75%|███████▍  | 123/165 [00:13<00:04,  8.76it/s]\u001b[A\n",
            " 75%|███████▌  | 124/165 [00:14<00:04,  8.79it/s]\u001b[A\n",
            " 76%|███████▌  | 125/165 [00:14<00:04,  8.79it/s]\u001b[A\n",
            " 76%|███████▋  | 126/165 [00:14<00:04,  8.78it/s]\u001b[A\n",
            " 77%|███████▋  | 127/165 [00:14<00:04,  8.78it/s]\u001b[A\n",
            " 78%|███████▊  | 128/165 [00:14<00:04,  8.81it/s]\u001b[A\n",
            " 78%|███████▊  | 129/165 [00:14<00:04,  8.83it/s]\u001b[A\n",
            " 79%|███████▉  | 130/165 [00:14<00:03,  8.85it/s]\u001b[A\n",
            " 79%|███████▉  | 131/165 [00:14<00:03,  8.84it/s]\u001b[A\n",
            " 80%|████████  | 132/165 [00:14<00:03,  8.84it/s]\u001b[A\n",
            " 81%|████████  | 133/165 [00:15<00:03,  8.79it/s]\u001b[A\n",
            " 81%|████████  | 134/165 [00:15<00:03,  8.76it/s]\u001b[A\n",
            " 82%|████████▏ | 135/165 [00:15<00:03,  8.77it/s]\u001b[A\n",
            " 82%|████████▏ | 136/165 [00:15<00:03,  8.76it/s]\u001b[A\n",
            " 83%|████████▎ | 137/165 [00:15<00:03,  8.79it/s]\u001b[A\n",
            " 84%|████████▎ | 138/165 [00:15<00:03,  8.81it/s]\u001b[A\n",
            " 84%|████████▍ | 139/165 [00:15<00:02,  8.81it/s]\u001b[A\n",
            " 85%|████████▍ | 140/165 [00:15<00:02,  8.81it/s]\u001b[A\n",
            " 85%|████████▌ | 141/165 [00:15<00:02,  8.84it/s]\u001b[A\n",
            " 86%|████████▌ | 142/165 [00:16<00:02,  8.84it/s]\u001b[A\n",
            " 87%|████████▋ | 143/165 [00:16<00:02,  8.82it/s]\u001b[A\n",
            " 87%|████████▋ | 144/165 [00:16<00:02,  8.75it/s]\u001b[A\n",
            " 88%|████████▊ | 145/165 [00:16<00:02,  8.73it/s]\u001b[A\n",
            " 88%|████████▊ | 146/165 [00:16<00:02,  8.75it/s]\u001b[A\n",
            " 89%|████████▉ | 147/165 [00:16<00:02,  8.78it/s]\u001b[A\n",
            " 90%|████████▉ | 148/165 [00:16<00:01,  8.78it/s]\u001b[A\n",
            " 90%|█████████ | 149/165 [00:16<00:01,  8.80it/s]\u001b[A\n",
            " 91%|█████████ | 150/165 [00:16<00:01,  8.80it/s]\u001b[A\n",
            " 92%|█████████▏| 151/165 [00:17<00:01,  8.73it/s]\u001b[A\n",
            " 92%|█████████▏| 152/165 [00:17<00:01,  8.75it/s]\u001b[A\n",
            " 93%|█████████▎| 153/165 [00:17<00:01,  8.76it/s]\u001b[A\n",
            " 93%|█████████▎| 154/165 [00:17<00:01,  8.80it/s]\u001b[A\n",
            " 94%|█████████▍| 155/165 [00:17<00:01,  8.79it/s]\u001b[A\n",
            " 95%|█████████▍| 156/165 [00:17<00:01,  8.73it/s]\u001b[A\n",
            " 95%|█████████▌| 157/165 [00:17<00:00,  8.71it/s]\u001b[A\n",
            " 96%|█████████▌| 158/165 [00:17<00:00,  8.72it/s]\u001b[A\n",
            " 96%|█████████▋| 159/165 [00:18<00:00,  8.74it/s]\u001b[A\n",
            " 97%|█████████▋| 160/165 [00:18<00:00,  8.74it/s]\u001b[A\n",
            " 98%|█████████▊| 161/165 [00:18<00:00,  8.78it/s]\u001b[A\n",
            " 98%|█████████▊| 162/165 [00:18<00:00,  8.81it/s]\u001b[A\n",
            " 99%|█████████▉| 163/165 [00:18<00:00,  8.82it/s]\u001b[A\n",
            "100%|██████████| 165/165 [00:18<00:00,  8.82it/s]\n",
            "========== model: bert-base-uncased fold: 3 inference ==========\n",
            "========== model: bert-base-uncased fold: 3 inference ==========\n",
            "========== model: bert-base-uncased fold: 3 inference ==========\n",
            "========== model: bert-base-uncased fold: 3 inference ==========\n",
            "========== model: bert-base-uncased fold: 3 inference ==========\n",
            "========== model: bert-base-uncased fold: 3 inference ==========\n",
            "========== model: bert-base-uncased fold: 3 inference ==========\n",
            "========== model: bert-base-uncased fold: 3 inference ==========\n",
            "========== model: bert-base-uncased fold: 3 inference ==========\n",
            "========== model: bert-base-uncased fold: 3 inference ==========\n",
            "========== model: bert-base-uncased fold: 3 inference ==========\n",
            "loading configuration file /content/activebus_bert_final/config.json\n",
            "Model config BertConfig {\n",
            "  \"_name_or_path\": \"activebus/BERT-XD_Review\",\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_ids\": 0,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.15.0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "loading weights file /content/activebus_bert_final/pytorch_model.bin\n",
            "Some weights of the model checkpoint at /content/activebus_bert_final were not used when initializing BertForSequenceClassification: ['cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /content/activebus_bert_final and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "\n",
            "  0%|          | 0/165 [00:00<?, ?it/s]\u001b[A\n",
            "  1%|          | 1/165 [00:00<00:19,  8.22it/s]\u001b[A\n",
            "  1%|          | 2/165 [00:00<00:19,  8.56it/s]\u001b[A\n",
            "  2%|▏         | 3/165 [00:00<00:19,  8.49it/s]\u001b[A\n",
            "  2%|▏         | 4/165 [00:00<00:18,  8.61it/s]\u001b[A\n",
            "  3%|▎         | 5/165 [00:00<00:18,  8.72it/s]\u001b[A\n",
            "  4%|▎         | 6/165 [00:00<00:18,  8.78it/s]\u001b[A\n",
            "  4%|▍         | 7/165 [00:00<00:17,  8.81it/s]\u001b[A\n",
            "  5%|▍         | 8/165 [00:00<00:17,  8.82it/s]\u001b[A\n",
            "  5%|▌         | 9/165 [00:01<00:17,  8.83it/s]\u001b[A\n",
            "  6%|▌         | 10/165 [00:01<00:17,  8.80it/s]\u001b[A\n",
            "  7%|▋         | 11/165 [00:01<00:17,  8.81it/s]\u001b[A\n",
            "  7%|▋         | 12/165 [00:01<00:17,  8.81it/s]\u001b[A\n",
            "  8%|▊         | 13/165 [00:01<00:17,  8.82it/s]\u001b[A\n",
            "  8%|▊         | 14/165 [00:01<00:17,  8.84it/s]\u001b[A\n",
            "  9%|▉         | 15/165 [00:01<00:16,  8.85it/s]\u001b[A\n",
            " 10%|▉         | 16/165 [00:01<00:16,  8.84it/s]\u001b[A\n",
            " 10%|█         | 17/165 [00:01<00:16,  8.86it/s]\u001b[A\n",
            " 11%|█         | 18/165 [00:02<00:16,  8.84it/s]\u001b[A\n",
            " 12%|█▏        | 19/165 [00:02<00:16,  8.85it/s]\u001b[A\n",
            " 12%|█▏        | 20/165 [00:02<00:16,  8.86it/s]\u001b[A\n",
            " 13%|█▎        | 21/165 [00:02<00:16,  8.87it/s]\u001b[A\n",
            " 13%|█▎        | 22/165 [00:02<00:16,  8.88it/s]\u001b[A\n",
            " 14%|█▍        | 23/165 [00:02<00:16,  8.87it/s]\u001b[A\n",
            " 15%|█▍        | 24/165 [00:02<00:15,  8.87it/s]\u001b[A\n",
            " 15%|█▌        | 25/165 [00:02<00:15,  8.86it/s]\u001b[A\n",
            " 16%|█▌        | 26/165 [00:02<00:15,  8.86it/s]\u001b[A\n",
            " 16%|█▋        | 27/165 [00:03<00:15,  8.88it/s]\u001b[A\n",
            " 17%|█▋        | 28/165 [00:03<00:15,  8.89it/s]\u001b[A\n",
            " 18%|█▊        | 29/165 [00:03<00:15,  8.87it/s]\u001b[A\n",
            " 18%|█▊        | 30/165 [00:03<00:15,  8.83it/s]\u001b[A\n",
            " 19%|█▉        | 31/165 [00:03<00:15,  8.83it/s]\u001b[A\n",
            " 19%|█▉        | 32/165 [00:03<00:15,  8.84it/s]\u001b[A\n",
            " 20%|██        | 33/165 [00:03<00:14,  8.84it/s]\u001b[A\n",
            " 21%|██        | 34/165 [00:03<00:14,  8.85it/s]\u001b[A\n",
            " 21%|██        | 35/165 [00:03<00:14,  8.85it/s]\u001b[A\n",
            " 22%|██▏       | 36/165 [00:04<00:14,  8.86it/s]\u001b[A\n",
            " 22%|██▏       | 37/165 [00:04<00:14,  8.87it/s]\u001b[A\n",
            " 23%|██▎       | 38/165 [00:04<00:14,  8.85it/s]\u001b[A\n",
            " 24%|██▎       | 39/165 [00:04<00:14,  8.87it/s]\u001b[A\n",
            " 24%|██▍       | 40/165 [00:04<00:14,  8.85it/s]\u001b[A\n",
            " 25%|██▍       | 41/165 [00:04<00:14,  8.85it/s]\u001b[A\n",
            " 25%|██▌       | 42/165 [00:04<00:13,  8.85it/s]\u001b[A\n",
            " 26%|██▌       | 43/165 [00:04<00:13,  8.85it/s]\u001b[A\n",
            " 27%|██▋       | 44/165 [00:04<00:13,  8.85it/s]\u001b[A\n",
            " 27%|██▋       | 45/165 [00:05<00:13,  8.85it/s]\u001b[A\n",
            " 28%|██▊       | 46/165 [00:05<00:13,  8.83it/s]\u001b[A\n",
            " 28%|██▊       | 47/165 [00:05<00:13,  8.83it/s]\u001b[A\n",
            " 29%|██▉       | 48/165 [00:05<00:13,  8.84it/s]\u001b[A\n",
            " 30%|██▉       | 49/165 [00:05<00:13,  8.85it/s]\u001b[A\n",
            " 30%|███       | 50/165 [00:05<00:12,  8.85it/s]\u001b[A\n",
            " 31%|███       | 51/165 [00:05<00:12,  8.85it/s]\u001b[A\n",
            " 32%|███▏      | 52/165 [00:05<00:12,  8.83it/s]\u001b[A\n",
            " 32%|███▏      | 53/165 [00:06<00:12,  8.84it/s]\u001b[A\n",
            " 33%|███▎      | 54/165 [00:06<00:12,  8.85it/s]\u001b[A\n",
            " 33%|███▎      | 55/165 [00:06<00:12,  8.84it/s]\u001b[A\n",
            " 34%|███▍      | 56/165 [00:06<00:12,  8.86it/s]\u001b[A\n",
            " 35%|███▍      | 57/165 [00:06<00:12,  8.86it/s]\u001b[A\n",
            " 35%|███▌      | 58/165 [00:06<00:12,  8.86it/s]\u001b[A\n",
            " 36%|███▌      | 59/165 [00:06<00:11,  8.85it/s]\u001b[A\n",
            " 36%|███▋      | 60/165 [00:06<00:11,  8.84it/s]\u001b[A\n",
            " 37%|███▋      | 61/165 [00:06<00:11,  8.85it/s]\u001b[A\n",
            " 38%|███▊      | 62/165 [00:07<00:11,  8.84it/s]\u001b[A\n",
            " 38%|███▊      | 63/165 [00:07<00:11,  8.84it/s]\u001b[A\n",
            " 39%|███▉      | 64/165 [00:07<00:11,  8.86it/s]\u001b[A\n",
            " 39%|███▉      | 65/165 [00:07<00:11,  8.87it/s]\u001b[A\n",
            " 40%|████      | 66/165 [00:07<00:11,  8.87it/s]\u001b[A\n",
            " 41%|████      | 67/165 [00:07<00:11,  8.85it/s]\u001b[A\n",
            " 41%|████      | 68/165 [00:07<00:10,  8.84it/s]\u001b[A\n",
            " 42%|████▏     | 69/165 [00:07<00:10,  8.86it/s]\u001b[A\n",
            " 42%|████▏     | 70/165 [00:07<00:10,  8.87it/s]\u001b[A\n",
            " 43%|████▎     | 71/165 [00:08<00:10,  8.85it/s]\u001b[A\n",
            " 44%|████▎     | 72/165 [00:08<00:10,  8.87it/s]\u001b[A\n",
            " 44%|████▍     | 73/165 [00:08<00:10,  8.89it/s]\u001b[A\n",
            " 45%|████▍     | 74/165 [00:08<00:10,  8.90it/s]\u001b[A\n",
            " 45%|████▌     | 75/165 [00:08<00:10,  8.90it/s]\u001b[A\n",
            " 46%|████▌     | 76/165 [00:08<00:10,  8.90it/s]\u001b[A\n",
            " 47%|████▋     | 77/165 [00:08<00:09,  8.89it/s]\u001b[A\n",
            " 47%|████▋     | 78/165 [00:08<00:09,  8.90it/s]\u001b[A\n",
            " 48%|████▊     | 79/165 [00:08<00:09,  8.86it/s]\u001b[A\n",
            " 48%|████▊     | 80/165 [00:09<00:09,  8.83it/s]\u001b[A\n",
            " 49%|████▉     | 81/165 [00:09<00:09,  8.85it/s]\u001b[A\n",
            " 50%|████▉     | 82/165 [00:09<00:09,  8.86it/s]\u001b[A\n",
            " 50%|█████     | 83/165 [00:09<00:09,  8.87it/s]\u001b[A\n",
            " 51%|█████     | 84/165 [00:09<00:09,  8.87it/s]\u001b[A\n",
            " 52%|█████▏    | 85/165 [00:09<00:09,  8.87it/s]\u001b[A\n",
            " 52%|█████▏    | 86/165 [00:09<00:08,  8.90it/s]\u001b[A\n",
            " 53%|█████▎    | 87/165 [00:09<00:08,  8.85it/s]\u001b[A\n",
            " 53%|█████▎    | 88/165 [00:09<00:08,  8.86it/s]\u001b[A\n",
            " 54%|█████▍    | 89/165 [00:10<00:08,  8.85it/s]\u001b[A\n",
            " 55%|█████▍    | 90/165 [00:10<00:08,  8.85it/s]\u001b[A\n",
            " 55%|█████▌    | 91/165 [00:10<00:08,  8.86it/s]\u001b[A\n",
            " 56%|█████▌    | 92/165 [00:10<00:08,  8.85it/s]\u001b[A\n",
            " 56%|█████▋    | 93/165 [00:10<00:08,  8.87it/s]\u001b[A\n",
            " 57%|█████▋    | 94/165 [00:10<00:08,  8.87it/s]\u001b[A\n",
            " 58%|█████▊    | 95/165 [00:10<00:07,  8.84it/s]\u001b[A\n",
            " 58%|█████▊    | 96/165 [00:10<00:07,  8.84it/s]\u001b[A\n",
            " 59%|█████▉    | 97/165 [00:10<00:07,  8.87it/s]\u001b[A\n",
            " 59%|█████▉    | 98/165 [00:11<00:07,  8.88it/s]\u001b[A\n",
            " 60%|██████    | 99/165 [00:11<00:07,  8.89it/s]\u001b[A\n",
            " 61%|██████    | 100/165 [00:11<00:07,  8.86it/s]\u001b[A\n",
            " 61%|██████    | 101/165 [00:11<00:07,  8.84it/s]\u001b[A\n",
            " 62%|██████▏   | 102/165 [00:11<00:07,  8.85it/s]\u001b[A\n",
            " 62%|██████▏   | 103/165 [00:11<00:07,  8.84it/s]\u001b[A\n",
            " 63%|██████▎   | 104/165 [00:11<00:06,  8.84it/s]\u001b[A\n",
            " 64%|██████▎   | 105/165 [00:11<00:06,  8.86it/s]\u001b[A\n",
            " 64%|██████▍   | 106/165 [00:11<00:06,  8.84it/s]\u001b[A\n",
            " 65%|██████▍   | 107/165 [00:12<00:06,  8.83it/s]\u001b[A\n",
            " 65%|██████▌   | 108/165 [00:12<00:06,  8.79it/s]\u001b[A\n",
            " 66%|██████▌   | 109/165 [00:12<00:06,  8.81it/s]\u001b[A\n",
            " 67%|██████▋   | 110/165 [00:12<00:06,  8.81it/s]\u001b[A\n",
            " 67%|██████▋   | 111/165 [00:12<00:06,  8.83it/s]\u001b[A\n",
            " 68%|██████▊   | 112/165 [00:12<00:05,  8.85it/s]\u001b[A\n",
            " 68%|██████▊   | 113/165 [00:12<00:05,  8.87it/s]\u001b[A\n",
            " 69%|██████▉   | 114/165 [00:12<00:05,  8.88it/s]\u001b[A\n",
            " 70%|██████▉   | 115/165 [00:13<00:05,  8.89it/s]\u001b[A\n",
            " 70%|███████   | 116/165 [00:13<00:05,  8.87it/s]\u001b[A\n",
            " 71%|███████   | 117/165 [00:13<00:05,  8.87it/s]\u001b[A\n",
            " 72%|███████▏  | 118/165 [00:13<00:05,  8.85it/s]\u001b[A\n",
            " 72%|███████▏  | 119/165 [00:13<00:05,  8.85it/s]\u001b[A\n",
            " 73%|███████▎  | 120/165 [00:13<00:05,  8.86it/s]\u001b[A\n",
            " 73%|███████▎  | 121/165 [00:13<00:04,  8.87it/s]\u001b[A\n",
            " 74%|███████▍  | 122/165 [00:13<00:04,  8.88it/s]\u001b[A\n",
            " 75%|███████▍  | 123/165 [00:13<00:04,  8.87it/s]\u001b[A\n",
            " 75%|███████▌  | 124/165 [00:14<00:04,  8.87it/s]\u001b[A\n",
            " 76%|███████▌  | 125/165 [00:14<00:04,  8.88it/s]\u001b[A\n",
            " 76%|███████▋  | 126/165 [00:14<00:04,  8.87it/s]\u001b[A\n",
            " 77%|███████▋  | 127/165 [00:14<00:04,  8.88it/s]\u001b[A\n",
            " 78%|███████▊  | 128/165 [00:14<00:04,  8.86it/s]\u001b[A\n",
            " 78%|███████▊  | 129/165 [00:14<00:04,  8.87it/s]\u001b[A\n",
            " 79%|███████▉  | 130/165 [00:14<00:03,  8.89it/s]\u001b[A\n",
            " 79%|███████▉  | 131/165 [00:14<00:03,  8.86it/s]\u001b[A\n",
            " 80%|████████  | 132/165 [00:14<00:03,  8.84it/s]\u001b[A\n",
            " 81%|████████  | 133/165 [00:15<00:03,  8.84it/s]\u001b[A\n",
            " 81%|████████  | 134/165 [00:15<00:03,  8.83it/s]\u001b[A\n",
            " 82%|████████▏ | 135/165 [00:15<00:03,  8.83it/s]\u001b[A\n",
            " 82%|████████▏ | 136/165 [00:15<00:03,  8.84it/s]\u001b[A\n",
            " 83%|████████▎ | 137/165 [00:15<00:03,  8.80it/s]\u001b[A\n",
            " 84%|████████▎ | 138/165 [00:15<00:03,  8.78it/s]\u001b[A\n",
            " 84%|████████▍ | 139/165 [00:15<00:02,  8.80it/s]\u001b[A\n",
            " 85%|████████▍ | 140/165 [00:15<00:02,  8.82it/s]\u001b[A\n",
            " 85%|████████▌ | 141/165 [00:15<00:02,  8.83it/s]\u001b[A\n",
            " 86%|████████▌ | 142/165 [00:16<00:02,  8.76it/s]\u001b[A\n",
            " 87%|████████▋ | 143/165 [00:16<00:02,  8.79it/s]\u001b[A\n",
            " 87%|████████▋ | 144/165 [00:16<00:02,  8.82it/s]\u001b[A\n",
            " 88%|████████▊ | 145/165 [00:16<00:02,  8.83it/s]\u001b[A\n",
            " 88%|████████▊ | 146/165 [00:16<00:02,  8.81it/s]\u001b[A\n",
            " 89%|████████▉ | 147/165 [00:16<00:02,  8.82it/s]\u001b[A\n",
            " 90%|████████▉ | 148/165 [00:16<00:01,  8.81it/s]\u001b[A\n",
            " 90%|█████████ | 149/165 [00:16<00:01,  8.81it/s]\u001b[A\n",
            " 91%|█████████ | 150/165 [00:16<00:01,  8.82it/s]\u001b[A\n",
            " 92%|█████████▏| 151/165 [00:17<00:01,  8.83it/s]\u001b[A\n",
            " 92%|█████████▏| 152/165 [00:17<00:01,  8.77it/s]\u001b[A\n",
            " 93%|█████████▎| 153/165 [00:17<00:01,  8.78it/s]\u001b[A\n",
            " 93%|█████████▎| 154/165 [00:17<00:01,  8.80it/s]\u001b[A\n",
            " 94%|█████████▍| 155/165 [00:17<00:01,  8.82it/s]\u001b[A\n",
            " 95%|█████████▍| 156/165 [00:17<00:01,  8.84it/s]\u001b[A\n",
            " 95%|█████████▌| 157/165 [00:17<00:00,  8.83it/s]\u001b[A\n",
            " 96%|█████████▌| 158/165 [00:17<00:00,  8.81it/s]\u001b[A\n",
            " 96%|█████████▋| 159/165 [00:17<00:00,  8.81it/s]\u001b[A\n",
            " 97%|█████████▋| 160/165 [00:18<00:00,  8.80it/s]\u001b[A\n",
            " 98%|█████████▊| 161/165 [00:18<00:00,  8.79it/s]\u001b[A\n",
            " 98%|█████████▊| 162/165 [00:18<00:00,  8.81it/s]\u001b[A\n",
            " 99%|█████████▉| 163/165 [00:18<00:00,  8.85it/s]\u001b[A\n",
            "100%|██████████| 165/165 [00:18<00:00,  8.83it/s]\n",
            "========== model: bert-base-uncased fold: 4 inference ==========\n",
            "========== model: bert-base-uncased fold: 4 inference ==========\n",
            "========== model: bert-base-uncased fold: 4 inference ==========\n",
            "========== model: bert-base-uncased fold: 4 inference ==========\n",
            "========== model: bert-base-uncased fold: 4 inference ==========\n",
            "========== model: bert-base-uncased fold: 4 inference ==========\n",
            "========== model: bert-base-uncased fold: 4 inference ==========\n",
            "========== model: bert-base-uncased fold: 4 inference ==========\n",
            "========== model: bert-base-uncased fold: 4 inference ==========\n",
            "========== model: bert-base-uncased fold: 4 inference ==========\n",
            "========== model: bert-base-uncased fold: 4 inference ==========\n",
            "loading configuration file /content/activebus_bert_final/config.json\n",
            "Model config BertConfig {\n",
            "  \"_name_or_path\": \"activebus/BERT-XD_Review\",\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_ids\": 0,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.15.0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "loading weights file /content/activebus_bert_final/pytorch_model.bin\n",
            "Some weights of the model checkpoint at /content/activebus_bert_final were not used when initializing BertForSequenceClassification: ['cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /content/activebus_bert_final and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "\n",
            "  0%|          | 0/165 [00:00<?, ?it/s]\u001b[A\n",
            "  1%|          | 1/165 [00:00<00:19,  8.53it/s]\u001b[A\n",
            "  1%|          | 2/165 [00:00<00:18,  8.72it/s]\u001b[A\n",
            "  2%|▏         | 3/165 [00:00<00:18,  8.80it/s]\u001b[A\n",
            "  2%|▏         | 4/165 [00:00<00:18,  8.81it/s]\u001b[A\n",
            "  3%|▎         | 5/165 [00:00<00:18,  8.81it/s]\u001b[A\n",
            "  4%|▎         | 6/165 [00:00<00:18,  8.79it/s]\u001b[A\n",
            "  4%|▍         | 7/165 [00:00<00:17,  8.80it/s]\u001b[A\n",
            "  5%|▍         | 8/165 [00:00<00:17,  8.81it/s]\u001b[A\n",
            "  5%|▌         | 9/165 [00:01<00:17,  8.80it/s]\u001b[A\n",
            "  6%|▌         | 10/165 [00:01<00:17,  8.82it/s]\u001b[A\n",
            "  7%|▋         | 11/165 [00:01<00:17,  8.83it/s]\u001b[A\n",
            "  7%|▋         | 12/165 [00:01<00:17,  8.81it/s]\u001b[A\n",
            "  8%|▊         | 13/165 [00:01<00:17,  8.79it/s]\u001b[A\n",
            "  8%|▊         | 14/165 [00:01<00:17,  8.81it/s]\u001b[A\n",
            "  9%|▉         | 15/165 [00:01<00:17,  8.82it/s]\u001b[A\n",
            " 10%|▉         | 16/165 [00:01<00:16,  8.84it/s]\u001b[A\n",
            " 10%|█         | 17/165 [00:01<00:16,  8.83it/s]\u001b[A\n",
            " 11%|█         | 18/165 [00:02<00:16,  8.83it/s]\u001b[A\n",
            " 12%|█▏        | 19/165 [00:02<00:16,  8.85it/s]\u001b[A\n",
            " 12%|█▏        | 20/165 [00:02<00:16,  8.87it/s]\u001b[A\n",
            " 13%|█▎        | 21/165 [00:02<00:16,  8.89it/s]\u001b[A\n",
            " 13%|█▎        | 22/165 [00:02<00:16,  8.90it/s]\u001b[A\n",
            " 14%|█▍        | 23/165 [00:02<00:16,  8.86it/s]\u001b[A\n",
            " 15%|█▍        | 24/165 [00:02<00:15,  8.85it/s]\u001b[A\n",
            " 15%|█▌        | 25/165 [00:02<00:15,  8.87it/s]\u001b[A\n",
            " 16%|█▌        | 26/165 [00:02<00:15,  8.89it/s]\u001b[A\n",
            " 16%|█▋        | 27/165 [00:03<00:15,  8.87it/s]\u001b[A\n",
            " 17%|█▋        | 28/165 [00:03<00:15,  8.87it/s]\u001b[A\n",
            " 18%|█▊        | 29/165 [00:03<00:15,  8.89it/s]\u001b[A\n",
            " 18%|█▊        | 30/165 [00:03<00:15,  8.90it/s]\u001b[A\n",
            " 19%|█▉        | 31/165 [00:03<00:15,  8.90it/s]\u001b[A\n",
            " 19%|█▉        | 32/165 [00:03<00:14,  8.89it/s]\u001b[A\n",
            " 20%|██        | 33/165 [00:03<00:14,  8.86it/s]\u001b[A\n",
            " 21%|██        | 34/165 [00:03<00:14,  8.85it/s]\u001b[A\n",
            " 21%|██        | 35/165 [00:03<00:14,  8.85it/s]\u001b[A\n",
            " 22%|██▏       | 36/165 [00:04<00:14,  8.85it/s]\u001b[A\n",
            " 22%|██▏       | 37/165 [00:04<00:14,  8.86it/s]\u001b[A\n",
            " 23%|██▎       | 38/165 [00:04<00:14,  8.88it/s]\u001b[A\n",
            " 24%|██▎       | 39/165 [00:04<00:14,  8.89it/s]\u001b[A\n",
            " 24%|██▍       | 40/165 [00:04<00:14,  8.89it/s]\u001b[A\n",
            " 25%|██▍       | 41/165 [00:04<00:13,  8.91it/s]\u001b[A\n",
            " 25%|██▌       | 42/165 [00:04<00:13,  8.90it/s]\u001b[A\n",
            " 26%|██▌       | 43/165 [00:04<00:13,  8.91it/s]\u001b[A\n",
            " 27%|██▋       | 44/165 [00:04<00:13,  8.87it/s]\u001b[A\n",
            " 27%|██▋       | 45/165 [00:05<00:13,  8.87it/s]\u001b[A\n",
            " 28%|██▊       | 46/165 [00:05<00:13,  8.88it/s]\u001b[A\n",
            " 28%|██▊       | 47/165 [00:05<00:13,  8.88it/s]\u001b[A\n",
            " 29%|██▉       | 48/165 [00:05<00:13,  8.90it/s]\u001b[A\n",
            " 30%|██▉       | 49/165 [00:05<00:13,  8.83it/s]\u001b[A\n",
            " 30%|███       | 50/165 [00:05<00:12,  8.86it/s]\u001b[A\n",
            " 31%|███       | 51/165 [00:05<00:12,  8.87it/s]\u001b[A\n",
            " 32%|███▏      | 52/165 [00:05<00:12,  8.83it/s]\u001b[A\n",
            " 32%|███▏      | 53/165 [00:05<00:12,  8.83it/s]\u001b[A\n",
            " 33%|███▎      | 54/165 [00:06<00:12,  8.85it/s]\u001b[A\n",
            " 33%|███▎      | 55/165 [00:06<00:12,  8.85it/s]\u001b[A\n",
            " 34%|███▍      | 56/165 [00:06<00:12,  8.86it/s]\u001b[A\n",
            " 35%|███▍      | 57/165 [00:06<00:12,  8.84it/s]\u001b[A\n",
            " 35%|███▌      | 58/165 [00:06<00:12,  8.83it/s]\u001b[A\n",
            " 36%|███▌      | 59/165 [00:06<00:12,  8.82it/s]\u001b[A\n",
            " 36%|███▋      | 60/165 [00:06<00:11,  8.82it/s]\u001b[A\n",
            " 37%|███▋      | 61/165 [00:06<00:11,  8.81it/s]\u001b[A\n",
            " 38%|███▊      | 62/165 [00:07<00:11,  8.82it/s]\u001b[A\n",
            " 38%|███▊      | 63/165 [00:07<00:11,  8.84it/s]\u001b[A\n",
            " 39%|███▉      | 64/165 [00:07<00:11,  8.86it/s]\u001b[A\n",
            " 39%|███▉      | 65/165 [00:07<00:11,  8.85it/s]\u001b[A\n",
            " 40%|████      | 66/165 [00:07<00:11,  8.85it/s]\u001b[A\n",
            " 41%|████      | 67/165 [00:07<00:11,  8.86it/s]\u001b[A\n",
            " 41%|████      | 68/165 [00:07<00:10,  8.85it/s]\u001b[A\n",
            " 42%|████▏     | 69/165 [00:07<00:10,  8.86it/s]\u001b[A\n",
            " 42%|████▏     | 70/165 [00:07<00:10,  8.86it/s]\u001b[A\n",
            " 43%|████▎     | 71/165 [00:08<00:10,  8.88it/s]\u001b[A\n",
            " 44%|████▎     | 72/165 [00:08<00:10,  8.89it/s]\u001b[A\n",
            " 44%|████▍     | 73/165 [00:08<00:10,  8.88it/s]\u001b[A\n",
            " 45%|████▍     | 74/165 [00:08<00:10,  8.89it/s]\u001b[A\n",
            " 45%|████▌     | 75/165 [00:08<00:10,  8.89it/s]\u001b[A\n",
            " 46%|████▌     | 76/165 [00:08<00:10,  8.89it/s]\u001b[A\n",
            " 47%|████▋     | 77/165 [00:08<00:09,  8.86it/s]\u001b[A\n",
            " 47%|████▋     | 78/165 [00:08<00:09,  8.88it/s]\u001b[A\n",
            " 48%|████▊     | 79/165 [00:08<00:09,  8.85it/s]\u001b[A\n",
            " 48%|████▊     | 80/165 [00:09<00:09,  8.87it/s]\u001b[A\n",
            " 49%|████▉     | 81/165 [00:09<00:09,  8.89it/s]\u001b[A\n",
            " 50%|████▉     | 82/165 [00:09<00:09,  8.87it/s]\u001b[A\n",
            " 50%|█████     | 83/165 [00:09<00:09,  8.86it/s]\u001b[A\n",
            " 51%|█████     | 84/165 [00:09<00:09,  8.85it/s]\u001b[A\n",
            " 52%|█████▏    | 85/165 [00:09<00:09,  8.84it/s]\u001b[A\n",
            " 52%|█████▏    | 86/165 [00:09<00:08,  8.85it/s]\u001b[A\n",
            " 53%|█████▎    | 87/165 [00:09<00:08,  8.85it/s]\u001b[A\n",
            " 53%|█████▎    | 88/165 [00:09<00:08,  8.84it/s]\u001b[A\n",
            " 54%|█████▍    | 89/165 [00:10<00:08,  8.85it/s]\u001b[A\n",
            " 55%|█████▍    | 90/165 [00:10<00:08,  8.86it/s]\u001b[A\n",
            " 55%|█████▌    | 91/165 [00:10<00:08,  8.85it/s]\u001b[A\n",
            " 56%|█████▌    | 92/165 [00:10<00:08,  8.85it/s]\u001b[A\n",
            " 56%|█████▋    | 93/165 [00:10<00:08,  8.85it/s]\u001b[A\n",
            " 57%|█████▋    | 94/165 [00:10<00:08,  8.86it/s]\u001b[A\n",
            " 58%|█████▊    | 95/165 [00:10<00:07,  8.84it/s]\u001b[A\n",
            " 58%|█████▊    | 96/165 [00:10<00:07,  8.83it/s]\u001b[A\n",
            " 59%|█████▉    | 97/165 [00:10<00:07,  8.84it/s]\u001b[A\n",
            " 59%|█████▉    | 98/165 [00:11<00:07,  8.86it/s]\u001b[A\n",
            " 60%|██████    | 99/165 [00:11<00:07,  8.86it/s]\u001b[A\n",
            " 61%|██████    | 100/165 [00:11<00:07,  8.86it/s]\u001b[A\n",
            " 61%|██████    | 101/165 [00:11<00:07,  8.84it/s]\u001b[A\n",
            " 62%|██████▏   | 102/165 [00:11<00:07,  8.85it/s]\u001b[A\n",
            " 62%|██████▏   | 103/165 [00:11<00:07,  8.85it/s]\u001b[A\n",
            " 63%|██████▎   | 104/165 [00:11<00:06,  8.86it/s]\u001b[A\n",
            " 64%|██████▎   | 105/165 [00:11<00:06,  8.86it/s]\u001b[A\n",
            " 64%|██████▍   | 106/165 [00:11<00:06,  8.86it/s]\u001b[A\n",
            " 65%|██████▍   | 107/165 [00:12<00:06,  8.87it/s]\u001b[A\n",
            " 65%|██████▌   | 108/165 [00:12<00:06,  8.88it/s]\u001b[A\n",
            " 66%|██████▌   | 109/165 [00:12<00:06,  8.87it/s]\u001b[A\n",
            " 67%|██████▋   | 110/165 [00:12<00:06,  8.87it/s]\u001b[A\n",
            " 67%|██████▋   | 111/165 [00:12<00:06,  8.88it/s]\u001b[A\n",
            " 68%|██████▊   | 112/165 [00:12<00:05,  8.87it/s]\u001b[A\n",
            " 68%|██████▊   | 113/165 [00:12<00:05,  8.86it/s]\u001b[A\n",
            " 69%|██████▉   | 114/165 [00:12<00:05,  8.88it/s]\u001b[A\n",
            " 70%|██████▉   | 115/165 [00:12<00:05,  8.84it/s]\u001b[A\n",
            " 70%|███████   | 116/165 [00:13<00:05,  8.84it/s]\u001b[A\n",
            " 71%|███████   | 117/165 [00:13<00:05,  8.83it/s]\u001b[A\n",
            " 72%|███████▏  | 118/165 [00:13<00:05,  8.78it/s]\u001b[A\n",
            " 72%|███████▏  | 119/165 [00:13<00:05,  8.80it/s]\u001b[A\n",
            " 73%|███████▎  | 120/165 [00:13<00:05,  8.83it/s]\u001b[A\n",
            " 73%|███████▎  | 121/165 [00:13<00:04,  8.83it/s]\u001b[A\n",
            " 74%|███████▍  | 122/165 [00:13<00:04,  8.84it/s]\u001b[A\n",
            " 75%|███████▍  | 123/165 [00:13<00:04,  8.85it/s]\u001b[A\n",
            " 75%|███████▌  | 124/165 [00:14<00:04,  8.86it/s]\u001b[A\n",
            " 76%|███████▌  | 125/165 [00:14<00:04,  8.88it/s]\u001b[A\n",
            " 76%|███████▋  | 126/165 [00:14<00:04,  8.89it/s]\u001b[A\n",
            " 77%|███████▋  | 127/165 [00:14<00:04,  8.89it/s]\u001b[A\n",
            " 78%|███████▊  | 128/165 [00:14<00:04,  8.90it/s]\u001b[A\n",
            " 78%|███████▊  | 129/165 [00:14<00:04,  8.89it/s]\u001b[A\n",
            " 79%|███████▉  | 130/165 [00:14<00:03,  8.86it/s]\u001b[A\n",
            " 79%|███████▉  | 131/165 [00:14<00:03,  8.87it/s]\u001b[A\n",
            " 80%|████████  | 132/165 [00:14<00:03,  8.86it/s]\u001b[A\n",
            " 81%|████████  | 133/165 [00:15<00:03,  8.87it/s]\u001b[A\n",
            " 81%|████████  | 134/165 [00:15<00:03,  8.87it/s]\u001b[A\n",
            " 82%|████████▏ | 135/165 [00:15<00:03,  8.87it/s]\u001b[A\n",
            " 82%|████████▏ | 136/165 [00:15<00:03,  8.86it/s]\u001b[A\n",
            " 83%|████████▎ | 137/165 [00:15<00:03,  8.85it/s]\u001b[A\n",
            " 84%|████████▎ | 138/165 [00:15<00:03,  8.82it/s]\u001b[A\n",
            " 84%|████████▍ | 139/165 [00:15<00:02,  8.84it/s]\u001b[A\n",
            " 85%|████████▍ | 140/165 [00:15<00:02,  8.83it/s]\u001b[A\n",
            " 85%|████████▌ | 141/165 [00:15<00:02,  8.86it/s]\u001b[A\n",
            " 86%|████████▌ | 142/165 [00:16<00:02,  8.85it/s]\u001b[A\n",
            " 87%|████████▋ | 143/165 [00:16<00:02,  8.83it/s]\u001b[A\n",
            " 87%|████████▋ | 144/165 [00:16<00:02,  8.85it/s]\u001b[A\n",
            " 88%|████████▊ | 145/165 [00:16<00:02,  8.87it/s]\u001b[A\n",
            " 88%|████████▊ | 146/165 [00:16<00:02,  8.88it/s]\u001b[A\n",
            " 89%|████████▉ | 147/165 [00:16<00:02,  8.87it/s]\u001b[A\n",
            " 90%|████████▉ | 148/165 [00:16<00:01,  8.89it/s]\u001b[A\n",
            " 90%|█████████ | 149/165 [00:16<00:01,  8.89it/s]\u001b[A\n",
            " 91%|█████████ | 150/165 [00:16<00:01,  8.89it/s]\u001b[A\n",
            " 92%|█████████▏| 151/165 [00:17<00:01,  8.87it/s]\u001b[A\n",
            " 92%|█████████▏| 152/165 [00:17<00:01,  8.89it/s]\u001b[A\n",
            " 93%|█████████▎| 153/165 [00:17<00:01,  8.92it/s]\u001b[A\n",
            " 93%|█████████▎| 154/165 [00:17<00:01,  8.92it/s]\u001b[A\n",
            " 94%|█████████▍| 155/165 [00:17<00:01,  8.92it/s]\u001b[A\n",
            " 95%|█████████▍| 156/165 [00:17<00:01,  8.91it/s]\u001b[A\n",
            " 95%|█████████▌| 157/165 [00:17<00:00,  8.90it/s]\u001b[A\n",
            " 96%|█████████▌| 158/165 [00:17<00:00,  8.90it/s]\u001b[A\n",
            " 96%|█████████▋| 159/165 [00:17<00:00,  8.87it/s]\u001b[A\n",
            " 97%|█████████▋| 160/165 [00:18<00:00,  8.87it/s]\u001b[A\n",
            " 98%|█████████▊| 161/165 [00:18<00:00,  8.88it/s]\u001b[A\n",
            " 98%|█████████▊| 162/165 [00:18<00:00,  8.89it/s]\u001b[A\n",
            " 99%|█████████▉| 163/165 [00:18<00:00,  8.89it/s]\u001b[A\n",
            "100%|██████████| 165/165 [00:18<00:00,  8.85it/s]\n",
            " 67%|██████▋   | 2/3 [1:38:11<49:06, 2946.01s/it]Didn't find file /content/activebus_bert_final/added_tokens.json. We won't load it.\n",
            "loading file /content/activebus_bert_final/vocab.txt\n",
            "loading file /content/activebus_bert_final/tokenizer.json\n",
            "loading file None\n",
            "loading file /content/activebus_bert_final/special_tokens_map.json\n",
            "loading file /content/activebus_bert_final/tokenizer_config.json\n",
            "========== fold: 0 training ==========\n",
            "========== fold: 0 training ==========\n",
            "========== fold: 0 training ==========\n",
            "========== fold: 0 training ==========\n",
            "========== fold: 0 training ==========\n",
            "========== fold: 0 training ==========\n",
            "========== fold: 0 training ==========\n",
            "========== fold: 0 training ==========\n",
            "========== fold: 0 training ==========\n",
            "========== fold: 0 training ==========\n",
            "========== fold: 0 training ==========\n",
            "========== fold: 0 training ==========\n",
            "Didn't find file /content/activebus_bert_final/added_tokens.json. We won't load it.\n",
            "loading file /content/activebus_bert_final/vocab.txt\n",
            "loading file None\n",
            "loading file /content/activebus_bert_final/special_tokens_map.json\n",
            "loading file /content/activebus_bert_final/tokenizer_config.json\n",
            "loading file /content/activebus_bert_final/tokenizer.json\n",
            "Didn't find file /content/activebus_bert_final/added_tokens.json. We won't load it.\n",
            "loading file /content/activebus_bert_final/vocab.txt\n",
            "loading file None\n",
            "loading file /content/activebus_bert_final/special_tokens_map.json\n",
            "loading file /content/activebus_bert_final/tokenizer_config.json\n",
            "loading file /content/activebus_bert_final/tokenizer.json\n",
            "loading configuration file /content/activebus_bert_final/config.json\n",
            "Model config BertConfig {\n",
            "  \"_name_or_path\": \"activebus/BERT-XD_Review\",\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_ids\": 0,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.15.0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "loading weights file /content/activebus_bert_final/pytorch_model.bin\n",
            "Some weights of the model checkpoint at /content/activebus_bert_final were not used when initializing BertForSequenceClassification: ['cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /content/activebus_bert_final and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: [1][0/306] Elapsed 0m 0s (remain 3m 8s) Loss: 0.7045 \n",
            "Epoch: [1][100/306] Elapsed 0m 34s (remain 1m 9s) Loss: 0.3571 \n",
            "Epoch: [1][200/306] Elapsed 1m 7s (remain 0m 35s) Loss: 0.2535 \n",
            "Epoch: [1][300/306] Elapsed 1m 40s (remain 0m 1s) Loss: 0.2192 \n",
            "Epoch: [1][305/306] Elapsed 1m 42s (remain 0m 0s) Loss: 0.2184 \n",
            "EVAL: [0/77] Elapsed 0m 0s (remain 0m 28s) Loss: 0.1968 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1 - avg_train_loss: 0.2184  avg_val_loss: 0.1580  time: 111s\n",
            "Epoch 1 - avg_train_loss: 0.2184  avg_val_loss: 0.1580  time: 111s\n",
            "Epoch 1 - avg_train_loss: 0.2184  avg_val_loss: 0.1580  time: 111s\n",
            "Epoch 1 - avg_train_loss: 0.2184  avg_val_loss: 0.1580  time: 111s\n",
            "Epoch 1 - avg_train_loss: 0.2184  avg_val_loss: 0.1580  time: 111s\n",
            "Epoch 1 - avg_train_loss: 0.2184  avg_val_loss: 0.1580  time: 111s\n",
            "Epoch 1 - avg_train_loss: 0.2184  avg_val_loss: 0.1580  time: 111s\n",
            "Epoch 1 - avg_train_loss: 0.2184  avg_val_loss: 0.1580  time: 111s\n",
            "Epoch 1 - avg_train_loss: 0.2184  avg_val_loss: 0.1580  time: 111s\n",
            "Epoch 1 - avg_train_loss: 0.2184  avg_val_loss: 0.1580  time: 111s\n",
            "Epoch 1 - avg_train_loss: 0.2184  avg_val_loss: 0.1580  time: 111s\n",
            "Epoch 1 - avg_train_loss: 0.2184  avg_val_loss: 0.1580  time: 111s\n",
            "Epoch 1 - Score: 0.1579806737033711\n",
            "Epoch 1 - Score: 0.1579806737033711\n",
            "Epoch 1 - Score: 0.1579806737033711\n",
            "Epoch 1 - Score: 0.1579806737033711\n",
            "Epoch 1 - Score: 0.1579806737033711\n",
            "Epoch 1 - Score: 0.1579806737033711\n",
            "Epoch 1 - Score: 0.1579806737033711\n",
            "Epoch 1 - Score: 0.1579806737033711\n",
            "Epoch 1 - Score: 0.1579806737033711\n",
            "Epoch 1 - Score: 0.1579806737033711\n",
            "Epoch 1 - Score: 0.1579806737033711\n",
            "Epoch 1 - Score: 0.1579806737033711\n",
            "Epoch 1 - Save Best Score: 0.1580 Model\n",
            "Epoch 1 - Save Best Score: 0.1580 Model\n",
            "Epoch 1 - Save Best Score: 0.1580 Model\n",
            "Epoch 1 - Save Best Score: 0.1580 Model\n",
            "Epoch 1 - Save Best Score: 0.1580 Model\n",
            "Epoch 1 - Save Best Score: 0.1580 Model\n",
            "Epoch 1 - Save Best Score: 0.1580 Model\n",
            "Epoch 1 - Save Best Score: 0.1580 Model\n",
            "Epoch 1 - Save Best Score: 0.1580 Model\n",
            "Epoch 1 - Save Best Score: 0.1580 Model\n",
            "Epoch 1 - Save Best Score: 0.1580 Model\n",
            "Epoch 1 - Save Best Score: 0.1580 Model\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EVAL: [76/77] Elapsed 0m 8s (remain 0m 0s) Loss: 0.1580 \n",
            "Log loss for 0 is 0.1579806737033711\n",
            "Epoch: [2][0/306] Elapsed 0m 0s (remain 3m 2s) Loss: 0.0708 \n",
            "Epoch: [2][100/306] Elapsed 0m 34s (remain 1m 9s) Loss: 0.1073 \n",
            "Epoch: [2][200/306] Elapsed 1m 7s (remain 0m 35s) Loss: 0.1073 \n",
            "Epoch: [2][300/306] Elapsed 1m 40s (remain 0m 1s) Loss: 0.0998 \n",
            "Epoch: [2][305/306] Elapsed 1m 42s (remain 0m 0s) Loss: 0.0985 \n",
            "EVAL: [0/77] Elapsed 0m 0s (remain 0m 28s) Loss: 0.1978 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2 - avg_train_loss: 0.0985  avg_val_loss: 0.1784  time: 111s\n",
            "Epoch 2 - avg_train_loss: 0.0985  avg_val_loss: 0.1784  time: 111s\n",
            "Epoch 2 - avg_train_loss: 0.0985  avg_val_loss: 0.1784  time: 111s\n",
            "Epoch 2 - avg_train_loss: 0.0985  avg_val_loss: 0.1784  time: 111s\n",
            "Epoch 2 - avg_train_loss: 0.0985  avg_val_loss: 0.1784  time: 111s\n",
            "Epoch 2 - avg_train_loss: 0.0985  avg_val_loss: 0.1784  time: 111s\n",
            "Epoch 2 - avg_train_loss: 0.0985  avg_val_loss: 0.1784  time: 111s\n",
            "Epoch 2 - avg_train_loss: 0.0985  avg_val_loss: 0.1784  time: 111s\n",
            "Epoch 2 - avg_train_loss: 0.0985  avg_val_loss: 0.1784  time: 111s\n",
            "Epoch 2 - avg_train_loss: 0.0985  avg_val_loss: 0.1784  time: 111s\n",
            "Epoch 2 - avg_train_loss: 0.0985  avg_val_loss: 0.1784  time: 111s\n",
            "Epoch 2 - avg_train_loss: 0.0985  avg_val_loss: 0.1784  time: 111s\n",
            "Epoch 2 - Score: 0.17835997389204775\n",
            "Epoch 2 - Score: 0.17835997389204775\n",
            "Epoch 2 - Score: 0.17835997389204775\n",
            "Epoch 2 - Score: 0.17835997389204775\n",
            "Epoch 2 - Score: 0.17835997389204775\n",
            "Epoch 2 - Score: 0.17835997389204775\n",
            "Epoch 2 - Score: 0.17835997389204775\n",
            "Epoch 2 - Score: 0.17835997389204775\n",
            "Epoch 2 - Score: 0.17835997389204775\n",
            "Epoch 2 - Score: 0.17835997389204775\n",
            "Epoch 2 - Score: 0.17835997389204775\n",
            "Epoch 2 - Score: 0.17835997389204775\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EVAL: [76/77] Elapsed 0m 8s (remain 0m 0s) Loss: 0.1784 \n",
            "Log loss for 1 is 0.17835997389204775\n",
            "Epoch: [3][0/306] Elapsed 0m 0s (remain 3m 11s) Loss: 0.0093 \n",
            "Epoch: [3][100/306] Elapsed 0m 34s (remain 1m 9s) Loss: 0.0617 \n",
            "Epoch: [3][200/306] Elapsed 1m 7s (remain 0m 35s) Loss: 0.0680 \n",
            "Epoch: [3][300/306] Elapsed 1m 41s (remain 0m 1s) Loss: 0.0660 \n",
            "Epoch: [3][305/306] Elapsed 1m 42s (remain 0m 0s) Loss: 0.0655 \n",
            "EVAL: [0/77] Elapsed 0m 0s (remain 0m 27s) Loss: 0.1993 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3 - avg_train_loss: 0.0655  avg_val_loss: 0.2108  time: 112s\n",
            "Epoch 3 - avg_train_loss: 0.0655  avg_val_loss: 0.2108  time: 112s\n",
            "Epoch 3 - avg_train_loss: 0.0655  avg_val_loss: 0.2108  time: 112s\n",
            "Epoch 3 - avg_train_loss: 0.0655  avg_val_loss: 0.2108  time: 112s\n",
            "Epoch 3 - avg_train_loss: 0.0655  avg_val_loss: 0.2108  time: 112s\n",
            "Epoch 3 - avg_train_loss: 0.0655  avg_val_loss: 0.2108  time: 112s\n",
            "Epoch 3 - avg_train_loss: 0.0655  avg_val_loss: 0.2108  time: 112s\n",
            "Epoch 3 - avg_train_loss: 0.0655  avg_val_loss: 0.2108  time: 112s\n",
            "Epoch 3 - avg_train_loss: 0.0655  avg_val_loss: 0.2108  time: 112s\n",
            "Epoch 3 - avg_train_loss: 0.0655  avg_val_loss: 0.2108  time: 112s\n",
            "Epoch 3 - avg_train_loss: 0.0655  avg_val_loss: 0.2108  time: 112s\n",
            "Epoch 3 - avg_train_loss: 0.0655  avg_val_loss: 0.2108  time: 112s\n",
            "Epoch 3 - Score: 0.2107810964942485\n",
            "Epoch 3 - Score: 0.2107810964942485\n",
            "Epoch 3 - Score: 0.2107810964942485\n",
            "Epoch 3 - Score: 0.2107810964942485\n",
            "Epoch 3 - Score: 0.2107810964942485\n",
            "Epoch 3 - Score: 0.2107810964942485\n",
            "Epoch 3 - Score: 0.2107810964942485\n",
            "Epoch 3 - Score: 0.2107810964942485\n",
            "Epoch 3 - Score: 0.2107810964942485\n",
            "Epoch 3 - Score: 0.2107810964942485\n",
            "Epoch 3 - Score: 0.2107810964942485\n",
            "Epoch 3 - Score: 0.2107810964942485\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EVAL: [76/77] Elapsed 0m 8s (remain 0m 0s) Loss: 0.2108 \n",
            "Log loss for 2 is 0.2107810964942485\n",
            "Epoch: [4][0/306] Elapsed 0m 0s (remain 3m 8s) Loss: 0.0145 \n",
            "Epoch: [4][100/306] Elapsed 0m 34s (remain 1m 9s) Loss: 0.0506 \n",
            "Epoch: [4][200/306] Elapsed 1m 7s (remain 0m 35s) Loss: 0.0463 \n",
            "Epoch: [4][300/306] Elapsed 1m 40s (remain 0m 1s) Loss: 0.0490 \n",
            "Epoch: [4][305/306] Elapsed 1m 42s (remain 0m 0s) Loss: 0.0490 \n",
            "EVAL: [0/77] Elapsed 0m 0s (remain 0m 27s) Loss: 0.2528 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4 - avg_train_loss: 0.0490  avg_val_loss: 0.1916  time: 111s\n",
            "Epoch 4 - avg_train_loss: 0.0490  avg_val_loss: 0.1916  time: 111s\n",
            "Epoch 4 - avg_train_loss: 0.0490  avg_val_loss: 0.1916  time: 111s\n",
            "Epoch 4 - avg_train_loss: 0.0490  avg_val_loss: 0.1916  time: 111s\n",
            "Epoch 4 - avg_train_loss: 0.0490  avg_val_loss: 0.1916  time: 111s\n",
            "Epoch 4 - avg_train_loss: 0.0490  avg_val_loss: 0.1916  time: 111s\n",
            "Epoch 4 - avg_train_loss: 0.0490  avg_val_loss: 0.1916  time: 111s\n",
            "Epoch 4 - avg_train_loss: 0.0490  avg_val_loss: 0.1916  time: 111s\n",
            "Epoch 4 - avg_train_loss: 0.0490  avg_val_loss: 0.1916  time: 111s\n",
            "Epoch 4 - avg_train_loss: 0.0490  avg_val_loss: 0.1916  time: 111s\n",
            "Epoch 4 - avg_train_loss: 0.0490  avg_val_loss: 0.1916  time: 111s\n",
            "Epoch 4 - avg_train_loss: 0.0490  avg_val_loss: 0.1916  time: 111s\n",
            "Epoch 4 - Score: 0.19161539198660082\n",
            "Epoch 4 - Score: 0.19161539198660082\n",
            "Epoch 4 - Score: 0.19161539198660082\n",
            "Epoch 4 - Score: 0.19161539198660082\n",
            "Epoch 4 - Score: 0.19161539198660082\n",
            "Epoch 4 - Score: 0.19161539198660082\n",
            "Epoch 4 - Score: 0.19161539198660082\n",
            "Epoch 4 - Score: 0.19161539198660082\n",
            "Epoch 4 - Score: 0.19161539198660082\n",
            "Epoch 4 - Score: 0.19161539198660082\n",
            "Epoch 4 - Score: 0.19161539198660082\n",
            "Epoch 4 - Score: 0.19161539198660082\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EVAL: [76/77] Elapsed 0m 8s (remain 0m 0s) Loss: 0.1916 \n",
            "Log loss for 3 is 0.19161539198660082\n",
            "Epoch: [5][0/306] Elapsed 0m 0s (remain 3m 11s) Loss: 0.0093 \n",
            "Epoch: [5][100/306] Elapsed 0m 34s (remain 1m 9s) Loss: 0.0215 \n",
            "Epoch: [5][200/306] Elapsed 1m 7s (remain 0m 35s) Loss: 0.0237 \n",
            "Epoch: [5][300/306] Elapsed 1m 41s (remain 0m 1s) Loss: 0.0239 \n",
            "Epoch: [5][305/306] Elapsed 1m 42s (remain 0m 0s) Loss: 0.0239 \n",
            "EVAL: [0/77] Elapsed 0m 0s (remain 0m 27s) Loss: 0.4312 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5 - avg_train_loss: 0.0239  avg_val_loss: 0.2902  time: 112s\n",
            "Epoch 5 - avg_train_loss: 0.0239  avg_val_loss: 0.2902  time: 112s\n",
            "Epoch 5 - avg_train_loss: 0.0239  avg_val_loss: 0.2902  time: 112s\n",
            "Epoch 5 - avg_train_loss: 0.0239  avg_val_loss: 0.2902  time: 112s\n",
            "Epoch 5 - avg_train_loss: 0.0239  avg_val_loss: 0.2902  time: 112s\n",
            "Epoch 5 - avg_train_loss: 0.0239  avg_val_loss: 0.2902  time: 112s\n",
            "Epoch 5 - avg_train_loss: 0.0239  avg_val_loss: 0.2902  time: 112s\n",
            "Epoch 5 - avg_train_loss: 0.0239  avg_val_loss: 0.2902  time: 112s\n",
            "Epoch 5 - avg_train_loss: 0.0239  avg_val_loss: 0.2902  time: 112s\n",
            "Epoch 5 - avg_train_loss: 0.0239  avg_val_loss: 0.2902  time: 112s\n",
            "Epoch 5 - avg_train_loss: 0.0239  avg_val_loss: 0.2902  time: 112s\n",
            "Epoch 5 - avg_train_loss: 0.0239  avg_val_loss: 0.2902  time: 112s\n",
            "Epoch 5 - Score: 0.2901877051919258\n",
            "Epoch 5 - Score: 0.2901877051919258\n",
            "Epoch 5 - Score: 0.2901877051919258\n",
            "Epoch 5 - Score: 0.2901877051919258\n",
            "Epoch 5 - Score: 0.2901877051919258\n",
            "Epoch 5 - Score: 0.2901877051919258\n",
            "Epoch 5 - Score: 0.2901877051919258\n",
            "Epoch 5 - Score: 0.2901877051919258\n",
            "Epoch 5 - Score: 0.2901877051919258\n",
            "Epoch 5 - Score: 0.2901877051919258\n",
            "Epoch 5 - Score: 0.2901877051919258\n",
            "Epoch 5 - Score: 0.2901877051919258\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EVAL: [76/77] Elapsed 0m 8s (remain 0m 0s) Loss: 0.2902 \n",
            "Log loss for 4 is 0.2901877051919258\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "========== fold: 0 result ==========\n",
            "========== fold: 0 result ==========\n",
            "========== fold: 0 result ==========\n",
            "========== fold: 0 result ==========\n",
            "========== fold: 0 result ==========\n",
            "========== fold: 0 result ==========\n",
            "========== fold: 0 result ==========\n",
            "========== fold: 0 result ==========\n",
            "========== fold: 0 result ==========\n",
            "========== fold: 0 result ==========\n",
            "========== fold: 0 result ==========\n",
            "========== fold: 0 result ==========\n",
            "Score: 0.15798\n",
            "Score: 0.15798\n",
            "Score: 0.15798\n",
            "Score: 0.15798\n",
            "Score: 0.15798\n",
            "Score: 0.15798\n",
            "Score: 0.15798\n",
            "Score: 0.15798\n",
            "Score: 0.15798\n",
            "Score: 0.15798\n",
            "Score: 0.15798\n",
            "Score: 0.15798\n",
            "========== fold: 1 training ==========\n",
            "========== fold: 1 training ==========\n",
            "========== fold: 1 training ==========\n",
            "========== fold: 1 training ==========\n",
            "========== fold: 1 training ==========\n",
            "========== fold: 1 training ==========\n",
            "========== fold: 1 training ==========\n",
            "========== fold: 1 training ==========\n",
            "========== fold: 1 training ==========\n",
            "========== fold: 1 training ==========\n",
            "========== fold: 1 training ==========\n",
            "========== fold: 1 training ==========\n",
            "Didn't find file /content/activebus_bert_final/added_tokens.json. We won't load it.\n",
            "loading file /content/activebus_bert_final/vocab.txt\n",
            "loading file None\n",
            "loading file /content/activebus_bert_final/special_tokens_map.json\n",
            "loading file /content/activebus_bert_final/tokenizer_config.json\n",
            "loading file /content/activebus_bert_final/tokenizer.json\n",
            "Didn't find file /content/activebus_bert_final/added_tokens.json. We won't load it.\n",
            "loading file /content/activebus_bert_final/vocab.txt\n",
            "loading file None\n",
            "loading file /content/activebus_bert_final/special_tokens_map.json\n",
            "loading file /content/activebus_bert_final/tokenizer_config.json\n",
            "loading file /content/activebus_bert_final/tokenizer.json\n",
            "loading configuration file /content/activebus_bert_final/config.json\n",
            "Model config BertConfig {\n",
            "  \"_name_or_path\": \"activebus/BERT-XD_Review\",\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_ids\": 0,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.15.0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "loading weights file /content/activebus_bert_final/pytorch_model.bin\n",
            "Some weights of the model checkpoint at /content/activebus_bert_final were not used when initializing BertForSequenceClassification: ['cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /content/activebus_bert_final and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: [1][0/306] Elapsed 0m 0s (remain 3m 10s) Loss: 0.6553 \n",
            "Epoch: [1][100/306] Elapsed 0m 34s (remain 1m 9s) Loss: 0.3339 \n",
            "Epoch: [1][200/306] Elapsed 1m 7s (remain 0m 35s) Loss: 0.2566 \n",
            "Epoch: [1][300/306] Elapsed 1m 41s (remain 0m 1s) Loss: 0.2243 \n",
            "Epoch: [1][305/306] Elapsed 1m 42s (remain 0m 0s) Loss: 0.2231 \n",
            "EVAL: [0/77] Elapsed 0m 0s (remain 0m 29s) Loss: 0.2058 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1 - avg_train_loss: 0.2231  avg_val_loss: 0.1624  time: 112s\n",
            "Epoch 1 - avg_train_loss: 0.2231  avg_val_loss: 0.1624  time: 112s\n",
            "Epoch 1 - avg_train_loss: 0.2231  avg_val_loss: 0.1624  time: 112s\n",
            "Epoch 1 - avg_train_loss: 0.2231  avg_val_loss: 0.1624  time: 112s\n",
            "Epoch 1 - avg_train_loss: 0.2231  avg_val_loss: 0.1624  time: 112s\n",
            "Epoch 1 - avg_train_loss: 0.2231  avg_val_loss: 0.1624  time: 112s\n",
            "Epoch 1 - avg_train_loss: 0.2231  avg_val_loss: 0.1624  time: 112s\n",
            "Epoch 1 - avg_train_loss: 0.2231  avg_val_loss: 0.1624  time: 112s\n",
            "Epoch 1 - avg_train_loss: 0.2231  avg_val_loss: 0.1624  time: 112s\n",
            "Epoch 1 - avg_train_loss: 0.2231  avg_val_loss: 0.1624  time: 112s\n",
            "Epoch 1 - avg_train_loss: 0.2231  avg_val_loss: 0.1624  time: 112s\n",
            "Epoch 1 - avg_train_loss: 0.2231  avg_val_loss: 0.1624  time: 112s\n",
            "Epoch 1 - Score: 0.1623548714371175\n",
            "Epoch 1 - Score: 0.1623548714371175\n",
            "Epoch 1 - Score: 0.1623548714371175\n",
            "Epoch 1 - Score: 0.1623548714371175\n",
            "Epoch 1 - Score: 0.1623548714371175\n",
            "Epoch 1 - Score: 0.1623548714371175\n",
            "Epoch 1 - Score: 0.1623548714371175\n",
            "Epoch 1 - Score: 0.1623548714371175\n",
            "Epoch 1 - Score: 0.1623548714371175\n",
            "Epoch 1 - Score: 0.1623548714371175\n",
            "Epoch 1 - Score: 0.1623548714371175\n",
            "Epoch 1 - Score: 0.1623548714371175\n",
            "Epoch 1 - Save Best Score: 0.1624 Model\n",
            "Epoch 1 - Save Best Score: 0.1624 Model\n",
            "Epoch 1 - Save Best Score: 0.1624 Model\n",
            "Epoch 1 - Save Best Score: 0.1624 Model\n",
            "Epoch 1 - Save Best Score: 0.1624 Model\n",
            "Epoch 1 - Save Best Score: 0.1624 Model\n",
            "Epoch 1 - Save Best Score: 0.1624 Model\n",
            "Epoch 1 - Save Best Score: 0.1624 Model\n",
            "Epoch 1 - Save Best Score: 0.1624 Model\n",
            "Epoch 1 - Save Best Score: 0.1624 Model\n",
            "Epoch 1 - Save Best Score: 0.1624 Model\n",
            "Epoch 1 - Save Best Score: 0.1624 Model\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EVAL: [76/77] Elapsed 0m 8s (remain 0m 0s) Loss: 0.1624 \n",
            "Log loss for 0 is 0.1623548714371175\n",
            "Epoch: [2][0/306] Elapsed 0m 0s (remain 3m 5s) Loss: 0.0888 \n",
            "Epoch: [2][100/306] Elapsed 0m 34s (remain 1m 9s) Loss: 0.1224 \n",
            "Epoch: [2][200/306] Elapsed 1m 7s (remain 0m 35s) Loss: 0.1172 \n",
            "Epoch: [2][300/306] Elapsed 1m 40s (remain 0m 1s) Loss: 0.1143 \n",
            "Epoch: [2][305/306] Elapsed 1m 42s (remain 0m 0s) Loss: 0.1157 \n",
            "EVAL: [0/77] Elapsed 0m 0s (remain 0m 29s) Loss: 0.0641 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2 - avg_train_loss: 0.1157  avg_val_loss: 0.1315  time: 111s\n",
            "Epoch 2 - avg_train_loss: 0.1157  avg_val_loss: 0.1315  time: 111s\n",
            "Epoch 2 - avg_train_loss: 0.1157  avg_val_loss: 0.1315  time: 111s\n",
            "Epoch 2 - avg_train_loss: 0.1157  avg_val_loss: 0.1315  time: 111s\n",
            "Epoch 2 - avg_train_loss: 0.1157  avg_val_loss: 0.1315  time: 111s\n",
            "Epoch 2 - avg_train_loss: 0.1157  avg_val_loss: 0.1315  time: 111s\n",
            "Epoch 2 - avg_train_loss: 0.1157  avg_val_loss: 0.1315  time: 111s\n",
            "Epoch 2 - avg_train_loss: 0.1157  avg_val_loss: 0.1315  time: 111s\n",
            "Epoch 2 - avg_train_loss: 0.1157  avg_val_loss: 0.1315  time: 111s\n",
            "Epoch 2 - avg_train_loss: 0.1157  avg_val_loss: 0.1315  time: 111s\n",
            "Epoch 2 - avg_train_loss: 0.1157  avg_val_loss: 0.1315  time: 111s\n",
            "Epoch 2 - avg_train_loss: 0.1157  avg_val_loss: 0.1315  time: 111s\n",
            "Epoch 2 - Score: 0.13154889282807647\n",
            "Epoch 2 - Score: 0.13154889282807647\n",
            "Epoch 2 - Score: 0.13154889282807647\n",
            "Epoch 2 - Score: 0.13154889282807647\n",
            "Epoch 2 - Score: 0.13154889282807647\n",
            "Epoch 2 - Score: 0.13154889282807647\n",
            "Epoch 2 - Score: 0.13154889282807647\n",
            "Epoch 2 - Score: 0.13154889282807647\n",
            "Epoch 2 - Score: 0.13154889282807647\n",
            "Epoch 2 - Score: 0.13154889282807647\n",
            "Epoch 2 - Score: 0.13154889282807647\n",
            "Epoch 2 - Score: 0.13154889282807647\n",
            "Epoch 2 - Save Best Score: 0.1315 Model\n",
            "Epoch 2 - Save Best Score: 0.1315 Model\n",
            "Epoch 2 - Save Best Score: 0.1315 Model\n",
            "Epoch 2 - Save Best Score: 0.1315 Model\n",
            "Epoch 2 - Save Best Score: 0.1315 Model\n",
            "Epoch 2 - Save Best Score: 0.1315 Model\n",
            "Epoch 2 - Save Best Score: 0.1315 Model\n",
            "Epoch 2 - Save Best Score: 0.1315 Model\n",
            "Epoch 2 - Save Best Score: 0.1315 Model\n",
            "Epoch 2 - Save Best Score: 0.1315 Model\n",
            "Epoch 2 - Save Best Score: 0.1315 Model\n",
            "Epoch 2 - Save Best Score: 0.1315 Model\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EVAL: [76/77] Elapsed 0m 8s (remain 0m 0s) Loss: 0.1315 \n",
            "Log loss for 1 is 0.13154889282807647\n",
            "Epoch: [3][0/306] Elapsed 0m 0s (remain 3m 4s) Loss: 0.0877 \n",
            "Epoch: [3][100/306] Elapsed 0m 34s (remain 1m 9s) Loss: 0.0634 \n",
            "Epoch: [3][200/306] Elapsed 1m 7s (remain 0m 35s) Loss: 0.0700 \n",
            "Epoch: [3][300/306] Elapsed 1m 41s (remain 0m 1s) Loss: 0.0698 \n",
            "Epoch: [3][305/306] Elapsed 1m 42s (remain 0m 0s) Loss: 0.0694 \n",
            "EVAL: [0/77] Elapsed 0m 0s (remain 0m 27s) Loss: 0.2416 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3 - avg_train_loss: 0.0694  avg_val_loss: 0.1729  time: 111s\n",
            "Epoch 3 - avg_train_loss: 0.0694  avg_val_loss: 0.1729  time: 111s\n",
            "Epoch 3 - avg_train_loss: 0.0694  avg_val_loss: 0.1729  time: 111s\n",
            "Epoch 3 - avg_train_loss: 0.0694  avg_val_loss: 0.1729  time: 111s\n",
            "Epoch 3 - avg_train_loss: 0.0694  avg_val_loss: 0.1729  time: 111s\n",
            "Epoch 3 - avg_train_loss: 0.0694  avg_val_loss: 0.1729  time: 111s\n",
            "Epoch 3 - avg_train_loss: 0.0694  avg_val_loss: 0.1729  time: 111s\n",
            "Epoch 3 - avg_train_loss: 0.0694  avg_val_loss: 0.1729  time: 111s\n",
            "Epoch 3 - avg_train_loss: 0.0694  avg_val_loss: 0.1729  time: 111s\n",
            "Epoch 3 - avg_train_loss: 0.0694  avg_val_loss: 0.1729  time: 111s\n",
            "Epoch 3 - avg_train_loss: 0.0694  avg_val_loss: 0.1729  time: 111s\n",
            "Epoch 3 - avg_train_loss: 0.0694  avg_val_loss: 0.1729  time: 111s\n",
            "Epoch 3 - Score: 0.17285706580928872\n",
            "Epoch 3 - Score: 0.17285706580928872\n",
            "Epoch 3 - Score: 0.17285706580928872\n",
            "Epoch 3 - Score: 0.17285706580928872\n",
            "Epoch 3 - Score: 0.17285706580928872\n",
            "Epoch 3 - Score: 0.17285706580928872\n",
            "Epoch 3 - Score: 0.17285706580928872\n",
            "Epoch 3 - Score: 0.17285706580928872\n",
            "Epoch 3 - Score: 0.17285706580928872\n",
            "Epoch 3 - Score: 0.17285706580928872\n",
            "Epoch 3 - Score: 0.17285706580928872\n",
            "Epoch 3 - Score: 0.17285706580928872\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EVAL: [76/77] Elapsed 0m 8s (remain 0m 0s) Loss: 0.1729 \n",
            "Log loss for 2 is 0.17285706580928872\n",
            "Epoch: [4][0/306] Elapsed 0m 0s (remain 3m 8s) Loss: 0.0040 \n",
            "Epoch: [4][100/306] Elapsed 0m 34s (remain 1m 9s) Loss: 0.0421 \n",
            "Epoch: [4][200/306] Elapsed 1m 7s (remain 0m 35s) Loss: 0.0465 \n",
            "Epoch: [4][300/306] Elapsed 1m 41s (remain 0m 1s) Loss: 0.0440 \n",
            "Epoch: [4][305/306] Elapsed 1m 42s (remain 0m 0s) Loss: 0.0434 \n",
            "EVAL: [0/77] Elapsed 0m 0s (remain 0m 26s) Loss: 0.0165 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4 - avg_train_loss: 0.0434  avg_val_loss: 0.1949  time: 112s\n",
            "Epoch 4 - avg_train_loss: 0.0434  avg_val_loss: 0.1949  time: 112s\n",
            "Epoch 4 - avg_train_loss: 0.0434  avg_val_loss: 0.1949  time: 112s\n",
            "Epoch 4 - avg_train_loss: 0.0434  avg_val_loss: 0.1949  time: 112s\n",
            "Epoch 4 - avg_train_loss: 0.0434  avg_val_loss: 0.1949  time: 112s\n",
            "Epoch 4 - avg_train_loss: 0.0434  avg_val_loss: 0.1949  time: 112s\n",
            "Epoch 4 - avg_train_loss: 0.0434  avg_val_loss: 0.1949  time: 112s\n",
            "Epoch 4 - avg_train_loss: 0.0434  avg_val_loss: 0.1949  time: 112s\n",
            "Epoch 4 - avg_train_loss: 0.0434  avg_val_loss: 0.1949  time: 112s\n",
            "Epoch 4 - avg_train_loss: 0.0434  avg_val_loss: 0.1949  time: 112s\n",
            "Epoch 4 - avg_train_loss: 0.0434  avg_val_loss: 0.1949  time: 112s\n",
            "Epoch 4 - avg_train_loss: 0.0434  avg_val_loss: 0.1949  time: 112s\n",
            "Epoch 4 - Score: 0.1949008660293818\n",
            "Epoch 4 - Score: 0.1949008660293818\n",
            "Epoch 4 - Score: 0.1949008660293818\n",
            "Epoch 4 - Score: 0.1949008660293818\n",
            "Epoch 4 - Score: 0.1949008660293818\n",
            "Epoch 4 - Score: 0.1949008660293818\n",
            "Epoch 4 - Score: 0.1949008660293818\n",
            "Epoch 4 - Score: 0.1949008660293818\n",
            "Epoch 4 - Score: 0.1949008660293818\n",
            "Epoch 4 - Score: 0.1949008660293818\n",
            "Epoch 4 - Score: 0.1949008660293818\n",
            "Epoch 4 - Score: 0.1949008660293818\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EVAL: [76/77] Elapsed 0m 8s (remain 0m 0s) Loss: 0.1949 \n",
            "Log loss for 3 is 0.1949008660293818\n",
            "Epoch: [5][0/306] Elapsed 0m 0s (remain 3m 5s) Loss: 0.0041 \n",
            "Epoch: [5][100/306] Elapsed 0m 34s (remain 1m 9s) Loss: 0.0218 \n",
            "Epoch: [5][200/306] Elapsed 1m 7s (remain 0m 35s) Loss: 0.0260 \n",
            "Epoch: [5][300/306] Elapsed 1m 41s (remain 0m 1s) Loss: 0.0261 \n",
            "Epoch: [5][305/306] Elapsed 1m 42s (remain 0m 0s) Loss: 0.0258 \n",
            "EVAL: [0/77] Elapsed 0m 0s (remain 0m 28s) Loss: 0.3168 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5 - avg_train_loss: 0.0258  avg_val_loss: 0.2430  time: 112s\n",
            "Epoch 5 - avg_train_loss: 0.0258  avg_val_loss: 0.2430  time: 112s\n",
            "Epoch 5 - avg_train_loss: 0.0258  avg_val_loss: 0.2430  time: 112s\n",
            "Epoch 5 - avg_train_loss: 0.0258  avg_val_loss: 0.2430  time: 112s\n",
            "Epoch 5 - avg_train_loss: 0.0258  avg_val_loss: 0.2430  time: 112s\n",
            "Epoch 5 - avg_train_loss: 0.0258  avg_val_loss: 0.2430  time: 112s\n",
            "Epoch 5 - avg_train_loss: 0.0258  avg_val_loss: 0.2430  time: 112s\n",
            "Epoch 5 - avg_train_loss: 0.0258  avg_val_loss: 0.2430  time: 112s\n",
            "Epoch 5 - avg_train_loss: 0.0258  avg_val_loss: 0.2430  time: 112s\n",
            "Epoch 5 - avg_train_loss: 0.0258  avg_val_loss: 0.2430  time: 112s\n",
            "Epoch 5 - avg_train_loss: 0.0258  avg_val_loss: 0.2430  time: 112s\n",
            "Epoch 5 - avg_train_loss: 0.0258  avg_val_loss: 0.2430  time: 112s\n",
            "Epoch 5 - Score: 0.24304642584795375\n",
            "Epoch 5 - Score: 0.24304642584795375\n",
            "Epoch 5 - Score: 0.24304642584795375\n",
            "Epoch 5 - Score: 0.24304642584795375\n",
            "Epoch 5 - Score: 0.24304642584795375\n",
            "Epoch 5 - Score: 0.24304642584795375\n",
            "Epoch 5 - Score: 0.24304642584795375\n",
            "Epoch 5 - Score: 0.24304642584795375\n",
            "Epoch 5 - Score: 0.24304642584795375\n",
            "Epoch 5 - Score: 0.24304642584795375\n",
            "Epoch 5 - Score: 0.24304642584795375\n",
            "Epoch 5 - Score: 0.24304642584795375\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EVAL: [76/77] Elapsed 0m 8s (remain 0m 0s) Loss: 0.2430 \n",
            "Log loss for 4 is 0.24304642584795375\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "========== fold: 1 result ==========\n",
            "========== fold: 1 result ==========\n",
            "========== fold: 1 result ==========\n",
            "========== fold: 1 result ==========\n",
            "========== fold: 1 result ==========\n",
            "========== fold: 1 result ==========\n",
            "========== fold: 1 result ==========\n",
            "========== fold: 1 result ==========\n",
            "========== fold: 1 result ==========\n",
            "========== fold: 1 result ==========\n",
            "========== fold: 1 result ==========\n",
            "========== fold: 1 result ==========\n",
            "Score: 0.13155\n",
            "Score: 0.13155\n",
            "Score: 0.13155\n",
            "Score: 0.13155\n",
            "Score: 0.13155\n",
            "Score: 0.13155\n",
            "Score: 0.13155\n",
            "Score: 0.13155\n",
            "Score: 0.13155\n",
            "Score: 0.13155\n",
            "Score: 0.13155\n",
            "Score: 0.13155\n",
            "========== fold: 2 training ==========\n",
            "========== fold: 2 training ==========\n",
            "========== fold: 2 training ==========\n",
            "========== fold: 2 training ==========\n",
            "========== fold: 2 training ==========\n",
            "========== fold: 2 training ==========\n",
            "========== fold: 2 training ==========\n",
            "========== fold: 2 training ==========\n",
            "========== fold: 2 training ==========\n",
            "========== fold: 2 training ==========\n",
            "========== fold: 2 training ==========\n",
            "========== fold: 2 training ==========\n",
            "Didn't find file /content/activebus_bert_final/added_tokens.json. We won't load it.\n",
            "loading file /content/activebus_bert_final/vocab.txt\n",
            "loading file None\n",
            "loading file /content/activebus_bert_final/special_tokens_map.json\n",
            "loading file /content/activebus_bert_final/tokenizer_config.json\n",
            "loading file /content/activebus_bert_final/tokenizer.json\n",
            "Didn't find file /content/activebus_bert_final/added_tokens.json. We won't load it.\n",
            "loading file /content/activebus_bert_final/vocab.txt\n",
            "loading file None\n",
            "loading file /content/activebus_bert_final/special_tokens_map.json\n",
            "loading file /content/activebus_bert_final/tokenizer_config.json\n",
            "loading file /content/activebus_bert_final/tokenizer.json\n",
            "loading configuration file /content/activebus_bert_final/config.json\n",
            "Model config BertConfig {\n",
            "  \"_name_or_path\": \"activebus/BERT-XD_Review\",\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_ids\": 0,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.15.0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "loading weights file /content/activebus_bert_final/pytorch_model.bin\n",
            "Some weights of the model checkpoint at /content/activebus_bert_final were not used when initializing BertForSequenceClassification: ['cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /content/activebus_bert_final and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: [1][0/306] Elapsed 0m 0s (remain 3m 5s) Loss: 0.7146 \n",
            "Epoch: [1][100/306] Elapsed 0m 34s (remain 1m 9s) Loss: 0.3826 \n",
            "Epoch: [1][200/306] Elapsed 1m 7s (remain 0m 35s) Loss: 0.2800 \n",
            "Epoch: [1][300/306] Elapsed 1m 40s (remain 0m 1s) Loss: 0.2371 \n",
            "Epoch: [1][305/306] Elapsed 1m 42s (remain 0m 0s) Loss: 0.2350 \n",
            "EVAL: [0/77] Elapsed 0m 0s (remain 0m 27s) Loss: 0.2247 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1 - avg_train_loss: 0.2350  avg_val_loss: 0.1384  time: 111s\n",
            "Epoch 1 - avg_train_loss: 0.2350  avg_val_loss: 0.1384  time: 111s\n",
            "Epoch 1 - avg_train_loss: 0.2350  avg_val_loss: 0.1384  time: 111s\n",
            "Epoch 1 - avg_train_loss: 0.2350  avg_val_loss: 0.1384  time: 111s\n",
            "Epoch 1 - avg_train_loss: 0.2350  avg_val_loss: 0.1384  time: 111s\n",
            "Epoch 1 - avg_train_loss: 0.2350  avg_val_loss: 0.1384  time: 111s\n",
            "Epoch 1 - avg_train_loss: 0.2350  avg_val_loss: 0.1384  time: 111s\n",
            "Epoch 1 - avg_train_loss: 0.2350  avg_val_loss: 0.1384  time: 111s\n",
            "Epoch 1 - avg_train_loss: 0.2350  avg_val_loss: 0.1384  time: 111s\n",
            "Epoch 1 - avg_train_loss: 0.2350  avg_val_loss: 0.1384  time: 111s\n",
            "Epoch 1 - avg_train_loss: 0.2350  avg_val_loss: 0.1384  time: 111s\n",
            "Epoch 1 - avg_train_loss: 0.2350  avg_val_loss: 0.1384  time: 111s\n",
            "Epoch 1 - Score: 0.1383658367798898\n",
            "Epoch 1 - Score: 0.1383658367798898\n",
            "Epoch 1 - Score: 0.1383658367798898\n",
            "Epoch 1 - Score: 0.1383658367798898\n",
            "Epoch 1 - Score: 0.1383658367798898\n",
            "Epoch 1 - Score: 0.1383658367798898\n",
            "Epoch 1 - Score: 0.1383658367798898\n",
            "Epoch 1 - Score: 0.1383658367798898\n",
            "Epoch 1 - Score: 0.1383658367798898\n",
            "Epoch 1 - Score: 0.1383658367798898\n",
            "Epoch 1 - Score: 0.1383658367798898\n",
            "Epoch 1 - Score: 0.1383658367798898\n",
            "Epoch 1 - Save Best Score: 0.1384 Model\n",
            "Epoch 1 - Save Best Score: 0.1384 Model\n",
            "Epoch 1 - Save Best Score: 0.1384 Model\n",
            "Epoch 1 - Save Best Score: 0.1384 Model\n",
            "Epoch 1 - Save Best Score: 0.1384 Model\n",
            "Epoch 1 - Save Best Score: 0.1384 Model\n",
            "Epoch 1 - Save Best Score: 0.1384 Model\n",
            "Epoch 1 - Save Best Score: 0.1384 Model\n",
            "Epoch 1 - Save Best Score: 0.1384 Model\n",
            "Epoch 1 - Save Best Score: 0.1384 Model\n",
            "Epoch 1 - Save Best Score: 0.1384 Model\n",
            "Epoch 1 - Save Best Score: 0.1384 Model\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EVAL: [76/77] Elapsed 0m 8s (remain 0m 0s) Loss: 0.1384 \n",
            "Log loss for 0 is 0.1383658367798898\n",
            "Epoch: [2][0/306] Elapsed 0m 0s (remain 3m 2s) Loss: 0.1452 \n",
            "Epoch: [2][100/306] Elapsed 0m 34s (remain 1m 9s) Loss: 0.1093 \n",
            "Epoch: [2][200/306] Elapsed 1m 7s (remain 0m 35s) Loss: 0.1109 \n",
            "Epoch: [2][300/306] Elapsed 1m 41s (remain 0m 1s) Loss: 0.1061 \n",
            "Epoch: [2][305/306] Elapsed 1m 42s (remain 0m 0s) Loss: 0.1075 \n",
            "EVAL: [0/77] Elapsed 0m 0s (remain 0m 28s) Loss: 0.2123 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2 - avg_train_loss: 0.1075  avg_val_loss: 0.1430  time: 111s\n",
            "Epoch 2 - avg_train_loss: 0.1075  avg_val_loss: 0.1430  time: 111s\n",
            "Epoch 2 - avg_train_loss: 0.1075  avg_val_loss: 0.1430  time: 111s\n",
            "Epoch 2 - avg_train_loss: 0.1075  avg_val_loss: 0.1430  time: 111s\n",
            "Epoch 2 - avg_train_loss: 0.1075  avg_val_loss: 0.1430  time: 111s\n",
            "Epoch 2 - avg_train_loss: 0.1075  avg_val_loss: 0.1430  time: 111s\n",
            "Epoch 2 - avg_train_loss: 0.1075  avg_val_loss: 0.1430  time: 111s\n",
            "Epoch 2 - avg_train_loss: 0.1075  avg_val_loss: 0.1430  time: 111s\n",
            "Epoch 2 - avg_train_loss: 0.1075  avg_val_loss: 0.1430  time: 111s\n",
            "Epoch 2 - avg_train_loss: 0.1075  avg_val_loss: 0.1430  time: 111s\n",
            "Epoch 2 - avg_train_loss: 0.1075  avg_val_loss: 0.1430  time: 111s\n",
            "Epoch 2 - avg_train_loss: 0.1075  avg_val_loss: 0.1430  time: 111s\n",
            "Epoch 2 - Score: 0.14298259074319256\n",
            "Epoch 2 - Score: 0.14298259074319256\n",
            "Epoch 2 - Score: 0.14298259074319256\n",
            "Epoch 2 - Score: 0.14298259074319256\n",
            "Epoch 2 - Score: 0.14298259074319256\n",
            "Epoch 2 - Score: 0.14298259074319256\n",
            "Epoch 2 - Score: 0.14298259074319256\n",
            "Epoch 2 - Score: 0.14298259074319256\n",
            "Epoch 2 - Score: 0.14298259074319256\n",
            "Epoch 2 - Score: 0.14298259074319256\n",
            "Epoch 2 - Score: 0.14298259074319256\n",
            "Epoch 2 - Score: 0.14298259074319256\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EVAL: [76/77] Elapsed 0m 8s (remain 0m 0s) Loss: 0.1430 \n",
            "Log loss for 1 is 0.14298259074319256\n",
            "Epoch: [3][0/306] Elapsed 0m 0s (remain 3m 19s) Loss: 0.0103 \n",
            "Epoch: [3][100/306] Elapsed 0m 34s (remain 1m 9s) Loss: 0.0700 \n",
            "Epoch: [3][200/306] Elapsed 1m 7s (remain 0m 35s) Loss: 0.0697 \n",
            "Epoch: [3][300/306] Elapsed 1m 41s (remain 0m 1s) Loss: 0.0667 \n",
            "Epoch: [3][305/306] Elapsed 1m 42s (remain 0m 0s) Loss: 0.0671 \n",
            "EVAL: [0/77] Elapsed 0m 0s (remain 0m 27s) Loss: 0.2344 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3 - avg_train_loss: 0.0671  avg_val_loss: 0.1604  time: 112s\n",
            "Epoch 3 - avg_train_loss: 0.0671  avg_val_loss: 0.1604  time: 112s\n",
            "Epoch 3 - avg_train_loss: 0.0671  avg_val_loss: 0.1604  time: 112s\n",
            "Epoch 3 - avg_train_loss: 0.0671  avg_val_loss: 0.1604  time: 112s\n",
            "Epoch 3 - avg_train_loss: 0.0671  avg_val_loss: 0.1604  time: 112s\n",
            "Epoch 3 - avg_train_loss: 0.0671  avg_val_loss: 0.1604  time: 112s\n",
            "Epoch 3 - avg_train_loss: 0.0671  avg_val_loss: 0.1604  time: 112s\n",
            "Epoch 3 - avg_train_loss: 0.0671  avg_val_loss: 0.1604  time: 112s\n",
            "Epoch 3 - avg_train_loss: 0.0671  avg_val_loss: 0.1604  time: 112s\n",
            "Epoch 3 - avg_train_loss: 0.0671  avg_val_loss: 0.1604  time: 112s\n",
            "Epoch 3 - avg_train_loss: 0.0671  avg_val_loss: 0.1604  time: 112s\n",
            "Epoch 3 - avg_train_loss: 0.0671  avg_val_loss: 0.1604  time: 112s\n",
            "Epoch 3 - Score: 0.1603949753777826\n",
            "Epoch 3 - Score: 0.1603949753777826\n",
            "Epoch 3 - Score: 0.1603949753777826\n",
            "Epoch 3 - Score: 0.1603949753777826\n",
            "Epoch 3 - Score: 0.1603949753777826\n",
            "Epoch 3 - Score: 0.1603949753777826\n",
            "Epoch 3 - Score: 0.1603949753777826\n",
            "Epoch 3 - Score: 0.1603949753777826\n",
            "Epoch 3 - Score: 0.1603949753777826\n",
            "Epoch 3 - Score: 0.1603949753777826\n",
            "Epoch 3 - Score: 0.1603949753777826\n",
            "Epoch 3 - Score: 0.1603949753777826\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EVAL: [76/77] Elapsed 0m 8s (remain 0m 0s) Loss: 0.1604 \n",
            "Log loss for 2 is 0.1603949753777826\n",
            "Epoch: [4][0/306] Elapsed 0m 0s (remain 3m 19s) Loss: 0.1158 \n",
            "Epoch: [4][100/306] Elapsed 0m 34s (remain 1m 9s) Loss: 0.0438 \n",
            "Epoch: [4][200/306] Elapsed 1m 7s (remain 0m 35s) Loss: 0.0454 \n",
            "Epoch: [4][300/306] Elapsed 1m 41s (remain 0m 1s) Loss: 0.0504 \n",
            "Epoch: [4][305/306] Elapsed 1m 42s (remain 0m 0s) Loss: 0.0517 \n",
            "EVAL: [0/77] Elapsed 0m 0s (remain 0m 28s) Loss: 0.2398 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4 - avg_train_loss: 0.0517  avg_val_loss: 0.1900  time: 112s\n",
            "Epoch 4 - avg_train_loss: 0.0517  avg_val_loss: 0.1900  time: 112s\n",
            "Epoch 4 - avg_train_loss: 0.0517  avg_val_loss: 0.1900  time: 112s\n",
            "Epoch 4 - avg_train_loss: 0.0517  avg_val_loss: 0.1900  time: 112s\n",
            "Epoch 4 - avg_train_loss: 0.0517  avg_val_loss: 0.1900  time: 112s\n",
            "Epoch 4 - avg_train_loss: 0.0517  avg_val_loss: 0.1900  time: 112s\n",
            "Epoch 4 - avg_train_loss: 0.0517  avg_val_loss: 0.1900  time: 112s\n",
            "Epoch 4 - avg_train_loss: 0.0517  avg_val_loss: 0.1900  time: 112s\n",
            "Epoch 4 - avg_train_loss: 0.0517  avg_val_loss: 0.1900  time: 112s\n",
            "Epoch 4 - avg_train_loss: 0.0517  avg_val_loss: 0.1900  time: 112s\n",
            "Epoch 4 - avg_train_loss: 0.0517  avg_val_loss: 0.1900  time: 112s\n",
            "Epoch 4 - avg_train_loss: 0.0517  avg_val_loss: 0.1900  time: 112s\n",
            "Epoch 4 - Score: 0.19002275986853304\n",
            "Epoch 4 - Score: 0.19002275986853304\n",
            "Epoch 4 - Score: 0.19002275986853304\n",
            "Epoch 4 - Score: 0.19002275986853304\n",
            "Epoch 4 - Score: 0.19002275986853304\n",
            "Epoch 4 - Score: 0.19002275986853304\n",
            "Epoch 4 - Score: 0.19002275986853304\n",
            "Epoch 4 - Score: 0.19002275986853304\n",
            "Epoch 4 - Score: 0.19002275986853304\n",
            "Epoch 4 - Score: 0.19002275986853304\n",
            "Epoch 4 - Score: 0.19002275986853304\n",
            "Epoch 4 - Score: 0.19002275986853304\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EVAL: [76/77] Elapsed 0m 8s (remain 0m 0s) Loss: 0.1900 \n",
            "Log loss for 3 is 0.19002275986853304\n",
            "Epoch: [5][0/306] Elapsed 0m 0s (remain 3m 16s) Loss: 0.1831 \n",
            "Epoch: [5][100/306] Elapsed 0m 34s (remain 1m 9s) Loss: 0.0430 \n",
            "Epoch: [5][200/306] Elapsed 1m 7s (remain 0m 35s) Loss: 0.0277 \n",
            "Epoch: [5][300/306] Elapsed 1m 41s (remain 0m 1s) Loss: 0.0303 \n",
            "Epoch: [5][305/306] Elapsed 1m 42s (remain 0m 0s) Loss: 0.0300 \n",
            "EVAL: [0/77] Elapsed 0m 0s (remain 0m 28s) Loss: 0.2393 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5 - avg_train_loss: 0.0300  avg_val_loss: 0.2328  time: 112s\n",
            "Epoch 5 - avg_train_loss: 0.0300  avg_val_loss: 0.2328  time: 112s\n",
            "Epoch 5 - avg_train_loss: 0.0300  avg_val_loss: 0.2328  time: 112s\n",
            "Epoch 5 - avg_train_loss: 0.0300  avg_val_loss: 0.2328  time: 112s\n",
            "Epoch 5 - avg_train_loss: 0.0300  avg_val_loss: 0.2328  time: 112s\n",
            "Epoch 5 - avg_train_loss: 0.0300  avg_val_loss: 0.2328  time: 112s\n",
            "Epoch 5 - avg_train_loss: 0.0300  avg_val_loss: 0.2328  time: 112s\n",
            "Epoch 5 - avg_train_loss: 0.0300  avg_val_loss: 0.2328  time: 112s\n",
            "Epoch 5 - avg_train_loss: 0.0300  avg_val_loss: 0.2328  time: 112s\n",
            "Epoch 5 - avg_train_loss: 0.0300  avg_val_loss: 0.2328  time: 112s\n",
            "Epoch 5 - avg_train_loss: 0.0300  avg_val_loss: 0.2328  time: 112s\n",
            "Epoch 5 - avg_train_loss: 0.0300  avg_val_loss: 0.2328  time: 112s\n",
            "Epoch 5 - Score: 0.2328140527700344\n",
            "Epoch 5 - Score: 0.2328140527700344\n",
            "Epoch 5 - Score: 0.2328140527700344\n",
            "Epoch 5 - Score: 0.2328140527700344\n",
            "Epoch 5 - Score: 0.2328140527700344\n",
            "Epoch 5 - Score: 0.2328140527700344\n",
            "Epoch 5 - Score: 0.2328140527700344\n",
            "Epoch 5 - Score: 0.2328140527700344\n",
            "Epoch 5 - Score: 0.2328140527700344\n",
            "Epoch 5 - Score: 0.2328140527700344\n",
            "Epoch 5 - Score: 0.2328140527700344\n",
            "Epoch 5 - Score: 0.2328140527700344\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EVAL: [76/77] Elapsed 0m 8s (remain 0m 0s) Loss: 0.2328 \n",
            "Log loss for 4 is 0.2328140527700344\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "========== fold: 2 result ==========\n",
            "========== fold: 2 result ==========\n",
            "========== fold: 2 result ==========\n",
            "========== fold: 2 result ==========\n",
            "========== fold: 2 result ==========\n",
            "========== fold: 2 result ==========\n",
            "========== fold: 2 result ==========\n",
            "========== fold: 2 result ==========\n",
            "========== fold: 2 result ==========\n",
            "========== fold: 2 result ==========\n",
            "========== fold: 2 result ==========\n",
            "========== fold: 2 result ==========\n",
            "Score: 0.13837\n",
            "Score: 0.13837\n",
            "Score: 0.13837\n",
            "Score: 0.13837\n",
            "Score: 0.13837\n",
            "Score: 0.13837\n",
            "Score: 0.13837\n",
            "Score: 0.13837\n",
            "Score: 0.13837\n",
            "Score: 0.13837\n",
            "Score: 0.13837\n",
            "Score: 0.13837\n",
            "========== fold: 3 training ==========\n",
            "========== fold: 3 training ==========\n",
            "========== fold: 3 training ==========\n",
            "========== fold: 3 training ==========\n",
            "========== fold: 3 training ==========\n",
            "========== fold: 3 training ==========\n",
            "========== fold: 3 training ==========\n",
            "========== fold: 3 training ==========\n",
            "========== fold: 3 training ==========\n",
            "========== fold: 3 training ==========\n",
            "========== fold: 3 training ==========\n",
            "========== fold: 3 training ==========\n",
            "Didn't find file /content/activebus_bert_final/added_tokens.json. We won't load it.\n",
            "loading file /content/activebus_bert_final/vocab.txt\n",
            "loading file None\n",
            "loading file /content/activebus_bert_final/special_tokens_map.json\n",
            "loading file /content/activebus_bert_final/tokenizer_config.json\n",
            "loading file /content/activebus_bert_final/tokenizer.json\n",
            "Didn't find file /content/activebus_bert_final/added_tokens.json. We won't load it.\n",
            "loading file /content/activebus_bert_final/vocab.txt\n",
            "loading file None\n",
            "loading file /content/activebus_bert_final/special_tokens_map.json\n",
            "loading file /content/activebus_bert_final/tokenizer_config.json\n",
            "loading file /content/activebus_bert_final/tokenizer.json\n",
            "loading configuration file /content/activebus_bert_final/config.json\n",
            "Model config BertConfig {\n",
            "  \"_name_or_path\": \"activebus/BERT-XD_Review\",\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_ids\": 0,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.15.0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "loading weights file /content/activebus_bert_final/pytorch_model.bin\n",
            "Some weights of the model checkpoint at /content/activebus_bert_final were not used when initializing BertForSequenceClassification: ['cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /content/activebus_bert_final and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: [1][0/306] Elapsed 0m 0s (remain 3m 4s) Loss: 0.7368 \n",
            "Epoch: [1][100/306] Elapsed 0m 34s (remain 1m 9s) Loss: 0.3443 \n",
            "Epoch: [1][200/306] Elapsed 1m 7s (remain 0m 35s) Loss: 0.2488 \n",
            "Epoch: [1][300/306] Elapsed 1m 41s (remain 0m 1s) Loss: 0.2220 \n",
            "Epoch: [1][305/306] Elapsed 1m 42s (remain 0m 0s) Loss: 0.2206 \n",
            "EVAL: [0/77] Elapsed 0m 0s (remain 0m 27s) Loss: 0.2623 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1 - avg_train_loss: 0.2206  avg_val_loss: 0.1060  time: 112s\n",
            "Epoch 1 - avg_train_loss: 0.2206  avg_val_loss: 0.1060  time: 112s\n",
            "Epoch 1 - avg_train_loss: 0.2206  avg_val_loss: 0.1060  time: 112s\n",
            "Epoch 1 - avg_train_loss: 0.2206  avg_val_loss: 0.1060  time: 112s\n",
            "Epoch 1 - avg_train_loss: 0.2206  avg_val_loss: 0.1060  time: 112s\n",
            "Epoch 1 - avg_train_loss: 0.2206  avg_val_loss: 0.1060  time: 112s\n",
            "Epoch 1 - avg_train_loss: 0.2206  avg_val_loss: 0.1060  time: 112s\n",
            "Epoch 1 - avg_train_loss: 0.2206  avg_val_loss: 0.1060  time: 112s\n",
            "Epoch 1 - avg_train_loss: 0.2206  avg_val_loss: 0.1060  time: 112s\n",
            "Epoch 1 - avg_train_loss: 0.2206  avg_val_loss: 0.1060  time: 112s\n",
            "Epoch 1 - avg_train_loss: 0.2206  avg_val_loss: 0.1060  time: 112s\n",
            "Epoch 1 - avg_train_loss: 0.2206  avg_val_loss: 0.1060  time: 112s\n",
            "Epoch 1 - Score: 0.10601357417181134\n",
            "Epoch 1 - Score: 0.10601357417181134\n",
            "Epoch 1 - Score: 0.10601357417181134\n",
            "Epoch 1 - Score: 0.10601357417181134\n",
            "Epoch 1 - Score: 0.10601357417181134\n",
            "Epoch 1 - Score: 0.10601357417181134\n",
            "Epoch 1 - Score: 0.10601357417181134\n",
            "Epoch 1 - Score: 0.10601357417181134\n",
            "Epoch 1 - Score: 0.10601357417181134\n",
            "Epoch 1 - Score: 0.10601357417181134\n",
            "Epoch 1 - Score: 0.10601357417181134\n",
            "Epoch 1 - Score: 0.10601357417181134\n",
            "Epoch 1 - Save Best Score: 0.1060 Model\n",
            "Epoch 1 - Save Best Score: 0.1060 Model\n",
            "Epoch 1 - Save Best Score: 0.1060 Model\n",
            "Epoch 1 - Save Best Score: 0.1060 Model\n",
            "Epoch 1 - Save Best Score: 0.1060 Model\n",
            "Epoch 1 - Save Best Score: 0.1060 Model\n",
            "Epoch 1 - Save Best Score: 0.1060 Model\n",
            "Epoch 1 - Save Best Score: 0.1060 Model\n",
            "Epoch 1 - Save Best Score: 0.1060 Model\n",
            "Epoch 1 - Save Best Score: 0.1060 Model\n",
            "Epoch 1 - Save Best Score: 0.1060 Model\n",
            "Epoch 1 - Save Best Score: 0.1060 Model\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EVAL: [76/77] Elapsed 0m 8s (remain 0m 0s) Loss: 0.1060 \n",
            "Log loss for 0 is 0.10601357417181134\n",
            "Epoch: [2][0/306] Elapsed 0m 0s (remain 2m 59s) Loss: 0.0755 \n",
            "Epoch: [2][100/306] Elapsed 0m 34s (remain 1m 9s) Loss: 0.1333 \n",
            "Epoch: [2][200/306] Elapsed 1m 7s (remain 0m 35s) Loss: 0.1127 \n",
            "Epoch: [2][300/306] Elapsed 1m 41s (remain 0m 1s) Loss: 0.1118 \n",
            "Epoch: [2][305/306] Elapsed 1m 42s (remain 0m 0s) Loss: 0.1110 \n",
            "EVAL: [0/77] Elapsed 0m 0s (remain 0m 28s) Loss: 0.3983 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2 - avg_train_loss: 0.1110  avg_val_loss: 0.1161  time: 112s\n",
            "Epoch 2 - avg_train_loss: 0.1110  avg_val_loss: 0.1161  time: 112s\n",
            "Epoch 2 - avg_train_loss: 0.1110  avg_val_loss: 0.1161  time: 112s\n",
            "Epoch 2 - avg_train_loss: 0.1110  avg_val_loss: 0.1161  time: 112s\n",
            "Epoch 2 - avg_train_loss: 0.1110  avg_val_loss: 0.1161  time: 112s\n",
            "Epoch 2 - avg_train_loss: 0.1110  avg_val_loss: 0.1161  time: 112s\n",
            "Epoch 2 - avg_train_loss: 0.1110  avg_val_loss: 0.1161  time: 112s\n",
            "Epoch 2 - avg_train_loss: 0.1110  avg_val_loss: 0.1161  time: 112s\n",
            "Epoch 2 - avg_train_loss: 0.1110  avg_val_loss: 0.1161  time: 112s\n",
            "Epoch 2 - avg_train_loss: 0.1110  avg_val_loss: 0.1161  time: 112s\n",
            "Epoch 2 - avg_train_loss: 0.1110  avg_val_loss: 0.1161  time: 112s\n",
            "Epoch 2 - avg_train_loss: 0.1110  avg_val_loss: 0.1161  time: 112s\n",
            "Epoch 2 - Score: 0.11613946418621757\n",
            "Epoch 2 - Score: 0.11613946418621757\n",
            "Epoch 2 - Score: 0.11613946418621757\n",
            "Epoch 2 - Score: 0.11613946418621757\n",
            "Epoch 2 - Score: 0.11613946418621757\n",
            "Epoch 2 - Score: 0.11613946418621757\n",
            "Epoch 2 - Score: 0.11613946418621757\n",
            "Epoch 2 - Score: 0.11613946418621757\n",
            "Epoch 2 - Score: 0.11613946418621757\n",
            "Epoch 2 - Score: 0.11613946418621757\n",
            "Epoch 2 - Score: 0.11613946418621757\n",
            "Epoch 2 - Score: 0.11613946418621757\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EVAL: [76/77] Elapsed 0m 8s (remain 0m 0s) Loss: 0.1161 \n",
            "Log loss for 1 is 0.11613946418621757\n",
            "Epoch: [3][0/306] Elapsed 0m 0s (remain 3m 12s) Loss: 0.0228 \n",
            "Epoch: [3][100/306] Elapsed 0m 34s (remain 1m 9s) Loss: 0.0648 \n",
            "Epoch: [3][200/306] Elapsed 1m 7s (remain 0m 35s) Loss: 0.0623 \n",
            "Epoch: [3][300/306] Elapsed 1m 41s (remain 0m 1s) Loss: 0.0654 \n",
            "Epoch: [3][305/306] Elapsed 1m 42s (remain 0m 0s) Loss: 0.0650 \n",
            "EVAL: [0/77] Elapsed 0m 0s (remain 0m 28s) Loss: 0.2390 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3 - avg_train_loss: 0.0650  avg_val_loss: 0.1347  time: 112s\n",
            "Epoch 3 - avg_train_loss: 0.0650  avg_val_loss: 0.1347  time: 112s\n",
            "Epoch 3 - avg_train_loss: 0.0650  avg_val_loss: 0.1347  time: 112s\n",
            "Epoch 3 - avg_train_loss: 0.0650  avg_val_loss: 0.1347  time: 112s\n",
            "Epoch 3 - avg_train_loss: 0.0650  avg_val_loss: 0.1347  time: 112s\n",
            "Epoch 3 - avg_train_loss: 0.0650  avg_val_loss: 0.1347  time: 112s\n",
            "Epoch 3 - avg_train_loss: 0.0650  avg_val_loss: 0.1347  time: 112s\n",
            "Epoch 3 - avg_train_loss: 0.0650  avg_val_loss: 0.1347  time: 112s\n",
            "Epoch 3 - avg_train_loss: 0.0650  avg_val_loss: 0.1347  time: 112s\n",
            "Epoch 3 - avg_train_loss: 0.0650  avg_val_loss: 0.1347  time: 112s\n",
            "Epoch 3 - avg_train_loss: 0.0650  avg_val_loss: 0.1347  time: 112s\n",
            "Epoch 3 - avg_train_loss: 0.0650  avg_val_loss: 0.1347  time: 112s\n",
            "Epoch 3 - Score: 0.1347150013397069\n",
            "Epoch 3 - Score: 0.1347150013397069\n",
            "Epoch 3 - Score: 0.1347150013397069\n",
            "Epoch 3 - Score: 0.1347150013397069\n",
            "Epoch 3 - Score: 0.1347150013397069\n",
            "Epoch 3 - Score: 0.1347150013397069\n",
            "Epoch 3 - Score: 0.1347150013397069\n",
            "Epoch 3 - Score: 0.1347150013397069\n",
            "Epoch 3 - Score: 0.1347150013397069\n",
            "Epoch 3 - Score: 0.1347150013397069\n",
            "Epoch 3 - Score: 0.1347150013397069\n",
            "Epoch 3 - Score: 0.1347150013397069\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EVAL: [76/77] Elapsed 0m 8s (remain 0m 0s) Loss: 0.1347 \n",
            "Log loss for 2 is 0.1347150013397069\n",
            "Epoch: [4][0/306] Elapsed 0m 0s (remain 3m 11s) Loss: 0.0172 \n",
            "Epoch: [4][100/306] Elapsed 0m 34s (remain 1m 9s) Loss: 0.0325 \n",
            "Epoch: [4][200/306] Elapsed 1m 7s (remain 0m 35s) Loss: 0.0340 \n",
            "Epoch: [4][300/306] Elapsed 1m 41s (remain 0m 1s) Loss: 0.0371 \n",
            "Epoch: [4][305/306] Elapsed 1m 42s (remain 0m 0s) Loss: 0.0375 \n",
            "EVAL: [0/77] Elapsed 0m 0s (remain 0m 27s) Loss: 0.2380 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4 - avg_train_loss: 0.0375  avg_val_loss: 0.1690  time: 112s\n",
            "Epoch 4 - avg_train_loss: 0.0375  avg_val_loss: 0.1690  time: 112s\n",
            "Epoch 4 - avg_train_loss: 0.0375  avg_val_loss: 0.1690  time: 112s\n",
            "Epoch 4 - avg_train_loss: 0.0375  avg_val_loss: 0.1690  time: 112s\n",
            "Epoch 4 - avg_train_loss: 0.0375  avg_val_loss: 0.1690  time: 112s\n",
            "Epoch 4 - avg_train_loss: 0.0375  avg_val_loss: 0.1690  time: 112s\n",
            "Epoch 4 - avg_train_loss: 0.0375  avg_val_loss: 0.1690  time: 112s\n",
            "Epoch 4 - avg_train_loss: 0.0375  avg_val_loss: 0.1690  time: 112s\n",
            "Epoch 4 - avg_train_loss: 0.0375  avg_val_loss: 0.1690  time: 112s\n",
            "Epoch 4 - avg_train_loss: 0.0375  avg_val_loss: 0.1690  time: 112s\n",
            "Epoch 4 - avg_train_loss: 0.0375  avg_val_loss: 0.1690  time: 112s\n",
            "Epoch 4 - avg_train_loss: 0.0375  avg_val_loss: 0.1690  time: 112s\n",
            "Epoch 4 - Score: 0.16898409704839623\n",
            "Epoch 4 - Score: 0.16898409704839623\n",
            "Epoch 4 - Score: 0.16898409704839623\n",
            "Epoch 4 - Score: 0.16898409704839623\n",
            "Epoch 4 - Score: 0.16898409704839623\n",
            "Epoch 4 - Score: 0.16898409704839623\n",
            "Epoch 4 - Score: 0.16898409704839623\n",
            "Epoch 4 - Score: 0.16898409704839623\n",
            "Epoch 4 - Score: 0.16898409704839623\n",
            "Epoch 4 - Score: 0.16898409704839623\n",
            "Epoch 4 - Score: 0.16898409704839623\n",
            "Epoch 4 - Score: 0.16898409704839623\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EVAL: [76/77] Elapsed 0m 8s (remain 0m 0s) Loss: 0.1690 \n",
            "Log loss for 3 is 0.16898409704839623\n",
            "Epoch: [5][0/306] Elapsed 0m 0s (remain 3m 26s) Loss: 0.0059 \n",
            "Epoch: [5][100/306] Elapsed 0m 34s (remain 1m 9s) Loss: 0.0308 \n",
            "Epoch: [5][200/306] Elapsed 1m 7s (remain 0m 35s) Loss: 0.0333 \n",
            "Epoch: [5][300/306] Elapsed 1m 41s (remain 0m 1s) Loss: 0.0279 \n",
            "Epoch: [5][305/306] Elapsed 1m 42s (remain 0m 0s) Loss: 0.0278 \n",
            "EVAL: [0/77] Elapsed 0m 0s (remain 0m 26s) Loss: 0.5156 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5 - avg_train_loss: 0.0278  avg_val_loss: 0.1979  time: 112s\n",
            "Epoch 5 - avg_train_loss: 0.0278  avg_val_loss: 0.1979  time: 112s\n",
            "Epoch 5 - avg_train_loss: 0.0278  avg_val_loss: 0.1979  time: 112s\n",
            "Epoch 5 - avg_train_loss: 0.0278  avg_val_loss: 0.1979  time: 112s\n",
            "Epoch 5 - avg_train_loss: 0.0278  avg_val_loss: 0.1979  time: 112s\n",
            "Epoch 5 - avg_train_loss: 0.0278  avg_val_loss: 0.1979  time: 112s\n",
            "Epoch 5 - avg_train_loss: 0.0278  avg_val_loss: 0.1979  time: 112s\n",
            "Epoch 5 - avg_train_loss: 0.0278  avg_val_loss: 0.1979  time: 112s\n",
            "Epoch 5 - avg_train_loss: 0.0278  avg_val_loss: 0.1979  time: 112s\n",
            "Epoch 5 - avg_train_loss: 0.0278  avg_val_loss: 0.1979  time: 112s\n",
            "Epoch 5 - avg_train_loss: 0.0278  avg_val_loss: 0.1979  time: 112s\n",
            "Epoch 5 - avg_train_loss: 0.0278  avg_val_loss: 0.1979  time: 112s\n",
            "Epoch 5 - Score: 0.19793466065055634\n",
            "Epoch 5 - Score: 0.19793466065055634\n",
            "Epoch 5 - Score: 0.19793466065055634\n",
            "Epoch 5 - Score: 0.19793466065055634\n",
            "Epoch 5 - Score: 0.19793466065055634\n",
            "Epoch 5 - Score: 0.19793466065055634\n",
            "Epoch 5 - Score: 0.19793466065055634\n",
            "Epoch 5 - Score: 0.19793466065055634\n",
            "Epoch 5 - Score: 0.19793466065055634\n",
            "Epoch 5 - Score: 0.19793466065055634\n",
            "Epoch 5 - Score: 0.19793466065055634\n",
            "Epoch 5 - Score: 0.19793466065055634\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EVAL: [76/77] Elapsed 0m 8s (remain 0m 0s) Loss: 0.1979 \n",
            "Log loss for 4 is 0.19793466065055634\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "========== fold: 3 result ==========\n",
            "========== fold: 3 result ==========\n",
            "========== fold: 3 result ==========\n",
            "========== fold: 3 result ==========\n",
            "========== fold: 3 result ==========\n",
            "========== fold: 3 result ==========\n",
            "========== fold: 3 result ==========\n",
            "========== fold: 3 result ==========\n",
            "========== fold: 3 result ==========\n",
            "========== fold: 3 result ==========\n",
            "========== fold: 3 result ==========\n",
            "========== fold: 3 result ==========\n",
            "Score: 0.10601\n",
            "Score: 0.10601\n",
            "Score: 0.10601\n",
            "Score: 0.10601\n",
            "Score: 0.10601\n",
            "Score: 0.10601\n",
            "Score: 0.10601\n",
            "Score: 0.10601\n",
            "Score: 0.10601\n",
            "Score: 0.10601\n",
            "Score: 0.10601\n",
            "Score: 0.10601\n",
            "========== fold: 4 training ==========\n",
            "========== fold: 4 training ==========\n",
            "========== fold: 4 training ==========\n",
            "========== fold: 4 training ==========\n",
            "========== fold: 4 training ==========\n",
            "========== fold: 4 training ==========\n",
            "========== fold: 4 training ==========\n",
            "========== fold: 4 training ==========\n",
            "========== fold: 4 training ==========\n",
            "========== fold: 4 training ==========\n",
            "========== fold: 4 training ==========\n",
            "========== fold: 4 training ==========\n",
            "Didn't find file /content/activebus_bert_final/added_tokens.json. We won't load it.\n",
            "loading file /content/activebus_bert_final/vocab.txt\n",
            "loading file None\n",
            "loading file /content/activebus_bert_final/special_tokens_map.json\n",
            "loading file /content/activebus_bert_final/tokenizer_config.json\n",
            "loading file /content/activebus_bert_final/tokenizer.json\n",
            "Didn't find file /content/activebus_bert_final/added_tokens.json. We won't load it.\n",
            "loading file /content/activebus_bert_final/vocab.txt\n",
            "loading file None\n",
            "loading file /content/activebus_bert_final/special_tokens_map.json\n",
            "loading file /content/activebus_bert_final/tokenizer_config.json\n",
            "loading file /content/activebus_bert_final/tokenizer.json\n",
            "loading configuration file /content/activebus_bert_final/config.json\n",
            "Model config BertConfig {\n",
            "  \"_name_or_path\": \"activebus/BERT-XD_Review\",\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_ids\": 0,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.15.0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "loading weights file /content/activebus_bert_final/pytorch_model.bin\n",
            "Some weights of the model checkpoint at /content/activebus_bert_final were not used when initializing BertForSequenceClassification: ['cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /content/activebus_bert_final and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: [1][0/306] Elapsed 0m 0s (remain 3m 10s) Loss: 0.6734 \n",
            "Epoch: [1][100/306] Elapsed 0m 34s (remain 1m 9s) Loss: 0.3407 \n",
            "Epoch: [1][200/306] Elapsed 1m 7s (remain 0m 35s) Loss: 0.2507 \n",
            "Epoch: [1][300/306] Elapsed 1m 41s (remain 0m 1s) Loss: 0.2183 \n",
            "Epoch: [1][305/306] Elapsed 1m 42s (remain 0m 0s) Loss: 0.2163 \n",
            "EVAL: [0/77] Elapsed 0m 0s (remain 0m 28s) Loss: 0.5270 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1 - avg_train_loss: 0.2163  avg_val_loss: 0.1662  time: 112s\n",
            "Epoch 1 - avg_train_loss: 0.2163  avg_val_loss: 0.1662  time: 112s\n",
            "Epoch 1 - avg_train_loss: 0.2163  avg_val_loss: 0.1662  time: 112s\n",
            "Epoch 1 - avg_train_loss: 0.2163  avg_val_loss: 0.1662  time: 112s\n",
            "Epoch 1 - avg_train_loss: 0.2163  avg_val_loss: 0.1662  time: 112s\n",
            "Epoch 1 - avg_train_loss: 0.2163  avg_val_loss: 0.1662  time: 112s\n",
            "Epoch 1 - avg_train_loss: 0.2163  avg_val_loss: 0.1662  time: 112s\n",
            "Epoch 1 - avg_train_loss: 0.2163  avg_val_loss: 0.1662  time: 112s\n",
            "Epoch 1 - avg_train_loss: 0.2163  avg_val_loss: 0.1662  time: 112s\n",
            "Epoch 1 - avg_train_loss: 0.2163  avg_val_loss: 0.1662  time: 112s\n",
            "Epoch 1 - avg_train_loss: 0.2163  avg_val_loss: 0.1662  time: 112s\n",
            "Epoch 1 - avg_train_loss: 0.2163  avg_val_loss: 0.1662  time: 112s\n",
            "Epoch 1 - Score: 0.16615268190261567\n",
            "Epoch 1 - Score: 0.16615268190261567\n",
            "Epoch 1 - Score: 0.16615268190261567\n",
            "Epoch 1 - Score: 0.16615268190261567\n",
            "Epoch 1 - Score: 0.16615268190261567\n",
            "Epoch 1 - Score: 0.16615268190261567\n",
            "Epoch 1 - Score: 0.16615268190261567\n",
            "Epoch 1 - Score: 0.16615268190261567\n",
            "Epoch 1 - Score: 0.16615268190261567\n",
            "Epoch 1 - Score: 0.16615268190261567\n",
            "Epoch 1 - Score: 0.16615268190261567\n",
            "Epoch 1 - Score: 0.16615268190261567\n",
            "Epoch 1 - Save Best Score: 0.1662 Model\n",
            "Epoch 1 - Save Best Score: 0.1662 Model\n",
            "Epoch 1 - Save Best Score: 0.1662 Model\n",
            "Epoch 1 - Save Best Score: 0.1662 Model\n",
            "Epoch 1 - Save Best Score: 0.1662 Model\n",
            "Epoch 1 - Save Best Score: 0.1662 Model\n",
            "Epoch 1 - Save Best Score: 0.1662 Model\n",
            "Epoch 1 - Save Best Score: 0.1662 Model\n",
            "Epoch 1 - Save Best Score: 0.1662 Model\n",
            "Epoch 1 - Save Best Score: 0.1662 Model\n",
            "Epoch 1 - Save Best Score: 0.1662 Model\n",
            "Epoch 1 - Save Best Score: 0.1662 Model\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EVAL: [76/77] Elapsed 0m 8s (remain 0m 0s) Loss: 0.1662 \n",
            "Log loss for 0 is 0.16615268190261567\n",
            "Epoch: [2][0/306] Elapsed 0m 0s (remain 3m 2s) Loss: 0.0757 \n",
            "Epoch: [2][100/306] Elapsed 0m 34s (remain 1m 9s) Loss: 0.1071 \n",
            "Epoch: [2][200/306] Elapsed 1m 7s (remain 0m 35s) Loss: 0.0996 \n",
            "Epoch: [2][300/306] Elapsed 1m 41s (remain 0m 1s) Loss: 0.1024 \n",
            "Epoch: [2][305/306] Elapsed 1m 42s (remain 0m 0s) Loss: 0.1033 \n",
            "EVAL: [0/77] Elapsed 0m 0s (remain 0m 28s) Loss: 0.6701 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2 - avg_train_loss: 0.1033  avg_val_loss: 0.1618  time: 112s\n",
            "Epoch 2 - avg_train_loss: 0.1033  avg_val_loss: 0.1618  time: 112s\n",
            "Epoch 2 - avg_train_loss: 0.1033  avg_val_loss: 0.1618  time: 112s\n",
            "Epoch 2 - avg_train_loss: 0.1033  avg_val_loss: 0.1618  time: 112s\n",
            "Epoch 2 - avg_train_loss: 0.1033  avg_val_loss: 0.1618  time: 112s\n",
            "Epoch 2 - avg_train_loss: 0.1033  avg_val_loss: 0.1618  time: 112s\n",
            "Epoch 2 - avg_train_loss: 0.1033  avg_val_loss: 0.1618  time: 112s\n",
            "Epoch 2 - avg_train_loss: 0.1033  avg_val_loss: 0.1618  time: 112s\n",
            "Epoch 2 - avg_train_loss: 0.1033  avg_val_loss: 0.1618  time: 112s\n",
            "Epoch 2 - avg_train_loss: 0.1033  avg_val_loss: 0.1618  time: 112s\n",
            "Epoch 2 - avg_train_loss: 0.1033  avg_val_loss: 0.1618  time: 112s\n",
            "Epoch 2 - avg_train_loss: 0.1033  avg_val_loss: 0.1618  time: 112s\n",
            "Epoch 2 - Score: 0.16180574325186192\n",
            "Epoch 2 - Score: 0.16180574325186192\n",
            "Epoch 2 - Score: 0.16180574325186192\n",
            "Epoch 2 - Score: 0.16180574325186192\n",
            "Epoch 2 - Score: 0.16180574325186192\n",
            "Epoch 2 - Score: 0.16180574325186192\n",
            "Epoch 2 - Score: 0.16180574325186192\n",
            "Epoch 2 - Score: 0.16180574325186192\n",
            "Epoch 2 - Score: 0.16180574325186192\n",
            "Epoch 2 - Score: 0.16180574325186192\n",
            "Epoch 2 - Score: 0.16180574325186192\n",
            "Epoch 2 - Score: 0.16180574325186192\n",
            "Epoch 2 - Save Best Score: 0.1618 Model\n",
            "Epoch 2 - Save Best Score: 0.1618 Model\n",
            "Epoch 2 - Save Best Score: 0.1618 Model\n",
            "Epoch 2 - Save Best Score: 0.1618 Model\n",
            "Epoch 2 - Save Best Score: 0.1618 Model\n",
            "Epoch 2 - Save Best Score: 0.1618 Model\n",
            "Epoch 2 - Save Best Score: 0.1618 Model\n",
            "Epoch 2 - Save Best Score: 0.1618 Model\n",
            "Epoch 2 - Save Best Score: 0.1618 Model\n",
            "Epoch 2 - Save Best Score: 0.1618 Model\n",
            "Epoch 2 - Save Best Score: 0.1618 Model\n",
            "Epoch 2 - Save Best Score: 0.1618 Model\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EVAL: [76/77] Elapsed 0m 8s (remain 0m 0s) Loss: 0.1618 \n",
            "Log loss for 1 is 0.16180574325186192\n",
            "Epoch: [3][0/306] Elapsed 0m 0s (remain 3m 9s) Loss: 0.0100 \n",
            "Epoch: [3][100/306] Elapsed 0m 34s (remain 1m 9s) Loss: 0.0405 \n",
            "Epoch: [3][200/306] Elapsed 1m 7s (remain 0m 35s) Loss: 0.0547 \n",
            "Epoch: [3][300/306] Elapsed 1m 41s (remain 0m 1s) Loss: 0.0620 \n",
            "Epoch: [3][305/306] Elapsed 1m 42s (remain 0m 0s) Loss: 0.0623 \n",
            "EVAL: [0/77] Elapsed 0m 0s (remain 0m 28s) Loss: 0.5483 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3 - avg_train_loss: 0.0623  avg_val_loss: 0.1862  time: 112s\n",
            "Epoch 3 - avg_train_loss: 0.0623  avg_val_loss: 0.1862  time: 112s\n",
            "Epoch 3 - avg_train_loss: 0.0623  avg_val_loss: 0.1862  time: 112s\n",
            "Epoch 3 - avg_train_loss: 0.0623  avg_val_loss: 0.1862  time: 112s\n",
            "Epoch 3 - avg_train_loss: 0.0623  avg_val_loss: 0.1862  time: 112s\n",
            "Epoch 3 - avg_train_loss: 0.0623  avg_val_loss: 0.1862  time: 112s\n",
            "Epoch 3 - avg_train_loss: 0.0623  avg_val_loss: 0.1862  time: 112s\n",
            "Epoch 3 - avg_train_loss: 0.0623  avg_val_loss: 0.1862  time: 112s\n",
            "Epoch 3 - avg_train_loss: 0.0623  avg_val_loss: 0.1862  time: 112s\n",
            "Epoch 3 - avg_train_loss: 0.0623  avg_val_loss: 0.1862  time: 112s\n",
            "Epoch 3 - avg_train_loss: 0.0623  avg_val_loss: 0.1862  time: 112s\n",
            "Epoch 3 - avg_train_loss: 0.0623  avg_val_loss: 0.1862  time: 112s\n",
            "Epoch 3 - Score: 0.18619935435518856\n",
            "Epoch 3 - Score: 0.18619935435518856\n",
            "Epoch 3 - Score: 0.18619935435518856\n",
            "Epoch 3 - Score: 0.18619935435518856\n",
            "Epoch 3 - Score: 0.18619935435518856\n",
            "Epoch 3 - Score: 0.18619935435518856\n",
            "Epoch 3 - Score: 0.18619935435518856\n",
            "Epoch 3 - Score: 0.18619935435518856\n",
            "Epoch 3 - Score: 0.18619935435518856\n",
            "Epoch 3 - Score: 0.18619935435518856\n",
            "Epoch 3 - Score: 0.18619935435518856\n",
            "Epoch 3 - Score: 0.18619935435518856\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EVAL: [76/77] Elapsed 0m 8s (remain 0m 0s) Loss: 0.1862 \n",
            "Log loss for 2 is 0.18619935435518856\n",
            "Epoch: [4][0/306] Elapsed 0m 0s (remain 3m 12s) Loss: 0.0392 \n",
            "Epoch: [4][100/306] Elapsed 0m 34s (remain 1m 9s) Loss: 0.0246 \n",
            "Epoch: [4][200/306] Elapsed 1m 7s (remain 0m 35s) Loss: 0.0338 \n",
            "Epoch: [4][300/306] Elapsed 1m 41s (remain 0m 1s) Loss: 0.0339 \n",
            "Epoch: [4][305/306] Elapsed 1m 42s (remain 0m 0s) Loss: 0.0350 \n",
            "EVAL: [0/77] Elapsed 0m 0s (remain 0m 29s) Loss: 0.7347 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4 - avg_train_loss: 0.0350  avg_val_loss: 0.1993  time: 112s\n",
            "Epoch 4 - avg_train_loss: 0.0350  avg_val_loss: 0.1993  time: 112s\n",
            "Epoch 4 - avg_train_loss: 0.0350  avg_val_loss: 0.1993  time: 112s\n",
            "Epoch 4 - avg_train_loss: 0.0350  avg_val_loss: 0.1993  time: 112s\n",
            "Epoch 4 - avg_train_loss: 0.0350  avg_val_loss: 0.1993  time: 112s\n",
            "Epoch 4 - avg_train_loss: 0.0350  avg_val_loss: 0.1993  time: 112s\n",
            "Epoch 4 - avg_train_loss: 0.0350  avg_val_loss: 0.1993  time: 112s\n",
            "Epoch 4 - avg_train_loss: 0.0350  avg_val_loss: 0.1993  time: 112s\n",
            "Epoch 4 - avg_train_loss: 0.0350  avg_val_loss: 0.1993  time: 112s\n",
            "Epoch 4 - avg_train_loss: 0.0350  avg_val_loss: 0.1993  time: 112s\n",
            "Epoch 4 - avg_train_loss: 0.0350  avg_val_loss: 0.1993  time: 112s\n",
            "Epoch 4 - avg_train_loss: 0.0350  avg_val_loss: 0.1993  time: 112s\n",
            "Epoch 4 - Score: 0.19928820504924205\n",
            "Epoch 4 - Score: 0.19928820504924205\n",
            "Epoch 4 - Score: 0.19928820504924205\n",
            "Epoch 4 - Score: 0.19928820504924205\n",
            "Epoch 4 - Score: 0.19928820504924205\n",
            "Epoch 4 - Score: 0.19928820504924205\n",
            "Epoch 4 - Score: 0.19928820504924205\n",
            "Epoch 4 - Score: 0.19928820504924205\n",
            "Epoch 4 - Score: 0.19928820504924205\n",
            "Epoch 4 - Score: 0.19928820504924205\n",
            "Epoch 4 - Score: 0.19928820504924205\n",
            "Epoch 4 - Score: 0.19928820504924205\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EVAL: [76/77] Elapsed 0m 8s (remain 0m 0s) Loss: 0.1993 \n",
            "Log loss for 3 is 0.19928820504924205\n",
            "Epoch: [5][0/306] Elapsed 0m 0s (remain 3m 10s) Loss: 0.0194 \n",
            "Epoch: [5][100/306] Elapsed 0m 34s (remain 1m 9s) Loss: 0.0104 \n",
            "Epoch: [5][200/306] Elapsed 1m 7s (remain 0m 35s) Loss: 0.0139 \n",
            "Epoch: [5][300/306] Elapsed 1m 41s (remain 0m 1s) Loss: 0.0195 \n",
            "Epoch: [5][305/306] Elapsed 1m 42s (remain 0m 0s) Loss: 0.0199 \n",
            "EVAL: [0/77] Elapsed 0m 0s (remain 0m 28s) Loss: 0.8257 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5 - avg_train_loss: 0.0199  avg_val_loss: 0.2720  time: 112s\n",
            "Epoch 5 - avg_train_loss: 0.0199  avg_val_loss: 0.2720  time: 112s\n",
            "Epoch 5 - avg_train_loss: 0.0199  avg_val_loss: 0.2720  time: 112s\n",
            "Epoch 5 - avg_train_loss: 0.0199  avg_val_loss: 0.2720  time: 112s\n",
            "Epoch 5 - avg_train_loss: 0.0199  avg_val_loss: 0.2720  time: 112s\n",
            "Epoch 5 - avg_train_loss: 0.0199  avg_val_loss: 0.2720  time: 112s\n",
            "Epoch 5 - avg_train_loss: 0.0199  avg_val_loss: 0.2720  time: 112s\n",
            "Epoch 5 - avg_train_loss: 0.0199  avg_val_loss: 0.2720  time: 112s\n",
            "Epoch 5 - avg_train_loss: 0.0199  avg_val_loss: 0.2720  time: 112s\n",
            "Epoch 5 - avg_train_loss: 0.0199  avg_val_loss: 0.2720  time: 112s\n",
            "Epoch 5 - avg_train_loss: 0.0199  avg_val_loss: 0.2720  time: 112s\n",
            "Epoch 5 - avg_train_loss: 0.0199  avg_val_loss: 0.2720  time: 112s\n",
            "Epoch 5 - Score: 0.2720260888865563\n",
            "Epoch 5 - Score: 0.2720260888865563\n",
            "Epoch 5 - Score: 0.2720260888865563\n",
            "Epoch 5 - Score: 0.2720260888865563\n",
            "Epoch 5 - Score: 0.2720260888865563\n",
            "Epoch 5 - Score: 0.2720260888865563\n",
            "Epoch 5 - Score: 0.2720260888865563\n",
            "Epoch 5 - Score: 0.2720260888865563\n",
            "Epoch 5 - Score: 0.2720260888865563\n",
            "Epoch 5 - Score: 0.2720260888865563\n",
            "Epoch 5 - Score: 0.2720260888865563\n",
            "Epoch 5 - Score: 0.2720260888865563\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EVAL: [76/77] Elapsed 0m 8s (remain 0m 0s) Loss: 0.2720 \n",
            "Log loss for 4 is 0.2720260888865563\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "========== fold: 4 result ==========\n",
            "========== fold: 4 result ==========\n",
            "========== fold: 4 result ==========\n",
            "========== fold: 4 result ==========\n",
            "========== fold: 4 result ==========\n",
            "========== fold: 4 result ==========\n",
            "========== fold: 4 result ==========\n",
            "========== fold: 4 result ==========\n",
            "========== fold: 4 result ==========\n",
            "========== fold: 4 result ==========\n",
            "========== fold: 4 result ==========\n",
            "========== fold: 4 result ==========\n",
            "Score: 0.16181\n",
            "Score: 0.16181\n",
            "Score: 0.16181\n",
            "Score: 0.16181\n",
            "Score: 0.16181\n",
            "Score: 0.16181\n",
            "Score: 0.16181\n",
            "Score: 0.16181\n",
            "Score: 0.16181\n",
            "Score: 0.16181\n",
            "Score: 0.16181\n",
            "Score: 0.16181\n",
            "========== CV ==========\n",
            "========== CV ==========\n",
            "========== CV ==========\n",
            "========== CV ==========\n",
            "========== CV ==========\n",
            "========== CV ==========\n",
            "========== CV ==========\n",
            "========== CV ==========\n",
            "========== CV ==========\n",
            "========== CV ==========\n",
            "========== CV ==========\n",
            "========== CV ==========\n",
            "Score: 0.13915\n",
            "Score: 0.13915\n",
            "Score: 0.13915\n",
            "Score: 0.13915\n",
            "Score: 0.13915\n",
            "Score: 0.13915\n",
            "Score: 0.13915\n",
            "Score: 0.13915\n",
            "Score: 0.13915\n",
            "Score: 0.13915\n",
            "Score: 0.13915\n",
            "Score: 0.13915\n",
            "Didn't find file /content/activebus_bert_final/added_tokens.json. We won't load it.\n",
            "loading file /content/activebus_bert_final/vocab.txt\n",
            "loading file None\n",
            "loading file /content/activebus_bert_final/special_tokens_map.json\n",
            "loading file /content/activebus_bert_final/tokenizer_config.json\n",
            "loading file /content/activebus_bert_final/tokenizer.json\n",
            "========== model: bert-base-uncased fold: 0 inference ==========\n",
            "========== model: bert-base-uncased fold: 0 inference ==========\n",
            "========== model: bert-base-uncased fold: 0 inference ==========\n",
            "========== model: bert-base-uncased fold: 0 inference ==========\n",
            "========== model: bert-base-uncased fold: 0 inference ==========\n",
            "========== model: bert-base-uncased fold: 0 inference ==========\n",
            "========== model: bert-base-uncased fold: 0 inference ==========\n",
            "========== model: bert-base-uncased fold: 0 inference ==========\n",
            "========== model: bert-base-uncased fold: 0 inference ==========\n",
            "========== model: bert-base-uncased fold: 0 inference ==========\n",
            "========== model: bert-base-uncased fold: 0 inference ==========\n",
            "========== model: bert-base-uncased fold: 0 inference ==========\n",
            "loading configuration file /content/activebus_bert_final/config.json\n",
            "Model config BertConfig {\n",
            "  \"_name_or_path\": \"activebus/BERT-XD_Review\",\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_ids\": 0,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.15.0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "loading weights file /content/activebus_bert_final/pytorch_model.bin\n",
            "Some weights of the model checkpoint at /content/activebus_bert_final were not used when initializing BertForSequenceClassification: ['cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /content/activebus_bert_final and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "\n",
            "  0%|          | 0/165 [00:00<?, ?it/s]\u001b[A\n",
            "  1%|          | 1/165 [00:00<00:21,  7.62it/s]\u001b[A\n",
            "  1%|          | 2/165 [00:00<00:20,  7.87it/s]\u001b[A\n",
            "  2%|▏         | 3/165 [00:00<00:19,  8.22it/s]\u001b[A\n",
            "  2%|▏         | 4/165 [00:00<00:19,  8.40it/s]\u001b[A\n",
            "  3%|▎         | 5/165 [00:00<00:18,  8.57it/s]\u001b[A\n",
            "  4%|▎         | 6/165 [00:00<00:18,  8.66it/s]\u001b[A\n",
            "  4%|▍         | 7/165 [00:00<00:18,  8.75it/s]\u001b[A\n",
            "  5%|▍         | 8/165 [00:00<00:17,  8.79it/s]\u001b[A\n",
            "  5%|▌         | 9/165 [00:01<00:17,  8.78it/s]\u001b[A\n",
            "  6%|▌         | 10/165 [00:01<00:17,  8.82it/s]\u001b[A\n",
            "  7%|▋         | 11/165 [00:01<00:17,  8.84it/s]\u001b[A\n",
            "  7%|▋         | 12/165 [00:01<00:17,  8.82it/s]\u001b[A\n",
            "  8%|▊         | 13/165 [00:01<00:17,  8.85it/s]\u001b[A\n",
            "  8%|▊         | 14/165 [00:01<00:17,  8.83it/s]\u001b[A\n",
            "  9%|▉         | 15/165 [00:01<00:17,  8.82it/s]\u001b[A\n",
            " 10%|▉         | 16/165 [00:01<00:16,  8.83it/s]\u001b[A\n",
            " 10%|█         | 17/165 [00:01<00:16,  8.84it/s]\u001b[A\n",
            " 11%|█         | 18/165 [00:02<00:16,  8.85it/s]\u001b[A\n",
            " 12%|█▏        | 19/165 [00:02<00:16,  8.85it/s]\u001b[A\n",
            " 12%|█▏        | 20/165 [00:02<00:16,  8.83it/s]\u001b[A\n",
            " 13%|█▎        | 21/165 [00:02<00:16,  8.84it/s]\u001b[A\n",
            " 13%|█▎        | 22/165 [00:02<00:16,  8.84it/s]\u001b[A\n",
            " 14%|█▍        | 23/165 [00:02<00:16,  8.85it/s]\u001b[A\n",
            " 15%|█▍        | 24/165 [00:02<00:15,  8.87it/s]\u001b[A\n",
            " 15%|█▌        | 25/165 [00:02<00:15,  8.88it/s]\u001b[A\n",
            " 16%|█▌        | 26/165 [00:02<00:15,  8.88it/s]\u001b[A\n",
            " 16%|█▋        | 27/165 [00:03<00:15,  8.87it/s]\u001b[A\n",
            " 17%|█▋        | 28/165 [00:03<00:15,  8.86it/s]\u001b[A\n",
            " 18%|█▊        | 29/165 [00:03<00:15,  8.89it/s]\u001b[A\n",
            " 18%|█▊        | 30/165 [00:03<00:15,  8.88it/s]\u001b[A\n",
            " 19%|█▉        | 31/165 [00:03<00:15,  8.88it/s]\u001b[A\n",
            " 19%|█▉        | 32/165 [00:03<00:15,  8.84it/s]\u001b[A\n",
            " 20%|██        | 33/165 [00:03<00:14,  8.87it/s]\u001b[A\n",
            " 21%|██        | 34/165 [00:03<00:14,  8.88it/s]\u001b[A\n",
            " 21%|██        | 35/165 [00:03<00:14,  8.90it/s]\u001b[A\n",
            " 22%|██▏       | 36/165 [00:04<00:14,  8.88it/s]\u001b[A\n",
            " 22%|██▏       | 37/165 [00:04<00:14,  8.86it/s]\u001b[A\n",
            " 23%|██▎       | 38/165 [00:04<00:14,  8.85it/s]\u001b[A\n",
            " 24%|██▎       | 39/165 [00:04<00:14,  8.86it/s]\u001b[A\n",
            " 24%|██▍       | 40/165 [00:04<00:14,  8.86it/s]\u001b[A\n",
            " 25%|██▍       | 41/165 [00:04<00:13,  8.90it/s]\u001b[A\n",
            " 25%|██▌       | 42/165 [00:04<00:13,  8.89it/s]\u001b[A\n",
            " 26%|██▌       | 43/165 [00:04<00:13,  8.89it/s]\u001b[A\n",
            " 27%|██▋       | 44/165 [00:04<00:13,  8.90it/s]\u001b[A\n",
            " 27%|██▋       | 45/165 [00:05<00:13,  8.88it/s]\u001b[A\n",
            " 28%|██▊       | 46/165 [00:05<00:13,  8.85it/s]\u001b[A\n",
            " 28%|██▊       | 47/165 [00:05<00:13,  8.83it/s]\u001b[A\n",
            " 29%|██▉       | 48/165 [00:05<00:13,  8.85it/s]\u001b[A\n",
            " 30%|██▉       | 49/165 [00:05<00:13,  8.86it/s]\u001b[A\n",
            " 30%|███       | 50/165 [00:05<00:13,  8.85it/s]\u001b[A\n",
            " 31%|███       | 51/165 [00:05<00:12,  8.86it/s]\u001b[A\n",
            " 32%|███▏      | 52/165 [00:05<00:12,  8.87it/s]\u001b[A\n",
            " 32%|███▏      | 53/165 [00:06<00:12,  8.87it/s]\u001b[A\n",
            " 33%|███▎      | 54/165 [00:06<00:12,  8.85it/s]\u001b[A\n",
            " 33%|███▎      | 55/165 [00:06<00:12,  8.86it/s]\u001b[A\n",
            " 34%|███▍      | 56/165 [00:06<00:12,  8.86it/s]\u001b[A\n",
            " 35%|███▍      | 57/165 [00:06<00:12,  8.87it/s]\u001b[A\n",
            " 35%|███▌      | 58/165 [00:06<00:12,  8.86it/s]\u001b[A\n",
            " 36%|███▌      | 59/165 [00:06<00:11,  8.87it/s]\u001b[A\n",
            " 36%|███▋      | 60/165 [00:06<00:11,  8.88it/s]\u001b[A\n",
            " 37%|███▋      | 61/165 [00:06<00:11,  8.89it/s]\u001b[A\n",
            " 38%|███▊      | 62/165 [00:07<00:11,  8.83it/s]\u001b[A\n",
            " 38%|███▊      | 63/165 [00:07<00:11,  8.85it/s]\u001b[A\n",
            " 39%|███▉      | 64/165 [00:07<00:11,  8.82it/s]\u001b[A\n",
            " 39%|███▉      | 65/165 [00:07<00:11,  8.81it/s]\u001b[A\n",
            " 40%|████      | 66/165 [00:07<00:11,  8.82it/s]\u001b[A\n",
            " 41%|████      | 67/165 [00:07<00:11,  8.84it/s]\u001b[A\n",
            " 41%|████      | 68/165 [00:07<00:10,  8.86it/s]\u001b[A\n",
            " 42%|████▏     | 69/165 [00:07<00:10,  8.85it/s]\u001b[A\n",
            " 42%|████▏     | 70/165 [00:07<00:10,  8.86it/s]\u001b[A\n",
            " 43%|████▎     | 71/165 [00:08<00:10,  8.85it/s]\u001b[A\n",
            " 44%|████▎     | 72/165 [00:08<00:10,  8.85it/s]\u001b[A\n",
            " 44%|████▍     | 73/165 [00:08<00:10,  8.82it/s]\u001b[A\n",
            " 45%|████▍     | 74/165 [00:08<00:10,  8.83it/s]\u001b[A\n",
            " 45%|████▌     | 75/165 [00:08<00:10,  8.83it/s]\u001b[A\n",
            " 46%|████▌     | 76/165 [00:08<00:10,  8.84it/s]\u001b[A\n",
            " 47%|████▋     | 77/165 [00:08<00:09,  8.85it/s]\u001b[A\n",
            " 47%|████▋     | 78/165 [00:08<00:09,  8.88it/s]\u001b[A\n",
            " 48%|████▊     | 79/165 [00:08<00:09,  8.84it/s]\u001b[A\n",
            " 48%|████▊     | 80/165 [00:09<00:09,  8.84it/s]\u001b[A\n",
            " 49%|████▉     | 81/165 [00:09<00:09,  8.83it/s]\u001b[A\n",
            " 50%|████▉     | 82/165 [00:09<00:09,  8.81it/s]\u001b[A\n",
            " 50%|█████     | 83/165 [00:09<00:09,  8.81it/s]\u001b[A\n",
            " 51%|█████     | 84/165 [00:09<00:09,  8.83it/s]\u001b[A\n",
            " 52%|█████▏    | 85/165 [00:09<00:09,  8.82it/s]\u001b[A\n",
            " 52%|█████▏    | 86/165 [00:09<00:08,  8.84it/s]\u001b[A\n",
            " 53%|█████▎    | 87/165 [00:09<00:08,  8.82it/s]\u001b[A\n",
            " 53%|█████▎    | 88/165 [00:09<00:08,  8.82it/s]\u001b[A\n",
            " 54%|█████▍    | 89/165 [00:10<00:08,  8.80it/s]\u001b[A\n",
            " 55%|█████▍    | 90/165 [00:10<00:08,  8.78it/s]\u001b[A\n",
            " 55%|█████▌    | 91/165 [00:10<00:08,  8.80it/s]\u001b[A\n",
            " 56%|█████▌    | 92/165 [00:10<00:08,  8.80it/s]\u001b[A\n",
            " 56%|█████▋    | 93/165 [00:10<00:08,  8.80it/s]\u001b[A\n",
            " 57%|█████▋    | 94/165 [00:10<00:08,  8.80it/s]\u001b[A\n",
            " 58%|█████▊    | 95/165 [00:10<00:07,  8.79it/s]\u001b[A\n",
            " 58%|█████▊    | 96/165 [00:10<00:07,  8.81it/s]\u001b[A\n",
            " 59%|█████▉    | 97/165 [00:10<00:07,  8.82it/s]\u001b[A\n",
            " 59%|█████▉    | 98/165 [00:11<00:07,  8.82it/s]\u001b[A\n",
            " 60%|██████    | 99/165 [00:11<00:07,  8.82it/s]\u001b[A\n",
            " 61%|██████    | 100/165 [00:11<00:07,  8.78it/s]\u001b[A\n",
            " 61%|██████    | 101/165 [00:11<00:07,  8.80it/s]\u001b[A\n",
            " 62%|██████▏   | 102/165 [00:11<00:07,  8.80it/s]\u001b[A\n",
            " 62%|██████▏   | 103/165 [00:11<00:07,  8.80it/s]\u001b[A\n",
            " 63%|██████▎   | 104/165 [00:11<00:06,  8.82it/s]\u001b[A\n",
            " 64%|██████▎   | 105/165 [00:11<00:06,  8.85it/s]\u001b[A\n",
            " 64%|██████▍   | 106/165 [00:12<00:06,  8.76it/s]\u001b[A\n",
            " 65%|██████▍   | 107/165 [00:12<00:06,  8.78it/s]\u001b[A\n",
            " 65%|██████▌   | 108/165 [00:12<00:06,  8.81it/s]\u001b[A\n",
            " 66%|██████▌   | 109/165 [00:12<00:06,  8.81it/s]\u001b[A\n",
            " 67%|██████▋   | 110/165 [00:12<00:06,  8.84it/s]\u001b[A\n",
            " 67%|██████▋   | 111/165 [00:12<00:06,  8.84it/s]\u001b[A\n",
            " 68%|██████▊   | 112/165 [00:12<00:05,  8.86it/s]\u001b[A\n",
            " 68%|██████▊   | 113/165 [00:12<00:05,  8.88it/s]\u001b[A\n",
            " 69%|██████▉   | 114/165 [00:12<00:05,  8.83it/s]\u001b[A\n",
            " 70%|██████▉   | 115/165 [00:13<00:05,  8.85it/s]\u001b[A\n",
            " 70%|███████   | 116/165 [00:13<00:05,  8.86it/s]\u001b[A\n",
            " 71%|███████   | 117/165 [00:13<00:05,  8.87it/s]\u001b[A\n",
            " 72%|███████▏  | 118/165 [00:13<00:05,  8.85it/s]\u001b[A\n",
            " 72%|███████▏  | 119/165 [00:13<00:05,  8.84it/s]\u001b[A\n",
            " 73%|███████▎  | 120/165 [00:13<00:05,  8.85it/s]\u001b[A\n",
            " 73%|███████▎  | 121/165 [00:13<00:04,  8.88it/s]\u001b[A\n",
            " 74%|███████▍  | 122/165 [00:13<00:04,  8.89it/s]\u001b[A\n",
            " 75%|███████▍  | 123/165 [00:13<00:04,  8.87it/s]\u001b[A\n",
            " 75%|███████▌  | 124/165 [00:14<00:04,  8.88it/s]\u001b[A\n",
            " 76%|███████▌  | 125/165 [00:14<00:04,  8.87it/s]\u001b[A\n",
            " 76%|███████▋  | 126/165 [00:14<00:04,  8.88it/s]\u001b[A\n",
            " 77%|███████▋  | 127/165 [00:14<00:04,  8.87it/s]\u001b[A\n",
            " 78%|███████▊  | 128/165 [00:14<00:04,  8.86it/s]\u001b[A\n",
            " 78%|███████▊  | 129/165 [00:14<00:04,  8.85it/s]\u001b[A\n",
            " 79%|███████▉  | 130/165 [00:14<00:03,  8.83it/s]\u001b[A\n",
            " 79%|███████▉  | 131/165 [00:14<00:03,  8.83it/s]\u001b[A\n",
            " 80%|████████  | 132/165 [00:14<00:03,  8.85it/s]\u001b[A\n",
            " 81%|████████  | 133/165 [00:15<00:03,  8.85it/s]\u001b[A\n",
            " 81%|████████  | 134/165 [00:15<00:03,  8.84it/s]\u001b[A\n",
            " 82%|████████▏ | 135/165 [00:15<00:03,  8.83it/s]\u001b[A\n",
            " 82%|████████▏ | 136/165 [00:15<00:03,  8.83it/s]\u001b[A\n",
            " 83%|████████▎ | 137/165 [00:15<00:03,  8.85it/s]\u001b[A\n",
            " 84%|████████▎ | 138/165 [00:15<00:03,  8.87it/s]\u001b[A\n",
            " 84%|████████▍ | 139/165 [00:15<00:02,  8.83it/s]\u001b[A\n",
            " 85%|████████▍ | 140/165 [00:15<00:02,  8.83it/s]\u001b[A\n",
            " 85%|████████▌ | 141/165 [00:15<00:02,  8.85it/s]\u001b[A\n",
            " 86%|████████▌ | 142/165 [00:16<00:02,  8.86it/s]\u001b[A\n",
            " 87%|████████▋ | 143/165 [00:16<00:02,  8.84it/s]\u001b[A\n",
            " 87%|████████▋ | 144/165 [00:16<00:02,  8.86it/s]\u001b[A\n",
            " 88%|████████▊ | 145/165 [00:16<00:02,  8.87it/s]\u001b[A\n",
            " 88%|████████▊ | 146/165 [00:16<00:02,  8.89it/s]\u001b[A\n",
            " 89%|████████▉ | 147/165 [00:16<00:02,  8.88it/s]\u001b[A\n",
            " 90%|████████▉ | 148/165 [00:16<00:01,  8.87it/s]\u001b[A\n",
            " 90%|█████████ | 149/165 [00:16<00:01,  8.85it/s]\u001b[A\n",
            " 91%|█████████ | 150/165 [00:16<00:01,  8.82it/s]\u001b[A\n",
            " 92%|█████████▏| 151/165 [00:17<00:01,  8.86it/s]\u001b[A\n",
            " 92%|█████████▏| 152/165 [00:17<00:01,  8.84it/s]\u001b[A\n",
            " 93%|█████████▎| 153/165 [00:17<00:01,  8.86it/s]\u001b[A\n",
            " 93%|█████████▎| 154/165 [00:17<00:01,  8.84it/s]\u001b[A\n",
            " 94%|█████████▍| 155/165 [00:17<00:01,  8.83it/s]\u001b[A\n",
            " 95%|█████████▍| 156/165 [00:17<00:01,  8.80it/s]\u001b[A\n",
            " 95%|█████████▌| 157/165 [00:17<00:00,  8.79it/s]\u001b[A\n",
            " 96%|█████████▌| 158/165 [00:17<00:00,  8.81it/s]\u001b[A\n",
            " 96%|█████████▋| 159/165 [00:18<00:00,  8.84it/s]\u001b[A\n",
            " 97%|█████████▋| 160/165 [00:18<00:00,  8.84it/s]\u001b[A\n",
            " 98%|█████████▊| 161/165 [00:18<00:00,  8.85it/s]\u001b[A\n",
            " 98%|█████████▊| 162/165 [00:18<00:00,  8.87it/s]\u001b[A\n",
            " 99%|█████████▉| 163/165 [00:18<00:00,  8.87it/s]\u001b[A\n",
            "100%|██████████| 165/165 [00:18<00:00,  8.82it/s]\n",
            "========== model: bert-base-uncased fold: 1 inference ==========\n",
            "========== model: bert-base-uncased fold: 1 inference ==========\n",
            "========== model: bert-base-uncased fold: 1 inference ==========\n",
            "========== model: bert-base-uncased fold: 1 inference ==========\n",
            "========== model: bert-base-uncased fold: 1 inference ==========\n",
            "========== model: bert-base-uncased fold: 1 inference ==========\n",
            "========== model: bert-base-uncased fold: 1 inference ==========\n",
            "========== model: bert-base-uncased fold: 1 inference ==========\n",
            "========== model: bert-base-uncased fold: 1 inference ==========\n",
            "========== model: bert-base-uncased fold: 1 inference ==========\n",
            "========== model: bert-base-uncased fold: 1 inference ==========\n",
            "========== model: bert-base-uncased fold: 1 inference ==========\n",
            "loading configuration file /content/activebus_bert_final/config.json\n",
            "Model config BertConfig {\n",
            "  \"_name_or_path\": \"activebus/BERT-XD_Review\",\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_ids\": 0,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.15.0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "loading weights file /content/activebus_bert_final/pytorch_model.bin\n",
            "Some weights of the model checkpoint at /content/activebus_bert_final were not used when initializing BertForSequenceClassification: ['cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /content/activebus_bert_final and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "\n",
            "  0%|          | 0/165 [00:00<?, ?it/s]\u001b[A\n",
            "  1%|          | 1/165 [00:00<00:20,  7.97it/s]\u001b[A\n",
            "  1%|          | 2/165 [00:00<00:19,  8.44it/s]\u001b[A\n",
            "  2%|▏         | 3/165 [00:00<00:18,  8.61it/s]\u001b[A\n",
            "  2%|▏         | 4/165 [00:00<00:18,  8.73it/s]\u001b[A\n",
            "  3%|▎         | 5/165 [00:00<00:18,  8.78it/s]\u001b[A\n",
            "  4%|▎         | 6/165 [00:00<00:18,  8.83it/s]\u001b[A\n",
            "  4%|▍         | 7/165 [00:00<00:17,  8.86it/s]\u001b[A\n",
            "  5%|▍         | 8/165 [00:00<00:17,  8.84it/s]\u001b[A\n",
            "  5%|▌         | 9/165 [00:01<00:17,  8.85it/s]\u001b[A\n",
            "  6%|▌         | 10/165 [00:01<00:17,  8.86it/s]\u001b[A\n",
            "  7%|▋         | 11/165 [00:01<00:17,  8.84it/s]\u001b[A\n",
            "  7%|▋         | 12/165 [00:01<00:17,  8.85it/s]\u001b[A\n",
            "  8%|▊         | 13/165 [00:01<00:17,  8.85it/s]\u001b[A\n",
            "  8%|▊         | 14/165 [00:01<00:17,  8.86it/s]\u001b[A\n",
            "  9%|▉         | 15/165 [00:01<00:16,  8.85it/s]\u001b[A\n",
            " 10%|▉         | 16/165 [00:01<00:16,  8.86it/s]\u001b[A\n",
            " 10%|█         | 17/165 [00:01<00:16,  8.87it/s]\u001b[A\n",
            " 11%|█         | 18/165 [00:02<00:16,  8.85it/s]\u001b[A\n",
            " 12%|█▏        | 19/165 [00:02<00:16,  8.86it/s]\u001b[A\n",
            " 12%|█▏        | 20/165 [00:02<00:16,  8.86it/s]\u001b[A\n",
            " 13%|█▎        | 21/165 [00:02<00:16,  8.88it/s]\u001b[A\n",
            " 13%|█▎        | 22/165 [00:02<00:16,  8.87it/s]\u001b[A\n",
            " 14%|█▍        | 23/165 [00:02<00:16,  8.85it/s]\u001b[A\n",
            " 15%|█▍        | 24/165 [00:02<00:15,  8.83it/s]\u001b[A\n",
            " 15%|█▌        | 25/165 [00:02<00:15,  8.83it/s]\u001b[A\n",
            " 16%|█▌        | 26/165 [00:02<00:15,  8.81it/s]\u001b[A\n",
            " 16%|█▋        | 27/165 [00:03<00:15,  8.80it/s]\u001b[A\n",
            " 17%|█▋        | 28/165 [00:03<00:15,  8.85it/s]\u001b[A\n",
            " 18%|█▊        | 29/165 [00:03<00:15,  8.81it/s]\u001b[A\n",
            " 18%|█▊        | 30/165 [00:03<00:15,  8.79it/s]\u001b[A\n",
            " 19%|█▉        | 31/165 [00:03<00:15,  8.81it/s]\u001b[A\n",
            " 19%|█▉        | 32/165 [00:03<00:15,  8.81it/s]\u001b[A\n",
            " 20%|██        | 33/165 [00:03<00:14,  8.81it/s]\u001b[A\n",
            " 21%|██        | 34/165 [00:03<00:14,  8.81it/s]\u001b[A\n",
            " 21%|██        | 35/165 [00:03<00:14,  8.82it/s]\u001b[A\n",
            " 22%|██▏       | 36/165 [00:04<00:14,  8.84it/s]\u001b[A\n",
            " 22%|██▏       | 37/165 [00:04<00:14,  8.86it/s]\u001b[A\n",
            " 23%|██▎       | 38/165 [00:04<00:14,  8.86it/s]\u001b[A\n",
            " 24%|██▎       | 39/165 [00:04<00:14,  8.84it/s]\u001b[A\n",
            " 24%|██▍       | 40/165 [00:04<00:14,  8.83it/s]\u001b[A\n",
            " 25%|██▍       | 41/165 [00:04<00:14,  8.85it/s]\u001b[A\n",
            " 25%|██▌       | 42/165 [00:04<00:13,  8.86it/s]\u001b[A\n",
            " 26%|██▌       | 43/165 [00:04<00:13,  8.87it/s]\u001b[A\n",
            " 27%|██▋       | 44/165 [00:04<00:13,  8.85it/s]\u001b[A\n",
            " 27%|██▋       | 45/165 [00:05<00:13,  8.85it/s]\u001b[A\n",
            " 28%|██▊       | 46/165 [00:05<00:13,  8.85it/s]\u001b[A\n",
            " 28%|██▊       | 47/165 [00:05<00:13,  8.83it/s]\u001b[A\n",
            " 29%|██▉       | 48/165 [00:05<00:13,  8.83it/s]\u001b[A\n",
            " 30%|██▉       | 49/165 [00:05<00:13,  8.79it/s]\u001b[A\n",
            " 30%|███       | 50/165 [00:05<00:13,  8.79it/s]\u001b[A\n",
            " 31%|███       | 51/165 [00:05<00:12,  8.80it/s]\u001b[A\n",
            " 32%|███▏      | 52/165 [00:05<00:12,  8.81it/s]\u001b[A\n",
            " 32%|███▏      | 53/165 [00:06<00:12,  8.77it/s]\u001b[A\n",
            " 33%|███▎      | 54/165 [00:06<00:12,  8.76it/s]\u001b[A\n",
            " 33%|███▎      | 55/165 [00:06<00:12,  8.79it/s]\u001b[A\n",
            " 34%|███▍      | 56/165 [00:06<00:12,  8.79it/s]\u001b[A\n",
            " 35%|███▍      | 57/165 [00:06<00:12,  8.80it/s]\u001b[A\n",
            " 35%|███▌      | 58/165 [00:06<00:12,  8.79it/s]\u001b[A\n",
            " 36%|███▌      | 59/165 [00:06<00:12,  8.83it/s]\u001b[A\n",
            " 36%|███▋      | 60/165 [00:06<00:11,  8.82it/s]\u001b[A\n",
            " 37%|███▋      | 61/165 [00:06<00:11,  8.73it/s]\u001b[A\n",
            " 38%|███▊      | 62/165 [00:07<00:11,  8.79it/s]\u001b[A\n",
            " 38%|███▊      | 63/165 [00:07<00:11,  8.80it/s]\u001b[A\n",
            " 39%|███▉      | 64/165 [00:07<00:11,  8.80it/s]\u001b[A\n",
            " 39%|███▉      | 65/165 [00:07<00:11,  8.82it/s]\u001b[A\n",
            " 40%|████      | 66/165 [00:07<00:11,  8.81it/s]\u001b[A\n",
            " 41%|████      | 67/165 [00:07<00:11,  8.78it/s]\u001b[A\n",
            " 41%|████      | 68/165 [00:07<00:10,  8.82it/s]\u001b[A\n",
            " 42%|████▏     | 69/165 [00:07<00:10,  8.83it/s]\u001b[A\n",
            " 42%|████▏     | 70/165 [00:07<00:10,  8.84it/s]\u001b[A\n",
            " 43%|████▎     | 71/165 [00:08<00:10,  8.83it/s]\u001b[A\n",
            " 44%|████▎     | 72/165 [00:08<00:10,  8.85it/s]\u001b[A\n",
            " 44%|████▍     | 73/165 [00:08<00:10,  8.86it/s]\u001b[A\n",
            " 45%|████▍     | 74/165 [00:08<00:10,  8.86it/s]\u001b[A\n",
            " 45%|████▌     | 75/165 [00:08<00:10,  8.85it/s]\u001b[A\n",
            " 46%|████▌     | 76/165 [00:08<00:10,  8.87it/s]\u001b[A\n",
            " 47%|████▋     | 77/165 [00:08<00:09,  8.85it/s]\u001b[A\n",
            " 47%|████▋     | 78/165 [00:08<00:09,  8.87it/s]\u001b[A\n",
            " 48%|████▊     | 79/165 [00:08<00:09,  8.90it/s]\u001b[A\n",
            " 48%|████▊     | 80/165 [00:09<00:09,  8.88it/s]\u001b[A\n",
            " 49%|████▉     | 81/165 [00:09<00:09,  8.88it/s]\u001b[A\n",
            " 50%|████▉     | 82/165 [00:09<00:09,  8.87it/s]\u001b[A\n",
            " 50%|█████     | 83/165 [00:09<00:09,  8.84it/s]\u001b[A\n",
            " 51%|█████     | 84/165 [00:09<00:09,  8.84it/s]\u001b[A\n",
            " 52%|█████▏    | 85/165 [00:09<00:09,  8.85it/s]\u001b[A\n",
            " 52%|█████▏    | 86/165 [00:09<00:08,  8.84it/s]\u001b[A\n",
            " 53%|█████▎    | 87/165 [00:09<00:08,  8.85it/s]\u001b[A\n",
            " 53%|█████▎    | 88/165 [00:09<00:08,  8.85it/s]\u001b[A\n",
            " 54%|█████▍    | 89/165 [00:10<00:08,  8.84it/s]\u001b[A\n",
            " 55%|█████▍    | 90/165 [00:10<00:08,  8.80it/s]\u001b[A\n",
            " 55%|█████▌    | 91/165 [00:10<00:08,  8.81it/s]\u001b[A\n",
            " 56%|█████▌    | 92/165 [00:10<00:08,  8.82it/s]\u001b[A\n",
            " 56%|█████▋    | 93/165 [00:10<00:08,  8.84it/s]\u001b[A\n",
            " 57%|█████▋    | 94/165 [00:10<00:08,  8.86it/s]\u001b[A\n",
            " 58%|█████▊    | 95/165 [00:10<00:07,  8.88it/s]\u001b[A\n",
            " 58%|█████▊    | 96/165 [00:10<00:07,  8.86it/s]\u001b[A\n",
            " 59%|█████▉    | 97/165 [00:10<00:07,  8.86it/s]\u001b[A\n",
            " 59%|█████▉    | 98/165 [00:11<00:07,  8.84it/s]\u001b[A\n",
            " 60%|██████    | 99/165 [00:11<00:07,  8.85it/s]\u001b[A\n",
            " 61%|██████    | 100/165 [00:11<00:07,  8.84it/s]\u001b[A\n",
            " 61%|██████    | 101/165 [00:11<00:07,  8.84it/s]\u001b[A\n",
            " 62%|██████▏   | 102/165 [00:11<00:07,  8.87it/s]\u001b[A\n",
            " 62%|██████▏   | 103/165 [00:11<00:06,  8.87it/s]\u001b[A\n",
            " 63%|██████▎   | 104/165 [00:11<00:06,  8.88it/s]\u001b[A\n",
            " 64%|██████▎   | 105/165 [00:11<00:06,  8.86it/s]\u001b[A\n",
            " 64%|██████▍   | 106/165 [00:12<00:06,  8.84it/s]\u001b[A\n",
            " 65%|██████▍   | 107/165 [00:12<00:06,  8.87it/s]\u001b[A\n",
            " 65%|██████▌   | 108/165 [00:12<00:06,  8.78it/s]\u001b[A\n",
            " 66%|██████▌   | 109/165 [00:12<00:06,  8.80it/s]\u001b[A\n",
            " 67%|██████▋   | 110/165 [00:12<00:06,  8.83it/s]\u001b[A\n",
            " 67%|██████▋   | 111/165 [00:12<00:06,  8.84it/s]\u001b[A\n",
            " 68%|██████▊   | 112/165 [00:12<00:05,  8.86it/s]\u001b[A\n",
            " 68%|██████▊   | 113/165 [00:12<00:05,  8.85it/s]\u001b[A\n",
            " 69%|██████▉   | 114/165 [00:12<00:05,  8.83it/s]\u001b[A\n",
            " 70%|██████▉   | 115/165 [00:13<00:05,  8.83it/s]\u001b[A\n",
            " 70%|███████   | 116/165 [00:13<00:05,  8.86it/s]\u001b[A\n",
            " 71%|███████   | 117/165 [00:13<00:05,  8.88it/s]\u001b[A\n",
            " 72%|███████▏  | 118/165 [00:13<00:05,  8.86it/s]\u001b[A\n",
            " 72%|███████▏  | 119/165 [00:13<00:05,  8.85it/s]\u001b[A\n",
            " 73%|███████▎  | 120/165 [00:13<00:05,  8.86it/s]\u001b[A\n",
            " 73%|███████▎  | 121/165 [00:13<00:04,  8.83it/s]\u001b[A\n",
            " 74%|███████▍  | 122/165 [00:13<00:04,  8.85it/s]\u001b[A\n",
            " 75%|███████▍  | 123/165 [00:13<00:04,  8.86it/s]\u001b[A\n",
            " 75%|███████▌  | 124/165 [00:14<00:04,  8.87it/s]\u001b[A\n",
            " 76%|███████▌  | 125/165 [00:14<00:04,  8.88it/s]\u001b[A\n",
            " 76%|███████▋  | 126/165 [00:14<00:04,  8.87it/s]\u001b[A\n",
            " 77%|███████▋  | 127/165 [00:14<00:04,  8.87it/s]\u001b[A\n",
            " 78%|███████▊  | 128/165 [00:14<00:04,  8.85it/s]\u001b[A\n",
            " 78%|███████▊  | 129/165 [00:14<00:04,  8.84it/s]\u001b[A\n",
            " 79%|███████▉  | 130/165 [00:14<00:03,  8.82it/s]\u001b[A\n",
            " 79%|███████▉  | 131/165 [00:14<00:03,  8.83it/s]\u001b[A\n",
            " 80%|████████  | 132/165 [00:14<00:03,  8.82it/s]\u001b[A\n",
            " 81%|████████  | 133/165 [00:15<00:03,  8.85it/s]\u001b[A\n",
            " 81%|████████  | 134/165 [00:15<00:03,  8.84it/s]\u001b[A\n",
            " 82%|████████▏ | 135/165 [00:15<00:03,  8.82it/s]\u001b[A\n",
            " 82%|████████▏ | 136/165 [00:15<00:03,  8.83it/s]\u001b[A\n",
            " 83%|████████▎ | 137/165 [00:15<00:03,  8.85it/s]\u001b[A\n",
            " 84%|████████▎ | 138/165 [00:15<00:03,  8.84it/s]\u001b[A\n",
            " 84%|████████▍ | 139/165 [00:15<00:02,  8.84it/s]\u001b[A\n",
            " 85%|████████▍ | 140/165 [00:15<00:02,  8.84it/s]\u001b[A\n",
            " 85%|████████▌ | 141/165 [00:15<00:02,  8.85it/s]\u001b[A\n",
            " 86%|████████▌ | 142/165 [00:16<00:02,  8.85it/s]\u001b[A\n",
            " 87%|████████▋ | 143/165 [00:16<00:02,  8.83it/s]\u001b[A\n",
            " 87%|████████▋ | 144/165 [00:16<00:02,  8.85it/s]\u001b[A\n",
            " 88%|████████▊ | 145/165 [00:16<00:02,  8.83it/s]\u001b[A\n",
            " 88%|████████▊ | 146/165 [00:16<00:02,  8.83it/s]\u001b[A\n",
            " 89%|████████▉ | 147/165 [00:16<00:02,  8.84it/s]\u001b[A\n",
            " 90%|████████▉ | 148/165 [00:16<00:01,  8.85it/s]\u001b[A\n",
            " 90%|█████████ | 149/165 [00:16<00:01,  8.87it/s]\u001b[A\n",
            " 91%|█████████ | 150/165 [00:16<00:01,  8.86it/s]\u001b[A\n",
            " 92%|█████████▏| 151/165 [00:17<00:01,  8.86it/s]\u001b[A\n",
            " 92%|█████████▏| 152/165 [00:17<00:01,  8.87it/s]\u001b[A\n",
            " 93%|█████████▎| 153/165 [00:17<00:01,  8.86it/s]\u001b[A\n",
            " 93%|█████████▎| 154/165 [00:17<00:01,  8.85it/s]\u001b[A\n",
            " 94%|█████████▍| 155/165 [00:17<00:01,  8.83it/s]\u001b[A\n",
            " 95%|█████████▍| 156/165 [00:17<00:01,  8.84it/s]\u001b[A\n",
            " 95%|█████████▌| 157/165 [00:17<00:00,  8.84it/s]\u001b[A\n",
            " 96%|█████████▌| 158/165 [00:17<00:00,  8.85it/s]\u001b[A\n",
            " 96%|█████████▋| 159/165 [00:17<00:00,  8.84it/s]\u001b[A\n",
            " 97%|█████████▋| 160/165 [00:18<00:00,  8.83it/s]\u001b[A\n",
            " 98%|█████████▊| 161/165 [00:18<00:00,  8.85it/s]\u001b[A\n",
            " 98%|█████████▊| 162/165 [00:18<00:00,  8.86it/s]\u001b[A\n",
            " 99%|█████████▉| 163/165 [00:18<00:00,  8.86it/s]\u001b[A\n",
            "100%|██████████| 165/165 [00:18<00:00,  8.82it/s]\n",
            "========== model: bert-base-uncased fold: 2 inference ==========\n",
            "========== model: bert-base-uncased fold: 2 inference ==========\n",
            "========== model: bert-base-uncased fold: 2 inference ==========\n",
            "========== model: bert-base-uncased fold: 2 inference ==========\n",
            "========== model: bert-base-uncased fold: 2 inference ==========\n",
            "========== model: bert-base-uncased fold: 2 inference ==========\n",
            "========== model: bert-base-uncased fold: 2 inference ==========\n",
            "========== model: bert-base-uncased fold: 2 inference ==========\n",
            "========== model: bert-base-uncased fold: 2 inference ==========\n",
            "========== model: bert-base-uncased fold: 2 inference ==========\n",
            "========== model: bert-base-uncased fold: 2 inference ==========\n",
            "========== model: bert-base-uncased fold: 2 inference ==========\n",
            "loading configuration file /content/activebus_bert_final/config.json\n",
            "Model config BertConfig {\n",
            "  \"_name_or_path\": \"activebus/BERT-XD_Review\",\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_ids\": 0,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.15.0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "loading weights file /content/activebus_bert_final/pytorch_model.bin\n",
            "Some weights of the model checkpoint at /content/activebus_bert_final were not used when initializing BertForSequenceClassification: ['cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /content/activebus_bert_final and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "\n",
            "  0%|          | 0/165 [00:00<?, ?it/s]\u001b[A\n",
            "  1%|          | 1/165 [00:00<00:20,  7.83it/s]\u001b[A\n",
            "  1%|          | 2/165 [00:00<00:19,  8.40it/s]\u001b[A\n",
            "  2%|▏         | 3/165 [00:00<00:18,  8.59it/s]\u001b[A\n",
            "  2%|▏         | 4/165 [00:00<00:18,  8.67it/s]\u001b[A\n",
            "  3%|▎         | 5/165 [00:00<00:18,  8.75it/s]\u001b[A\n",
            "  4%|▎         | 6/165 [00:00<00:18,  8.79it/s]\u001b[A\n",
            "  4%|▍         | 7/165 [00:00<00:17,  8.81it/s]\u001b[A\n",
            "  5%|▍         | 8/165 [00:00<00:17,  8.81it/s]\u001b[A\n",
            "  5%|▌         | 9/165 [00:01<00:17,  8.84it/s]\u001b[A\n",
            "  6%|▌         | 10/165 [00:01<00:17,  8.85it/s]\u001b[A\n",
            "  7%|▋         | 11/165 [00:01<00:17,  8.84it/s]\u001b[A\n",
            "  7%|▋         | 12/165 [00:01<00:17,  8.81it/s]\u001b[A\n",
            "  8%|▊         | 13/165 [00:01<00:17,  8.81it/s]\u001b[A\n",
            "  8%|▊         | 14/165 [00:01<00:17,  8.81it/s]\u001b[A\n",
            "  9%|▉         | 15/165 [00:01<00:17,  8.82it/s]\u001b[A\n",
            " 10%|▉         | 16/165 [00:01<00:16,  8.84it/s]\u001b[A\n",
            " 10%|█         | 17/165 [00:01<00:16,  8.83it/s]\u001b[A\n",
            " 11%|█         | 18/165 [00:02<00:16,  8.80it/s]\u001b[A\n",
            " 12%|█▏        | 19/165 [00:02<00:16,  8.82it/s]\u001b[A\n",
            " 12%|█▏        | 20/165 [00:02<00:16,  8.82it/s]\u001b[A\n",
            " 13%|█▎        | 21/165 [00:02<00:16,  8.83it/s]\u001b[A\n",
            " 13%|█▎        | 22/165 [00:02<00:16,  8.73it/s]\u001b[A\n",
            " 14%|█▍        | 23/165 [00:02<00:16,  8.74it/s]\u001b[A\n",
            " 15%|█▍        | 24/165 [00:02<00:16,  8.76it/s]\u001b[A\n",
            " 15%|█▌        | 25/165 [00:02<00:15,  8.79it/s]\u001b[A\n",
            " 16%|█▌        | 26/165 [00:02<00:15,  8.81it/s]\u001b[A\n",
            " 16%|█▋        | 27/165 [00:03<00:15,  8.81it/s]\u001b[A\n",
            " 17%|█▋        | 28/165 [00:03<00:15,  8.80it/s]\u001b[A\n",
            " 18%|█▊        | 29/165 [00:03<00:15,  8.78it/s]\u001b[A\n",
            " 18%|█▊        | 30/165 [00:03<00:15,  8.81it/s]\u001b[A\n",
            " 19%|█▉        | 31/165 [00:03<00:15,  8.84it/s]\u001b[A\n",
            " 19%|█▉        | 32/165 [00:03<00:15,  8.82it/s]\u001b[A\n",
            " 20%|██        | 33/165 [00:03<00:14,  8.83it/s]\u001b[A\n",
            " 21%|██        | 34/165 [00:03<00:14,  8.83it/s]\u001b[A\n",
            " 21%|██        | 35/165 [00:03<00:14,  8.81it/s]\u001b[A\n",
            " 22%|██▏       | 36/165 [00:04<00:14,  8.82it/s]\u001b[A\n",
            " 22%|██▏       | 37/165 [00:04<00:14,  8.84it/s]\u001b[A\n",
            " 23%|██▎       | 38/165 [00:04<00:14,  8.84it/s]\u001b[A\n",
            " 24%|██▎       | 39/165 [00:04<00:14,  8.85it/s]\u001b[A\n",
            " 24%|██▍       | 40/165 [00:04<00:14,  8.83it/s]\u001b[A\n",
            " 25%|██▍       | 41/165 [00:04<00:13,  8.86it/s]\u001b[A\n",
            " 25%|██▌       | 42/165 [00:04<00:13,  8.88it/s]\u001b[A\n",
            " 26%|██▌       | 43/165 [00:04<00:13,  8.87it/s]\u001b[A\n",
            " 27%|██▋       | 44/165 [00:05<00:13,  8.83it/s]\u001b[A\n",
            " 27%|██▋       | 45/165 [00:05<00:13,  8.83it/s]\u001b[A\n",
            " 28%|██▊       | 46/165 [00:05<00:13,  8.83it/s]\u001b[A\n",
            " 28%|██▊       | 47/165 [00:05<00:13,  8.82it/s]\u001b[A\n",
            " 29%|██▉       | 48/165 [00:05<00:13,  8.86it/s]\u001b[A\n",
            " 30%|██▉       | 49/165 [00:05<00:13,  8.86it/s]\u001b[A\n",
            " 30%|███       | 50/165 [00:05<00:12,  8.88it/s]\u001b[A\n",
            " 31%|███       | 51/165 [00:05<00:12,  8.88it/s]\u001b[A\n",
            " 32%|███▏      | 52/165 [00:05<00:12,  8.90it/s]\u001b[A\n",
            " 32%|███▏      | 53/165 [00:06<00:12,  8.87it/s]\u001b[A\n",
            " 33%|███▎      | 54/165 [00:06<00:12,  8.85it/s]\u001b[A\n",
            " 33%|███▎      | 55/165 [00:06<00:12,  8.87it/s]\u001b[A\n",
            " 34%|███▍      | 56/165 [00:06<00:12,  8.86it/s]\u001b[A\n",
            " 35%|███▍      | 57/165 [00:06<00:12,  8.86it/s]\u001b[A\n",
            " 35%|███▌      | 58/165 [00:06<00:12,  8.87it/s]\u001b[A\n",
            " 36%|███▌      | 59/165 [00:06<00:12,  8.83it/s]\u001b[A\n",
            " 36%|███▋      | 60/165 [00:06<00:11,  8.83it/s]\u001b[A\n",
            " 37%|███▋      | 61/165 [00:06<00:11,  8.83it/s]\u001b[A\n",
            " 38%|███▊      | 62/165 [00:07<00:11,  8.84it/s]\u001b[A\n",
            " 38%|███▊      | 63/165 [00:07<00:11,  8.82it/s]\u001b[A\n",
            " 39%|███▉      | 64/165 [00:07<00:11,  8.83it/s]\u001b[A\n",
            " 39%|███▉      | 65/165 [00:07<00:11,  8.83it/s]\u001b[A\n",
            " 40%|████      | 66/165 [00:07<00:11,  8.84it/s]\u001b[A\n",
            " 41%|████      | 67/165 [00:07<00:11,  8.83it/s]\u001b[A\n",
            " 41%|████      | 68/165 [00:07<00:10,  8.84it/s]\u001b[A\n",
            " 42%|████▏     | 69/165 [00:07<00:10,  8.85it/s]\u001b[A\n",
            " 42%|████▏     | 70/165 [00:07<00:10,  8.86it/s]\u001b[A\n",
            " 43%|████▎     | 71/165 [00:08<00:10,  8.86it/s]\u001b[A\n",
            " 44%|████▎     | 72/165 [00:08<00:10,  8.86it/s]\u001b[A\n",
            " 44%|████▍     | 73/165 [00:08<00:10,  8.85it/s]\u001b[A\n",
            " 45%|████▍     | 74/165 [00:08<00:10,  8.85it/s]\u001b[A\n",
            " 45%|████▌     | 75/165 [00:08<00:10,  8.87it/s]\u001b[A\n",
            " 46%|████▌     | 76/165 [00:08<00:10,  8.87it/s]\u001b[A\n",
            " 47%|████▋     | 77/165 [00:08<00:09,  8.88it/s]\u001b[A\n",
            " 47%|████▋     | 78/165 [00:08<00:09,  8.88it/s]\u001b[A\n",
            " 48%|████▊     | 79/165 [00:08<00:09,  8.86it/s]\u001b[A\n",
            " 48%|████▊     | 80/165 [00:09<00:09,  8.86it/s]\u001b[A\n",
            " 49%|████▉     | 81/165 [00:09<00:09,  8.88it/s]\u001b[A\n",
            " 50%|████▉     | 82/165 [00:09<00:09,  8.89it/s]\u001b[A\n",
            " 50%|█████     | 83/165 [00:09<00:09,  8.90it/s]\u001b[A\n",
            " 51%|█████     | 84/165 [00:09<00:09,  8.87it/s]\u001b[A\n",
            " 52%|█████▏    | 85/165 [00:09<00:09,  8.89it/s]\u001b[A\n",
            " 52%|█████▏    | 86/165 [00:09<00:08,  8.86it/s]\u001b[A\n",
            " 53%|█████▎    | 87/165 [00:09<00:08,  8.87it/s]\u001b[A\n",
            " 53%|█████▎    | 88/165 [00:09<00:08,  8.88it/s]\u001b[A\n",
            " 54%|█████▍    | 89/165 [00:10<00:08,  8.87it/s]\u001b[A\n",
            " 55%|█████▍    | 90/165 [00:10<00:08,  8.88it/s]\u001b[A\n",
            " 55%|█████▌    | 91/165 [00:10<00:08,  8.90it/s]\u001b[A\n",
            " 56%|█████▌    | 92/165 [00:10<00:08,  8.87it/s]\u001b[A\n",
            " 56%|█████▋    | 93/165 [00:10<00:08,  8.88it/s]\u001b[A\n",
            " 57%|█████▋    | 94/165 [00:10<00:08,  8.87it/s]\u001b[A\n",
            " 58%|█████▊    | 95/165 [00:10<00:07,  8.87it/s]\u001b[A\n",
            " 58%|█████▊    | 96/165 [00:10<00:07,  8.88it/s]\u001b[A\n",
            " 59%|█████▉    | 97/165 [00:10<00:07,  8.89it/s]\u001b[A\n",
            " 59%|█████▉    | 98/165 [00:11<00:07,  8.90it/s]\u001b[A\n",
            " 60%|██████    | 99/165 [00:11<00:07,  8.89it/s]\u001b[A\n",
            " 61%|██████    | 100/165 [00:11<00:07,  8.90it/s]\u001b[A\n",
            " 61%|██████    | 101/165 [00:11<00:07,  8.89it/s]\u001b[A\n",
            " 62%|██████▏   | 102/165 [00:11<00:07,  8.87it/s]\u001b[A\n",
            " 62%|██████▏   | 103/165 [00:11<00:06,  8.87it/s]\u001b[A\n",
            " 63%|██████▎   | 104/165 [00:11<00:06,  8.86it/s]\u001b[A\n",
            " 64%|██████▎   | 105/165 [00:11<00:06,  8.85it/s]\u001b[A\n",
            " 64%|██████▍   | 106/165 [00:11<00:06,  8.84it/s]\u001b[A\n",
            " 65%|██████▍   | 107/165 [00:12<00:06,  8.82it/s]\u001b[A\n",
            " 65%|██████▌   | 108/165 [00:12<00:06,  8.81it/s]\u001b[A\n",
            " 66%|██████▌   | 109/165 [00:12<00:06,  8.82it/s]\u001b[A\n",
            " 67%|██████▋   | 110/165 [00:12<00:06,  8.84it/s]\u001b[A\n",
            " 67%|██████▋   | 111/165 [00:12<00:06,  8.84it/s]\u001b[A\n",
            " 68%|██████▊   | 112/165 [00:12<00:06,  8.82it/s]\u001b[A\n",
            " 68%|██████▊   | 113/165 [00:12<00:05,  8.80it/s]\u001b[A\n",
            " 69%|██████▉   | 114/165 [00:12<00:05,  8.82it/s]\u001b[A\n",
            " 70%|██████▉   | 115/165 [00:13<00:05,  8.79it/s]\u001b[A\n",
            " 70%|███████   | 116/165 [00:13<00:05,  8.80it/s]\u001b[A\n",
            " 71%|███████   | 117/165 [00:13<00:05,  8.81it/s]\u001b[A\n",
            " 72%|███████▏  | 118/165 [00:13<00:05,  8.85it/s]\u001b[A\n",
            " 72%|███████▏  | 119/165 [00:13<00:05,  8.85it/s]\u001b[A\n",
            " 73%|███████▎  | 120/165 [00:13<00:05,  8.85it/s]\u001b[A\n",
            " 73%|███████▎  | 121/165 [00:13<00:04,  8.85it/s]\u001b[A\n",
            " 74%|███████▍  | 122/165 [00:13<00:04,  8.88it/s]\u001b[A\n",
            " 75%|███████▍  | 123/165 [00:13<00:04,  8.88it/s]\u001b[A\n",
            " 75%|███████▌  | 124/165 [00:14<00:04,  8.89it/s]\u001b[A\n",
            " 76%|███████▌  | 125/165 [00:14<00:04,  8.88it/s]\u001b[A\n",
            " 76%|███████▋  | 126/165 [00:14<00:04,  8.86it/s]\u001b[A\n",
            " 77%|███████▋  | 127/165 [00:14<00:04,  8.85it/s]\u001b[A\n",
            " 78%|███████▊  | 128/165 [00:14<00:04,  8.81it/s]\u001b[A\n",
            " 78%|███████▊  | 129/165 [00:14<00:04,  8.81it/s]\u001b[A\n",
            " 79%|███████▉  | 130/165 [00:14<00:03,  8.80it/s]\u001b[A\n",
            " 79%|███████▉  | 131/165 [00:14<00:03,  8.81it/s]\u001b[A\n",
            " 80%|████████  | 132/165 [00:14<00:03,  8.83it/s]\u001b[A\n",
            " 81%|████████  | 133/165 [00:15<00:03,  8.84it/s]\u001b[A\n",
            " 81%|████████  | 134/165 [00:15<00:03,  8.86it/s]\u001b[A\n",
            " 82%|████████▏ | 135/165 [00:15<00:03,  8.87it/s]\u001b[A\n",
            " 82%|████████▏ | 136/165 [00:15<00:03,  8.88it/s]\u001b[A\n",
            " 83%|████████▎ | 137/165 [00:15<00:03,  8.89it/s]\u001b[A\n",
            " 84%|████████▎ | 138/165 [00:15<00:03,  8.89it/s]\u001b[A\n",
            " 84%|████████▍ | 139/165 [00:15<00:02,  8.89it/s]\u001b[A\n",
            " 85%|████████▍ | 140/165 [00:15<00:02,  8.89it/s]\u001b[A\n",
            " 85%|████████▌ | 141/165 [00:15<00:02,  8.88it/s]\u001b[A\n",
            " 86%|████████▌ | 142/165 [00:16<00:02,  8.86it/s]\u001b[A\n",
            " 87%|████████▋ | 143/165 [00:16<00:02,  8.86it/s]\u001b[A\n",
            " 87%|████████▋ | 144/165 [00:16<00:02,  8.85it/s]\u001b[A\n",
            " 88%|████████▊ | 145/165 [00:16<00:02,  8.86it/s]\u001b[A\n",
            " 88%|████████▊ | 146/165 [00:16<00:02,  8.86it/s]\u001b[A\n",
            " 89%|████████▉ | 147/165 [00:16<00:02,  8.87it/s]\u001b[A\n",
            " 90%|████████▉ | 148/165 [00:16<00:01,  8.87it/s]\u001b[A\n",
            " 90%|█████████ | 149/165 [00:16<00:01,  8.87it/s]\u001b[A\n",
            " 91%|█████████ | 150/165 [00:16<00:01,  8.88it/s]\u001b[A\n",
            " 92%|█████████▏| 151/165 [00:17<00:01,  8.86it/s]\u001b[A\n",
            " 92%|█████████▏| 152/165 [00:17<00:01,  8.87it/s]\u001b[A\n",
            " 93%|█████████▎| 153/165 [00:17<00:01,  8.85it/s]\u001b[A\n",
            " 93%|█████████▎| 154/165 [00:17<00:01,  8.87it/s]\u001b[A\n",
            " 94%|█████████▍| 155/165 [00:17<00:01,  8.87it/s]\u001b[A\n",
            " 95%|█████████▍| 156/165 [00:17<00:01,  8.88it/s]\u001b[A\n",
            " 95%|█████████▌| 157/165 [00:17<00:00,  8.87it/s]\u001b[A\n",
            " 96%|█████████▌| 158/165 [00:17<00:00,  8.86it/s]\u001b[A\n",
            " 96%|█████████▋| 159/165 [00:17<00:00,  8.86it/s]\u001b[A\n",
            " 97%|█████████▋| 160/165 [00:18<00:00,  8.88it/s]\u001b[A\n",
            " 98%|█████████▊| 161/165 [00:18<00:00,  8.89it/s]\u001b[A\n",
            " 98%|█████████▊| 162/165 [00:18<00:00,  8.89it/s]\u001b[A\n",
            " 99%|█████████▉| 163/165 [00:18<00:00,  8.90it/s]\u001b[A\n",
            "100%|██████████| 165/165 [00:18<00:00,  8.83it/s]\n",
            "========== model: bert-base-uncased fold: 3 inference ==========\n",
            "========== model: bert-base-uncased fold: 3 inference ==========\n",
            "========== model: bert-base-uncased fold: 3 inference ==========\n",
            "========== model: bert-base-uncased fold: 3 inference ==========\n",
            "========== model: bert-base-uncased fold: 3 inference ==========\n",
            "========== model: bert-base-uncased fold: 3 inference ==========\n",
            "========== model: bert-base-uncased fold: 3 inference ==========\n",
            "========== model: bert-base-uncased fold: 3 inference ==========\n",
            "========== model: bert-base-uncased fold: 3 inference ==========\n",
            "========== model: bert-base-uncased fold: 3 inference ==========\n",
            "========== model: bert-base-uncased fold: 3 inference ==========\n",
            "========== model: bert-base-uncased fold: 3 inference ==========\n",
            "loading configuration file /content/activebus_bert_final/config.json\n",
            "Model config BertConfig {\n",
            "  \"_name_or_path\": \"activebus/BERT-XD_Review\",\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_ids\": 0,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.15.0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "loading weights file /content/activebus_bert_final/pytorch_model.bin\n",
            "Some weights of the model checkpoint at /content/activebus_bert_final were not used when initializing BertForSequenceClassification: ['cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /content/activebus_bert_final and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "\n",
            "  0%|          | 0/165 [00:00<?, ?it/s]\u001b[A\n",
            "  1%|          | 1/165 [00:00<00:20,  8.13it/s]\u001b[A\n",
            "  1%|          | 2/165 [00:00<00:19,  8.53it/s]\u001b[A\n",
            "  2%|▏         | 3/165 [00:00<00:18,  8.68it/s]\u001b[A\n",
            "  2%|▏         | 4/165 [00:00<00:18,  8.76it/s]\u001b[A\n",
            "  3%|▎         | 5/165 [00:00<00:18,  8.82it/s]\u001b[A\n",
            "  4%|▎         | 6/165 [00:00<00:17,  8.84it/s]\u001b[A\n",
            "  4%|▍         | 7/165 [00:00<00:17,  8.87it/s]\u001b[A\n",
            "  5%|▍         | 8/165 [00:00<00:17,  8.88it/s]\u001b[A\n",
            "  5%|▌         | 9/165 [00:01<00:17,  8.88it/s]\u001b[A\n",
            "  6%|▌         | 10/165 [00:01<00:17,  8.80it/s]\u001b[A\n",
            "  7%|▋         | 11/165 [00:01<00:17,  8.82it/s]\u001b[A\n",
            "  7%|▋         | 12/165 [00:01<00:17,  8.84it/s]\u001b[A\n",
            "  8%|▊         | 13/165 [00:01<00:17,  8.86it/s]\u001b[A\n",
            "  8%|▊         | 14/165 [00:01<00:17,  8.86it/s]\u001b[A\n",
            "  9%|▉         | 15/165 [00:01<00:16,  8.83it/s]\u001b[A\n",
            " 10%|▉         | 16/165 [00:01<00:16,  8.85it/s]\u001b[A\n",
            " 10%|█         | 17/165 [00:01<00:16,  8.86it/s]\u001b[A\n",
            " 11%|█         | 18/165 [00:02<00:16,  8.86it/s]\u001b[A\n",
            " 12%|█▏        | 19/165 [00:02<00:16,  8.86it/s]\u001b[A\n",
            " 12%|█▏        | 20/165 [00:02<00:16,  8.86it/s]\u001b[A\n",
            " 13%|█▎        | 21/165 [00:02<00:16,  8.88it/s]\u001b[A\n",
            " 13%|█▎        | 22/165 [00:02<00:16,  8.85it/s]\u001b[A\n",
            " 14%|█▍        | 23/165 [00:02<00:16,  8.85it/s]\u001b[A\n",
            " 15%|█▍        | 24/165 [00:02<00:15,  8.86it/s]\u001b[A\n",
            " 15%|█▌        | 25/165 [00:02<00:15,  8.79it/s]\u001b[A\n",
            " 16%|█▌        | 26/165 [00:02<00:15,  8.82it/s]\u001b[A\n",
            " 16%|█▋        | 27/165 [00:03<00:15,  8.83it/s]\u001b[A\n",
            " 17%|█▋        | 28/165 [00:03<00:15,  8.83it/s]\u001b[A\n",
            " 18%|█▊        | 29/165 [00:03<00:15,  8.83it/s]\u001b[A\n",
            " 18%|█▊        | 30/165 [00:03<00:15,  8.84it/s]\u001b[A\n",
            " 19%|█▉        | 31/165 [00:03<00:15,  8.84it/s]\u001b[A\n",
            " 19%|█▉        | 32/165 [00:03<00:15,  8.84it/s]\u001b[A\n",
            " 20%|██        | 33/165 [00:03<00:14,  8.83it/s]\u001b[A\n",
            " 21%|██        | 34/165 [00:03<00:14,  8.83it/s]\u001b[A\n",
            " 21%|██        | 35/165 [00:03<00:14,  8.79it/s]\u001b[A\n",
            " 22%|██▏       | 36/165 [00:04<00:14,  8.80it/s]\u001b[A\n",
            " 22%|██▏       | 37/165 [00:04<00:14,  8.83it/s]\u001b[A\n",
            " 23%|██▎       | 38/165 [00:04<00:14,  8.85it/s]\u001b[A\n",
            " 24%|██▎       | 39/165 [00:04<00:14,  8.85it/s]\u001b[A\n",
            " 24%|██▍       | 40/165 [00:04<00:14,  8.88it/s]\u001b[A\n",
            " 25%|██▍       | 41/165 [00:04<00:14,  8.84it/s]\u001b[A\n",
            " 25%|██▌       | 42/165 [00:04<00:13,  8.84it/s]\u001b[A\n",
            " 26%|██▌       | 43/165 [00:04<00:13,  8.84it/s]\u001b[A\n",
            " 27%|██▋       | 44/165 [00:04<00:13,  8.87it/s]\u001b[A\n",
            " 27%|██▋       | 45/165 [00:05<00:13,  8.86it/s]\u001b[A\n",
            " 28%|██▊       | 46/165 [00:05<00:13,  8.85it/s]\u001b[A\n",
            " 28%|██▊       | 47/165 [00:05<00:13,  8.86it/s]\u001b[A\n",
            " 29%|██▉       | 48/165 [00:05<00:13,  8.87it/s]\u001b[A\n",
            " 30%|██▉       | 49/165 [00:05<00:13,  8.86it/s]\u001b[A\n",
            " 30%|███       | 50/165 [00:05<00:12,  8.86it/s]\u001b[A\n",
            " 31%|███       | 51/165 [00:05<00:12,  8.87it/s]\u001b[A\n",
            " 32%|███▏      | 52/165 [00:05<00:12,  8.85it/s]\u001b[A\n",
            " 32%|███▏      | 53/165 [00:06<00:12,  8.83it/s]\u001b[A\n",
            " 33%|███▎      | 54/165 [00:06<00:12,  8.84it/s]\u001b[A\n",
            " 33%|███▎      | 55/165 [00:06<00:12,  8.82it/s]\u001b[A\n",
            " 34%|███▍      | 56/165 [00:06<00:12,  8.83it/s]\u001b[A\n",
            " 35%|███▍      | 57/165 [00:06<00:12,  8.85it/s]\u001b[A\n",
            " 35%|███▌      | 58/165 [00:06<00:12,  8.85it/s]\u001b[A\n",
            " 36%|███▌      | 59/165 [00:06<00:11,  8.86it/s]\u001b[A\n",
            " 36%|███▋      | 60/165 [00:06<00:11,  8.84it/s]\u001b[A\n",
            " 37%|███▋      | 61/165 [00:06<00:11,  8.82it/s]\u001b[A\n",
            " 38%|███▊      | 62/165 [00:07<00:11,  8.83it/s]\u001b[A\n",
            " 38%|███▊      | 63/165 [00:07<00:11,  8.83it/s]\u001b[A\n",
            " 39%|███▉      | 64/165 [00:07<00:11,  8.85it/s]\u001b[A\n",
            " 39%|███▉      | 65/165 [00:07<00:11,  8.88it/s]\u001b[A\n",
            " 40%|████      | 66/165 [00:07<00:11,  8.89it/s]\u001b[A\n",
            " 41%|████      | 67/165 [00:07<00:11,  8.89it/s]\u001b[A\n",
            " 41%|████      | 68/165 [00:07<00:10,  8.87it/s]\u001b[A\n",
            " 42%|████▏     | 69/165 [00:07<00:10,  8.87it/s]\u001b[A\n",
            " 42%|████▏     | 70/165 [00:07<00:10,  8.87it/s]\u001b[A\n",
            " 43%|████▎     | 71/165 [00:08<00:10,  8.89it/s]\u001b[A\n",
            " 44%|████▎     | 72/165 [00:08<00:10,  8.88it/s]\u001b[A\n",
            " 44%|████▍     | 73/165 [00:08<00:10,  8.87it/s]\u001b[A\n",
            " 45%|████▍     | 74/165 [00:08<00:10,  8.86it/s]\u001b[A\n",
            " 45%|████▌     | 75/165 [00:08<00:10,  8.86it/s]\u001b[A\n",
            " 46%|████▌     | 76/165 [00:08<00:10,  8.83it/s]\u001b[A\n",
            " 47%|████▋     | 77/165 [00:08<00:09,  8.86it/s]\u001b[A\n",
            " 47%|████▋     | 78/165 [00:08<00:09,  8.87it/s]\u001b[A\n",
            " 48%|████▊     | 79/165 [00:08<00:09,  8.87it/s]\u001b[A\n",
            " 48%|████▊     | 80/165 [00:09<00:09,  8.89it/s]\u001b[A\n",
            " 49%|████▉     | 81/165 [00:09<00:09,  8.84it/s]\u001b[A\n",
            " 50%|████▉     | 82/165 [00:09<00:09,  8.85it/s]\u001b[A\n",
            " 50%|█████     | 83/165 [00:09<00:09,  8.86it/s]\u001b[A\n",
            " 51%|█████     | 84/165 [00:09<00:09,  8.85it/s]\u001b[A\n",
            " 52%|█████▏    | 85/165 [00:09<00:09,  8.85it/s]\u001b[A\n",
            " 52%|█████▏    | 86/165 [00:09<00:08,  8.85it/s]\u001b[A\n",
            " 53%|█████▎    | 87/165 [00:09<00:08,  8.86it/s]\u001b[A\n",
            " 53%|█████▎    | 88/165 [00:09<00:08,  8.84it/s]\u001b[A\n",
            " 54%|█████▍    | 89/165 [00:10<00:08,  8.85it/s]\u001b[A\n",
            " 55%|█████▍    | 90/165 [00:10<00:08,  8.82it/s]\u001b[A\n",
            " 55%|█████▌    | 91/165 [00:10<00:08,  8.82it/s]\u001b[A\n",
            " 56%|█████▌    | 92/165 [00:10<00:08,  8.81it/s]\u001b[A\n",
            " 56%|█████▋    | 93/165 [00:10<00:08,  8.81it/s]\u001b[A\n",
            " 57%|█████▋    | 94/165 [00:10<00:08,  8.80it/s]\u001b[A\n",
            " 58%|█████▊    | 95/165 [00:10<00:07,  8.84it/s]\u001b[A\n",
            " 58%|█████▊    | 96/165 [00:10<00:07,  8.81it/s]\u001b[A\n",
            " 59%|█████▉    | 97/165 [00:10<00:07,  8.85it/s]\u001b[A\n",
            " 59%|█████▉    | 98/165 [00:11<00:07,  8.84it/s]\u001b[A\n",
            " 60%|██████    | 99/165 [00:11<00:07,  8.81it/s]\u001b[A\n",
            " 61%|██████    | 100/165 [00:11<00:07,  8.81it/s]\u001b[A\n",
            " 61%|██████    | 101/165 [00:11<00:07,  8.77it/s]\u001b[A\n",
            " 62%|██████▏   | 102/165 [00:11<00:07,  8.79it/s]\u001b[A\n",
            " 62%|██████▏   | 103/165 [00:11<00:07,  8.79it/s]\u001b[A\n",
            " 63%|██████▎   | 104/165 [00:11<00:06,  8.79it/s]\u001b[A\n",
            " 64%|██████▎   | 105/165 [00:11<00:06,  8.82it/s]\u001b[A\n",
            " 64%|██████▍   | 106/165 [00:11<00:06,  8.80it/s]\u001b[A\n",
            " 65%|██████▍   | 107/165 [00:12<00:06,  8.80it/s]\u001b[A\n",
            " 65%|██████▌   | 108/165 [00:12<00:06,  8.78it/s]\u001b[A\n",
            " 66%|██████▌   | 109/165 [00:12<00:06,  8.79it/s]\u001b[A\n",
            " 67%|██████▋   | 110/165 [00:12<00:06,  8.80it/s]\u001b[A\n",
            " 67%|██████▋   | 111/165 [00:12<00:06,  8.80it/s]\u001b[A\n",
            " 68%|██████▊   | 112/165 [00:12<00:06,  8.81it/s]\u001b[A\n",
            " 68%|██████▊   | 113/165 [00:12<00:05,  8.85it/s]\u001b[A\n",
            " 69%|██████▉   | 114/165 [00:12<00:05,  8.87it/s]\u001b[A\n",
            " 70%|██████▉   | 115/165 [00:13<00:05,  8.89it/s]\u001b[A\n",
            " 70%|███████   | 116/165 [00:13<00:05,  8.86it/s]\u001b[A\n",
            " 71%|███████   | 117/165 [00:13<00:05,  8.87it/s]\u001b[A\n",
            " 72%|███████▏  | 118/165 [00:13<00:05,  8.87it/s]\u001b[A\n",
            " 72%|███████▏  | 119/165 [00:13<00:05,  8.88it/s]\u001b[A\n",
            " 73%|███████▎  | 120/165 [00:13<00:05,  8.89it/s]\u001b[A\n",
            " 73%|███████▎  | 121/165 [00:13<00:04,  8.87it/s]\u001b[A\n",
            " 74%|███████▍  | 122/165 [00:13<00:04,  8.89it/s]\u001b[A\n",
            " 75%|███████▍  | 123/165 [00:13<00:04,  8.86it/s]\u001b[A\n",
            " 75%|███████▌  | 124/165 [00:14<00:04,  8.85it/s]\u001b[A\n",
            " 76%|███████▌  | 125/165 [00:14<00:04,  8.85it/s]\u001b[A\n",
            " 76%|███████▋  | 126/165 [00:14<00:04,  8.83it/s]\u001b[A\n",
            " 77%|███████▋  | 127/165 [00:14<00:04,  8.83it/s]\u001b[A\n",
            " 78%|███████▊  | 128/165 [00:14<00:04,  8.82it/s]\u001b[A\n",
            " 78%|███████▊  | 129/165 [00:14<00:04,  8.80it/s]\u001b[A\n",
            " 79%|███████▉  | 130/165 [00:14<00:03,  8.79it/s]\u001b[A\n",
            " 79%|███████▉  | 131/165 [00:14<00:03,  8.81it/s]\u001b[A\n",
            " 80%|████████  | 132/165 [00:14<00:03,  8.81it/s]\u001b[A\n",
            " 81%|████████  | 133/165 [00:15<00:03,  8.81it/s]\u001b[A\n",
            " 81%|████████  | 134/165 [00:15<00:03,  8.81it/s]\u001b[A\n",
            " 82%|████████▏ | 135/165 [00:15<00:03,  8.80it/s]\u001b[A\n",
            " 82%|████████▏ | 136/165 [00:15<00:03,  8.81it/s]\u001b[A\n",
            " 83%|████████▎ | 137/165 [00:15<00:03,  8.82it/s]\u001b[A\n",
            " 84%|████████▎ | 138/165 [00:15<00:03,  8.85it/s]\u001b[A\n",
            " 84%|████████▍ | 139/165 [00:15<00:02,  8.85it/s]\u001b[A\n",
            " 85%|████████▍ | 140/165 [00:15<00:02,  8.86it/s]\u001b[A\n",
            " 85%|████████▌ | 141/165 [00:15<00:02,  8.86it/s]\u001b[A\n",
            " 86%|████████▌ | 142/165 [00:16<00:02,  8.85it/s]\u001b[A\n",
            " 87%|████████▋ | 143/165 [00:16<00:02,  8.88it/s]\u001b[A\n",
            " 87%|████████▋ | 144/165 [00:16<00:02,  8.86it/s]\u001b[A\n",
            " 88%|████████▊ | 145/165 [00:16<00:02,  8.87it/s]\u001b[A\n",
            " 88%|████████▊ | 146/165 [00:16<00:02,  8.88it/s]\u001b[A\n",
            " 89%|████████▉ | 147/165 [00:16<00:02,  8.87it/s]\u001b[A\n",
            " 90%|████████▉ | 148/165 [00:16<00:01,  8.88it/s]\u001b[A\n",
            " 90%|█████████ | 149/165 [00:16<00:01,  8.87it/s]\u001b[A\n",
            " 91%|█████████ | 150/165 [00:16<00:01,  8.86it/s]\u001b[A\n",
            " 92%|█████████▏| 151/165 [00:17<00:01,  8.87it/s]\u001b[A\n",
            " 92%|█████████▏| 152/165 [00:17<00:01,  8.88it/s]\u001b[A\n",
            " 93%|█████████▎| 153/165 [00:17<00:01,  8.86it/s]\u001b[A\n",
            " 93%|█████████▎| 154/165 [00:17<00:01,  8.86it/s]\u001b[A\n",
            " 94%|█████████▍| 155/165 [00:17<00:01,  8.86it/s]\u001b[A\n",
            " 95%|█████████▍| 156/165 [00:17<00:01,  8.86it/s]\u001b[A\n",
            " 95%|█████████▌| 157/165 [00:17<00:00,  8.88it/s]\u001b[A\n",
            " 96%|█████████▌| 158/165 [00:17<00:00,  8.88it/s]\u001b[A\n",
            " 96%|█████████▋| 159/165 [00:17<00:00,  8.89it/s]\u001b[A\n",
            " 97%|█████████▋| 160/165 [00:18<00:00,  8.90it/s]\u001b[A\n",
            " 98%|█████████▊| 161/165 [00:18<00:00,  8.89it/s]\u001b[A\n",
            " 98%|█████████▊| 162/165 [00:18<00:00,  8.88it/s]\u001b[A\n",
            " 99%|█████████▉| 163/165 [00:18<00:00,  8.89it/s]\u001b[A\n",
            "100%|██████████| 165/165 [00:18<00:00,  8.83it/s]\n",
            "========== model: bert-base-uncased fold: 4 inference ==========\n",
            "========== model: bert-base-uncased fold: 4 inference ==========\n",
            "========== model: bert-base-uncased fold: 4 inference ==========\n",
            "========== model: bert-base-uncased fold: 4 inference ==========\n",
            "========== model: bert-base-uncased fold: 4 inference ==========\n",
            "========== model: bert-base-uncased fold: 4 inference ==========\n",
            "========== model: bert-base-uncased fold: 4 inference ==========\n",
            "========== model: bert-base-uncased fold: 4 inference ==========\n",
            "========== model: bert-base-uncased fold: 4 inference ==========\n",
            "========== model: bert-base-uncased fold: 4 inference ==========\n",
            "========== model: bert-base-uncased fold: 4 inference ==========\n",
            "========== model: bert-base-uncased fold: 4 inference ==========\n",
            "loading configuration file /content/activebus_bert_final/config.json\n",
            "Model config BertConfig {\n",
            "  \"_name_or_path\": \"activebus/BERT-XD_Review\",\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_ids\": 0,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.15.0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "loading weights file /content/activebus_bert_final/pytorch_model.bin\n",
            "Some weights of the model checkpoint at /content/activebus_bert_final were not used when initializing BertForSequenceClassification: ['cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /content/activebus_bert_final and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "\n",
            "  0%|          | 0/165 [00:00<?, ?it/s]\u001b[A\n",
            "  1%|          | 1/165 [00:00<00:19,  8.47it/s]\u001b[A\n",
            "  1%|          | 2/165 [00:00<00:18,  8.64it/s]\u001b[A\n",
            "  2%|▏         | 3/165 [00:00<00:18,  8.69it/s]\u001b[A\n",
            "  2%|▏         | 4/165 [00:00<00:18,  8.74it/s]\u001b[A\n",
            "  3%|▎         | 5/165 [00:00<00:18,  8.79it/s]\u001b[A\n",
            "  4%|▎         | 6/165 [00:00<00:18,  8.81it/s]\u001b[A\n",
            "  4%|▍         | 7/165 [00:00<00:17,  8.80it/s]\u001b[A\n",
            "  5%|▍         | 8/165 [00:00<00:17,  8.81it/s]\u001b[A\n",
            "  5%|▌         | 9/165 [00:01<00:17,  8.85it/s]\u001b[A\n",
            "  6%|▌         | 10/165 [00:01<00:17,  8.84it/s]\u001b[A\n",
            "  7%|▋         | 11/165 [00:01<00:17,  8.83it/s]\u001b[A\n",
            "  7%|▋         | 12/165 [00:01<00:17,  8.84it/s]\u001b[A\n",
            "  8%|▊         | 13/165 [00:01<00:17,  8.83it/s]\u001b[A\n",
            "  8%|▊         | 14/165 [00:01<00:17,  8.86it/s]\u001b[A\n",
            "  9%|▉         | 15/165 [00:01<00:16,  8.85it/s]\u001b[A\n",
            " 10%|▉         | 16/165 [00:01<00:16,  8.83it/s]\u001b[A\n",
            " 10%|█         | 17/165 [00:01<00:16,  8.84it/s]\u001b[A\n",
            " 11%|█         | 18/165 [00:02<00:16,  8.85it/s]\u001b[A\n",
            " 12%|█▏        | 19/165 [00:02<00:16,  8.87it/s]\u001b[A\n",
            " 12%|█▏        | 20/165 [00:02<00:16,  8.88it/s]\u001b[A\n",
            " 13%|█▎        | 21/165 [00:02<00:16,  8.86it/s]\u001b[A\n",
            " 13%|█▎        | 22/165 [00:02<00:16,  8.84it/s]\u001b[A\n",
            " 14%|█▍        | 23/165 [00:02<00:16,  8.82it/s]\u001b[A\n",
            " 15%|█▍        | 24/165 [00:02<00:15,  8.82it/s]\u001b[A\n",
            " 15%|█▌        | 25/165 [00:02<00:15,  8.84it/s]\u001b[A\n",
            " 16%|█▌        | 26/165 [00:02<00:15,  8.85it/s]\u001b[A\n",
            " 16%|█▋        | 27/165 [00:03<00:15,  8.84it/s]\u001b[A\n",
            " 17%|█▋        | 28/165 [00:03<00:15,  8.86it/s]\u001b[A\n",
            " 18%|█▊        | 29/165 [00:03<00:15,  8.87it/s]\u001b[A\n",
            " 18%|█▊        | 30/165 [00:03<00:15,  8.88it/s]\u001b[A\n",
            " 19%|█▉        | 31/165 [00:03<00:15,  8.88it/s]\u001b[A\n",
            " 19%|█▉        | 32/165 [00:03<00:14,  8.88it/s]\u001b[A\n",
            " 20%|██        | 33/165 [00:03<00:14,  8.88it/s]\u001b[A\n",
            " 21%|██        | 34/165 [00:03<00:14,  8.88it/s]\u001b[A\n",
            " 21%|██        | 35/165 [00:03<00:14,  8.88it/s]\u001b[A\n",
            " 22%|██▏       | 36/165 [00:04<00:14,  8.85it/s]\u001b[A\n",
            " 22%|██▏       | 37/165 [00:04<00:14,  8.87it/s]\u001b[A\n",
            " 23%|██▎       | 38/165 [00:04<00:14,  8.87it/s]\u001b[A\n",
            " 24%|██▎       | 39/165 [00:04<00:14,  8.88it/s]\u001b[A\n",
            " 24%|██▍       | 40/165 [00:04<00:14,  8.87it/s]\u001b[A\n",
            " 25%|██▍       | 41/165 [00:04<00:14,  8.84it/s]\u001b[A\n",
            " 25%|██▌       | 42/165 [00:04<00:13,  8.86it/s]\u001b[A\n",
            " 26%|██▌       | 43/165 [00:04<00:13,  8.89it/s]\u001b[A\n",
            " 27%|██▋       | 44/165 [00:04<00:13,  8.90it/s]\u001b[A\n",
            " 27%|██▋       | 45/165 [00:05<00:13,  8.90it/s]\u001b[A\n",
            " 28%|██▊       | 46/165 [00:05<00:13,  8.88it/s]\u001b[A\n",
            " 28%|██▊       | 47/165 [00:05<00:13,  8.87it/s]\u001b[A\n",
            " 29%|██▉       | 48/165 [00:05<00:13,  8.87it/s]\u001b[A\n",
            " 30%|██▉       | 49/165 [00:05<00:13,  8.86it/s]\u001b[A\n",
            " 30%|███       | 50/165 [00:05<00:13,  8.82it/s]\u001b[A\n",
            " 31%|███       | 51/165 [00:05<00:12,  8.82it/s]\u001b[A\n",
            " 32%|███▏      | 52/165 [00:05<00:12,  8.85it/s]\u001b[A\n",
            " 32%|███▏      | 53/165 [00:05<00:12,  8.87it/s]\u001b[A\n",
            " 33%|███▎      | 54/165 [00:06<00:12,  8.85it/s]\u001b[A\n",
            " 33%|███▎      | 55/165 [00:06<00:12,  8.86it/s]\u001b[A\n",
            " 34%|███▍      | 56/165 [00:06<00:12,  8.81it/s]\u001b[A\n",
            " 35%|███▍      | 57/165 [00:06<00:12,  8.85it/s]\u001b[A\n",
            " 35%|███▌      | 58/165 [00:06<00:12,  8.85it/s]\u001b[A\n",
            " 36%|███▌      | 59/165 [00:06<00:12,  8.83it/s]\u001b[A\n",
            " 36%|███▋      | 60/165 [00:06<00:11,  8.84it/s]\u001b[A\n",
            " 37%|███▋      | 61/165 [00:06<00:11,  8.85it/s]\u001b[A\n",
            " 38%|███▊      | 62/165 [00:07<00:11,  8.87it/s]\u001b[A\n",
            " 38%|███▊      | 63/165 [00:07<00:11,  8.86it/s]\u001b[A\n",
            " 39%|███▉      | 64/165 [00:07<00:11,  8.85it/s]\u001b[A\n",
            " 39%|███▉      | 65/165 [00:07<00:11,  8.84it/s]\u001b[A\n",
            " 40%|████      | 66/165 [00:07<00:11,  8.83it/s]\u001b[A\n",
            " 41%|████      | 67/165 [00:07<00:11,  8.82it/s]\u001b[A\n",
            " 41%|████      | 68/165 [00:07<00:11,  8.81it/s]\u001b[A\n",
            " 42%|████▏     | 69/165 [00:07<00:10,  8.84it/s]\u001b[A\n",
            " 42%|████▏     | 70/165 [00:07<00:10,  8.86it/s]\u001b[A\n",
            " 43%|████▎     | 71/165 [00:08<00:10,  8.86it/s]\u001b[A\n",
            " 44%|████▎     | 72/165 [00:08<00:10,  8.85it/s]\u001b[A\n",
            " 44%|████▍     | 73/165 [00:08<00:10,  8.87it/s]\u001b[A\n",
            " 45%|████▍     | 74/165 [00:08<00:10,  8.85it/s]\u001b[A\n",
            " 45%|████▌     | 75/165 [00:08<00:10,  8.88it/s]\u001b[A\n",
            " 46%|████▌     | 76/165 [00:08<00:10,  8.89it/s]\u001b[A\n",
            " 47%|████▋     | 77/165 [00:08<00:09,  8.85it/s]\u001b[A\n",
            " 47%|████▋     | 78/165 [00:08<00:09,  8.85it/s]\u001b[A\n",
            " 48%|████▊     | 79/165 [00:08<00:09,  8.87it/s]\u001b[A\n",
            " 48%|████▊     | 80/165 [00:09<00:09,  8.86it/s]\u001b[A\n",
            " 49%|████▉     | 81/165 [00:09<00:09,  8.86it/s]\u001b[A\n",
            " 50%|████▉     | 82/165 [00:09<00:09,  8.86it/s]\u001b[A\n",
            " 50%|█████     | 83/165 [00:09<00:09,  8.87it/s]\u001b[A\n",
            " 51%|█████     | 84/165 [00:09<00:09,  8.87it/s]\u001b[A\n",
            " 52%|█████▏    | 85/165 [00:09<00:09,  8.86it/s]\u001b[A\n",
            " 52%|█████▏    | 86/165 [00:09<00:08,  8.85it/s]\u001b[A\n",
            " 53%|█████▎    | 87/165 [00:09<00:08,  8.83it/s]\u001b[A\n",
            " 53%|█████▎    | 88/165 [00:09<00:08,  8.86it/s]\u001b[A\n",
            " 54%|█████▍    | 89/165 [00:10<00:08,  8.87it/s]\u001b[A\n",
            " 55%|█████▍    | 90/165 [00:10<00:08,  8.87it/s]\u001b[A\n",
            " 55%|█████▌    | 91/165 [00:10<00:08,  8.88it/s]\u001b[A\n",
            " 56%|█████▌    | 92/165 [00:10<00:08,  8.87it/s]\u001b[A\n",
            " 56%|█████▋    | 93/165 [00:10<00:08,  8.87it/s]\u001b[A\n",
            " 57%|█████▋    | 94/165 [00:10<00:07,  8.88it/s]\u001b[A\n",
            " 58%|█████▊    | 95/165 [00:10<00:07,  8.87it/s]\u001b[A\n",
            " 58%|█████▊    | 96/165 [00:10<00:07,  8.87it/s]\u001b[A\n",
            " 59%|█████▉    | 97/165 [00:10<00:07,  8.85it/s]\u001b[A\n",
            " 59%|█████▉    | 98/165 [00:11<00:07,  8.88it/s]\u001b[A\n",
            " 60%|██████    | 99/165 [00:11<00:07,  8.88it/s]\u001b[A\n",
            " 61%|██████    | 100/165 [00:11<00:07,  8.89it/s]\u001b[A\n",
            " 61%|██████    | 101/165 [00:11<00:07,  8.86it/s]\u001b[A\n",
            " 62%|██████▏   | 102/165 [00:11<00:07,  8.85it/s]\u001b[A\n",
            " 62%|██████▏   | 103/165 [00:11<00:06,  8.87it/s]\u001b[A\n",
            " 63%|██████▎   | 104/165 [00:11<00:06,  8.85it/s]\u001b[A\n",
            " 64%|██████▎   | 105/165 [00:11<00:06,  8.85it/s]\u001b[A\n",
            " 64%|██████▍   | 106/165 [00:11<00:06,  8.87it/s]\u001b[A\n",
            " 65%|██████▍   | 107/165 [00:12<00:06,  8.84it/s]\u001b[A\n",
            " 65%|██████▌   | 108/165 [00:12<00:06,  8.86it/s]\u001b[A\n",
            " 66%|██████▌   | 109/165 [00:12<00:06,  8.88it/s]\u001b[A\n",
            " 67%|██████▋   | 110/165 [00:12<00:06,  8.88it/s]\u001b[A\n",
            " 67%|██████▋   | 111/165 [00:12<00:06,  8.86it/s]\u001b[A\n",
            " 68%|██████▊   | 112/165 [00:12<00:05,  8.84it/s]\u001b[A\n",
            " 68%|██████▊   | 113/165 [00:12<00:05,  8.86it/s]\u001b[A\n",
            " 69%|██████▉   | 114/165 [00:12<00:05,  8.86it/s]\u001b[A\n",
            " 70%|██████▉   | 115/165 [00:12<00:05,  8.89it/s]\u001b[A\n",
            " 70%|███████   | 116/165 [00:13<00:05,  8.89it/s]\u001b[A\n",
            " 71%|███████   | 117/165 [00:13<00:05,  8.86it/s]\u001b[A\n",
            " 72%|███████▏  | 118/165 [00:13<00:05,  8.87it/s]\u001b[A\n",
            " 72%|███████▏  | 119/165 [00:13<00:05,  8.85it/s]\u001b[A\n",
            " 73%|███████▎  | 120/165 [00:13<00:05,  8.87it/s]\u001b[A\n",
            " 73%|███████▎  | 121/165 [00:13<00:04,  8.86it/s]\u001b[A\n",
            " 74%|███████▍  | 122/165 [00:13<00:04,  8.87it/s]\u001b[A\n",
            " 75%|███████▍  | 123/165 [00:13<00:04,  8.86it/s]\u001b[A\n",
            " 75%|███████▌  | 124/165 [00:14<00:04,  8.88it/s]\u001b[A\n",
            " 76%|███████▌  | 125/165 [00:14<00:04,  8.86it/s]\u001b[A\n",
            " 76%|███████▋  | 126/165 [00:14<00:04,  8.86it/s]\u001b[A\n",
            " 77%|███████▋  | 127/165 [00:14<00:04,  8.88it/s]\u001b[A\n",
            " 78%|███████▊  | 128/165 [00:14<00:04,  8.89it/s]\u001b[A\n",
            " 78%|███████▊  | 129/165 [00:14<00:04,  8.88it/s]\u001b[A\n",
            " 79%|███████▉  | 130/165 [00:14<00:03,  8.88it/s]\u001b[A\n",
            " 79%|███████▉  | 131/165 [00:14<00:03,  8.88it/s]\u001b[A\n",
            " 80%|████████  | 132/165 [00:14<00:03,  8.87it/s]\u001b[A\n",
            " 81%|████████  | 133/165 [00:15<00:03,  8.87it/s]\u001b[A\n",
            " 81%|████████  | 134/165 [00:15<00:03,  8.84it/s]\u001b[A\n",
            " 82%|████████▏ | 135/165 [00:15<00:03,  8.86it/s]\u001b[A\n",
            " 82%|████████▏ | 136/165 [00:15<00:03,  8.87it/s]\u001b[A\n",
            " 83%|████████▎ | 137/165 [00:15<00:03,  8.89it/s]\u001b[A\n",
            " 84%|████████▎ | 138/165 [00:15<00:03,  8.89it/s]\u001b[A\n",
            " 84%|████████▍ | 139/165 [00:15<00:02,  8.88it/s]\u001b[A\n",
            " 85%|████████▍ | 140/165 [00:15<00:02,  8.86it/s]\u001b[A\n",
            " 85%|████████▌ | 141/165 [00:15<00:02,  8.85it/s]\u001b[A\n",
            " 86%|████████▌ | 142/165 [00:16<00:02,  8.86it/s]\u001b[A\n",
            " 87%|████████▋ | 143/165 [00:16<00:02,  8.87it/s]\u001b[A\n",
            " 87%|████████▋ | 144/165 [00:16<00:02,  8.86it/s]\u001b[A\n",
            " 88%|████████▊ | 145/165 [00:16<00:02,  8.83it/s]\u001b[A\n",
            " 88%|████████▊ | 146/165 [00:16<00:02,  8.86it/s]\u001b[A\n",
            " 89%|████████▉ | 147/165 [00:16<00:02,  8.86it/s]\u001b[A\n",
            " 90%|████████▉ | 148/165 [00:16<00:01,  8.87it/s]\u001b[A\n",
            " 90%|█████████ | 149/165 [00:16<00:01,  8.87it/s]\u001b[A\n",
            " 91%|█████████ | 150/165 [00:16<00:01,  8.88it/s]\u001b[A\n",
            " 92%|█████████▏| 151/165 [00:17<00:01,  8.84it/s]\u001b[A\n",
            " 92%|█████████▏| 152/165 [00:17<00:01,  8.87it/s]\u001b[A\n",
            " 93%|█████████▎| 153/165 [00:17<00:01,  8.88it/s]\u001b[A\n",
            " 93%|█████████▎| 154/165 [00:17<00:01,  8.88it/s]\u001b[A\n",
            " 94%|█████████▍| 155/165 [00:17<00:01,  8.88it/s]\u001b[A\n",
            " 95%|█████████▍| 156/165 [00:17<00:01,  8.88it/s]\u001b[A\n",
            " 95%|█████████▌| 157/165 [00:17<00:00,  8.87it/s]\u001b[A\n",
            " 96%|█████████▌| 158/165 [00:17<00:00,  8.89it/s]\u001b[A\n",
            " 96%|█████████▋| 159/165 [00:17<00:00,  8.89it/s]\u001b[A\n",
            " 97%|█████████▋| 160/165 [00:18<00:00,  8.89it/s]\u001b[A\n",
            " 98%|█████████▊| 161/165 [00:18<00:00,  8.91it/s]\u001b[A\n",
            " 98%|█████████▊| 162/165 [00:18<00:00,  8.93it/s]\u001b[A\n",
            " 99%|█████████▉| 163/165 [00:18<00:00,  8.93it/s]\u001b[A\n",
            "100%|██████████| 165/165 [00:18<00:00,  8.84it/s]\n",
            "100%|██████████| 3/3 [2:27:13<00:00, 2944.63s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sub.loc[sub['Delivery and Customer Support']>0.95,'Delivery and Customer Support']=1\n",
        "sub.loc[sub['Installation']>0.99,'Installation']=1\n",
        "sub.loc[sub['Polarity']>0.995,'Polarity']=1"
      ],
      "metadata": {
        "id": "UQAI4iu-KY79"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sub.to_csv(\"final_submission.csv\",index=False)"
      ],
      "metadata": {
        "id": "X3grvTwDirjc"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sub"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 520
        },
        "id": "8arQ-2KKlFUo",
        "outputId": "22af96b8-7ca5-47b5-dce0-c19956efdcd7"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-c997ef17-ffd4-4b1b-bb8b-384909c962d8\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Components</th>\n",
              "      <th>Delivery and Customer Support</th>\n",
              "      <th>Design and Aesthetics</th>\n",
              "      <th>Dimensions</th>\n",
              "      <th>Features</th>\n",
              "      <th>Functionality</th>\n",
              "      <th>Installation</th>\n",
              "      <th>Material</th>\n",
              "      <th>Price</th>\n",
              "      <th>Quality</th>\n",
              "      <th>Usability</th>\n",
              "      <th>Polarity</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.064288</td>\n",
              "      <td>0.004370</td>\n",
              "      <td>0.006367</td>\n",
              "      <td>0.465626</td>\n",
              "      <td>0.003052</td>\n",
              "      <td>0.008083</td>\n",
              "      <td>0.003644</td>\n",
              "      <td>0.684759</td>\n",
              "      <td>0.007097</td>\n",
              "      <td>0.796686</td>\n",
              "      <td>0.006844</td>\n",
              "      <td>0.008028</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.003568</td>\n",
              "      <td>0.001271</td>\n",
              "      <td>0.003796</td>\n",
              "      <td>0.004844</td>\n",
              "      <td>0.003948</td>\n",
              "      <td>0.966002</td>\n",
              "      <td>0.002018</td>\n",
              "      <td>0.001610</td>\n",
              "      <td>0.003577</td>\n",
              "      <td>0.025209</td>\n",
              "      <td>0.005528</td>\n",
              "      <td>0.984463</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.003849</td>\n",
              "      <td>0.000802</td>\n",
              "      <td>0.004170</td>\n",
              "      <td>0.003153</td>\n",
              "      <td>0.018387</td>\n",
              "      <td>0.011440</td>\n",
              "      <td>0.004288</td>\n",
              "      <td>0.001159</td>\n",
              "      <td>0.001668</td>\n",
              "      <td>0.013245</td>\n",
              "      <td>0.982984</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.004342</td>\n",
              "      <td>0.001007</td>\n",
              "      <td>0.003264</td>\n",
              "      <td>0.004065</td>\n",
              "      <td>0.003459</td>\n",
              "      <td>0.991359</td>\n",
              "      <td>0.002927</td>\n",
              "      <td>0.001221</td>\n",
              "      <td>0.002014</td>\n",
              "      <td>0.004203</td>\n",
              "      <td>0.011061</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.003362</td>\n",
              "      <td>0.000911</td>\n",
              "      <td>0.003433</td>\n",
              "      <td>0.004024</td>\n",
              "      <td>0.003178</td>\n",
              "      <td>0.079682</td>\n",
              "      <td>0.002456</td>\n",
              "      <td>0.001270</td>\n",
              "      <td>0.002046</td>\n",
              "      <td>0.927882</td>\n",
              "      <td>0.005519</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2626</th>\n",
              "      <td>0.017728</td>\n",
              "      <td>0.000819</td>\n",
              "      <td>0.004584</td>\n",
              "      <td>0.003877</td>\n",
              "      <td>0.004603</td>\n",
              "      <td>0.008004</td>\n",
              "      <td>0.988757</td>\n",
              "      <td>0.001877</td>\n",
              "      <td>0.001912</td>\n",
              "      <td>0.990025</td>\n",
              "      <td>0.930305</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2627</th>\n",
              "      <td>0.007733</td>\n",
              "      <td>0.006487</td>\n",
              "      <td>0.103094</td>\n",
              "      <td>0.056577</td>\n",
              "      <td>0.003429</td>\n",
              "      <td>0.021570</td>\n",
              "      <td>0.006162</td>\n",
              "      <td>0.001244</td>\n",
              "      <td>0.011111</td>\n",
              "      <td>0.005483</td>\n",
              "      <td>0.008538</td>\n",
              "      <td>0.910537</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2628</th>\n",
              "      <td>0.003074</td>\n",
              "      <td>0.001187</td>\n",
              "      <td>0.003138</td>\n",
              "      <td>0.003783</td>\n",
              "      <td>0.002732</td>\n",
              "      <td>0.976333</td>\n",
              "      <td>0.001824</td>\n",
              "      <td>0.002818</td>\n",
              "      <td>0.001916</td>\n",
              "      <td>0.972331</td>\n",
              "      <td>0.004255</td>\n",
              "      <td>0.994582</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2629</th>\n",
              "      <td>0.003634</td>\n",
              "      <td>0.000894</td>\n",
              "      <td>0.914709</td>\n",
              "      <td>0.013211</td>\n",
              "      <td>0.003502</td>\n",
              "      <td>0.003982</td>\n",
              "      <td>0.002336</td>\n",
              "      <td>0.024591</td>\n",
              "      <td>0.001898</td>\n",
              "      <td>0.917942</td>\n",
              "      <td>0.008805</td>\n",
              "      <td>0.896557</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2630</th>\n",
              "      <td>0.003335</td>\n",
              "      <td>0.000775</td>\n",
              "      <td>0.004203</td>\n",
              "      <td>0.004537</td>\n",
              "      <td>0.061854</td>\n",
              "      <td>0.013038</td>\n",
              "      <td>0.989171</td>\n",
              "      <td>0.001515</td>\n",
              "      <td>0.001553</td>\n",
              "      <td>0.006408</td>\n",
              "      <td>0.954890</td>\n",
              "      <td>0.994655</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2631 rows × 12 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-c997ef17-ffd4-4b1b-bb8b-384909c962d8')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-c997ef17-ffd4-4b1b-bb8b-384909c962d8 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-c997ef17-ffd4-4b1b-bb8b-384909c962d8');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "      Components  Delivery and Customer Support  ...  Usability  Polarity\n",
              "0       0.064288                       0.004370  ...   0.006844  0.008028\n",
              "1       0.003568                       0.001271  ...   0.005528  0.984463\n",
              "2       0.003849                       0.000802  ...   0.982984  1.000000\n",
              "3       0.004342                       0.001007  ...   0.011061  1.000000\n",
              "4       0.003362                       0.000911  ...   0.005519  1.000000\n",
              "...          ...                            ...  ...        ...       ...\n",
              "2626    0.017728                       0.000819  ...   0.930305  1.000000\n",
              "2627    0.007733                       0.006487  ...   0.008538  0.910537\n",
              "2628    0.003074                       0.001187  ...   0.004255  0.994582\n",
              "2629    0.003634                       0.000894  ...   0.008805  0.896557\n",
              "2630    0.003335                       0.000775  ...   0.954890  0.994655\n",
              "\n",
              "[2631 rows x 12 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "4Bg4xG84lRq2"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "machine_shape": "hm",
      "name": "uHack Sentiments 2.0: Decode Code Words_Rank3.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "79c669321bb44061950e37bd94e4a259": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_c8b81f14afd742c481feb776f784d373",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_63ea255d6b8e4344843cbd202199ef39",
              "IPY_MODEL_55cb5bb5132a4561a26495837b42cb53",
              "IPY_MODEL_d182d9b06ce64c37ade3155e757d7b54"
            ]
          }
        },
        "c8b81f14afd742c481feb776f784d373": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "63ea255d6b8e4344843cbd202199ef39": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_4339806721fb4226b95ef36b143b8abf",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "Downloading: 100%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_8ee569af346e4bfeb9e33ade8bc4cc32"
          }
        },
        "55cb5bb5132a4561a26495837b42cb53": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_f8c1a50e12864335a1bc1a87dd2217a7",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 1020,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1020,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_1de3eaca2d5748508882b63a9d22da41"
          }
        },
        "d182d9b06ce64c37ade3155e757d7b54": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_59ae600e3bea4fc6aa3e1b493055b0a4",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 1.00k/1.00k [00:00&lt;00:00, 26.2kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_a5fc9e2be1aa478eb74fb36d09f2b5b4"
          }
        },
        "4339806721fb4226b95ef36b143b8abf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "8ee569af346e4bfeb9e33ade8bc4cc32": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "f8c1a50e12864335a1bc1a87dd2217a7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "1de3eaca2d5748508882b63a9d22da41": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "59ae600e3bea4fc6aa3e1b493055b0a4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "a5fc9e2be1aa478eb74fb36d09f2b5b4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "604bf0e27fb54ba796606a90d2495f7f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_dfa246c09a03470ab44d0a3d03dd34f5",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_a40d411dcd6c400d8c8722fb6b40a56b",
              "IPY_MODEL_dea6ed6a3db34d0397abc3ab2e593d01",
              "IPY_MODEL_d677ec818ea945a7a7049908fc89f4ac"
            ]
          }
        },
        "dfa246c09a03470ab44d0a3d03dd34f5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "a40d411dcd6c400d8c8722fb6b40a56b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_25ba258e241842c094451cf60ae9077c",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "Downloading: 100%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_80440ad1112642df9f96fad9e95c4f59"
          }
        },
        "dea6ed6a3db34d0397abc3ab2e593d01": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_12b2d39d4bcc47a4b313686942a102fc",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 534356664,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 534356664,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_f76b892816384499b0f99c450f13f89f"
          }
        },
        "d677ec818ea945a7a7049908fc89f4ac": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_344fff8984f24981945aa2444ac700be",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 510M/510M [00:20&lt;00:00, 29.3MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_db8edb44026645d18c7ea7ba32582ade"
          }
        },
        "25ba258e241842c094451cf60ae9077c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "80440ad1112642df9f96fad9e95c4f59": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "12b2d39d4bcc47a4b313686942a102fc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "f76b892816384499b0f99c450f13f89f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "344fff8984f24981945aa2444ac700be": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "db8edb44026645d18c7ea7ba32582ade": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "d5c9c39f7b2145c1932eef10ef1053b5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_0552864714f843a7ba676b7d81ee78ee",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_f2f551636d1c4a0f85b732a006e86de6",
              "IPY_MODEL_eb42b0ed7503427cbf611e6c10e0b18e",
              "IPY_MODEL_8a9d265c97564e13ada9606e634e2dfa"
            ]
          }
        },
        "0552864714f843a7ba676b7d81ee78ee": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "f2f551636d1c4a0f85b732a006e86de6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_b74c1406476749ab93303c73657a257a",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "Downloading: 100%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_30a64747440144a6b120d65c905eacbe"
          }
        },
        "eb42b0ed7503427cbf611e6c10e0b18e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_04459085433a49659fa7398178831b42",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 39,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 39,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_dc0a6e3129fb47b99fddb315095f0c5f"
          }
        },
        "8a9d265c97564e13ada9606e634e2dfa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_0ffe54217a5747a08547564fda3066f2",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 39.0/39.0 [00:00&lt;00:00, 1.33kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_742448ecae5649109b7874e5175f7622"
          }
        },
        "b74c1406476749ab93303c73657a257a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "30a64747440144a6b120d65c905eacbe": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "04459085433a49659fa7398178831b42": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "dc0a6e3129fb47b99fddb315095f0c5f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "0ffe54217a5747a08547564fda3066f2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "742448ecae5649109b7874e5175f7622": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "304903f465214ea2a58292b87205b7e2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_f5679a4c21084b1b9489ae5bd55ff649",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_7a38201ffa5645c3a71ecfe1080526cf",
              "IPY_MODEL_19d079cfea414db3b8c4ffbfa3bc09ae",
              "IPY_MODEL_e5949ad8849f483ca5922b88e9ac6337"
            ]
          }
        },
        "f5679a4c21084b1b9489ae5bd55ff649": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "7a38201ffa5645c3a71ecfe1080526cf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_ed35b2367710430aa803f48dd4e9f654",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "Downloading: 100%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_93fcb41580b44df7baf9cf30be332866"
          }
        },
        "19d079cfea414db3b8c4ffbfa3bc09ae": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_999db374173d4672b7cb0eaedc20bcd4",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 231508,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 231508,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_cb47b246cb7c4140b9c7f07b20aa6ce0"
          }
        },
        "e5949ad8849f483ca5922b88e9ac6337": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_db953f5f0489452d873b251c0a47f29e",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 226k/226k [00:00&lt;00:00, 645kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_9bb623193d7345efbf503ec8268e9f2a"
          }
        },
        "ed35b2367710430aa803f48dd4e9f654": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "93fcb41580b44df7baf9cf30be332866": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "999db374173d4672b7cb0eaedc20bcd4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "cb47b246cb7c4140b9c7f07b20aa6ce0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "db953f5f0489452d873b251c0a47f29e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "9bb623193d7345efbf503ec8268e9f2a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "dd2efccff6ae46f080a2a95ee1507420": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_a5fbcadc429c496395cc9a26666cacb1",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_91675cf4f0ae4ef09cc84fc1f74f9d24",
              "IPY_MODEL_64477bfc60994688a1382c4573476345",
              "IPY_MODEL_7969c221547e4bf4b1b5dc4a0bf2b6d9"
            ]
          }
        },
        "a5fbcadc429c496395cc9a26666cacb1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "91675cf4f0ae4ef09cc84fc1f74f9d24": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_1693983495f7446aa45cb7091dad1d10",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "Downloading: 100%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_66c21ec35db344ec8807df3b92e99f85"
          }
        },
        "64477bfc60994688a1382c4573476345": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_80e1a188ddc7419f9cd58e3410606e65",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 112,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 112,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_e62c65f4e53d4ad7a29bd1fb7e52b7c6"
          }
        },
        "7969c221547e4bf4b1b5dc4a0bf2b6d9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_307ff0069df244a5b027cbf6188b9c17",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 112/112 [00:00&lt;00:00, 3.68kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_b09a5a3fb7444536bf9ac2575a8723bf"
          }
        },
        "1693983495f7446aa45cb7091dad1d10": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "66c21ec35db344ec8807df3b92e99f85": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "80e1a188ddc7419f9cd58e3410606e65": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "e62c65f4e53d4ad7a29bd1fb7e52b7c6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "307ff0069df244a5b027cbf6188b9c17": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "b09a5a3fb7444536bf9ac2575a8723bf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}