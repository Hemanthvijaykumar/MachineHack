{"cells":[{"cell_type":"markdown","id":"5fbbb601-cbc9-4d43-9617-4e9d882943a8","metadata":{"id":"5fbbb601-cbc9-4d43-9617-4e9d882943a8"},"source":["# Data reading and cleaning (1 block)"]},{"cell_type":"code","execution_count":null,"id":"1177HTsbbSvD","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":13446,"status":"ok","timestamp":1640600794766,"user":{"displayName":"Aishwarya Verma","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhaxwkmPjoi5xkTRkOll9xQZy4hrlug6tIFdoSWvQ=s64","userId":"06108390091304498033"},"user_tz":-330},"id":"1177HTsbbSvD","outputId":"8ee877a4-41cc-4a91-a783-fbc00898aa8d"},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting catboost\n","  Downloading catboost-1.0.3-cp37-none-manylinux1_x86_64.whl (76.3 MB)\n","\u001b[K     |████████████████████████████████| 76.3 MB 1.2 MB/s \n","\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from catboost) (1.4.1)\n","Requirement already satisfied: plotly in /usr/local/lib/python3.7/dist-packages (from catboost) (4.4.1)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from catboost) (1.15.0)\n","Requirement already satisfied: pandas>=0.24.0 in /usr/local/lib/python3.7/dist-packages (from catboost) (1.1.5)\n","Requirement already satisfied: graphviz in /usr/local/lib/python3.7/dist-packages (from catboost) (0.10.1)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from catboost) (3.2.2)\n","Requirement already satisfied: numpy>=1.16.0 in /usr/local/lib/python3.7/dist-packages (from catboost) (1.19.5)\n","Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.24.0->catboost) (2.8.2)\n","Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.24.0->catboost) (2018.9)\n","Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->catboost) (3.0.6)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->catboost) (0.11.0)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->catboost) (1.3.2)\n","Requirement already satisfied: retrying>=1.3.3 in /usr/local/lib/python3.7/dist-packages (from plotly->catboost) (1.3.3)\n","Installing collected packages: catboost\n","Successfully installed catboost-1.0.3\n"]}],"source":["!pip install catboost"]},{"cell_type":"code","execution_count":null,"id":"89955d65-c452-4510-abb2-3ed46e02152b","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1428,"status":"ok","timestamp":1640602749646,"user":{"displayName":"Aishwarya Verma","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhaxwkmPjoi5xkTRkOll9xQZy4hrlug6tIFdoSWvQ=s64","userId":"06108390091304498033"},"user_tz":-330},"id":"89955d65-c452-4510-abb2-3ed46e02152b","outputId":"3196bf4d-4037-4f1c-c1ed-6c89c18dd053"},"outputs":[{"name":"stdout","output_type":"stream","text":["(914, 11)\n","(167, 11)\n"]}],"source":["import pandas as pd\n","import numpy as np\n","from sklearn.preprocessing import LabelEncoder\n","from sklearn.model_selection import train_test_split\n","from sklearn.ensemble import RandomForestRegressor\n","from sklearn.metrics import r2_score, mean_absolute_error, mean_absolute_percentage_error, mean_squared_log_error\n","from catboost import CatBoostRegressor\n","from scipy.stats import zscore\n","\n","pd.set_option('max_rows', 1000)\n","pd.set_option('max_columns', 1000)\n","\n","def metrics_print(actual, predicted, data_set):\n","    print(f'{data_set}')\n","    print('RMSLE', np.sqrt(mean_squared_log_error(actual, predicted)))\n","    print('MAE', mean_absolute_error(actual, predicted))\n","    print('MAPE', mean_absolute_percentage_error(actual, predicted))\n","    print('r2_score', r2_score(actual, predicted))\n","    \n","# weather data\n","df1 = pd.read_csv('train_weather.csv', encoding = 'utf8')\n","cols = df1.columns\n","df1.columns = [c.strip() for c in cols]\n","print(df1.shape)\n","\n","df0 = pd.read_csv('test_weather.csv', encoding = 'utf8')\n","cols = df0.columns\n","df0.columns = [c.strip() for c in cols]\n","df0.rename(columns = {'EVENTS':'EVENT'}, inplace = True)\n","print(df0.shape)\n","\n","df1 = pd.concat([df1,df0])\n","\n","# main data\n","df = pd.read_csv('train.csv', encoding = 'utf8')\n","cols = df.columns\n","df.columns = [c.strip() for c in cols]\n","df.fillna('', inplace = True)\n","df['is_train'] = 1\n","\n","# adding test to the single dataset\n","df0 = pd.read_csv('test.csv', encoding = 'utf8')\n","cols = df0.columns\n","df0.columns = [c.strip() for c in cols]\n","df0.fillna('', inplace = True)\n","df0['is_train'] = 0\n","\n","df = pd.concat([df,df0])\n","\n","# convertion to float\n","def conv2float(x):\n","    try:\n","        x = float(x)\n","    except:\n","        print(x)\n","    return x\n","\n","# cleaning\n","feats_agg = ['AIR_TEMP', 'TRACK_TEMP','HUMIDITY', 'PRESSURE', 'WIND_SPEED', 'WIND_DIRECTION', 'RAIN']\n","l = lambda x: len(str(x).split('.'))\n","for f in feats_agg:\n","    df1[f] = df1[f].map(lambda x: str(x).replace(',','.'))\n","    df1 = df1[df1[f].map(l) < 3]\n","    df1[f] = df1[f].map(conv2float)\n","\n","# weather analysis\n","df1['TIME_UTC_STR'] = pd.to_datetime(df1['TIME_UTC_STR'])\n","\n","feats_gr  = ['LOCATION','EVENT']\n","feats_agg = ['TIME_UTC_STR','AIR_TEMP', 'TRACK_TEMP','HUMIDITY', 'PRESSURE', 'WIND_SPEED', 'WIND_DIRECTION', 'RAIN']\n","\n","df2 = df1.groupby(feats_gr)[feats_agg].agg([np.mean, min, max, np.std])\n","\n","# creating race_number\n","df1.sort_values(['LOCATION','EVENT','TIME_UTC_STR'], inplace = True)\n","#df1['d'] = df1.groupby(['LOCATION','EVENT']).TIME_UTC_SECONDS.diff()\n","df1['day'] = df1['TIME_UTC_STR'].astype(str).str[:10]\n","\n","feats = ['LOCATION','EVENT','day']\n","df2 = df1.groupby(feats).size().reset_index()\n","df2['race_number'] = df2.groupby(feats[:2]).cumcount() + 1\n","del df2[0]\n","\n","# join with initial table\n","df3 = df1.merge(df2)\n","# group by feats_gr and calc mean,std of feats_agg\n","feats_gr  = ['LOCATION','EVENT','race_number','day']\n","feats_agg = ['AIR_TEMP', 'TRACK_TEMP','HUMIDITY', 'PRESSURE', 'WIND_SPEED', 'WIND_DIRECTION', 'RAIN']\n","df4 = df3.groupby(feats_gr)[feats_agg].agg([np.mean, np.std])\n","cols = df4.columns\n","df4.columns = ['_'.join(c) for c in cols]\n","df4.reset_index(inplace = True)\n","\n","# and then we will merge it with main data\n","feats_weather = list(df4.columns[3:])\n","\n","for f in ['DRIVER_NUMBER','GROUP']:\n","    del df[f]\n","        \n","df['driver'] = df['TEAM'] + '_' + df['DRIVER_NAME']\n","del df['TEAM']\n","del df['DRIVER_NAME']\n","\n","\n","# we should create new feature: race_number = number of trial(appearance) per group\n","feats = ['LOCATION', 'EVENT','driver'] + ['LAP_NUMBER']\n","df['race_number'] = df.groupby(feats).cumcount()+1\n","\n","# number of unique NUMBER per driver\n","# FOR EACH DRIVER THERE IS ONLY ONE UNIQUE NUMBER NOT CHANGING BY LAPS! \n","pd.options.display.float_format = '{:,.0f}'.format\n","feats = ['LOCATION', 'EVENT','driver']\n","df.groupby(feats)['NUMBER'].nunique().unstack().max(axis = 1).unstack().fillna(0)\n","\n","feats_gr  = ['LOCATION','EVENT','driver','race_number']\n","feats_lap = ['LAP_NUMBER']\n","feats_time = ['S1','S2','S3','ELAPSED','HOUR','S1_LARGE','S2_LARGE','S3_LARGE','PIT_TIME']\n","\n","for f in feats_time:\n","    df[f] = df[f].map(lambda x: str(x).strip() )\n","df[feats_time][:5]\n","\n","\n","def str2time(x):\n","    x = x.strip()\n","    if x == '':\n","        return np.nan\n","    x = x.split(':')\n","    out = float(x[-1])\n","    for i,t in enumerate(x[::-1]):\n","        out+=float(t)*60*i\n","    return out\n","\n","for f in feats_time:\n","    df[f] = df[f].map(str2time)\n","df[feats_time].isnull().sum()\n","\n","for f in ['S1','S2','S3']:\n","    del df[f+'_LARGE']\n","    \n","# check 2: cumsum of s1 + s2 + s3 in the group = elapsed\n","# yes, it's true before appearance of null in s1,s2,s3\n","feats_gr  = ['LOCATION','EVENT','driver','race_number']\n","df.sort_values(feats_gr + ['LAP_NUMBER'], inplace = True)\n","\n","df['S'] = \\\n","    df.groupby(feats_gr)['S1'].cumsum() + \\\n","    df.groupby(feats_gr)['S2'].cumsum() + \\\n","    df.groupby(feats_gr)['S3'].cumsum()\n","df['delta'] = df['S'] - df['ELAPSED']\n","\n","\n","f1 = df.delta.abs()>1\n","f2 = df.LAP_NUMBER < 8\n","\n","feats_time = ['S1','S2','S3','ELAPSED','HOUR','PIT_TIME','S','delta']\n","df[f1][feats_gr + feats_time][:5]\n","\n","\n","# check 3: s1+s2+s3 = diff of hour\n","del df['HOUR']\n","\n","# create feature s with correction by pit_time\n","df['S'] = df['S1'] + df['S2'] + df['S3']\n","df['S_PIT'] = df['S1'] + df['S2'] + df['S3'] - df['PIT_TIME'].fillna(0)\n","\n","\n","df['pit_b'] = (df.CROSSING_FINISH_LINE_IN_PIT == 'B').astype(int)\n","del df['CROSSING_FINISH_LINE_IN_PIT']\n","\n","df['POWER'] = df.POWER.replace({'':0, 235.0:1, 250.0:2})\n","\n","feats_real   = ['KPH']\n","feats_target = ['LAP_TIME']\n","\n","df['KPH'] = df.KPH.replace('',np.nan).astype(np.float)\n","feats_join = ['LOCATION', 'EVENT', 'race_number']\n","df = df.merge(df4, on = feats_join, how = 'left')\n","\n","# DATA FOR TRAINING CREATED"]},{"cell_type":"markdown","id":"618b713f-e609-4439-806d-bc9ded7bc93e","metadata":{"id":"618b713f-e609-4439-806d-bc9ded7bc93e"},"source":["# Model training and submission creation (1 block)"]},{"cell_type":"code","execution_count":null,"id":"b60a4c44-30da-4ebf-af0b-d61b4a1b590a","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":69365,"status":"ok","timestamp":1640602819009,"user":{"displayName":"Aishwarya Verma","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhaxwkmPjoi5xkTRkOll9xQZy4hrlug6tIFdoSWvQ=s64","userId":"06108390091304498033"},"user_tz":-330},"id":"b60a4c44-30da-4ebf-af0b-d61b4a1b590a","outputId":"6a25a8f4-6488-4b67-e2be-814ea95939e3"},"outputs":[{"name":"stdout","output_type":"stream","text":["(10696, 25)\n","(10696, 25)\n","(10696, 26)\n","(10696, 27)\n","(10696, 28)\n","Test\n","S1 NaNs (0, 28)\n","S2 NaNs (0, 28)\n","S3 NaNs (2, 28)\n","KPH NaNs (2, 28)\n","Train\n","S1 NaNs (4, 28)\n","S2 NaNs (8, 28)\n","S3 NaNs (34, 28)\n","KPH NaNs (30, 28)\n","(10696, 28)\n","(10660, 41)\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/numpy/core/_methods.py:234: RuntimeWarning: Degrees of freedom <= 0 for slice\n","  keepdims=keepdims)\n","/usr/local/lib/python3.7/dist-packages/numpy/core/_methods.py:224: RuntimeWarning: invalid value encountered in true_divide\n","  ret, rcount, out=ret, casting='unsafe', subok=False)\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:157: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame.\n","Try using .loc[row_indexer,col_indexer] = value instead\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"]},{"name":"stdout","output_type":"stream","text":["df-------- (10660, 48)\n","df_test--- 420\n","df_train-- 10240\n","df_model_2 4075\n","(3164, 45) (792, 45)\n"]}],"source":["le, le1, le2, le3, le4, le5 = LabelEncoder(), LabelEncoder(), LabelEncoder(), LabelEncoder(), LabelEncoder(), LabelEncoder()\n","le1.fit(df['LOCATION'])\n","le2.fit(df['EVENT'])\n","le3.fit(df['driver'])\n","le4.fit(df['race_number'])\n","le5.fit(df['LAP_NUMBER'])\n","\n","# Creating race_id as  \n","# race_id = LOCATION + EVENT + race_number\n","\n","df['race_id'] = pd.Series(le1.transform(df['LOCATION'])).astype(str) + '_' + \\\n","                pd.Series(le2.transform(df['EVENT'])).astype(str) + '_' + \\\n","                pd.Series(le4.transform(df['race_number'])).astype(str)\n","\n","# Creating race_driver_id as  \n","# race_id = LOCATION + EVENT + race_number + driver\n","df['race_driver_id'] = df['race_id'] + '_' + pd.Series(le3.transform(df['driver'])).astype(str)\n","\n","\n","# Finalising the features in the specific order\n","df = df[['race_id', 'race_driver_id', 'driver', 'race_number', 'LOCATION', 'EVENT', 'NUMBER',\n","         'LAP_NUMBER', 'LAP_IMPROVEMENT', 'S1', 'S1_IMPROVEMENT', 'S2', 'S2_IMPROVEMENT', 'S3', 'S3_IMPROVEMENT', \n","         'KPH', 'ELAPSED', 'PIT_TIME', 'POWER', 'is_train', 'S', 'delta', 'S_PIT', 'LAP_TIME', 'pit_b']]\n","print(df.shape)\n","\n","\n","print(df.shape)\n","# LAPS_in_RACE\n","temp = df.groupby(['race_id'])['LAP_NUMBER'].max().reset_index()\n","temp = temp.rename(columns={'LAP_NUMBER':'LAPS_in_RACE'})\n","\n","df = df.merge(temp, on=['race_id'], how='left')\n","\n","print(df.shape)\n","\n","# filling NaNs in PIT_TIME with 0\n","df['PIT_TIME'] = df['PIT_TIME'].fillna(0)\n","\n","temp = df.groupby('race_id')['driver'].nunique().reset_index().rename(columns={'driver':'drivers_in_RACE'})\n","df = df.merge(temp, on= 'race_id', how='left')\n","\n","print(df.shape)\n","\n","# drivers_in_LAP\n","temp = df.groupby(['race_id', 'LAP_NUMBER'])['driver'].nunique().reset_index().rename(columns={'driver':'drivers_in_LAP'})\n","df = df.merge(temp, on=['race_id', 'LAP_NUMBER'], how='left')\n","\n","# drivers_droped\n","df['drivers_droped'] = abs(df.sort_values(['race_id', 'LAP_NUMBER']).groupby('race_id')['drivers_in_LAP'].diff().fillna(0))\n","\n","df = df.drop(columns=['delta'])\n","\n","print(df.shape)\n","\n","# Test\n","print('Test')\n","print('S1 NaNs', df[(df['S1'].isna())&(df['is_train'] == 0)].shape)\n","print('S2 NaNs', df[(df['S2'].isna())&(df['is_train'] == 0)].shape)\n","print('S3 NaNs', df[(df['S3'].isna())&(df['is_train'] == 0)].shape)\n","print('KPH NaNs', df[(df['KPH'].isna())&(df['is_train'] == 0)].shape)\n","\n","# Train\n","print('Train')\n","print('S1 NaNs', df[(df['S1'].isna())&(df['is_train'] == 1)].shape)\n","print('S2 NaNs', df[(df['S2'].isna())&(df['is_train'] == 1)].shape)\n","print('S3 NaNs', df[(df['S3'].isna())&(df['is_train'] == 1)].shape)\n","print('KPH NaNs', df[(df['KPH'].isna())&(df['is_train'] == 1)].shape)\n","\n","print(df.shape)\n","df.loc[(df['race_id'] == '5_3_0')&(df['LAP_NUMBER'] == 3)&(df['KPH'].isna()), 'KPH'] = 50\n","df.loc[(df['race_id']=='5_5_0')&(df['LAP_NUMBER'] == 4), 'KPH'] = 100\n","\n","df.loc[(df['race_driver_id'] == '5_3_0_8')&(df['LAP_NUMBER'] == 3), 'S3'] = 18\n","df.loc[(df['race_driver_id'] == '5_5_0_24')&(df['S3'].isna()), 'S3'] = 23.\n","\n","df['S'] = df['S1'] + df['S2'] + df['S3']\n","df['S_PIT'] = df['S'] - df['PIT_TIME']\n","\n","df = df.dropna(subset=['S1', 'S2', 'S3'])\n","assert df.isna().mean().sum() == 0\n","\n","\n","# creating features\n","df['S1_dist'] = df['KPH']*df['S1']\n","df['S2_dist'] = df['KPH']*df['S2']\n","df['S3_dist'] = df['KPH']*df['S3']\n","df['S_dist'] = df['S1_dist'] + df['S2_dist'] + df['S3_dist']\n","\n","temp = df.groupby(['race_id', 'LAP_NUMBER'])['S1_dist']\\\n","    .median()\\\n","    .reset_index()\\\n","    .rename(columns={'S1_dist':'S1_dist_race_id_LAP_median'})\n","\n","df = df.merge(temp, on=['race_id', 'LAP_NUMBER'], how='left')\n","\n","temp = df.groupby(['race_id', 'LAP_NUMBER'])['S2_dist']\\\n","    .median()\\\n","    .reset_index()\\\n","    .rename(columns={'S2_dist':'S2_dist_race_id_LAP_median'})\n","\n","df = df.merge(temp, on=['race_id', 'LAP_NUMBER'], how='left')\n","\n","temp = df.groupby(['race_id', 'LAP_NUMBER'])['S3_dist']\\\n","    .median()\\\n","    .reset_index()\\\n","    .rename(columns={'S3_dist':'S3_dist_race_id_LAP_median'})\n","\n","df = df.merge(temp, on=['race_id', 'LAP_NUMBER'], how='left')\n","\n","temp = df.groupby(['race_id', 'LAP_NUMBER'])['S_dist']\\\n","    .median()\\\n","    .reset_index()\\\n","    .rename(columns={'S_dist':'S_dist_race_id_LAP_median'})\n","\n","df = df.merge(temp, on=['race_id', 'LAP_NUMBER'], how='left')\n","\n","# creating features\n","df['estimated_time_S'] = df['S_dist_race_id_LAP_median']/df['KPH']\n","df['estimated_time_S1'] = df['S1_dist_race_id_LAP_median']/df['KPH']\n","df['estimated_time_S2'] = df['S2_dist_race_id_LAP_median']/df['KPH']\n","df['estimated_time_S3'] = df['S3_dist_race_id_LAP_median']/df['KPH']\n","\n","# defining the submission Locations and events\n","test_LOCATION = ['Location 6', 'Location 7', 'Location 8']\n","test_EVENT = ['Qualifying Group 1', 'Qualifying Group 2', 'Qualifying Group 3', 'Qualifying Group 4']\n","\n","# creating features\n","df['place_in_LAP'] = df.sort_values(['race_id', 'S']).groupby(['race_id', 'LAP_NUMBER']).cumcount()+1\n","\n","print(df.shape)\n","\n","# creating features\n","df['S1_zscore'] = df.groupby(['race_driver_id']).S1.transform(lambda x : zscore(x,ddof=1))\n","df['S2_zscore'] = df.groupby(['race_driver_id']).S2.transform(lambda x : zscore(x,ddof=1))\n","df['S3_zscore'] = df.groupby(['race_driver_id']).S3.transform(lambda x : zscore(x,ddof=1))\n","df['S1_zscore'] = df['S1_zscore'].fillna(df['S1_zscore'].median())\n","df['S2_zscore'] = df['S2_zscore'].fillna(df['S2_zscore'].median())\n","df['S3_zscore'] = df['S3_zscore'].fillna(df['S3_zscore'].median())\n","df['min_zscore'] = df[['S1_zscore', 'S2_zscore', 'S3_zscore']].min(axis=1)\n","df['max_zscore'] = df[['S1_zscore', 'S2_zscore', 'S3_zscore']].max(axis=1)\n","df['estimated_ELAPSED'] = (df['estimated_time_S']*df['LAP_NUMBER'])\n","df['ELAPSED_RATIO'] = abs(df['estimated_ELAPSED'] - df['ELAPSED'])/df['ELAPSED']\n","\n","df_test = df[df['is_train'] == 0].copy()\n","df_train = df[df['is_train'] == 1].copy()\n","        \n","df_train['LAP_TIME'] = df_train['LAP_TIME'].astype(float)\n","\n","# creating filters\n","f_test_loc = df_train['LOCATION'].isin(test_LOCATION)\n","f_test_event = df_train['EVENT'].isin(test_EVENT)\n","\n","# training only on the submission locations\n","\n","df_model_2 = df_train[(f_test_loc)&(~f_test_event)]\n","\n","# !!!!!!!!!!!!!!!!!! training only on this data\n","# Location 6,  Free Practice 1\n","# Location 6,  Free Practice 2\n","# Location 6,  Free Practice 3\n","# Location 7,  Free Practice 1\n","# Location 7,  Free Practice 2\n","# Location 8,  Free Practice 1\n","# Location 8,  Free Practice 2\n","\n","# Encoding categorical features\n","for x in ['driver', 'LOCATION', 'EVENT', 'race_id', 'race_driver_id']:\n","    df_model_2[x] = le.fit_transform(df_model_2[x])\n","    df_test[x] = le.fit_transform(df_test[x])\n","\n","print('df--------', df.shape)\n","print('df_test---', df_test.shape[0])\n","print('df_train--', df_train.shape[0])\n","print('df_model_2', df_model_2.shape[0])\n","\n","# availiable features\n","feat0 = ['race_id', 'race_driver_id', 'driver', 'race_number', 'LOCATION',\n","       'EVENT', 'NUMBER', 'LAP_NUMBER', 'LAP_IMPROVEMENT', 'S1',\n","       'S1_IMPROVEMENT', 'S2', 'S2_IMPROVEMENT', 'S3', 'S3_IMPROVEMENT', 'KPH',\n","       'ELAPSED', 'PIT_TIME', 'POWER', 'is_train', 'S', 'S_PIT',\n","       'pit_b', 'LAPS_in_RACE', 'drivers_in_RACE', 'drivers_in_LAP',\n","       'drivers_droped', 'S1_dist', 'S2_dist', 'S3_dist', 'S_dist',\n","       'S1_dist_race_id_LAP_median', 'S2_dist_race_id_LAP_median',\n","       'S3_dist_race_id_LAP_median', 'S_dist_race_id_LAP_median',\n","       'estimated_time_S', 'estimated_time_S1', 'estimated_time_S2',\n","       'estimated_time_S3', 'place_in_LAP', 'S1_zscore', 'S2_zscore', 'S3_zscore', 'min_zscore', 'max_zscore']\n","\n","# filtering 0 values\n","df_model_2 = df_model_2[df_model_2['LAP_TIME'] > 0]\n","\n","# defining list of features\n","feat = feat0\n","\n","# train test split\n","x, xv, y, yv = train_test_split(df_model_2[feat], df_model_2['LAP_TIME'], test_size=0.2, random_state=4)\n","print(x.shape, xv.shape)\n","\n","# creating lof from target\n","y_log = np.log1p(y)\n","yv_log = np.log1p(yv)\n","\n","# training 2 models\n","model2 = RandomForestRegressor(n_estimators=2000, max_depth=5)\n","model3 = CatBoostRegressor(verbose=False)\n","\n","model2.fit(x, y_log)\n","model3.fit(x, y_log)\n","\n","pred_test  = \n","(np.expm1(model2.predict(df_test[feat]))+np.expm1(model3.predict(df_test[feat])))/2\n","\n","# creating submission\n","df_test['LAP_TIME'] = pred_test\n","\n","df_test[['LAP_TIME']].to_csv('my_submission_file.csv', index=False)"]},{"cell_type":"code","execution_count":null,"id":"2f9b6a7a-2ee2-4213-9629-02ebb5eea345","metadata":{"id":"2f9b6a7a-2ee2-4213-9629-02ebb5eea345"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"id":"65095589-b396-4ea0-b737-1f420b5ce07e","metadata":{"id":"65095589-b396-4ea0-b737-1f420b5ce07e"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"id":"8718ae00-1111-46f7-8c7e-a55b296a43f0","metadata":{"id":"8718ae00-1111-46f7-8c7e-a55b296a43f0"},"outputs":[],"source":[]}],"metadata":{"colab":{"collapsed_sections":[],"name":"final_submission.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.8"}},"nbformat":4,"nbformat_minor":5}
